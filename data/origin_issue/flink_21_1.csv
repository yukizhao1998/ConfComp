Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Outward issue link (Container),Inward issue link (Dependency),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Update mem_setup_tm documentation,FLINK-22796,13380920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tonywei,tonywei,tonywei,28/May/21 07:14,28/Aug/21 12:13,13/Jul/23 08:07,01/Jun/21 06:00,1.13.1,,,,,,,,1.13.2,1.14.0,,,,,,Documentation,,,,,0,pull-request-available,,,,,"In [FLINK-20860], there are two config options introduced.
we should update the corresponding docs as well.

https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#managed-memory
https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup_tm/#%e6%b6%88%e8%b4%b9%e8%80%85%e6%9d%83%e9%87%8d",,tonywei,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 01 06:00:04 UTC 2021,,,,,,,,,,"0|z0rgf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/21 07:15;tonywei;Hi [~jark]
could you assign this issue to me? thanks.;;;","01/Jun/21 06:00;xtsong;Fixed via
- master (1.14): 59fd4c60f4ca664c4ba16480e9700e4f9bb9b8c0
- release-1.13: 5332c9898a1c891c2f97867304f912ae8d339c60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw better exception when executing remote SQL file in SQL Client,FLINK-22795,13380918,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,JasonLee,mangguozhi,mangguozhi,28/May/21 07:03,23/Sep/21 17:22,13/Jul/23 08:07,03/Jun/21 03:55,1.13.0,,,,,,,,1.13.2,1.14.0,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"hi, all

When I executed following command in Flink 1.13
{code:java}
bin/sql-client.sh -f hdfs:/user/test.sql
{code}
The exception is unclear to me:
  

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
  at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)
  at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
  Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Fail to read content from the /opt/flink-1.13.0/hdfs:/user/test.sql.
  at org.apache.flink.table.client.SqlClient.readFromURL(SqlClient.java:250)
  at org.apache.flink.table.client.SqlClient.readExecutionContent(SqlClient.java:239)
  at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:153)
  at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
  at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
  ... 1 more
 {color:#de350b}  Caused by: java.io.FileNotFoundException: /opt/flink-1.13.0/hdfs:/user/test.sql (No such file or directory){color}  at java.io.FileInputStream.open0(Native Method)
  at java.io.FileInputStream.open(FileInputStream.java:195)
  at java.io.FileInputStream.<init>(FileInputStream.java:138)
  at java.io.FileInputStream.<init>(FileInputStream.java:93)
  at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
  at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
  at java.net.URL.openStream(URL.java:1045)
  at org.apache.commons.io.IOUtils.toString(IOUtils.java:2764)
  at org.apache.flink.table.client.SqlClient.readFromURL(SqlClient.java:247)
  ... 5 more

 Shutting down the session...
  done.

I think we should improve the exception message at first, and then it's great if we can support load file from HDFS.","flink 1.13

hadoop2.6.0",fsk119,jark,JasonLee,leonard,mangguozhi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 03 03:55:20 UTC 2021,,,,,,,,,,"0|z0rgeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/21 07:05;dwysakowicz;Please use English in the JIRA. Feel free to reopen once you translate the ticket.;;;","28/May/21 07:17;leonard;Thanks [~mangguozhi] for report this.

Currently both sql file and sql jars in sql client loaded by `-f`, `-j` command option only support local file, and does not support remote file system files.

I think we can check file path and throw better exception message.

For support loading remote files, it should be another topic we can also discuss here.;;;","28/May/21 07:37;jark;+1 to improve exception messages. cc [~fsk119];;;","29/May/21 12:21;JasonLee;[~jark] Hi Jark, I want to help solve this problem. I think we just need to add a parameter check in the startClient method and throw a clearer exception. Please assign it to me Thanks.;;;","29/May/21 14:22;jark;[~JasonLee], yes. You can learn how to check from this code: https://github.com/apache/flink/blob/34ff7537d3746ca6c6e14742eab0982ebde733b0/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/context/SessionContext.java#L350;;;","30/May/21 02:32;JasonLee;After reading the above code, I think we should put the sqlexecutionexception under the client package so that embedded and gateway modes can share the sqlexecutionexception. What do you think;;;","30/May/21 12:59;mangguozhi;It would be better if remote files can be supported in subsequent versions;;;","31/May/21 02:19;fsk119;[~JasonLee] You can just submit a code patch and we can understand more clearly. ;;;","03/Jun/21 03:55;jark;Fixed in
 - master: b9d60216435af9e0e3c5c0b3bfaa2c193e89280e
 - release-1.13: 43da9f5394e1054a984f56a416566d140b752412;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code of equals method grows beyond 64 KB,FLINK-22788,13380716,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,airblader,maver1ck,maver1ck,27/May/21 09:57,14/Apr/22 07:30,13/Jul/23 08:07,22/Jun/21 17:18,1.12.2,,,,,,,,1.12.5,1.13.3,1.14.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"I'm getting following exception when running SQL with many (more than 500) columns
{code:java}
java.lang.RuntimeException: Could not instantiate generated class 'GroupAggValueEqualiser$38011'
        at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:56) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.open(GroupAggFunction.java:112) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.streaming.api.operators.KeyedProcessOperator.open(KeyedProcessOperator.java:55) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:428) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:543) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:533) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_282]
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
        at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:77) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:50) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        ... 12 more
Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:77) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:50) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        ... 12 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
        at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]        at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:77) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:50) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        ... 12 more
Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""GroupAggValueEqualiser$38011"": Code of method ""equals(Lorg/apache/flink/table/data/RowData;Lorg/apache/flink/table/data/RowData;)Z"" of class ""GroupAggValueEqualiser$38011"" grows beyond 64 KB
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]        at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]        at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) ~[flink-dist_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:77) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:50) ~[flink-table-blink_2.12-1.12.2-stream2.jar:1.12.2-stream2]
        ... 12 more
{code}",,FrankZou,joemoe,libenchao,maver1ck,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24480,,,,,,,,,,,,,,,,FLINK-23007,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 22 17:18:10 UTC 2021,,,,,,,,,,"0|z0rf5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/21 10:03;maver1ck;Root cause
 Compiling ""GroupAggValueEqualiser$38011"": Code of method ""equals(Lorg/apache/flink/table/data/RowData;Lorg/apache/flink/table/data/RowData;)Z"" of class ""GroupAggValueEqualiser$38011"" grows beyond 64 KB

CC: [~libenchao] [~jark];;;","09/Jun/21 13:52;joemoe;our team will take care of that.;;;","09/Jun/21 14:02;maver1ck;[~joemoe] 
Thanks a lot;;;","09/Jun/21 15:35;twalthr;[~maver1ck] can you provide a little SQL snippet that produces the exception above (of course you can replace the 500 columns with {{col1,.., n}} using pseudo code).;;;","10/Jun/21 10:39;maver1ck;[~twalthr] 
 This SQL is triggering the bug in SQL client
{code:java}
CREATE TABLE source (id INT, col1 DECIMAL(18,4),col2 DECIMAL(18,4),col3 DECIMAL(18,4),col4 DECIMAL(18,4),col5 DECIMAL(18,4),col6 DECIMAL(18,4),col7 DECIMAL(18,4),col8 DECIMAL(18,4),col9 DECIMAL(18,4),col10 DECIMAL(18,4),col11 DECIMAL(18,4),col12 DECIMAL(18,4),col13 DECIMAL(18,4),col14 DECIMAL(18,4),col15 DECIMAL(18,4),col16 DECIMAL(18,4),col17 DECIMAL(18,4),col18 DECIMAL(18,4),col19 DECIMAL(18,4),col20 DECIMAL(18,4),col21 DECIMAL(18,4),col22 DECIMAL(18,4),col23 DECIMAL(18,4),col24 DECIMAL(18,4),col25 DECIMAL(18,4),col26 DECIMAL(18,4),col27 DECIMAL(18,4),col28 DECIMAL(18,4),col29 DECIMAL(18,4),col30 DECIMAL(18,4),col31 DECIMAL(18,4),col32 DECIMAL(18,4),col33 DECIMAL(18,4),col34 DECIMAL(18,4),col35 DECIMAL(18,4),col36 DECIMAL(18,4),col37 DECIMAL(18,4),col38 DECIMAL(18,4),col39 DECIMAL(18,4),col40 DECIMAL(18,4),col41 DECIMAL(18,4),col42 DECIMAL(18,4),col43 DECIMAL(18,4),col44 DECIMAL(18,4),col45 DECIMAL(18,4),col46 DECIMAL(18,4),col47 DECIMAL(18,4),col48 DECIMAL(18,4),col49 DECIMAL(18,4),col50 DECIMAL(18,4),col51 DECIMAL(18,4),col52 DECIMAL(18,4),col53 DECIMAL(18,4),col54 DECIMAL(18,4),col55 DECIMAL(18,4),col56 DECIMAL(18,4),col57 DECIMAL(18,4),col58 DECIMAL(18,4),col59 DECIMAL(18,4),col60 DECIMAL(18,4),col61 DECIMAL(18,4),col62 DECIMAL(18,4),col63 DECIMAL(18,4),col64 DECIMAL(18,4),col65 DECIMAL(18,4),col66 DECIMAL(18,4),col67 DECIMAL(18,4),col68 DECIMAL(18,4),col69 DECIMAL(18,4),col70 DECIMAL(18,4),col71 DECIMAL(18,4),col72 DECIMAL(18,4),col73 DECIMAL(18,4),col74 DECIMAL(18,4),col75 DECIMAL(18,4),col76 DECIMAL(18,4),col77 DECIMAL(18,4),col78 DECIMAL(18,4),col79 DECIMAL(18,4),col80 DECIMAL(18,4),col81 DECIMAL(18,4),col82 DECIMAL(18,4),col83 DECIMAL(18,4),col84 DECIMAL(18,4),col85 DECIMAL(18,4),col86 DECIMAL(18,4),col87 DECIMAL(18,4),col88 DECIMAL(18,4),col89 DECIMAL(18,4),col90 DECIMAL(18,4),col91 DECIMAL(18,4),col92 DECIMAL(18,4),col93 DECIMAL(18,4),col94 DECIMAL(18,4),col95 DECIMAL(18,4),col96 DECIMAL(18,4),col97 DECIMAL(18,4),col98 DECIMAL(18,4),col99 DECIMAL(18,4),col100 DECIMAL(18,4),col101 DECIMAL(18,4),col102 DECIMAL(18,4),col103 DECIMAL(18,4),col104 DECIMAL(18,4),col105 DECIMAL(18,4),col106 DECIMAL(18,4),col107 DECIMAL(18,4),col108 DECIMAL(18,4),col109 DECIMAL(18,4),col110 DECIMAL(18,4),col111 DECIMAL(18,4),col112 DECIMAL(18,4),col113 DECIMAL(18,4),col114 DECIMAL(18,4),col115 DECIMAL(18,4),col116 DECIMAL(18,4),col117 DECIMAL(18,4),col118 DECIMAL(18,4),col119 DECIMAL(18,4),col120 DECIMAL(18,4),col121 DECIMAL(18,4),col122 DECIMAL(18,4),col123 DECIMAL(18,4),col124 DECIMAL(18,4),col125 DECIMAL(18,4),col126 DECIMAL(18,4),col127 DECIMAL(18,4),col128 DECIMAL(18,4),col129 DECIMAL(18,4),col130 DECIMAL(18,4),col131 DECIMAL(18,4),col132 DECIMAL(18,4),col133 DECIMAL(18,4),col134 DECIMAL(18,4),col135 DECIMAL(18,4),col136 DECIMAL(18,4),col137 DECIMAL(18,4),col138 DECIMAL(18,4),col139 DECIMAL(18,4),col140 DECIMAL(18,4),col141 DECIMAL(18,4),col142 DECIMAL(18,4),col143 DECIMAL(18,4),col144 DECIMAL(18,4),col145 DECIMAL(18,4),col146 DECIMAL(18,4),col147 DECIMAL(18,4),col148 DECIMAL(18,4),col149 DECIMAL(18,4),col150 DECIMAL(18,4),col151 DECIMAL(18,4),col152 DECIMAL(18,4),col153 DECIMAL(18,4),col154 DECIMAL(18,4),col155 DECIMAL(18,4),col156 DECIMAL(18,4),col157 DECIMAL(18,4),col158 DECIMAL(18,4),col159 DECIMAL(18,4),col160 DECIMAL(18,4),col161 DECIMAL(18,4),col162 DECIMAL(18,4),col163 DECIMAL(18,4),col164 DECIMAL(18,4),col165 DECIMAL(18,4),col166 DECIMAL(18,4),col167 DECIMAL(18,4),col168 DECIMAL(18,4),col169 DECIMAL(18,4),col170 DECIMAL(18,4),col171 DECIMAL(18,4),col172 DECIMAL(18,4),col173 DECIMAL(18,4),col174 DECIMAL(18,4),col175 DECIMAL(18,4),col176 DECIMAL(18,4),col177 DECIMAL(18,4),col178 DECIMAL(18,4),col179 DECIMAL(18,4),col180 DECIMAL(18,4),col181 DECIMAL(18,4),col182 DECIMAL(18,4),col183 DECIMAL(18,4),col184 DECIMAL(18,4),col185 DECIMAL(18,4),col186 DECIMAL(18,4),col187 DECIMAL(18,4),col188 DECIMAL(18,4),col189 DECIMAL(18,4),col190 DECIMAL(18,4),col191 DECIMAL(18,4),col192 DECIMAL(18,4),col193 DECIMAL(18,4),col194 DECIMAL(18,4),col195 DECIMAL(18,4),col196 DECIMAL(18,4),col197 DECIMAL(18,4),col198 DECIMAL(18,4),col199 DECIMAL(18,4),col200 DECIMAL(18,4),col201 DECIMAL(18,4),col202 DECIMAL(18,4),col203 DECIMAL(18,4),col204 DECIMAL(18,4),col205 DECIMAL(18,4),col206 DECIMAL(18,4),col207 DECIMAL(18,4),col208 DECIMAL(18,4),col209 DECIMAL(18,4),col210 DECIMAL(18,4),col211 DECIMAL(18,4),col212 DECIMAL(18,4),col213 DECIMAL(18,4),col214 DECIMAL(18,4),col215 DECIMAL(18,4),col216 DECIMAL(18,4),col217 DECIMAL(18,4),col218 DECIMAL(18,4),col219 DECIMAL(18,4),col220 DECIMAL(18,4),col221 DECIMAL(18,4),col222 DECIMAL(18,4),col223 DECIMAL(18,4),col224 DECIMAL(18,4),col225 DECIMAL(18,4),col226 DECIMAL(18,4),col227 DECIMAL(18,4),col228 DECIMAL(18,4),col229 DECIMAL(18,4),col230 DECIMAL(18,4),col231 DECIMAL(18,4),col232 DECIMAL(18,4),col233 DECIMAL(18,4),col234 DECIMAL(18,4),col235 DECIMAL(18,4),col236 DECIMAL(18,4),col237 DECIMAL(18,4),col238 DECIMAL(18,4),col239 DECIMAL(18,4),col240 DECIMAL(18,4),col241 DECIMAL(18,4),col242 DECIMAL(18,4),col243 DECIMAL(18,4),col244 DECIMAL(18,4),col245 DECIMAL(18,4),col246 DECIMAL(18,4),col247 DECIMAL(18,4),col248 DECIMAL(18,4),col249 DECIMAL(18,4),col250 DECIMAL(18,4),col251 DECIMAL(18,4),col252 DECIMAL(18,4),col253 DECIMAL(18,4),col254 DECIMAL(18,4),col255 DECIMAL(18,4),col256 DECIMAL(18,4),col257 DECIMAL(18,4),col258 DECIMAL(18,4),col259 DECIMAL(18,4),col260 DECIMAL(18,4),col261 DECIMAL(18,4),col262 DECIMAL(18,4),col263 DECIMAL(18,4),col264 DECIMAL(18,4),col265 DECIMAL(18,4),col266 DECIMAL(18,4),col267 DECIMAL(18,4),col268 DECIMAL(18,4),col269 DECIMAL(18,4),col270 DECIMAL(18,4),col271 DECIMAL(18,4),col272 DECIMAL(18,4),col273 DECIMAL(18,4),col274 DECIMAL(18,4),col275 DECIMAL(18,4),col276 DECIMAL(18,4),col277 DECIMAL(18,4),col278 DECIMAL(18,4),col279 DECIMAL(18,4),col280 DECIMAL(18,4),col281 DECIMAL(18,4),col282 DECIMAL(18,4),col283 DECIMAL(18,4),col284 DECIMAL(18,4),col285 DECIMAL(18,4),col286 DECIMAL(18,4),col287 DECIMAL(18,4),col288 DECIMAL(18,4),col289 DECIMAL(18,4),col290 DECIMAL(18,4),col291 DECIMAL(18,4),col292 DECIMAL(18,4),col293 DECIMAL(18,4),col294 DECIMAL(18,4),col295 DECIMAL(18,4),col296 DECIMAL(18,4),col297 DECIMAL(18,4),col298 DECIMAL(18,4),col299 DECIMAL(18,4),col300 DECIMAL(18,4),col301 DECIMAL(18,4),col302 DECIMAL(18,4),col303 DECIMAL(18,4),col304 DECIMAL(18,4),col305 DECIMAL(18,4),col306 DECIMAL(18,4),col307 DECIMAL(18,4),col308 DECIMAL(18,4),col309 DECIMAL(18,4),col310 DECIMAL(18,4),col311 DECIMAL(18,4),col312 DECIMAL(18,4),col313 DECIMAL(18,4),col314 DECIMAL(18,4),col315 DECIMAL(18,4),col316 DECIMAL(18,4),col317 DECIMAL(18,4),col318 DECIMAL(18,4),col319 DECIMAL(18,4),col320 DECIMAL(18,4),col321 DECIMAL(18,4),col322 DECIMAL(18,4),col323 DECIMAL(18,4),col324 DECIMAL(18,4),col325 DECIMAL(18,4),col326 DECIMAL(18,4),col327 DECIMAL(18,4),col328 DECIMAL(18,4),col329 DECIMAL(18,4),col330 DECIMAL(18,4),col331 DECIMAL(18,4),col332 DECIMAL(18,4),col333 DECIMAL(18,4),col334 DECIMAL(18,4),col335 DECIMAL(18,4),col336 DECIMAL(18,4),col337 DECIMAL(18,4),col338 DECIMAL(18,4),col339 DECIMAL(18,4),col340 DECIMAL(18,4),col341 DECIMAL(18,4),col342 DECIMAL(18,4),col343 DECIMAL(18,4),col344 DECIMAL(18,4),col345 DECIMAL(18,4),col346 DECIMAL(18,4),col347 DECIMAL(18,4),col348 DECIMAL(18,4),col349 DECIMAL(18,4),col350 DECIMAL(18,4),col351 DECIMAL(18,4),col352 DECIMAL(18,4),col353 DECIMAL(18,4),col354 DECIMAL(18,4),col355 DECIMAL(18,4),col356 DECIMAL(18,4),col357 DECIMAL(18,4),col358 DECIMAL(18,4),col359 DECIMAL(18,4),col360 DECIMAL(18,4),col361 DECIMAL(18,4),col362 DECIMAL(18,4),col363 DECIMAL(18,4),col364 DECIMAL(18,4),col365 DECIMAL(18,4),col366 DECIMAL(18,4),col367 DECIMAL(18,4),col368 DECIMAL(18,4),col369 DECIMAL(18,4),col370 DECIMAL(18,4),col371 DECIMAL(18,4),col372 DECIMAL(18,4),col373 DECIMAL(18,4),col374 DECIMAL(18,4),col375 DECIMAL(18,4),col376 DECIMAL(18,4),col377 DECIMAL(18,4),col378 DECIMAL(18,4),col379 DECIMAL(18,4),col380 DECIMAL(18,4),col381 DECIMAL(18,4),col382 DECIMAL(18,4),col383 DECIMAL(18,4),col384 DECIMAL(18,4),col385 DECIMAL(18,4),col386 DECIMAL(18,4),col387 DECIMAL(18,4),col388 DECIMAL(18,4),col389 DECIMAL(18,4),col390 DECIMAL(18,4),col391 DECIMAL(18,4),col392 DECIMAL(18,4),col393 DECIMAL(18,4),col394 DECIMAL(18,4),col395 DECIMAL(18,4),col396 DECIMAL(18,4),col397 DECIMAL(18,4),col398 DECIMAL(18,4),col399 DECIMAL(18,4),col400 DECIMAL(18,4),col401 DECIMAL(18,4),col402 DECIMAL(18,4),col403 DECIMAL(18,4),col404 DECIMAL(18,4),col405 DECIMAL(18,4),col406 DECIMAL(18,4),col407 DECIMAL(18,4),col408 DECIMAL(18,4),col409 DECIMAL(18,4),col410 DECIMAL(18,4),col411 DECIMAL(18,4),col412 DECIMAL(18,4),col413 DECIMAL(18,4),col414 DECIMAL(18,4),col415 DECIMAL(18,4),col416 DECIMAL(18,4),col417 DECIMAL(18,4),col418 DECIMAL(18,4),col419 DECIMAL(18,4),col420 DECIMAL(18,4),col421 DECIMAL(18,4),col422 DECIMAL(18,4),col423 DECIMAL(18,4),col424 DECIMAL(18,4),col425 DECIMAL(18,4),col426 DECIMAL(18,4),col427 DECIMAL(18,4),col428 DECIMAL(18,4),col429 DECIMAL(18,4),col430 DECIMAL(18,4),col431 DECIMAL(18,4),col432 DECIMAL(18,4),col433 DECIMAL(18,4),col434 DECIMAL(18,4),col435 DECIMAL(18,4),col436 DECIMAL(18,4),col437 DECIMAL(18,4),col438 DECIMAL(18,4),col439 DECIMAL(18,4),col440 DECIMAL(18,4),col441 DECIMAL(18,4),col442 DECIMAL(18,4),col443 DECIMAL(18,4),col444 DECIMAL(18,4),col445 DECIMAL(18,4),col446 DECIMAL(18,4),col447 DECIMAL(18,4),col448 DECIMAL(18,4),col449 DECIMAL(18,4),col450 DECIMAL(18,4),col451 DECIMAL(18,4),col452 DECIMAL(18,4),col453 DECIMAL(18,4),col454 DECIMAL(18,4),col455 DECIMAL(18,4),col456 DECIMAL(18,4),col457 DECIMAL(18,4),col458 DECIMAL(18,4),col459 DECIMAL(18,4),col460 DECIMAL(18,4),col461 DECIMAL(18,4),col462 DECIMAL(18,4),col463 DECIMAL(18,4),col464 DECIMAL(18,4),col465 DECIMAL(18,4),col466 DECIMAL(18,4),col467 DECIMAL(18,4),col468 DECIMAL(18,4),col469 DECIMAL(18,4),col470 DECIMAL(18,4),col471 DECIMAL(18,4),col472 DECIMAL(18,4),col473 DECIMAL(18,4),col474 DECIMAL(18,4),col475 DECIMAL(18,4),col476 DECIMAL(18,4),col477 DECIMAL(18,4),col478 DECIMAL(18,4),col479 DECIMAL(18,4),col480 DECIMAL(18,4),col481 DECIMAL(18,4),col482 DECIMAL(18,4),col483 DECIMAL(18,4),col484 DECIMAL(18,4),col485 DECIMAL(18,4),col486 DECIMAL(18,4),col487 DECIMAL(18,4),col488 DECIMAL(18,4),col489 DECIMAL(18,4),col490 DECIMAL(18,4),col491 DECIMAL(18,4),col492 DECIMAL(18,4),col493 DECIMAL(18,4),col494 DECIMAL(18,4),col495 DECIMAL(18,4),col496 DECIMAL(18,4),col497 DECIMAL(18,4),col498 DECIMAL(18,4),col499 DECIMAL(18,4),col500 DECIMAL(18,4)) WITH ('connector' = 'datagen');
SELECT id, max(col1),max(col2),max(col3),max(col4),max(col5),max(col6),max(col7),max(col8),max(col9),max(col10),max(col11),max(col12),max(col13),max(col14),max(col15),max(col16),max(col17),max(col18),max(col19),max(col20),max(col21),max(col22),max(col23),max(col24),max(col25),max(col26),max(col27),max(col28),max(col29),max(col30),max(col31),max(col32),max(col33),max(col34),max(col35),max(col36),max(col37),max(col38),max(col39),max(col40),max(col41),max(col42),max(col43),max(col44),max(col45),max(col46),max(col47),max(col48),max(col49),max(col50),max(col51),max(col52),max(col53),max(col54),max(col55),max(col56),max(col57),max(col58),max(col59),max(col60),max(col61),max(col62),max(col63),max(col64),max(col65),max(col66),max(col67),max(col68),max(col69),max(col70),max(col71),max(col72),max(col73),max(col74),max(col75),max(col76),max(col77),max(col78),max(col79),max(col80),max(col81),max(col82),max(col83),max(col84),max(col85),max(col86),max(col87),max(col88),max(col89),max(col90),max(col91),max(col92),max(col93),max(col94),max(col95),max(col96),max(col97),max(col98),max(col99),max(col100),max(col101),max(col102),max(col103),max(col104),max(col105),max(col106),max(col107),max(col108),max(col109),max(col110),max(col111),max(col112),max(col113),max(col114),max(col115),max(col116),max(col117),max(col118),max(col119),max(col120),max(col121),max(col122),max(col123),max(col124),max(col125),max(col126),max(col127),max(col128),max(col129),max(col130),max(col131),max(col132),max(col133),max(col134),max(col135),max(col136),max(col137),max(col138),max(col139),max(col140),max(col141),max(col142),max(col143),max(col144),max(col145),max(col146),max(col147),max(col148),max(col149),max(col150),max(col151),max(col152),max(col153),max(col154),max(col155),max(col156),max(col157),max(col158),max(col159),max(col160),max(col161),max(col162),max(col163),max(col164),max(col165),max(col166),max(col167),max(col168),max(col169),max(col170),max(col171),max(col172),max(col173),max(col174),max(col175),max(col176),max(col177),max(col178),max(col179),max(col180),max(col181),max(col182),max(col183),max(col184),max(col185),max(col186),max(col187),max(col188),max(col189),max(col190),max(col191),max(col192),max(col193),max(col194),max(col195),max(col196),max(col197),max(col198),max(col199),max(col200),max(col201),max(col202),max(col203),max(col204),max(col205),max(col206),max(col207),max(col208),max(col209),max(col210),max(col211),max(col212),max(col213),max(col214),max(col215),max(col216),max(col217),max(col218),max(col219),max(col220),max(col221),max(col222),max(col223),max(col224),max(col225),max(col226),max(col227),max(col228),max(col229),max(col230),max(col231),max(col232),max(col233),max(col234),max(col235),max(col236),max(col237),max(col238),max(col239),max(col240),max(col241),max(col242),max(col243),max(col244),max(col245),max(col246),max(col247),max(col248),max(col249),max(col250),max(col251),max(col252),max(col253),max(col254),max(col255),max(col256),max(col257),max(col258),max(col259),max(col260),max(col261),max(col262),max(col263),max(col264),max(col265),max(col266),max(col267),max(col268),max(col269),max(col270),max(col271),max(col272),max(col273),max(col274),max(col275),max(col276),max(col277),max(col278),max(col279),max(col280),max(col281),max(col282),max(col283),max(col284),max(col285),max(col286),max(col287),max(col288),max(col289),max(col290),max(col291),max(col292),max(col293),max(col294),max(col295),max(col296),max(col297),max(col298),max(col299),max(col300),max(col301),max(col302),max(col303),max(col304),max(col305),max(col306),max(col307),max(col308),max(col309),max(col310),max(col311),max(col312),max(col313),max(col314),max(col315),max(col316),max(col317),max(col318),max(col319),max(col320),max(col321),max(col322),max(col323),max(col324),max(col325),max(col326),max(col327),max(col328),max(col329),max(col330),max(col331),max(col332),max(col333),max(col334),max(col335),max(col336),max(col337),max(col338),max(col339),max(col340),max(col341),max(col342),max(col343),max(col344),max(col345),max(col346),max(col347),max(col348),max(col349),max(col350),max(col351),max(col352),max(col353),max(col354),max(col355),max(col356),max(col357),max(col358),max(col359),max(col360),max(col361),max(col362),max(col363),max(col364),max(col365),max(col366),max(col367),max(col368),max(col369),max(col370),max(col371),max(col372),max(col373),max(col374),max(col375),max(col376),max(col377),max(col378),max(col379),max(col380),max(col381),max(col382),max(col383),max(col384),max(col385),max(col386),max(col387),max(col388),max(col389),max(col390),max(col391),max(col392),max(col393),max(col394),max(col395),max(col396),max(col397),max(col398),max(col399),max(col400),max(col401),max(col402),max(col403),max(col404),max(col405),max(col406),max(col407),max(col408),max(col409),max(col410),max(col411),max(col412),max(col413),max(col414),max(col415),max(col416),max(col417),max(col418),max(col419),max(col420),max(col421),max(col422),max(col423),max(col424),max(col425),max(col426),max(col427),max(col428),max(col429),max(col430),max(col431),max(col432),max(col433),max(col434),max(col435),max(col436),max(col437),max(col438),max(col439),max(col440),max(col441),max(col442),max(col443),max(col444),max(col445),max(col446),max(col447),max(col448),max(col449),max(col450),max(col451),max(col452),max(col453),max(col454),max(col455),max(col456),max(col457),max(col458),max(col459),max(col460),max(col461),max(col462),max(col463),max(col464),max(col465),max(col466),max(col467),max(col468),max(col469),max(col470),max(col471),max(col472),max(col473),max(col474),max(col475),max(col476),max(col477),max(col478),max(col479),max(col480),max(col481),max(col482),max(col483),max(col484),max(col485),max(col486),max(col487),max(col488),max(col489),max(col490),max(col491),max(col492),max(col493),max(col494),max(col495),max(col496),max(col497),max(col498),max(col499),max(col500) FROM source GROUP BY id;
{code}
Effect
{code:java}
Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""GroupAggValueEqualiser$4506"": Code of method ""equals(Lorg/apache/flink/table/data/RowData;Lorg/apache/flink/table/data/RowData;)Z"" of class ""GroupAggValueEqualiser$4506"" grows beyond 64 KB
{code};;;","22/Jun/21 17:18;twalthr;Fixed in 1.14.0: cb116be7bd3516778c4494ce1b9c7470977a1281
Fixed in 1.13.3: 8f5b0126d8f0521f01a731143bd5b293b3939b92
Fixed in 1.12.6: 3d93d67e6477da82cbe71f9877b24202b6310786;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql-client can not create .flink-sql-history file,FLINK-22786,13380682,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pensz,pensz,pensz,27/May/21 06:51,28/Aug/21 12:14,13/Jul/23 08:07,03/Jun/21 03:46,1.12.2,,,,,,,,1.13.2,1.14.0,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"When I run sql-client.sh， I find a warning 

Unable to create history file: /home/www/.flink-sql-history 

In fact, the user I used have permission to write /home/www/.

After debug sql-client.sh. I think this is a bug in org.apache.flink.table.client.cli.CliUtils#createFile

Files.notExists(parent) should be run before Files.createDirectories(parent) .",,fsk119,jark,pensz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 03 03:46:27 UTC 2021,,,,,,,,,,"0|z0reyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/21 09:20;jark;cc [~fsk119];;;","27/May/21 09:38;pensz;Add : /home/www/ is a symbolic link of directory.

Nature of the bug is we should deal with symbolic link.

Many classes have similar problem.

 

Can I fix the bug?   It doesn't look complicated.;;;","28/May/21 06:12;fsk119;Thanks for your report! But I think the main problem is we try to build a directory that already exists. 

I write a test in my local branch to reproduce this problem.
{code:java}
/** Test {@link CliUtils}. */
public class CliUtilsTest {

    @Rule public TemporaryFolder realFolder = new TemporaryFolder();

    @Rule public TemporaryFolder linkFolder = new TemporaryFolder();

    @Test
    public void testCreate() throws IOException {
        Path link = Paths.get(linkFolder.getRoot().getAbsolutePath(), ""link"");
        Files.createSymbolicLink(link, realFolder.getRoot().toPath());

        Path historyFile = Paths.get(link.toAbsolutePath().toString(), ""test.file"");
        CliUtils.createFile(historyFile);

        assertTrue(Files.exists(historyFile));
    }
}
{code};;;","31/May/21 02:16;fsk119;[~pensz] Are you still willing to contribute? I think it will not take too much time.;;;","31/May/21 02:48;pensz;Yes, I will create a PR about this bug.

Similar problem in following classes:
  
{code:java}
./flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java 
./flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServer.java 
./flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServerArchiveFetcher.java 
./flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateDownloader.java 
./flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/JaasModule.java no 
./flink-runtime/src/main/java/org/apache/flink/runtime/state/SnapshotDirectory.java 
./flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java 
./flink-runtime/src/main/java/org/apache/flink/runtime/rest/FileUploadHandler.java 
./flink-runtime/src/main/java/org/apache/flink/runtime/rest/RestServerEndpoint.java no 

./flink-python/src/main/java/org/apache/flink/client/python/PythonEnvUtils.java 
./flink-core/src/main/java/org/apache/flink/util/FileUtils.java no 
./flink-clients/src/main/java/org/apache/flink/client/program/PackagedProgramUtils.java 
./flink-yarn/src/main/java/org/apache/flink/yarn/YarnApplicationFileUploader.java 
./flink-runtime/src/main/java/org/apache/flink/runtime/clusterframework/overlays/AbstractContainerOverlay.java 
./flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/FileUploads.java no{code}
 
  

I setup my IDEA recently :(;;;","31/May/21 02:59;jark;Please only fix SQL Client. I'm not sure if other places are also the problem and they should be disucssed separately. ;;;","31/May/21 03:31;pensz;OK.;;;","03/Jun/21 03:46;jark;Fixed in 
 - master: bf56611cd19be57b9dbd3c66e7e30b97d0f77632
 - release-1.13: df0ae16bbd3ecb59203d30fe88571ead598788b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jepsen tests broken due to change in zNode layout,FLINK-22784,13380528,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/May/21 11:39,28/May/21 11:10,13/Jul/23 08:07,26/May/21 12:41,1.14.0,,,,,,,,1.14.0,,,,,,,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22636,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 26 12:41:20 UTC 2021,,,,,,,,,,"0|z0re08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/21 12:41;chesnay;master: 200bc4f5d222930aa4972d1a1d84e4a3904b47d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Jira Bot effectively does not apply all rules anymore due to throtteling,FLINK-22783,13380525,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,knaufk,knaufk,knaufk,26/May/21 11:31,26/May/21 13:16,13/Jul/23 08:07,26/May/21 13:16,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"The Flink Jira Bot is only allowed to update a certain number of tickets per run. They way this has been implemented it first the bot only considers the first n tickets returned for a given filter. *Afterwards* there is an additional filter that filters out tickets that have updated subtasks.

This can lead to situation where the bot does not make progress anymore, because it always considers the first 10 tickets, all of which have updated Sub-Tasks. So, no tickets are updated although the rule would apply to some tickets.

The effect can be seen https://github.com/apache/flink-jira-bot/runs/2674419533?check_suite_focus=true",,knaufk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22569,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-05-26 11:31:33.0,,,,,,,,,,"0|z0rdzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result for group window aggregate when mini-batch is enabled,FLINK-22781,13380504,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jingzhang,godfreyhe,godfreyhe,26/May/21 09:15,22/Apr/22 06:59,13/Jul/23 08:07,06/Jul/21 08:54,1.14.0,,,,,,,,1.14.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"We can reproduce this issue through adding the following code to {{GroupWindowITCase#testWindowAggregateOnUpsertSource}} method:

{code:java}
    tEnv.getConfig.getConfiguration.setBoolean(
      ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ENABLED, true)
    tEnv.getConfig.getConfiguration.set(
      ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ALLOW_LATENCY, Duration.ofSeconds(1))
    tEnv.getConfig.getConfiguration.setLong(
      ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_SIZE, 10L)
{code}

The reason is the group window without any data (the data may be retracted) should not send any record.",,aitozi,godfreyhe,hackergin,icshuo,jark,jingzhang,libenchao,lsy,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20487,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 06 08:54:39 UTC 2021,,,,,,,,,,"0|z0rduw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/21 09:16;godfreyhe;cc [~qingru zhang];;;","26/May/21 10:23;jingzhang;[~godfreyhe] Thanks for point out the drawback.

After I look deep into the problem, I found the output of. `StreamExecChangelogNormalize` is changed when enable miniBatch.

When disable minibatch, the output of `StreamExecChangelogNormalize` for key 'Euro' is :

changelogRow(""+U"", ""Euro"", ""no1"", JLong.valueOf(114L), localDateTime(1L)),
changelogRow(""-U"", ""Euro"", ""no1"", JLong.valueOf(114L), localDateTime(1L)),
changelogRow(""+U"", ""Euro"", ""no1"", JLong.valueOf(118L), localDateTime(6L)) 

when enable minibatch, the output of `StreamExecChangelogNormalize` for key 'Euro' is :
changelogRow(""+U"", ""Euro"", ""no1"", JLong.valueOf(118L), localDateTime(6L))
the following window aggregate would send out a 'Euro,0,1970-01-01T00:00,1970-01-01T00:00:05' when disable  minibatch while the record would not appear when enbale minibatch.
 
*In conclude, the problem is caused by two reasons:*
*1. When enable minibatch, emit behavior is different for `StreamExecChangelogNormalize`*
*2. WindowOperator should not emit data if the counter becomes to 0*
 
*because WindowOperator already stores the counter for per key, we could do a simple fix without destroy state compatibility.*
 
It's my pleasure to fix the bug, please assign the issue to me.;;;","26/May/21 10:23;godfreyhe;Assign to you [~qingru zhang];;;","26/May/21 11:06;jingzhang;[~godfreyhe] After I think back the problem, I found there still exists a problem even after group window without any data (the data may be retracted) should not send any record :(.

The result still may be different with disable/enable mini-batch because there exists some uncertain factors in these two cases. For example, If disable mini-batch, the upsert source sends a update message, `StreamExecChangelogNormalize` would send a update-before message and a update-after message to downstream. *The update-before maybe dropped as late data when it comes to windowOperator because it has the original row-time as last insert message at the same primary key.* *Whether it would be dropped or not is not nondeterministic*. Let's see what happened if enable mini-batch, the `StreamExecChangelogNormalize` maybe reduces the temporary change, *whether the intermediate result is visible to the following window operator is nondeterministic*.

 ;;;","28/May/21 09:24;icshuo;[~qingru zhang], I think what you mentioned above is the main concern why window do not support updating input stream before, either for CDC source or updating stream from other operator (e.g, rowtime deduplicate), because we do not know when the updating message for a specific key will happen, and window state will be cleaned once the window is fired. It is possible to keep the result correct by increasing the ttl for window (e.g, set allow lateness for window), but I'think that's not an elegant solution...;;;","31/May/21 06:42;jingzhang;[~icshuo] Thanks for your reply. WindowAggregate support retract input stream is a common requirement. For example, it would limit usage if thrown an exception if do a window aggregate upon a changlog with pk.

I admit we need an elegant solution here. I need some time to think through the problem, any suggestion is welcome.

Do we need to revert  FLINK-20487 temporarily before find out a good solution ? ;;;","31/May/21 08:04;lzljs3620320;[~qingru zhang] I don't think we need to revert FLINK-20487, we can think more about the story of CDC users. We can create a document and start a discussion in DEV mail list.;;;","14/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","19/Jun/21 04:45;jingzhang;[~flink-jira-bot] I'm still working on this. [~godfreyhe] Would you please remove the tag because I have no permission to edit. Thanks a lot.;;;","21/Jun/21 12:23;jingzhang;[~lzljs3620320] I agree with you. It's better to split the issue into two sub issues:
 # Fix bug in `WindowOperator` not emit data if the counter is 0, which is in progressed ([github pull request # 16219|https://github.com/apache/flink/pull/16219])now.
 # Find an elegant solution to define behavior(how to coordinate three relative configuration: allow lateness, state TTL and enable late-fire emit) for WindowAggregate which works on Changlog with primary key or other upsert source. I would start a discussion later this week.;;;","25/Jun/21 08:38;jark;Regarding to the second problem (late behavior and state ttl), I think current behavior is as expected, because the UB message is truly late. I'm in favor of keeping consistent behavior for changelog input and insert-only input, that means do not set differnet default value when input stream is changelog, otherwise, I think it would confuse users a lot. In order to make users better understand the late behavior for changelog input, I think we can add more explanation, examples, instructions, recommended configurations in docs. ;;;","25/Jun/21 12:07;jingzhang;[~jark] Thanks a lot for reply.

I would like to summarize the current solution (without introducing extra mechanism) to handle late UB message for window aggregate upon changelog(contains update message).

User needs to set all the following 3 parameters:

(1) enable late fire by setting 
{code:java}
table.exec.emit.late-fire.enabled : true{code}
(2) set per record emit behavior for late records by setting
{code:java}
table.exec.emit.late-fire.delay : 0 s
{code}
(3) keep window state for extra time after window is fired by setting
{code:java}
table.exec.emit.allow-latenss : 1 h
// 或者
table.exec.state.ttl: 1h{code}
 

The solution has two disadvantages:

(1) User may not realize that UB message maybe dropped as late event, so they will not set related parameters.

(2) When use look for a solution to solve dropped UB message problem, current solution is a bit of unconvinient for them because they need to set all the 3 parameters. 

 

I agree with you provides recommended configurations in docs could help.

Besides, could we simplify the 3 parameters a little. for example, user only need set allow-lateness, framework could atom set `table.exec.emit.late-fire.enabled` to true and set`table.exec.emit.late-fire.delay ` to 0s. Just like the behavior on Datastream(user only need set allowedLateness on Windowedstream).

What do you think?;;;","28/Jun/21 02:46;lzljs3620320;Hi Jing, Thanks for the summarization.

First of all, I don't recommend the user to configure {{table.exec.emit.allow-latenss}}. TTL should be configured for normal operations. (I believe TTL may even have a default value on every platform.)

Why needs to configure {{table.exec.emit.late-fire.delay}} ? I think firing on every element is a good start. So can we just set the default value to zero?

So, actually, users just need to configure late-fire.

Another choice is your recommend, just configure allow-lateness.

I think we should make a decision for long-term development. Always take one out. (Deprecated one and remove it after next few release);;;","28/Jun/21 03:08;jark;If we don't consider backward-compatibility, I think using just {{table.exec.emit.allow-lateness}} to enable late firing is a good idea. {{table.exec.emit.late-fire.delay}} can be zero to fire on every late elements, {{table.exec.emit.late-fire.enabled}} can be removed because it has been substituted by allow-lateness.  
Actually they are still Experimental APIs and never public on docs. 

For the temporary solution, I think we can go with [~lzljs3620320]'s proposal and deprecate {{table.exec.emit.late-fire.enabled}}, but keep backward-compatibility. 

For the long term, I think we should provide EMIT sytanx to make the behavior more explicitly and can have more fine-grained emit strategies. With the EMIT syntax, users should be able to declare {{ALLOW LATENESS}} clause. For example: 

{code}
EMIT 
  DELAY '1' MINUTE BEFORE WATERMARK,
  NO DELAY AFTER WATERMARK WITH ALLOW LATENESS '1' HOUR
{code}

;;;","29/Jun/21 02:09;lzljs3620320;Although it is an Experimental API, we already exposed it through user mail list and other channels.

So I think it is better to start a discussion on the mail list and finally make one really Public.;;;","06/Jul/21 08:54;godfreyhe;Fixed in 1.14.0: c919edff962e218e543b1e97550d35d66b7172b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaChangelogTableITCase.testKafkaDebeziumChangelogSource fail due to ConcurrentModificationException,FLINK-22779,13380454,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,maguowei,maguowei,26/May/21 04:04,02/Aug/21 03:30,13/Jul/23 08:07,26/May/21 04:15,1.13.0,,,,,,,,1.13.2,,,,,,,Connectors / Kafka,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18330&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6608

",,jark,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 26 04:15:58 UTC 2021,,,,,,,,,,"0|z0rdjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/21 04:13;jark;I will cherry-pick FLINK-22203 to release-1.13.;;;","26/May/21 04:15;jark;Fixed in release-1.13: fae3e70532056465be3a2376a47ba20b5f80b13b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore lost sections in Try Flink DataStream API example,FLINK-22777,13380448,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sgloutnikov,sgloutnikov,sgloutnikov,26/May/21 02:45,28/Aug/21 12:12,13/Jul/23 08:07,26/May/21 16:42,1.14.0,,,,,,,,1.13.2,1.14.0,,,,,,Documentation,,,,,0,pull-request-available,,,,,"During the migration of the docs from Jekyll to Hugo (FLINK-21193) in 1.13 large parts of the fraud detection DataStream API example in the ""Try Flink"" section were lost. Restore it for future doc versions.

 
 1.12 - [https://ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/datastream_api.html]
 1.13 - [https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/try-flink/datastream/] - (missing parts start from the {{Writing a Real Application}} section.
  
  ",,sgloutnikov,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 26 16:42:08 UTC 2021,,,,,,,,,,"0|z0rdig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/21 16:42;sjwiesman;fixed in release-1.13: 7d97a004e698779d8799e0dc0b48e0faec538285

fixed in master: 8e6f152d56edbcd4b88e0158a43311e7f820868c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraConnectorITCase.testCassandraTableSink Fail,FLINK-22775,13380445,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,echauchot,maguowei,maguowei,26/May/21 02:26,15/Dec/21 08:30,13/Jul/23 08:07,14/Dec/21 15:03,1.14.0,,,,,,,,1.13.6,1.14.3,1.15.0,,,,,Connectors / Cassandra,,,,,0,auto-deprioritized-major,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18328&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14105


{code:java}
 2021-05-25T23:03:44.0756266Z May 25 23:03:44 [ERROR] testCassandraTableSink(org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase)  Time elapsed: 13.673 s  <<< ERROR!
2021-05-25T23:03:44.0757635Z May 25 23:03:44 java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
2021-05-25T23:03:44.0760262Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-05-25T23:03:44.0761504Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-05-25T23:03:44.0762906Z May 25 23:03:44 	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:129)
2021-05-25T23:03:44.0763878Z May 25 23:03:44 	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:92)
2021-05-25T23:03:44.0764918Z May 25 23:03:44 	at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.testCassandraTableSink(CassandraConnectorITCase.java:520)
2021-05-25T23:03:44.0768225Z May 25 23:03:44 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-25T23:03:44.0769100Z May 25 23:03:44 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-25T23:03:44.0769917Z May 25 23:03:44 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-25T23:03:44.0770645Z May 25 23:03:44 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-25T23:03:44.0771387Z May 25 23:03:44 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-25T23:03:44.0772228Z May 25 23:03:44 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-25T23:03:44.0773541Z May 25 23:03:44 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-25T23:03:44.0774367Z May 25 23:03:44 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-25T23:03:44.0775246Z May 25 23:03:44 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-05-25T23:03:44.0776088Z May 25 23:03:44 	at org.apache.flink.testutils.junit.RetryRule$RetryOnExceptionStatement.evaluate(RetryRule.java:192)
2021-05-25T23:03:44.0776946Z May 25 23:03:44 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-25T23:03:44.0777685Z May 25 23:03:44 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-25T23:03:44.0778447Z May 25 23:03:44 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-25T23:03:44.0779110Z May 25 23:03:44 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-25T23:03:44.0779893Z May 25 23:03:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-25T23:03:44.0780744Z May 25 23:03:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-25T23:03:44.0781493Z May 25 23:03:44 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-25T23:03:44.0782154Z May 25 23:03:44 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-25T23:03:44.0782899Z May 25 23:03:44 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-25T23:03:44.0783576Z May 25 23:03:44 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-25T23:03:44.0784312Z May 25 23:03:44 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-25T23:03:44.0785020Z May 25 23:03:44 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-05-25T23:03:44.0785815Z May 25 23:03:44 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-25T23:03:44.0786619Z May 25 23:03:44 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-25T23:03:44.0787343Z May 25 23:03:44 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-25T23:03:44.0788202Z May 25 23:03:44 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-25T23:03:44.0789018Z May 25 23:03:44 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-05-25T23:03:44.0789860Z May 25 23:03:44 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-05-25T23:03:44.0790915Z May 25 23:03:44 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-05-25T23:03:44.0791764Z May 25 23:03:44 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-05-25T23:03:44.0795986Z May 25 23:03:44 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-25T23:03:44.0797022Z May 25 23:03:44 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-25T23:03:44.0797839Z May 25 23:03:44 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-25T23:03:44.0798758Z May 25 23:03:44 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-05-25T23:03:44.0799520Z May 25 23:03:44 Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
2021-05-25T23:03:44.0800276Z May 25 23:03:44 	at org.apache.flink.table.api.internal.InsertResultIterator.hasNext(InsertResultIterator.java:56)
2021-05-25T23:03:44.0801231Z May 25 23:03:44 	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
2021-05-25T23:03:44.0802456Z May 25 23:03:44 	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.isFirstRowReady(TableResultImpl.java:383)
2021-05-25T23:03:44.0803713Z May 25 23:03:44 	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:116)
2021-05-25T23:03:44.0804612Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2021-05-25T23:03:44.0805457Z May 25 23:03:44 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-05-25T23:03:44.0806266Z May 25 23:03:44 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-05-25T23:03:44.0806967Z May 25 23:03:44 	at java.lang.Thread.run(Thread.java:748)
2021-05-25T23:03:44.0866172Z May 25 23:03:44 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-05-25T23:03:44.0867449Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-05-25T23:03:44.0868377Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-05-25T23:03:44.0869237Z May 25 23:03:44 	at org.apache.flink.table.api.internal.InsertResultIterator.hasNext(InsertResultIterator.java:54)
2021-05-25T23:03:44.0869892Z May 25 23:03:44 	... 7 more
2021-05-25T23:03:44.0870472Z May 25 23:03:44 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-05-25T23:03:44.0871291Z May 25 23:03:44 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-05-25T23:03:44.0872219Z May 25 23:03:44 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-05-25T23:03:44.0873253Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-05-25T23:03:44.0873845Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-05-25T23:03:44.0874364Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-25T23:03:44.0874886Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-05-25T23:03:44.0875451Z May 25 23:03:44 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
2021-05-25T23:03:44.0876134Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-05-25T23:03:44.0876678Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-05-25T23:03:44.0877195Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-25T23:03:44.0878074Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-05-25T23:03:44.0878579Z May 25 23:03:44 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1081)
2021-05-25T23:03:44.0879202Z May 25 23:03:44 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2021-05-25T23:03:44.0879670Z May 25 23:03:44 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-05-25T23:03:44.0880089Z May 25 23:03:44 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-05-25T23:03:44.0880532Z May 25 23:03:44 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-05-25T23:03:44.0880961Z May 25 23:03:44 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-05-25T23:03:44.0881476Z May 25 23:03:44 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-05-25T23:03:44.0882021Z May 25 23:03:44 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-05-25T23:03:44.0882659Z May 25 23:03:44 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-05-25T23:03:44.0883141Z May 25 23:03:44 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-05-25T23:03:44.0883807Z May 25 23:03:44 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2021-05-25T23:03:44.0884370Z May 25 23:03:44 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-05-25T23:03:44.0884903Z May 25 23:03:44 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-05-25T23:03:44.0885378Z May 25 23:03:44 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-05-25T23:03:44.0885828Z May 25 23:03:44 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-05-25T23:03:44.0886332Z May 25 23:03:44 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-05-25T23:03:44.0886941Z May 25 23:03:44 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-05-25T23:03:44.0887820Z May 25 23:03:44 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-05-25T23:03:44.0888701Z May 25 23:03:44 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-05-25T23:03:44.0889222Z May 25 23:03:44 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-05-25T23:03:44.0889731Z May 25 23:03:44 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-05-25T23:03:44.0890217Z May 25 23:03:44 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-05-25T23:03:44.0890739Z May 25 23:03:44 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-05-25T23:03:44.0891284Z May 25 23:03:44 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-05-25T23:03:44.0891773Z May 25 23:03:44 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-05-25T23:03:44.0892407Z May 25 23:03:44 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-05-25T23:03:44.0893148Z May 25 23:03:44 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-05-25T23:03:44.0893889Z May 25 23:03:44 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-05-25T23:03:44.0894831Z May 25 23:03:44 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-05-25T23:03:44.0895906Z May 25 23:03:44 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-05-25T23:03:44.0896837Z May 25 23:03:44 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:207)
2021-05-25T23:03:44.0898172Z May 25 23:03:44 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:197)
2021-05-25T23:03:44.0898968Z May 25 23:03:44 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:188)
2021-05-25T23:03:44.0899722Z May 25 23:03:44 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:677)
2021-05-25T23:03:44.0900322Z May 25 23:03:44 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-05-25T23:03:44.0901200Z May 25 23:03:44 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
2021-05-25T23:03:44.0901916Z May 25 23:03:44 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-25T23:03:44.0903255Z May 25 23:03:44 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-25T23:03:44.0904101Z May 25 23:03:44 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-25T23:03:44.0904577Z May 25 23:03:44 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-25T23:03:44.0905279Z May 25 23:03:44 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
2021-05-25T23:03:44.0906348Z May 25 23:03:44 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
2021-05-25T23:03:44.0907237Z May 25 23:03:44 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-05-25T23:03:44.0908054Z May 25 23:03:44 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-05-25T23:03:44.0908759Z May 25 23:03:44 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-05-25T23:03:44.0909461Z May 25 23:03:44 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-05-25T23:03:44.0910160Z May 25 23:03:44 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-05-25T23:03:44.0910838Z May 25 23:03:44 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-05-25T23:03:44.0911556Z May 25 23:03:44 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-05-25T23:03:44.0912465Z May 25 23:03:44 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-25T23:03:44.0913219Z May 25 23:03:44 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-25T23:03:44.0913951Z May 25 23:03:44 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-05-25T23:03:44.0914628Z May 25 23:03:44 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-05-25T23:03:44.0915324Z May 25 23:03:44 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-05-25T23:03:44.0915959Z May 25 23:03:44 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-05-25T23:03:44.0916576Z May 25 23:03:44 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-05-25T23:03:44.0916993Z May 25 23:03:44 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-05-25T23:03:44.0917377Z May 25 23:03:44 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-05-25T23:03:44.0917712Z May 25 23:03:44 	... 4 more
2021-05-25T23:03:44.0918361Z May 25 23:03:44 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Error while sending value.
2021-05-25T23:03:44.0919000Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-05-25T23:03:44.0919569Z May 25 23:03:44 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-05-25T23:03:44.0920156Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:168)
2021-05-25T23:03:44.0921043Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:131)
2021-05-25T23:03:44.0922194Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:135)
2021-05-25T23:03:44.0923156Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:135)
2021-05-25T23:03:44.0923976Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:135)
2021-05-25T23:03:44.0924785Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:429)
2021-05-25T23:03:44.0925329Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:690)
2021-05-25T23:03:44.0925970Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:641)
2021-05-25T23:03:44.0926619Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646)
2021-05-25T23:03:44.0927160Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:619)
2021-05-25T23:03:44.0927650Z May 25 23:03:44 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
2021-05-25T23:03:44.0928212Z May 25 23:03:44 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
2021-05-25T23:03:44.0928778Z May 25 23:03:44 	at java.lang.Thread.run(Thread.java:748)
2021-05-25T23:03:44.0929177Z May 25 23:03:44 Caused by: java.io.IOException: Error while sending value.
2021-05-25T23:03:44.0929915Z May 25 23:03:44 	at org.apache.flink.streaming.connectors.cassandra.NoOpCassandraFailureHandler.onFailure(NoOpCassandraFailureHandler.java:33)
2021-05-25T23:03:44.0930633Z May 25 23:03:44 	at org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.checkAsyncErrors(CassandraSinkBase.java:169)
2021-05-25T23:03:44.0931258Z May 25 23:03:44 	at org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.close(CassandraSinkBase.java:103)
2021-05-25T23:03:44.0931832Z May 25 23:03:44 	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2021-05-25T23:03:44.0932610Z May 25 23:03:44 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:109)
2021-05-25T23:03:44.0933249Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$closeOperator$5(StreamOperatorWrapper.java:213)
2021-05-25T23:03:44.0933970Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
2021-05-25T23:03:44.0934671Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:210)
2021-05-25T23:03:44.0935319Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:185)
2021-05-25T23:03:44.0936064Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
2021-05-25T23:03:44.0936840Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
2021-05-25T23:03:44.0937623Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:97)
2021-05-25T23:03:44.0938728Z May 25 23:03:44 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:162)
2021-05-25T23:03:44.0939446Z May 25 23:03:44 	... 12 more
2021-05-25T23:03:44.0940208Z May 25 23:03:44 Caused by: com.datastax.driver.core.exceptions.WriteTimeoutException: Cassandra timeout during write query at consistency ONE (1 replica were required but only 0 acknowledged the write)
2021-05-25T23:03:44.0941288Z May 25 23:03:44 	at com.datastax.driver.core.exceptions.WriteTimeoutException.copy(WriteTimeoutException.java:100)
2021-05-25T23:03:44.0942041Z May 25 23:03:44 	at com.datastax.driver.core.Responses$Error.asException(Responses.java:122)
2021-05-25T23:03:44.0942764Z May 25 23:03:44 	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)
2021-05-25T23:03:44.0943324Z May 25 23:03:44 	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)
2021-05-25T23:03:44.0943960Z May 25 23:03:44 	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)
2021-05-25T23:03:44.0944782Z May 25 23:03:44 	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
2021-05-25T23:03:44.0945489Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
2021-05-25T23:03:44.0946107Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
2021-05-25T23:03:44.0946713Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
2021-05-25T23:03:44.0947281Z May 25 23:03:44 	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
2021-05-25T23:03:44.0948081Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
2021-05-25T23:03:44.0948676Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
2021-05-25T23:03:44.0949281Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
2021-05-25T23:03:44.0949866Z May 25 23:03:44 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
2021-05-25T23:03:44.0950438Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
2021-05-25T23:03:44.0951042Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
2021-05-25T23:03:44.0951648Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
2021-05-25T23:03:44.0952212Z May 25 23:03:44 	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
2021-05-25T23:03:44.0952930Z May 25 23:03:44 	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:308)
2021-05-25T23:03:44.0953465Z May 25 23:03:44 	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:422)
2021-05-25T23:03:44.0954012Z May 25 23:03:44 	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
2021-05-25T23:03:44.0954589Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
2021-05-25T23:03:44.0955184Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
2021-05-25T23:03:44.0955784Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
2021-05-25T23:03:44.0956375Z May 25 23:03:44 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
2021-05-25T23:03:44.0956955Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
2021-05-25T23:03:44.0957584Z May 25 23:03:44 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
2021-05-25T23:03:44.0958522Z May 25 23:03:44 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
2021-05-25T23:03:44.0959393Z May 25 23:03:44 	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:792)
2021-05-25T23:03:44.0960464Z May 25 23:03:44 	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475)
2021-05-25T23:03:44.0961208Z May 25 23:03:44 	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
2021-05-25T23:03:44.0961971Z May 25 23:03:44 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
2021-05-25T23:03:44.0962810Z May 25 23:03:44 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2021-05-25T23:03:44.0963276Z May 25 23:03:44 	... 1 more
2021-05-25T23:03:44.0964059Z May 25 23:03:44 Caused by: com.datastax.driver.core.exceptions.WriteTimeoutException: Cassandra timeout during write query at consistency ONE (1 replica were required but only 0 acknowledged the write)
2021-05-25T23:03:44.0965028Z May 25 23:03:44 	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:59)
2021-05-25T23:03:44.0965751Z May 25 23:03:44 	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)
2021-05-25T23:03:44.0966501Z May 25 23:03:44 	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)
2021-05-25T23:03:44.0967135Z May 25 23:03:44 	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)
2021-05-25T23:03:44.0968262Z May 25 23:03:44 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:88)
2021-05-25T23:03:44.0968877Z May 25 23:03:44 	... 20 more

{code}
",,akalashnikov,echauchot,gaoyunhaii,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25147,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 14 15:03:46 UTC 2021,,,,,,,,,,"0|z0rdhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","03/Jul/21 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","12/Nov/21 15:21;akalashnikov;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26175&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13913;;;","16/Nov/21 09:14;arvid;[~echauchot] could you PTAL when you have time?;;;","16/Nov/21 09:58;echauchot;[~arvid] sure, I'll take a look when I have time. Can you assign to me ?;;;","16/Nov/21 10:58;arvid;Thank you, I assigned you.;;;","19/Nov/21 15:52;echauchot;I took a look: it seems to be due to a load problem. When Cassandra writes data it waits for a given number of replicas to ack the write before responding to the client. In that case it writes to a node and to a replica. And it is the replica that did not respond in time hence the timeout at Cassandra client level. As the Cassandra cluster is an embedded daemon it is not very surprising that it is sensitive to load. I'd approach like this: 
 # first try to reduce the expectations in cassandra consistency by setting consistency to _ConsistencyLevel.ANY_ rather than _ConsistencyLevel.ONE_ which is ok for a test. Later see in time if the ITest is still flaky
 # If the test is still flaky, then migrate the cassandra test cluster from embedded daemon to either testContainers (relies on docker so less sensitive to load) or ASF v2 licenced test component such as Achilles (that I used in Apache Beam and that I contributed to) which has a lot of knobs for configuring the cluster.;;;","27/Nov/21 13:08;chesnay;potential fix merged to master: c40bbf1e87cc62880905cd567dca05a4e15aff35;;;","29/Nov/21 08:59;echauchot;thanks [~chesnay] for review/merge. So far (2 days), there was no failure on AZP in test connectors CI;;;","01/Dec/21 13:50;chesnay;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27357&view=logs&jobId=d44f43ce-542c-597d-bf94-b0718c71e5e8&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d

Not sure if it's still the same issue; there are too many cassandra test-instability tickets to make sense of things.;;;","02/Dec/21 08:48;echauchot;Hi Chesnay, thanks for the pointer. Indeed it is not the same ticket but I can definitely take a look. I'll open the related ticket.;;;","02/Dec/21 09:20;gaoyunhaii;Hi [~echauchot] , there is currently a pending issue for the above ""no host available..."" problem: https://issues.apache.org/jira/browse/FLINK-23047, it would be great if you could also have a look~;;;","02/Dec/21 09:33;echauchot;[~chesnay]  [~gaoyunhaii] yes I just found it as well. I'l comment in the related ticket

 ;;;","02/Dec/21 09:44;echauchot;FYI: https://issues.apache.org/jira/browse/FLINK-23047?focusedCommentId=17452266&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17452266;;;","07/Dec/21 11:44;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27666&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=14478;;;","10/Dec/21 11:51;echauchot; 

[~trohrmann] thanks for pointing out but the failure indicates:
{code:java}
Caused by: com.datastax.driver.core.exceptions.WriteTimeoutException: Cassandra timeout during write query at consistency ONE (1 replica were required but only 0 acknowledged the write) {code}
This should be fixed in [this merged PR|https://github.com/apache/flink/pull/17849]  Now every write request writes at consistency = ANY so there is no replica. 

My guess is that this failure did not happen on master but rather on an older branch or am I mistaken ?;;;","10/Dec/21 12:41;chesnay;It happened on 1.14; we only merged the potential fix to master.;;;","10/Dec/21 14:22;echauchot;[~chesnay] that is what I thought. Thanks for the confirmation.

It's been 13 days since we merged the fix to master and AFAIK there was no occurrence of this replica timeout failure on master since then. I think we can close this ticket.

Still I'm working on [migrating Cassandra to test containers|https://issues.apache.org/jira/browse/FLINK-25147 ] to try to fix the other flakiness issues we have on Cassandra connector.;;;","10/Dec/21 17:23;trohrmann;Can we backport the fix to {{1.14}} and {{1.13}} [~echauchot]?;;;","13/Dec/21 09:22;echauchot;[~trohrmann] of course !;;;","13/Dec/21 09:43;echauchot;[~trohrmann] [~chesnay] here are the backport PRs:

[to release 1.14|https://github.com/apache/flink/pull/18091]

[ to release 1.13|https://github.com/apache/flink/pull/18093]
;;;","14/Dec/21 15:03;chesnay;master: c40bbf1e87cc62880905cd567dca05a4e15aff35
1.14: 1aa12316267d1b80b3cce4ae11e98213a28c390d
1.13: b749e4fc41f7de5d107b8fe64f1965c02105b1bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveParser::setCurrentTimestamp fails with hive-3.1.2,FLINK-22760,13380084,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,lirui,lirui,24/May/21 11:53,10/Jun/21 05:49,13/Jul/23 08:07,28/May/21 06:11,,,,,,,,,1.13.2,1.14.0,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,leonard,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22762,FLINK-22943,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 28 06:11:50 UTC 2021,,,,,,,,,,"0|z0rb9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/21 06:11;lirui;Fixed in master: 1bfa29244c0d2d72ab6c72cf4f79a742ea6c3401
Fixed in release-1.13: a873ef925a3807292a7463cf66be0abad11a52db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the applicability of RocksDB related options as per operator,FLINK-22759,13380039,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,24/May/21 07:55,28/Aug/21 12:13,13/Jul/23 08:07,31/May/21 02:50,,,,,,,,,1.13.2,1.14.0,,,,,,Documentation,,,,,0,pull-request-available,,,,,"Hotfix of https://github.com/apache/flink/commit/f93d350e14ce3e56790672740cba91c06a77b940 tries to clarify RocksDB thread options applicability per operator/TM. However, some descriptions are not correct, and this ticket targets to resolve them.
",,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 31 02:50:51 UTC 2021,,,,,,,,,,"0|z0razs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/21 02:50;yunta;Merged
master: c0d216b606e9ba3d6e82a7e79abc97c8878ec365
release-1.13: 1518f96310c97209de139af5727741f7115f1db0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherTest.testJobStatusIsShownDuringTermination fail,FLINK-22756,13379932,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,maguowei,maguowei,22/May/21 15:20,17/Nov/21 15:21,13/Jul/23 08:07,02/Jun/21 14:01,1.12.4,,,,,,,,1.12.5,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18255&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=7871


{code:java}
2021-05-21T21:11:09.4635978Z java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: JobMaster has been shut down.
2021-05-21T21:11:09.4637019Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-05-21T21:11:09.4637464Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-05-21T21:11:09.4637981Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest.lambda$testJobStatusIsShownDuringTermination$4(DispatcherTest.java:883)
2021-05-21T21:11:09.4638553Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2021-05-21T21:11:09.4639055Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:129)
2021-05-21T21:11:09.4639610Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest.testJobStatusIsShownDuringTermination(DispatcherTest.java:878)
2021-05-21T21:11:09.4640216Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-21T21:11:09.4640602Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-21T21:11:09.4641247Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-21T21:11:09.4641741Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-21T21:11:09.4642140Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-21T21:11:09.4642614Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-21T21:11:09.4643071Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-21T21:11:09.4643542Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-21T21:11:09.4644005Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-05-21T21:11:09.4644439Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-21T21:11:09.4644856Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-21T21:11:09.4645262Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-21T21:11:09.4645791Z 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.evaluate(TestingFatalErrorHandlerResource.java:91)
2021-05-21T21:11:09.4646462Z 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.access$200(TestingFatalErrorHandlerResource.java:83)
2021-05-21T21:11:09.4647082Z 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:55)
2021-05-21T21:11:09.4647575Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-21T21:11:09.4654167Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-21T21:11:09.4654899Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-21T21:11:09.4655596Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-21T21:11:09.4656331Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-21T21:11:09.4657027Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-21T21:11:09.4657627Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-21T21:11:09.4658233Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-21T21:11:09.4658829Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-21T21:11:09.4659467Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-21T21:11:09.4660188Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-05-21T21:11:09.4660863Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-21T21:11:09.4661874Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-21T21:11:09.4662519Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-05-21T21:11:09.4663240Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-05-21T21:11:09.4664297Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-05-21T21:11:09.4665029Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-05-21T21:11:09.4665782Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-21T21:11:09.4666612Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-21T21:11:09.4667311Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-21T21:11:09.4667949Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-05-21T21:11:09.4668793Z Caused by: org.apache.flink.util.FlinkException: JobMaster has been shut down.
2021-05-21T21:11:09.4669512Z 	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.closeAsync(JobManagerRunnerImpl.java:204)
2021-05-21T21:11:09.4670398Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest$BlockingTerminationJobMangerService.closeAsync(DispatcherTest.java:1073)
2021-05-21T21:11:09.4671409Z 	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995)
2021-05-21T21:11:09.4672199Z 	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137)
2021-05-21T21:11:09.4672950Z 	at org.apache.flink.runtime.dispatcher.DispatcherJob.lambda$closeAsync$7(DispatcherJob.java:253)
2021-05-21T21:11:09.4673662Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2021-05-21T21:11:09.4674426Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2021-05-21T21:11:09.4675124Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-21T21:11:09.4675810Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)
2021-05-21T21:11:09.4676528Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1596)
2021-05-21T21:11:09.4677229Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-05-21T21:11:09.4677922Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-05-21T21:11:09.4678648Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-05-21T21:11:09.4679341Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

{code}
",,dwysakowicz,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 02 14:01:04 UTC 2021,,,,,,,,,,"0|z0rac8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/21 02:57;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18329&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=7890;;;","26/May/21 07:58;trohrmann;cc [~fpaul];;;","01/Jun/21 07:05;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18466&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7889;;;","01/Jun/21 07:06;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18466&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6&l=6456;;;","01/Jun/21 07:08;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18466&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada&l=8044;;;","02/Jun/21 14:01;chesnay;1.12: 9f1de31edc818a54852996e844ae940e96d18efb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SequenceStreamingFileSinkITCase.testWriteSequenceFile fail,FLINK-22755,13379929,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaoyunhaii,maguowei,maguowei,22/May/21 14:00,25/Aug/21 11:34,13/Jul/23 08:07,25/Aug/21 11:34,1.14.0,,,,,,,,1.14.0,,,,,,,Connectors / FileSystem,,,,,0,auto-deprioritized-major,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18224&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=14321


{code:java}
May 21 09:00:00 [ERROR] Failures: 
May 21 09:00:00 [ERROR]   SequenceStreamingFileSinkITCase.testWriteSequenceFile:97->validateResults:119 expected:<1> but was:<2>
May 21 09:00:00 [INFO] 

{code}
",,gaoyunhaii,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22710,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 25 11:34:25 UTC 2021,,,,,,,,,,"0|z0rabk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","29/Jun/21 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","25/Aug/21 11:34;gaoyunhaii;Fixed on master via 5390e91bd47219adde15d5d515a4f5baf4231fc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Links to connectors in docs are broken,FLINK-22746,13379771,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Authuir,roman,roman,21/May/21 13:38,28/Aug/21 12:13,13/Jul/23 08:07,27/May/21 14:28,,,,,,,,,1.13.2,1.14.0,,,,,,Documentation,,,,,1,pull-request-available,,,,,"Links to specific connectors, e.g. Kafka from [https://ci.apache.org/projects/flink/flink-docs-master/docs/connectors/datastream/overview/] lead to HTTP/404.",,Authuir,rmetzger,roman,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/21 18:05;rmetzger;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13026419/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 10 17:56:26 UTC 2021,,,,,,,,,,"0|z0r9cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/21 18:33;Authuir;Confirmed in master branch and 1.13 branch.

[~sjwiesman] [~rmetzger] Could you please assign this bug for me and help review this fix PR: [https://github.com/apache/flink/pull/15990] ?;;;","27/May/21 14:28;sjwiesman;fixed in master: 83402127c1e1ba264096f1e70fbbdbe005a0e8b4

fixed in 1.13: 520aa94fbb887437ca8750a19475a942dd554763;;;","04/Jun/21 18:05;rmetzger;Looks like the links are still broken :(

 !screenshot-1.png! ;;;","04/Jun/21 18:06;rmetzger;Well, it looks like our docs are not building at all: https://ci.apache.org/builders/flink-docs-master;;;","10/Jun/21 17:43;roman;I see builds resumed yesterday and continued today.;;;","10/Jun/21 17:56;rmetzger;Yes indeed! Finally :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MesosWorkerStore is started with an illegal namespace,FLINK-22745,13379751,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,21/May/21 11:36,28/Aug/21 12:12,13/Jul/23 08:07,22/May/21 09:09,1.14.0,,,,,,,,1.14.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,The MesosWorkerStore is started with an illegal namespace because of FLINK-22636.,,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22636,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 22 09:09:07 UTC 2021,,,,,,,,,,"0|z0r980:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/21 09:09;trohrmann;Fixed via 9693b11bdc05ec9cf68b8bba7db284cb34d143ba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraSinkBaseTest.testTimeoutExceptionOnInvoke fail due to TestTimedOutException,FLINK-22739,13379688,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,echauchot,maguowei,maguowei,21/May/21 08:01,07/Jul/22 14:31,13/Jul/23 08:07,07/Jul/22 14:31,1.13.0,,,,,,,,1.13.6,1.14.3,,,,,,Connectors / Cassandra,,,,,0,auto-deprioritized-major,stale-assigned,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18207&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13438


{code:java}
2021-05-20T22:21:49.9790943Z May 20 22:21:49 [ERROR] testTimeoutExceptionOnInvoke(org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest)  Time elapsed: 5.742 s  <<< ERROR!
2021-05-20T22:21:49.9791886Z May 20 22:21:49 org.junit.runners.model.TestTimedOutException: test timed out after 5000 milliseconds
2021-05-20T22:21:49.9792584Z May 20 22:21:49 	at java.base@11.0.10/java.io.RandomAccessFile.readBytes(Native Method)
2021-05-20T22:21:49.9793258Z May 20 22:21:49 	at java.base@11.0.10/java.io.RandomAccessFile.read(RandomAccessFile.java:406)
2021-05-20T22:21:49.9794205Z May 20 22:21:49 	at java.base@11.0.10/java.util.zip.ZipFile$Source.readAt(ZipFile.java:1331)
2021-05-20T22:21:49.9794931Z May 20 22:21:49 	at java.base@11.0.10/java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:1028)
2021-05-20T22:21:49.9795769Z May 20 22:21:49 	at java.base@11.0.10/java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:468)
2021-05-20T22:21:49.9796538Z May 20 22:21:49 	at java.base@11.0.10/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:159)
2021-05-20T22:21:49.9797988Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.loader.Resource.getBytes(Resource.java:124)
2021-05-20T22:21:49.9798766Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:797)
2021-05-20T22:21:49.9799605Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:698)
2021-05-20T22:21:49.9800472Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:621)
2021-05-20T22:21:49.9801568Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:579)
2021-05-20T22:21:49.9802449Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
2021-05-20T22:21:49.9803167Z May 20 22:21:49 	at java.base@11.0.10/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
2021-05-20T22:21:49.9803984Z May 20 22:21:49 	at app//com.google.common.collect.ImmutableMap.of(ImmutableMap.java:70)
2021-05-20T22:21:49.9804648Z May 20 22:21:49 	at app//com.google.common.reflect.TypeResolver$TypeTable.<init>(TypeResolver.java:218)
2021-05-20T22:21:49.9805331Z May 20 22:21:49 	at app//com.google.common.reflect.TypeResolver.<init>(TypeResolver.java:60)
2021-05-20T22:21:49.9805972Z May 20 22:21:49 	at app//com.google.common.reflect.TypeToken.where(TypeToken.java:215)
2021-05-20T22:21:49.9806669Z May 20 22:21:49 	at app//com.google.common.reflect.TypeToken.where(TypeToken.java:239)
2021-05-20T22:21:49.9807514Z May 20 22:21:49 	at app//com.datastax.driver.core.TypeTokens.mapOf(TypeTokens.java:106)
2021-05-20T22:21:49.9808329Z May 20 22:21:49 	at app//com.datastax.driver.core.SanityChecks.checkGuava(SanityChecks.java:50)
2021-05-20T22:21:49.9809035Z May 20 22:21:49 	at app//com.datastax.driver.core.SanityChecks.check(SanityChecks.java:36)
2021-05-20T22:21:49.9809696Z May 20 22:21:49 	at app//com.datastax.driver.core.Cluster.<clinit>(Cluster.java:67)
2021-05-20T22:21:49.9810253Z May 20 22:21:49 	at jdk.internal.reflect.GeneratedSerializationConstructorAccessor2.newInstance(Unknown Source)
2021-05-20T22:21:49.9810930Z May 20 22:21:49 	at java.base@11.0.10/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
2021-05-20T22:21:49.9811707Z May 20 22:21:49 	at app//org.objenesis.instantiator.sun.SunReflectionFactoryInstantiator.newInstance(SunReflectionFactoryInstantiator.java:45)
2021-05-20T22:21:49.9812475Z May 20 22:21:49 	at app//org.objenesis.ObjenesisBase.newInstance(ObjenesisBase.java:73)
2021-05-20T22:21:49.9813232Z May 20 22:21:49 	at app//org.mockito.internal.creation.instance.ObjenesisInstantiator.newInstance(ObjenesisInstantiator.java:19)
2021-05-20T22:21:49.9814154Z May 20 22:21:49 	at app//org.mockito.internal.creation.bytebuddy.SubclassByteBuddyMockMaker.createMock(SubclassByteBuddyMockMaker.java:47)
2021-05-20T22:21:49.9814990Z May 20 22:21:49 	at app//org.mockito.internal.creation.bytebuddy.ByteBuddyMockMaker.createMock(ByteBuddyMockMaker.java:25)
2021-05-20T22:21:49.9815814Z May 20 22:21:49 	at app//org.powermock.api.mockito.mockmaker.PowerMockMaker.createMock(PowerMockMaker.java:41)
2021-05-20T22:21:49.9816471Z May 20 22:21:49 	at app//org.mockito.internal.util.MockUtil.createMock(MockUtil.java:35)
2021-05-20T22:21:49.9817261Z May 20 22:21:49 	at app//org.mockito.internal.MockitoCore.mock(MockitoCore.java:69)
2021-05-20T22:21:49.9817854Z May 20 22:21:49 	at app//org.mockito.Mockito.mock(Mockito.java:1895)
2021-05-20T22:21:49.9818331Z May 20 22:21:49 	at app//org.mockito.Mockito.mock(Mockito.java:1804)
2021-05-20T22:21:49.9818872Z May 20 22:21:49 	at app//org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest$TestCassandraSink.<clinit>(CassandraSinkBaseTest.java:386)
2021-05-20T22:21:49.9819569Z May 20 22:21:49 	at app//org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.createOpenedTestCassandraSink(CassandraSinkBaseTest.java:352)
2021-05-20T22:21:49.9820464Z May 20 22:21:49 	at app//org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.testTimeoutExceptionOnInvoke(CassandraSinkBaseTest.java:321)
2021-05-20T22:21:49.9821063Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-20T22:21:49.9821616Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-20T22:21:49.9822250Z May 20 22:21:49 	at java.base@11.0.10/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-20T22:21:49.9822933Z May 20 22:21:49 	at java.base@11.0.10/java.lang.reflect.Method.invoke(Method.java:566)
2021-05-20T22:21:49.9823510Z May 20 22:21:49 	at app//org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-20T22:21:49.9824043Z May 20 22:21:49 	at app//org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-20T22:21:49.9824570Z May 20 22:21:49 	at app//org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-20T22:21:49.9825065Z May 20 22:21:49 	at app//org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-20T22:21:49.9825604Z May 20 22:21:49 	at app//org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-05-20T22:21:49.9826173Z May 20 22:21:49 	at app//org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-05-20T22:21:49.9826686Z May 20 22:21:49 	at java.base@11.0.10/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2021-05-20T22:21:49.9827278Z May 20 22:21:49 	at java.base@11.0.10/java.lang.Thread.run(Thread.java:834)

{code}
",,echauchot,gaoyunhaii,maguowei,martijnvisser,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25147,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 14:26:16 UTC 2022,,,,,,,,,,"0|z0r8u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","28/Jun/21 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","12/Jul/21 02:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20256&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13770;;;","10/Sep/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Sep/21 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Dec/21 19:01;martijnvisser;As can be seen in the umbrella ticket https://issues.apache.org/jira/browse/FLINK-25147 that Cassandra tests are now using the testcontainers. We're going to monitor the Cassandra tests until the new year and if no new occurences are popping up, we are closing this (and the other subtasks) as resolved;;;","13/Jan/22 11:29;trohrmann;Fixed via FLINK-25147.;;;","30/Mar/22 07:42;gaoyunhaii;This issue seems reproduced on 1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33864&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14354;;;","07/Jul/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","07/Jul/22 14:23;martijnvisser;[~echauchot] Do you think we can close this ticket? I do think it has been resolved;;;","07/Jul/22 14:26;echauchot;[~martijnvisser] yes I think so;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink-SQL-Client HiveModuleFactory has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0",FLINK-22736,13379663,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhangjian201,zhangjian201,21/May/21 07:07,21/May/21 07:19,13/Jul/23 08:07,21/May/21 07:19,1.12.2,,,,,,,,1.12.2,,,,,,,Table SQL / Client,,,,,0,,,,,,"bin/sql-client.sh embedded  启动 SQL client 时发生以下问题

[root@yqznwl3zhongjj22 flink-1.12.2]# bin/sql-client.sh embedded[root@yqznwl3zhongjj22 flink-1.12.2]# bin/sql-client.sh embeddedSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/flink-1.12.2/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]2021-05-21 14:46:22,387 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /tmp/.yarn-properties-root.2021-05-21 14:46:22,387 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /tmp/.yarn-properties-root.No default environment specified.Searching for '/usr/local/flink-1.12.2/conf/sql-client-defaults.yaml'...found.Reading default environment from: file:/usr/local/flink-1.12.2/conf/sql-client-defaults.yamlNo session environment specified.

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue. at org.apache.flink.table.client.SqlClient.main(SqlClient.java:215)Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context. at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:972) at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:225) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:108) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)Caused by: java.lang.UnsupportedClassVersionError: org/apache/flink/table/module/hive/HiveModuleFactory has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:468) at java.net.URLClassLoader.access$100(URLClassLoader.java:74) at java.net.URLClassLoader$1.run(URLClassLoader.java:369) at java.net.URLClassLoader$1.run(URLClassLoader.java:363) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:362) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:370) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at java.util.Iterator.forEachRemaining(Iterator.java:116) at org.apache.flink.table.factories.TableFactoryService.discoverFactories(TableFactoryService.java:194) at org.apache.flink.table.factories.TableFactoryService.findAllInternal(TableFactoryService.java:164) at org.apache.flink.table.factories.TableFactoryService.findAll(TableFactoryService.java:122) at org.apache.flink.table.factories.ComponentFactoryService.find(ComponentFactoryService.java:50) at org.apache.flink.table.client.gateway.local.ExecutionContext.lookupExecutor(ExecutionContext.java:487) at org.apache.flink.table.client.gateway.local.ExecutionContext.createTableEnvironment(ExecutionContext.java:651) at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:536) at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:187) at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:138) at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:961) ... 3 more

!image-2021-05-21-15-05-26-870.png!","Flink version：flink-1.12.2

Apache Maven 3.6.3
Java version: 1.8.0_221, vendor: Oracle Corporation, runtime: /usr/java/jdk1.8.0_221/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""3.10.0-957.12.2.el7.x86_64"", arch: ""amd64"", family: ""unix""",zhangjian201,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/21 07:05;zhangjian201;image-2021-05-21-15-05-26-870.png;https://issues.apache.org/jira/secure/attachment/13025743/image-2021-05-21-15-05-26-870.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 07:19:43 UTC 2021,,,,,,,,,,"0|z0r8og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/21 07:19;zhangjian201;This is because of the mismatched version of the jar package introduced。

Replace jar package with version 1.12

flink-connector-hive_2.11-1.13-SNAPSHOT.jar;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ExtractionUtils#getClassReader causes ""open too many files"" error",FLINK-22734,13379657,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,zhiping,zhiping,21/May/21 06:44,23/Sep/21 17:25,13/Jul/23 08:07,09/Jun/21 06:29,1.12.2,,,,,,,,1.14.0,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"when we get classReader in ExtractionUtils 

 
{code:java}
    private static ClassReader getClassReader(Class<?> cls) {        
        final String className = cls.getName().replaceFirst(""^.*\\."", """") + "".class"";        
        try {            
            return new ClassReader(cls.getResourceAsStream(className));        
        } catch (IOException e) {
            throw new IllegalStateException(""Could not instantiate ClassReader."", e);
        }
    }
{code}
we open a inputStream by ""cls.getResourceAsStream(className)"" and set it as a construct param for  ClassReader without any other steps to close it. Also ClassReader's construct method with only a outside inputStream, classReader won't close the inputStream while it finished read the file. This will leads to fd leak with ""open too many files"" error when we parse a lot independent sqls with different envs in one jvm(the case is that we want to extract the table/data lineage of our flink jobs).
{code:java}
lsof -p 8011 | wc -l 
10534
{code}
After checked all files and I found they are opened when loaded the udf jars for each loop to build the env. Then I test this method with try-with-source and it works well. So I wonder if it is necessary to change a little bit just like this

 
{code:java}
private static ClassReader getClassReader(Class<?> cls) {
   final String className = cls.getName().replaceFirst(""^.*\\."", """") + "".class"";
   try (InputStream inputStream = cls.getResourceAsStream(className)) {
      return new ClassReader(inputStream);
   } catch (IOException e) {
      throw new IllegalStateException(""Could not instantiate ClassReader."", e);
   }
}
{code}
 

 

 ",,zhiping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 09 06:29:08 UTC 2021,,,,,,,,,,"0|z0r8n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/21 06:29;chesnay;master: 7a2f080ad372a4fb35aba4a9b4d504cfe0e5b417;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type mismatch thrown in DataStream.union if parameter is KeyedStream for Python DataStream API,FLINK-22733,13379633,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,21/May/21 03:22,28/Aug/21 12:11,13/Jul/23 08:07,21/May/21 11:08,1.12.0,1.13.0,,,,,,,1.12.5,1.13.1,,,,,,API / Python,,,,,0,pull-request-available,,,,,See [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-DataStream-union-type-mismatch-td43855.html] for more details.,,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 11:08:54 UTC 2021,,,,,,,,,,"0|z0r8hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/21 11:08;dian.fu;Fixed to
- master via 09168af6c8491dfccd85ece9943123c968957e17
- release-1.13 via 256cf8d634917ce6b71ede67c8f5bdea54768002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lookup join condition with CURRENT_DATE fails to filter records,FLINK-22730,13379544,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,20/May/21 16:00,28/Aug/21 12:16,13/Jul/23 08:07,15/Jun/21 02:50,1.12.0,1.13.0,1.14.0,,,,,,1.13.2,1.14.0,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"Add the following test case to org.apache.flink.table.api.TableEnvironmentITCase to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  val id1 = TestValuesTableFactory.registerData(
    Seq(Row.of(""abc"", LocalDateTime.of(2000, 1, 1, 0, 0))))
  val ddl1 =
    s""""""
       |CREATE TABLE Ta (
       |  id VARCHAR,
       |  ts TIMESTAMP,
       |  proc AS PROCTIME()
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$id1',
       |  'bounded' = 'true'
       |)
       |"""""".stripMargin
  tEnv.executeSql(ddl1)

  val id2 = TestValuesTableFactory.registerData(
    Seq(Row.of(""abc"", LocalDateTime.of(2000, 1, 2, 0, 0))))
  val ddl2 =
    s""""""
       |CREATE TABLE Tb (
       |  id VARCHAR,
       |  ts TIMESTAMP
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$id2',
       |  'bounded' = 'true'
       |)
       |"""""".stripMargin
  tEnv.executeSql(ddl2)

  val it = tEnv.executeSql(
    """"""
      |SELECT * FROM Ta AS t1
      |INNER JOIN Tb FOR SYSTEM_TIME AS OF t1.proc AS t2
      |ON t1.id = t2.id
      |WHERE CAST(coalesce(t1.ts, t2.ts) AS VARCHAR) >= CONCAT(CAST(CURRENT_DATE AS VARCHAR), ' 00:00:00')
      |"""""".stripMargin).collect()

  while (it.hasNext) {
    System.out.println(it.next())
  }
}
{code}

The result is
{code}
+I[abc, 2000-01-01T00:00, 2021-05-20T14:30:47.735Z, abc, 2000-01-02T00:00]
{code}

which is obviously incorrect.

The generated operator is as follows

{code:java}
public class JoinTableFuncCollector$22 extends org.apache.flink.table.runtime.collector.TableFunctionCollector {

    org.apache.flink.table.data.GenericRowData out = new org.apache.flink.table.data.GenericRowData(2);
    org.apache.flink.table.data.utils.JoinedRowData joinedRow$9 = new org.apache.flink.table.data.utils.JoinedRowData();
    private static final java.util.TimeZone timeZone =
            java.util.TimeZone.getTimeZone(""Asia/Shanghai"");
    private org.apache.flink.table.data.TimestampData timestamp;
    private org.apache.flink.table.data.TimestampData localTimestamp;
    private int date;

    private final org.apache.flink.table.data.binary.BinaryStringData str$17 = org.apache.flink.table.data.binary.BinaryStringData.fromString("" 00:00:00"");


    public JoinTableFuncCollector$22(Object[] references) throws Exception {

    }

    @Override
    public void open(org.apache.flink.configuration.Configuration parameters) throws Exception {

    }

    @Override
    public void collect(Object record) throws Exception {
        org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) getInput();
        org.apache.flink.table.data.RowData in2 = (org.apache.flink.table.data.RowData) record;
        org.apache.flink.table.data.binary.BinaryStringData field$7;
        boolean isNull$7;
        org.apache.flink.table.data.TimestampData field$8;
        boolean isNull$8;
        org.apache.flink.table.data.TimestampData field$10;
        boolean isNull$10;
        boolean isNull$13;
        org.apache.flink.table.data.binary.BinaryStringData result$14;
        boolean isNull$15;
        org.apache.flink.table.data.binary.BinaryStringData result$16;
        boolean isNull$18;
        org.apache.flink.table.data.binary.BinaryStringData result$19;
        boolean isNull$20;
        boolean result$21;
        isNull$8 = in2.isNullAt(1);
        field$8 = null;
        if (!isNull$8) {
            field$8 = in2.getTimestamp(1, 6);
        }
        isNull$7 = in2.isNullAt(0);
        field$7 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        if (!isNull$7) {
            field$7 = ((org.apache.flink.table.data.binary.BinaryStringData) in2.getString(0));
        }
        isNull$10 = in1.isNullAt(1);
        field$10 = null;
        if (!isNull$10) {
            field$10 = in1.getTimestamp(1, 6);
        }



        boolean result$11 = !isNull$10;
        org.apache.flink.table.data.TimestampData result$12 = null;
        boolean isNull$12;
        if (result$11) {

            isNull$12 = isNull$10;
            if (!isNull$12) {
                result$12 = field$10;
            }
        }
        else {

            isNull$12 = isNull$8;
            if (!isNull$12) {
                result$12 = field$8;
            }
        }
        isNull$13 = isNull$12;
        result$14 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        if (!isNull$13) {

            result$14 = org.apache.flink.table.data.binary.BinaryStringData.fromString(org.apache.flink.table.runtime.functions.SqlDateTimeUtils.timestampToString(result$12, 6));
            isNull$13 = (result$14 == null);
        }




        isNull$15 = false;
        result$16 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        if (!isNull$15) {

            result$16 = org.apache.flink.table.data.binary.BinaryStringData.fromString(org.apache.calcite.avatica.util.DateTimeUtils.unixDateToString(((int) date)));
            isNull$15 = (result$16 == null);
        }


        result$19 = org.apache.flink.table.data.binary.BinaryStringDataUtil.concat(( isNull$15 ) ? null : (result$16), ( false ) ? null : (((org.apache.flink.table.data.binary.BinaryStringData) str$17)));
        isNull$18 = (result$19 == null);
        if (isNull$18) {
            result$19 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        }

        isNull$20 = isNull$13 || isNull$18;
        result$21 = false;
        if (!isNull$20) {

            result$21 = ((result$14 == null) ? ((result$19 == null) ? 0 : -1) : ((result$19 == null) ? 1 : (result$14.compareTo(result$19)))) >= 0;

        }

        if (result$21) {





            if (isNull$7) {
                out.setField(0, null);
            } else {
                out.setField(0, field$7);
            }



            if (isNull$8) {
                out.setField(1, null);
            } else {
                out.setField(1, field$8);
            }


            joinedRow$9.replace(in1, out);
            joinedRow$9.setRowKind(in1.getRowKind());
            outputResult(joinedRow$9);

        }

    }

    @Override
    public void close() throws Exception {

    }
}
{code}

The member variable {{date}} is not initialized before use, thus causing this bug.

This is because {{LookupJoinCodeGenerator#generateTableFunctionCollectorForJoinTable}} forgets to use {{${ctx.reusePerRecordCode()}}} when generating {{collect}} method.",,jingzhang,libenchao,lzljs3620320,TsReaper,xmarker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 15 02:50:37 UTC 2021,,,,,,,,,,"0|z0r7y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/21 12:55;xmarker;Could you assign this jira to me ?  ;;;","15/Jun/21 02:48;lzljs3620320;[~xmarker] Sorry, [~TsReaper] already create a PR for this.;;;","15/Jun/21 02:50;lzljs3620320;Fixed via:

master: 1635f9326c8993114d8a5aa1881c2078393f8eb5

release-1.13: 229b9c6be3521fea2e5c8fe51a4468918ecf8bb2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlotManagers should unregister metrics at the start of suspend(),FLINK-22725,13379497,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,20/May/21 11:53,28/Aug/21 12:11,13/Jul/23 08:07,25/May/21 20:34,1.13.0,,,,,,,,1.13.2,1.14.0,,,,,,Runtime / Coordination,Runtime / Metrics,,,,0,pull-request-available,,,,,"Slotmanagers register metrics in start(), but only unregister them in close().

This has 2 issues:

a) If the SM is restarted it cannot re-register the metrics because the old ones are still present; this isn't a critical problem (because the old ones still work) but it produces unnecesasry logging noise

b) The metric may produce an NPE when accessed by a reporter during suspend().",,complone,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22646,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 25 20:34:29 UTC 2021,,,,,,,,,,"0|z0r7nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/21 13:56;complone;i can do it;;;","25/May/21 20:34;chesnay;master: 75a2aaa4d091a54257e1620dd6d50055dff69e61

1.13: 5363432300e4c102a73ac1365fee198d21289790;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Breaking HighAvailabilityServices interface by adding new method,FLINK-22721,13379458,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,20/May/21 08:26,28/Aug/21 12:11,13/Jul/23 08:07,20/May/21 15:54,1.12.5,1.13.1,1.14.0,,,,,,1.12.5,1.13.1,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"As part of FLINK-20695 we introduced a new method to the {{HighAvailabilityServices.cleanupJobData}} interface. Since this method has not default implementation it is currently breaking change. Since Flink allows to implement custom Ha services using this interface, I suggest adding a default implementation for this method.",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 15:54:14 UTC 2021,,,,,,,,,,"0|z0r7ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/21 08:42;chesnay;What exact guarantees do we provide for this interface? It's not tagged as an API (so maybe let's add {{@PublicEvolving}}?) and it exposes various various other interfaces (BlobStore, JobGraphStore, LeaderRetrievalService, and further down even the JobGraph); are we just maintaining compatibility on a best-effort basis?;;;","20/May/21 08:44;trohrmann;The short answer is yes. We noticed the breaking behavior with the vvp-ha service implementation. I think it is worthwhile trying to be as compatible as possible since the default method won't hurt us much.;;;","20/May/21 15:54;trohrmann;Fixed via

1.14.0: 7753542057cd498b5932b0babea78aad8786b88e
1.13.1: d6ef576a308fe8aa204ca9636af3e2d4ec231da3
1.12.5: 1e5d808ad06906987ad5eb42e75e8414b2e5ad00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowJoinUtil.containsWindowStartEqualityAndEndEquality should not throw exception,FLINK-22719,13379420,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingzhang,lzljs3620320,lzljs3620320,20/May/21 03:32,28/Aug/21 12:12,13/Jul/23 08:07,24/May/21 02:12,,,,,,,,,1.14.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,This will broke regular join sql.,,aitozi,godfreyhe,jingzhang,lzljs3620320,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 21 02:22:21 UTC 2021,,,,,,,,,,"0|z0r76g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/21 03:35;jingzhang;[~lzljs3620320] I agree with you, If join has two window nodes as input nodes, however join condition does not satisfy window join condition restrict,  it could fall back to regular join.;;;","20/May/21 06:10;jingzhang;[~lzljs3620320], There exists multiple cases in WindowJoinTest to test those cases which window join condition is not valid. After I remove the exception thrown logical in WindowJoinUtil.containsWindowStartEqualityAndEndEquality, those cases also thrown a Exception,
{code:java}
aused by: org.apache.flink.table.api.TableException: Rowtime attributes must not be in the input rows of a regular join. As a workaround you can cast the time attributes of input tables to TIMESTAMP before.aused by: org.apache.flink.table.api.TableException: Rowtime attributes must not be in the input rows of a regular join. As a workaround you can cast the time attributes of input tables to TIMESTAMP before. at org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalJoinRule.matches(StreamPhysicalJoinRule.scala:78) at org.apache.calcite.plan.volcano.VolcanoRuleCall.matchRecurse(VolcanoRuleCall.java:284) at org.apache.calcite.plan.volcano.VolcanoRuleCall.matchRecurse(VolcanoRuleCall.java:411) at org.apache.calcite.plan.volcano.VolcanoRuleCall.matchRecurse(VolcanoRuleCall.java:411) at org.apache.calcite.plan.volcano.VolcanoRuleCall.match(VolcanoRuleCall.java:268) at org.apache.calcite.plan.volcano.VolcanoPlanner.fireRules(VolcanoPlanner.java:985) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1245) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:84) at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:268) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1132) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604) at org.apache.calcite.plan.volcano.VolcanoPlanner.changeTraits(VolcanoPlanner.java:486) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:309) at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64) ... 45 more
{code}
 ;;;","20/May/21 06:13;jingzhang;Now there is no possible to do a regular join directly after two window operations because of the above exception. Is there any need to remove thrown exception in `WindowJoinUtil` to let those cases fall back to regular join? ;;;","20/May/21 06:32;lzljs3620320;[~qingru zhang] Do not select row time field, we just need window start or window end.;;;","20/May/21 06:41;jingzhang;[~lzljs3620320] Thanks for point out the root cause , I would update these cases to remove row time.;;;","24/May/21 02:12;lzljs3620320;Fixed via master: 5ec4755def4f7705b45e2ea154ed8e2857cceda0;;;","25/May/21 07:56;twalthr;[~lzljs3620320] I'm not sure if the issue has been fixed properly. I haven't looked into the PR yet but I worked on this issue before as part of FLINK-10211 before the Blink merge interrupted a complete fix. Can we guarantee that the downstream operators still receive a valid rowtime attribute when removing this check? ;;;","19/Jun/21 03:24;jingzhang;[~twalthr] Sorry for late response. FLINK-22719 aims to fallback to regular join if the logicalJoin does not satisfy the requirements to convert to WindowJoin. It does not aim to fix the problems describe in FLINK-10211. We just avoid the exception in the unit test of previous WindowJoinTest by updating SQL query in unit test not select row time field. It means FLINK-10211  is still present if we keep the previous unit test of WindowJoinTest.

About FLINK-10211, I totally agree with you moving the time indicator materialization closely before the physical optimization.;;;","21/Jun/21 02:22;lzljs3620320;[~qingru zhang] is right, they are different problems. In some cases(select rowtime field in downstream join),  FLINK-10211 is the blocker of FLINK-22719. But in other cases(select window start/end and not select rowtime field in downstream join),  FLINK-10211 is not the blocker of FLINK-22719.

But yes, it is good to fix FLINK-10211 too~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetProtoStreamingFileSinkITCase.testParquetProtoWriters failed,FLINK-22710,13379228,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,maguowei,maguowei,19/May/21 07:52,07/Mar/22 09:03,13/Jul/23 08:07,25/Aug/21 11:32,1.14.0,,,,,,,,1.14.0,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,auto-deprioritized-major,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18114&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13181


{code:java}

May 19 03:00:31 [ERROR]   ParquetProtoStreamingFileSinkITCase.testParquetProtoWriters:83->validateResults:92 expected:<1> but was:<2>

{code}
",,gaoyunhaii,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22755,FLINK-20558,FLINK-18456,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 07 09:03:12 UTC 2022,,,,,,,,,,"0|z0r5zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Jun/21 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","23/Aug/21 12:30;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22573&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13781;;;","23/Aug/21 12:31;trohrmann;cc [~gaoyunhaii] do you have an idea what could be going wrong here?;;;","23/Aug/21 13:41;gaoyunhaii;Very thanks [~trohrmann] for checking this issue! I tried a quick look and still not found yet, and I'll have a more close look~ ;;;","23/Aug/21 17:13;gaoyunhaii;This issue should be caused by the `StreamFileSink` in the test uses the default `DateTimeBucketAssigner` which partitioned data into buckets according to hours, thus if the tests is run at the boundary of an hour, there might be two buckets. I'll have a check of all the similar tests and open a PR to fix them. ;;;","25/Aug/21 11:32;gaoyunhaii;Fixed on the master via 5390e91bd47219adde15d5d515a4f5baf4231fc2;;;","07/Mar/22 09:03;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32583&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361] It happen again on 1.13, I'll pick the fix back to 1.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source NOTICE outdated regarding docs/,FLINK-22706,13379189,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Fuyao Li,Fuyao Li,Fuyao Li,19/May/21 04:50,23/Sep/21 17:21,13/Jul/23 08:07,20/May/21 08:55,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Release System,,,,,0,pull-request-available,,,,,"With the PR introduced in [1], flink documentation is upgraded to Hugo instead of Jekyll. However, we fail to update the NOTICE file. [2]

 

The jQuery, bootstrap dependencies are not used anymore and AnchorJS JS library path should also be updated.

 

In addition, bootstrap version 3.3.4 is known to have security vulnerabilities. See link [3]. This could cause legal approval rejection while adopting Apache Flink. (I am facing such a issue.)

 

I can create a pull request to fix this. Please assign the task to me.

 

[1] https://issues.apache.org/jira/browse/FLINK-21193

[2] [https://github.com/apache/flink/blob/master/NOTICE#L10]

[3]https://snyk.io/test/npm/bootstrap/3.3.4",,Fuyao Li,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 08:55:17 UTC 2021,,,,,,,,,,"0|z0r5r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/21 04:55;Fuyao Li;[~jark] Hello Jark, please assign the task to me. I will update the NOTICE file and we can have a discussion at the PR.;;;","19/May/21 06:10;jark;cc [~rmetzger], [~sjwiesman];;;","19/May/21 08:05;chesnay;[~Fuyao Li] Nice catch, go ahead and open a PR.;;;","20/May/21 08:55;chesnay;master: f1132f338c7bf56bff83147ab9842227c3d01e97

1.13: d1dd346aa0ce7c6300e4e392384b8ac40be0c13c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperHaServicesTest.testCleanupJobData failed,FLINK-22704,13379178,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,maguowei,maguowei,19/May/21 03:24,23/Sep/21 17:21,13/Jul/23 08:07,20/May/21 08:18,1.13.0,1.14.0,,,,,,,1.12.5,1.13.1,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18108&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=8172


{code:java}
May 19 01:30:02 Expected: a collection containing ""1a2850d5759a2e1f4fef9cc7e6abc675""
May 19 01:30:02      but: was ""resource_manager_lock""

{code}
",,maguowei,mapohl,trohrmann,yittg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20695,FLINK-22494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 08:18:16 UTC 2021,,,,,,,,,,"0|z0r5oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/21 06:58;mapohl;The test got introduced by FLINK-20695 and started [failing on the backport branch|https://dev.azure.com/mapohl/flink/_build/results?buildId=434&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=6765] of FLINK-22494 when rebasing {{release-1.13}}.;;;","19/May/21 07:47;mapohl;[~yittg] may you have a look at it. The test seem to expect the {{jobId}} does not seem to be included in every path making the assert in [ZooKeeperHaServicesTest:179|https://github.com/apache/flink/blob/release-1.13/flink-runtime/src/test/java/org/apache/flink/runtime/highavailability/zookeeper/ZooKeeperHaServicesTest.java#L179]. Unfortunately, I am not able to reproduce it locally.

That's the stacktrace I collected from [this build|https://dev.azure.com/mapohl/flink/_build/results?buildId=434&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=6765]:
{code}
Expected: a collection containing ""c3278fc11e864fa29888d8016fbcefa6""
     but: was ""resource_manager_lock""
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
        at org.junit.Assert.assertThat(Assert.java:956)
        at org.junit.Assert.assertThat(Assert.java:923)
        at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.lambda$testCleanupJobData$1(ZooKeeperHaServicesTest.java:179)
        at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.runCleanupTestWithJob(ZooKeeperHaServicesTest.java:260)
        at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.testCleanupJobData(ZooKeeperHaServicesTest.java:172)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code};;;","19/May/21 08:13;trohrmann;I think the problem is caused by not waiting on the leader election of the {{JobMaster}} to have completed. That's why it can happen that we stop the {{LeaderElectionService}} before it has created the job specific leader nodes.

I will fix this problem if it is ok [~mapohl].;;;","19/May/21 11:28;yittg;[~mapohl] Thanks for letting me know, and [~trohrmann] thanks for your fix.
Sorry for introducing this issue.;;;","20/May/21 04:37;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18151&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada&l=8170

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18153&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=6768;;;","20/May/21 08:18;trohrmann;Fixed via 

1.14.0: 3558ff172c73fad95cb08bb2e6cda96daf13db91
1.13.1: 7390981e088e618af91a4493d720a36572e8672c
1.12.5: 7b9351e26d2356ac1d2f4f1ad02fd7f6860257be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceITCase.testRedundantParallelism failed,FLINK-22702,13379174,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,maguowei,maguowei,19/May/21 03:08,09/Mar/23 01:52,13/Jul/23 08:07,26/Aug/21 10:29,1.12.3,1.14.0,,,,,,,1.14.0,,,,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18107&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6847

{code:java}
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:199)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:154)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:116)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:275)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:67)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:398)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:619)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:583)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:758)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:573)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:146)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:101)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.IllegalStateException: Consumer is not subscribed to any topics or assigned any partitions
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1223)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:97)
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:56)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:138)
	... 6 more

{code}
",,becket_qin,dwysakowicz,maguowei,roman,ruanhang1993,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24811,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 08 06:01:05 UTC 2021,,,,,,,,,,"0|z0r5ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/21 04:40;maguowei;KafkaSourceITCase.testValueOnlyDeserializer fail because of same errors

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18151&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=7009

;;;","02/Jun/21 04:56;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18527&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7893;;;","07/Jun/21 01:56;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18708&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6586;;;","07/Jun/21 02:33;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18709&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7893;;;","10/Jun/21 02:48;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18851&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7620;;;","10/Jun/21 02:49;xtsong;[~renqs], could you help take a look into this?;;;","15/Jun/21 03:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18959&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6651;;;","17/Jun/21 03:41;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19037&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6563;;;","21/Jun/21 02:31;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19168&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6673;;;","21/Jun/21 02:32;xtsong;[~renqs], any insights on this instability?;;;","20/Jul/21 01:37;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20709&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6177;;;","28/Jul/21 06:43;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21053&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=7211;;;","28/Jul/21 06:43;dwysakowicz;ping [~renqs];;;","04/Aug/21 16:46;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21503&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7521;;;","07/Aug/21 12:32;roman;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21676&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7207]

(same error in testValueOnlyDeserializer);;;","16/Aug/21 03:32;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22179&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7094;;;","24/Aug/21 09:52;ruanhang1993;This issue is related to https://issues.apache.org/jira/browse/FLINK-22198.

When add splits to _KafkaPartitionSplitReader_, the split reader will delete the completed partitions whose offset is over the stopping offset in the _removeEmptySplits_ method.  All assigned partitions are deleted here in the test case.

The reason is that the timestamp of the test data is more than 7 days earlier than the test time. These messages are deleted ,and the beginning offset of the partition is set to the end, which causes  the split reader judge the partition as a completed partition and delete it.

We should change the timestamp of the test data to null or a recent timestamp.;;;","26/Aug/21 10:29;becket_qin;Merged to master. 
83b9ee8a3afd3e3c5512b4a495f97c01c1be11c2;;;","08/Oct/21 06:01;xtsong;Instance on 1.12:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24808&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6576;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RabbitMQ source does not stop unless message arrives in queue,FLINK-22698,13379137,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cmick,austince,austince,18/May/21 20:34,17/Nov/21 15:21,13/Jul/23 08:07,22/Jun/21 11:35,1.12.4,1.13.1,,,,,,,1.12.5,1.13.2,1.14.0,,,,,Connectors/ RabbitMQ,,,,,0,pull-request-available,,,,,"In a streaming job with multiple RMQSources, a stop-with-savepoint request has unexpected behavior. Regular checkpoints and savepoints complete successfully, it is only the stop-with-savepoint request where this behavior is seen.
 
*Expected Behavior:*
The stop-with-savepoint request stops the job with a FINISHED state.
 
*Actual Behavior:*
The stop-with-savepoint request either times out or hangs indefinitely unless a message arrives in all the queues that the job consumes from after the stop-with-savepoint request is made.
 
*Current workaround:*
Send a sentinel value to each of the queues consumed by the job that the deserialization schema checks in its isEndOfStream method. This is cumbersome and makes it difficult to do stateful upgrades, as coordination with another system is now necessary. 
 
 
The TaskManager thread dump is attached.
 ",,austince,cmick,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23322,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/21 20:33;austince;taskmanager_thread_dump.json;https://issues.apache.org/jira/secure/attachment/13025617/taskmanager_thread_dump.json",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 22 11:35:08 UTC 2021,,,,,,,,,,"0|z0r5fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/21 06:53;nicholasjiang;[~austince], thank you for your research. From your description, the problem lies in the inspection of IsEndStream method. And the check of the deserialization schema cause difficult upgrades, therefore this check should consider the state of Job not the isEndOfStream in deserialization schema. Right?;;;","19/May/21 16:48;austince;Hey [~nicholasjiang] – I think that is correct. I'm coming from [this ML thread|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/RabbitMQ-source-does-not-stop-unless-message-arrives-in-queue-td43705.html] and am trying to get some more information from the OP.;;;","20/May/21 02:08;nicholasjiang;[~austince], thanks for tracking this issue. From this ML thread, the discussion is in the obstruct progress. IMO, we could fix the IsEndStream method to solve the above problem possiblely.;;;","21/May/21 12:35;cmick;Hi [~austince], we've encountered a similar issue after upgrading to Flink 1.12. Seems recent changes (introduced in 1.12) in the RMQSource caused this issue to appear (so most likely 1.13 is affected as well). Actually, it happens with a single queue as well - it's not possible to stop the job with a savepoint when there is no data on the queue (the job ends up in some weird state). You can still cancel it though (which seems to be a good workaround here).

I've investigated the issue, and its caused by the way how message delivery is implemented in the {color:#000000}QueueingConsumer (or rather on how it's used). In the run function it is waiting indefinitely for the next message to arrive (without checking if the source was cancelled):
{color}

{color:#000000}[https://github.com/apache/flink/blob/0a7ec4fe3f11dbcc1f56685c2211b60d8f496b2d/flink-connectors/flink-connector-rabbitmq/src/main/java/org/apache/flink/streaming/connectors/rabbitmq/RMQSource.java#L329]{color}

{color:#000000}So I believe it's not about inspecting the isEndOfStream method as [~nicholasjiang] suggested.{color}

 

{color:#000000}This can be easily solved by adding a delivery timeout (setting it to some low values such as 15s instead of infinity solves the issue).{color}

{color:#000000}We've already tested this out, and were able to reproduce it with additional unit tests. I suggest to add a deliveryTimeout parameter to the configuration (so by default we could leave the current behavior). If you think it's a good approach I could create a PR solving the problem.{color}

 

 ;;;","21/May/21 14:47;austince;Hi [~cmick] – that cause makes perfect sense to me. And the proposed solution also sounds good, just a couple of questions:
 * What happens when a delivery times out?
 * How do other sources handle this, i.e. Kafka? Do they wait for messages on another thread than where the source's `run` is called?

 

Anyway, I'll assign it to you + thanks for looking into this! Also, I'm going to cc' [~fpaul], as he's been working with this connector (+ others) and its FLIP-27 upgrades. He might have some other ideas.;;;","24/May/21 09:28;cmick;Thanks.

When the delivery times out, but the the consumer was not cancelled yet, nothing will happen - the consumer internal queue (LinkedBlockingQueue) will be checked again (one null check needs to be added, as the poll returns null instead of blocking the thread).

If the consumer was cancelled, the main loop in the source will be completed and the job will be able to stop gracefully.

 

I will checkout the implementation of other sources. I will write here if I find something that might be of help. Thanks for the suggestion.

 ;;;","26/May/21 01:32;nicholasjiang;Thanks for [~cmick]'s investigation. The solution which adds a deliveryTimeout parameter to the configuration makes sense to me. IMO, this deliveryTimeout could be regarded as configuration option and users could configure this timeout. 
[~cmick], In the unit test, how do you configure the delivery timeout which maybe cause the failure of the unit test?
[~austince], should the FLIP-27 upgrade of RabbiteMQ Source concern this problem?;;;","26/May/21 08:10;cmick;[~austince] In other data sources it's handled similarly (via timeout) or by interrupting the consumer thread (for instance Kafka source has a consumer thread running in the background). Adding a delivery timeout will be simple and will do the job here, so I would not recommend refactoring the whole source into thread-based (but maybe its worth considering when implementing the FLIP-27 source).

[~nicholasjiang] Yes, I will add it to configuration. I'm still not sure what should be the default. I would probably go with 0 - meaning no deliveryTimeout (and wait forever) - as this is the current approach (and if fixed in 1.12 or 1.13 this is the way to go). Although, probably in all real-world cases it makes no sense to wait forever for something, so maybe something like 30s would be better here (so in worst case, when there is no input data, the local LinkedBlockingQueue would be checked for new elements every 30s)

The unit test should be quite straightforward like:
1) start the source (with very low deliveryTimeout)
2) cancel the source
3) wait until source stops (in case of timeout the test fails)
4) check if there was no exception (there shouldn't be any cause its stopped gracefully)
The current tests use a mock that always returns some value from the consumer immediately. That is why the above case would pass the tests now (so additionally the mock will have to be slightly adjusted for this specific test only);;;","26/May/21 16:04;austince;[~nicholasjiang] – yes, I agree w/ Michał that this is something that should be considered w/ the FLIP-27 source, if it has not already been.

[~cmick] Thanks for the research into other sources, the approach still sounds good to me, including the user-facing configuration option. I'm not sure if there is precedent for it, but we could potentially introduce the fix in 1.14 with a reasonable timeout default (like the suggested 30s), and use the 0s default when backporting to 1.12 + 1.13 to maintain functionality. What do you think of that? Would it be more confusing to users?;;;","28/May/21 14:08;cmick;[~austince] I think it's a good idea. If someone faces the issue now, it will be easy to solve by setting this parameter, and for all others it will be resolved by default in the next releases.

I will prepare the PR based on this discussion shortly. Back-port should be easy (I can prepare later as well), just need to modify the default value.;;;","22/Jun/21 11:35;arvid;Merged into master as 9df4d26530c3dc12345d3c4b26ef1eb7cb672a4, into 1.13 as 19cf47ba7740b149413b418685bc2ec3e9d6b1da, and into 1.12 as 4b4b61485b8675aad177fe1a44d0170d28ae2c01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Flink's Code Style and Quality Guide to match the Google Java Format style introduced ,FLINK-22695,13379008,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,mapohl,mapohl,18/May/21 11:37,08/Jun/21 12:52,13/Jul/23 08:07,08/Jun/21 12:52,,,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,starter,,,,"FLINK-20651 introduced a new code formatting rule set which does not comply with what's stated in [Flink's Code Style and Quality Guide|https://flink.apache.org/contributing/code-style-and-quality-formatting.html] (e.g. spaces vs tabs).

The docs should be updated accordingly.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 12:52:39 UTC 2021,,,,,,,,,,"0|z0r4mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/21 12:52;chesnay;asf-site: f611ab27429bc5d14ace81227037f0486add8299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXPLAIN cannot be used on Hbase table when useing ROW type,FLINK-22693,13378968,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sadfdddd,sadfdddd,18/May/21 08:11,21/May/21 07:17,13/Jul/23 08:07,21/May/21 07:17,1.12.2,,,,,,,,,,,,,,,Connectors / HBase,Table SQL / API,,,,0,,,,,,"We use ’EXPLAIN PLAN FOR‘ as a way of validating SQL

calcite will no longer work and throw an exception when we sink to HBase table and use row type

!hbase.PNG!",,fsk119,jark,sadfdddd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/21 06:27;sadfdddd;Driver.java;https://issues.apache.org/jira/secure/attachment/13025677/Driver.java","18/May/21 08:04;sadfdddd;hbase.PNG;https://issues.apache.org/jira/secure/attachment/13025568/hbase.PNG","20/May/21 06:27;sadfdddd;pom.xml;https://issues.apache.org/jira/secure/attachment/13025678/pom.xml",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 07:14:12 UTC 2021,,,,,,,,,,"0|z0r4e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/21 11:55;jark;[~sadfdddd] could you share the program code above, so that we can reproduce this error easily?;;;","20/May/21 06:33;sadfdddd;[~jark]  I've uploaded my code，This error can stably be reproduced [^Driver.java];;;","20/May/21 06:44;sadfdddd;[~jark] The program can run without EXPLAIN PLAN FOR

First of all, I found that the difference between having EXPLAIN and not having EXPLAIN is in class org.apache.flink.table.planner.calcite.FlinkPlannerImpl .

On line 143, INSERT sqlNode direct return .but explain will validator.validate(explain.getExplicandum)

 ;;;","20/May/21 07:06;sadfdddd;I guess from the error exception that the type does not match, so I made the following attempt

To make the type match, I changed this class org.apache.flink.table.planner.calcite.FlinkTypeFactory to let it return StructKind.NONE ,but it didn't work

 

Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 60 to line 1, column 70: Cannot assign to target field 'f1' of type RecordType(INTEGER name) from source field 'f1' of type RecordType(INTEGER EXPR$0)Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 60 to line 1, column 70: Cannot assign to target field 'f1' of type RecordType(INTEGER name) from source field 'f1' of type RecordType(INTEGER EXPR$0) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5043) at org.apache.calcite.sql.validate.SqlValidatorImpl.checkTypeAssignment(SqlValidatorImpl.java:4714) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateInsert(SqlValidatorImpl.java:4417) at org.apache.calcite.sql.SqlInsert.validate(SqlInsert.java:158) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1016);;;","21/May/21 06:57;fsk119;I rerun the program in master. It seems the problem has been solved by FLINK-22155.  ;;;","21/May/21 07:14;sadfdddd;[~fsk119]  Thank you for your reply;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointStoreITCase.testRestartOnRecoveryFailure fails with RuntimeException,FLINK-22692,13378967,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,rmetzger,rmetzger,18/May/21 08:09,20/May/21 13:41,13/Jul/23 08:07,20/May/21 13:41,1.13.0,1.14.0,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"Not sure if it is related to the adaptive scheduler: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18052&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228

{code}
May 17 22:29:11 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.351 s <<< FAILURE! - in org.apache.flink.test.checkpointing.CheckpointStoreITCase
May 17 22:29:11 [ERROR] testRestartOnRecoveryFailure(org.apache.flink.test.checkpointing.CheckpointStoreITCase)  Time elapsed: 1.138 s  <<< ERROR!
May 17 22:29:11 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 17 22:29:11 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
May 17 22:29:11 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
May 17 22:29:11 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
May 17 22:29:11 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1081)
May 17 22:29:11 	at akka.dispatch.OnComplete.internal(Future.scala:264)
May 17 22:29:11 	at akka.dispatch.OnComplete.internal(Future.scala:261)
May 17 22:29:11 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
May 17 22:29:11 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
May 17 22:29:11 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
May 17 22:29:11 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
May 17 22:29:11 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
May 17 22:29:11 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
May 17 22:29:11 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
May 17 22:29:11 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
{code}",,maguowei,rmetzger,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22502,,,,,FLINK-22483,FLINK-22717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 13:41:19 UTC 2021,,,,,,,,,,"0|z0r4ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/21 08:10;rmetzger;[~roman_khachatryan] can you take a look at this failure?;;;","19/May/21 03:18;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18106&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4516

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18108&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4510

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18151&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4516;;;","19/May/21 07:44;roman;I couldn't reproduce  the issue with the default scheduler.

However, it is easily reproducible with the adaptive scheduler (e.g. add .set(JobManagerOptions.SCHEDULER, JobManagerOptions.SchedulerType.Adaptive) to CheckpointStoreITCase on line 67).

 

Which shows that the behavior of Adaptive scheduler differs from the Default one: it doesn't restart the job if a failure occurs during recovery.

 

Is the difference above something intentional [~rmetzger]?;;;","19/May/21 07:54;trohrmann;Yes, the {{AdaptiveScheduler}} fails the job if it cannot create the {{ExeuctionGraph}} (see FLINK-21846). Consequently, it does not restart the process in case of recovery failure. The problem is that we only do the recovery of the checkpoints once we create the {{ExecutionGraph}}. Since the {{ExecutionGraph}} is not immediately created, it will only be realized later. I think that FLINK-22483 might actually solve this particular problem because it moves the recovery of checkpoints to the point where we create the {{JobMaster}}.;;;","19/May/21 08:35;roman;Makes sense, thanks for the clarification.

So currently a combination of: (FLINK-22502 with Adaptive scheduler without FLINK-22483) will fail a job in case of a transient failure.

Which still seems a better behavior than before FLINK-22502. 

So I propose to add an assumption to tests that the Default scheduer is used (and drop it once FLINK-22483 is implemented).

WDYT?;;;","19/May/21 09:03;trohrmann;Sounds good to me.;;;","19/May/21 17:17;roman;I've created a [PR|https://github.com/apache/flink/pull/15960] to adjust CheckpointStoreITCase.

However, JobMasterStopWithSavepointITCase failed for another reason - I've opened FLINK-22717 to track it.
  ;;;","20/May/21 13:41;roman;Merged into master: b43ffda36917881f29b923e4d58a350423bb8840

into 1.13: e4c626dc6eee910de31f0ed7aadd1ca25917703d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table API Documentation Row-Based Operations Example Fails,FLINK-22689,13378930,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paul8263,yunfengzhou,yunfengzhou,18/May/21 02:50,28/Aug/21 12:13,13/Jul/23 08:07,01/Jun/21 04:09,1.12.1,,,,,,,,1.13.2,1.14.0,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"I wrote the following program according to the example code provided in [Documentation/Table API/Row-based operations|https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/tableapi/#row-based-operations]

{code:java}
public class TableUDF {
    public static void main(String[] args) {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
​
        Table input = tEnv.fromValues(
                DataTypes.of(""ROW<c STRING>""),
                Row.of(""name"")
        );
​
        ScalarFunction func = new MyMapFunction();
        tEnv.registerFunction(""func"", func);
​
        Table table = input
                .map(call(""func"", $(""c"")).as(""a"", ""b"")); // exception occurs here
​
        table.execute().print();
    }
​
    public static class MyMapFunction extends ScalarFunction {
        public Row eval(String a) {
            return Row.of(a, ""pre-"" + a);
        }
​
        @Override
        public TypeInformation<?> getResultType(Class<?>[] signature) {
            return Types.ROW(Types.STRING, Types.STRING);
        }
    }
}
{code}

The code above would throw an exception like this:

{code}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Only a scalar function can be used in the map operator.
  at org.apache.flink.table.operations.utils.OperationTreeBuilder.map(OperationTreeBuilder.java:480)
  at org.apache.flink.table.api.internal.TableImpl.map(TableImpl.java:519)
  at org.apache.flink.ml.common.function.TableUDFBug.main(TableUDF.java:29)
{code}


  The core of the program above is identical to that provided in flink documentation, but it cannot function correctly. This might affect users who want to use custom function with table API.

 ",,jark,paul8263,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 01 04:09:20 UTC 2021,,,,,,,,,,"0|z0r45k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/21 07:08;paul8263;I reproduced this problem. The ""as"" method call in BaseExpressions.java simply transformed the expression to UnresolvedCallExpression with the functionDefinition of BuiltInFunctionDefinitions.AS, which does not meet the requirement of ""map"", who needs the kind of resolvedMapFunction to be FunctionKind.SCALAR.

Also, the ""as"" method in BaseExpressions.java fails to expand ROW type to multiple fields.

Could you pls assign this issue to me?;;;","01/Jun/21 04:09;jark;Fixed in 
- master: 997fa27e462c612f70401939c0fa9b217490dd73
- release-1.13: dab3cf240ada16aa754f7855dff4a364abf91e5f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Root Exception can not be shown on Web UI in Flink 1.13.0,FLINK-22688,13378918,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,gary.wu,gary.wu,18/May/21 01:36,30/Nov/21 20:37,13/Jul/23 08:07,21/May/21 11:58,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Hi,
 
We have upgraded our Flink applications to 1.13.0 but we found that Root Exception can not be shown on Web UI with an internal server error message. After opening a browser development console and trace the message, we found that there is an exception in job manager:
 
_{color:#000000}2021-05-12 13:30:45,589 ERROR org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler [] - Unhandled exception.{color}_
_{color:#000000}java.lang.IllegalArgumentException: The location must not be null for a non-global failure.{color}_
    _{color:#000000}at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.assertLocalExceptionInfo(JobExceptionsHandler.java:218) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.createRootExceptionInfo(JobExceptionsHandler.java:191) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.SliceOps$1$1.accept(SliceOps.java:199) ~[?:?]{color}_
    _{color:#000000}at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1632) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:127) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:502) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:488) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ~[?:?]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.createJobExceptionHistory(JobExceptionsHandler.java:169) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.createJobExceptionsInfo(JobExceptionsHandler.java:154) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.handleRequest(JobExceptionsHandler.java:101) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.handleRequest(JobExceptionsHandler.java:63) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$0(AbstractExecutionGraphHandler.java:87) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]{color}_
    _{color:#000000}at java.lang.Thread.run(Thread.java:834) [?:?]{color}_
 
I see there are some exceptions in task managers and I remember the kind of exception can be shown in UI in version 1.12.1 :
 
_2021-05-18 00:50:30,261 WARN org.apache.flink.runtime.taskmanager.Task [] - xxx (23/90)#13 (c345fb009b5d93628b5a6d890c8f4226) switched from RUNNING to FAILED with failure cause: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager '10.194.65.3/10.194.65.3:44273'. This might indicate that the remote task manager was lost._
    _at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelInactive(CreditBasedPartitionRequestClientHandler.java:160)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)_
    _at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelInactive(NettyMessageClientDecoderDelegate.java:94)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)_
    _at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)_
    _at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)_
    _at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)_
    _at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)_
    _at java.base/java.lang.Thread.run(Thread.java:834)_
 
 
 
The issue has been reported in flink-user mailing list before: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Root-Exception-can-not-be-shown-on-Web-UI-in-Flink-1-13-0-td43673.html",,gary.wu,guoyangze,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/21 12:41;gary.wu;jobmanager_log_v1.txt.zip;https://issues.apache.org/jira/secure/attachment/13025694/jobmanager_log_v1.txt.zip","20/May/21 12:44;gary.wu;taskmanager_log_v1.txt;https://issues.apache.org/jira/secure/attachment/13025695/taskmanager_log_v1.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 11:58:48 UTC 2021,,,,,,,,,,"0|z0r42w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/21 01:41;gary.wu;Hi, [~guoyangze] and [~maguowei], 

I have created a bug for the issue discussed in the mailing list. Thank you in advance!;;;","18/May/21 02:15;guoyangze;cc [~mapohl] who can shed some light on this issue.;;;","18/May/21 08:23;mapohl;Thanks for sharing the information. [~gary.wu]. I'm gonna start looking into it.;;;","18/May/21 12:14;mapohl;[~gary.wu] could you share your JobManager and TaskManager logs with us? We would like to understand the context of the issue a bit more.;;;","20/May/21 12:41;gary.wu;Hi, [~mapohl],

Here are the logs of jobmanager and taskmanager logs. There is some information removed due to company policy. Thanks.

[^jobmanager_log_v1.txt.zip][^taskmanager_log_v1.txt];;;","20/May/21 14:23;mapohl;Awesome, thanks for sharing. It helped verifying that the assert was, indeed, too strict. (y);;;","21/May/21 11:58;chesnay;master: 0a7ec4fe3f11dbcc1f56685c2211b60d8f496b2d

1.13: 1aeb7e3fab771d8ecfbd0efbdc5a60f4dea41b51;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incompatible subtask mappings while resuming from unaligned checkpoints,FLINK-22686,13378788,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,arvid,arvid,17/May/21 11:19,09/Nov/21 15:22,13/Jul/23 08:07,03/Jun/21 07:41,1.13.0,,,,,,,,1.13.2,1.14.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"A user [reported|https://lists.apache.org/x/list.html?user@flink.apache.org:lte=1M:Flink%201.13.0%20reactive%20mode:%20Job%20stop%20and%20cannot%20restore%20from%20checkpoint] that he encountered an internal error while resuming during reactive mode. There isn't an immediate connection to reactive mode, so it's safe to assume that one rescaling case was not covered.

{noformat}
Caused by: java.lang.IllegalStateException: Incompatible subtask mappings: are multiple operators ingesting/producing intermediate results with varying degrees of parallelism?Found RescaleMappings{mappings=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149], [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179], [180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]]} and RescaleMappings{mappings=[[0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98, 105, 112, 119, 126, 133, 140, 147, 154, 161, 168, 175, 182, 189, 196, 203], [1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99, 106, 113, 120, 127, 134, 141, 148, 155, 162, 169, 176, 183, 190, 197, 204], [2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86, 93, 100, 107, 114, 121, 128, 135, 142, 149, 156, 163, 170, 177, 184, 191, 198, 205], [3, 10, 17, 24, 31, 38, 45, 52, 59, 66, 73, 80, 87, 94, 101, 108, 115, 122, 129, 136, 143, 150, 157, 164, 171, 178, 185, 192, 199, 206], [4, 11, 18, 25, 32, 39, 46, 53, 60, 67, 74, 81, 88, 95, 102, 109, 116, 123, 130, 137, 144, 151, 158, 165, 172, 179, 186, 193, 200, 207], [5, 12, 19, 26, 33, 40, 47, 54, 61, 68, 75, 82, 89, 96, 103, 110, 117, 124, 131, 138, 145, 152, 159, 166, 173, 180, 187, 194, 201, 208], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97, 104, 111, 118, 125, 132, 139, 146, 153, 160, 167, 174, 181, 188, 195, 202, 209]]}.
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.checkSubtaskMapping(TaskStateAssignment.java:322) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.getInputMapping(TaskStateAssignment.java:306) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.reDistributeInputChannelStates(StateAssignmentOperation.java:409) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignAttemptState(StateAssignmentOperation.java:193) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignStates(StateAssignmentOperation.java:139) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1566) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1646) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphAndRestoreState(AdaptiveScheduler.java:986) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$createExecutionGraphAndRestoreStateAsync$25(AdaptiveScheduler.java:976) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.BackgroundTask.lambda$new$0(BackgroundTask.java:57) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) ~[?:?]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.lang.Thread.run(Thread.java:834) ~[?:?]
{noformat}

Here it seems that the same gate gets input from a range-partitioned and a round-robin partitioned channel at the same time. During the implementation of FLINK-19801, we couldn't find such a case and optimized the implementation accordingly.

We have asked the user to provide his topology.",,czchen,dwysakowicz,gaoyunhaii,gary.wu,kezhuw,klion26,pnowojski,rmetzger,roman,Terry1897,trohrmann,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22815,,,,,,,,,,,,,,,,FLINK-24621,,,,,,,,,,"18/May/21 02:50;czchen;topology_1.png;https://issues.apache.org/jira/secure/attachment/13025557/topology_1.png","18/May/21 02:50;czchen;topology_2.png;https://issues.apache.org/jira/secure/attachment/13025558/topology_2.png","18/May/21 02:50;czchen;topology_3.png;https://issues.apache.org/jira/secure/attachment/13025559/topology_3.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 03 07:41:16 UTC 2021,,,,,,,,,,"0|z0r3a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/21 02:52;czchen;[~arvid]
 * The topology is in attachments topology_1.png, topology_2.png, topology_3.png (From top to bottom)
 * All operators are replaced with their base class name due to company policy.
 * The `EnrichedData` in topology is a `map` to change data format from Kafka.;;;","18/May/21 06:57;trohrmann;Thanks a lot for sharing the topology with us [~czchen]. I think the {{KeyedBroadcastProcessFunction}} is the culprit here because it combines the broadcast stream with a keyed stream.;;;","19/May/21 22:56;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/May/21 11:43;dwysakowicz;The reason for the particular exception is that the two input gates have different partitioning policies which breaks the assumption made for the implementation of UC rescaling. During implementing a fix for it I realized that there is another problem underneath that UC rescaling does not work well with BROADCAST partitioning whatsoever. For UC we cannot guarantee that the state of each operator is consistent in regards to processed records. Some of the operators might have already consumed a copy of a particular event, while others might have not. There is no way to reliably rescale the state of the channels so that each operator sees the same set of events after rescaling.

We decided not to fix the original issue in 1.13.1 as it might actually enable the discovered one, which in turn can lead to silent inconsistencies in the restored state. We will include a fix for both issues in 1.14.;;;","21/May/21 15:55;trohrmann;Would it make sense to disable UC for broadcast partitionings?;;;","21/May/21 15:58;dwysakowicz;That's what we want to do for 1.14. I'd prefer not to rush it for 1.13.1. Unless we want to block the 1.13.1 for a few more days. So far it is kind of disabled by the reported bug 😅;;;","03/Jun/21 07:41;dwysakowicz;Fixed in
* master
** 4011bde30cf3dee5cccb8eeb0faa7658d77a7f82 abd321c04d87799800d0b0dada9334fd46f99960
* 1.13.2
** 8327f4486841cd1d6beb05418e6d4206a6f4858b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The total Flink/process memory of memoryConfiguration in /taskmanagers can be null or incorrect value,FLINK-22683,13378764,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,17/May/21 09:22,23/Sep/21 17:22,13/Jul/23 08:07,04/Jun/21 06:21,1.12.4,1.13.0,,,,,,,1.12.5,1.13.2,1.14.0,,,,,Runtime / Metrics,,,,,0,pull-request-available,,,,,"In FLINK-14435, we add the `memoryConfiguration` to /taskmanagers REST API. However, that field can be `null` if it is not configured by the user or the incorrect value in fine-grained resource management. We need to calculate them proactively.",,guoyangze,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 04 06:21:10 UTC 2021,,,,,,,,,,"0|z0r34o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/21 09:23;guoyangze;[~xintongsong] Could you assign this to me?;;;","04/Jun/21 06:21;xtsong;Fixed via:
- master (1.14): 9e8c551958f30d5b54cbb26d2a232d96e1e0988c
- release-1.13: 2ea2f9f3bc21cfdaeec732d0c692eafa68665b2c
- release-1.12: 68f1812017aa027750daf0281e8257edb85965f9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An `IndexOutOfBoundsException` is thrown out when apply `WatermarkAssignerChangelogNormalizeTransposeRule`,FLINK-22680,13378751,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingzhang,jingzhang,jingzhang,17/May/21 08:53,23/Sep/21 17:22,13/Jul/23 08:07,05/Jun/21 07:14,,,,,,,,,1.14.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"{code:java}
@Test
def testUnResolvedWindowAggregateOnUpsertSource(): Unit = {

  def localDateTime(epochSecond: Long): LocalDateTime = {
    LocalDateTime.ofEpochSecond(epochSecond, 0, ZoneOffset.UTC)
  }

  val upsertSourceCurrencyData = List(
    changelogRow(""+U"", ""Euro"", ""no1"", JLong.valueOf(114L), localDateTime(1L)),
    changelogRow(""+U"", ""US Dollar"", ""no1"", JLong.valueOf(102L), localDateTime(2L)),
    changelogRow(""+U"", ""Yen"", ""no1"", JLong.valueOf(1L), localDateTime(3L)),
    changelogRow(""+U"", ""RMB"", ""no1"", JLong.valueOf(702L), localDateTime(4L)),
    changelogRow(""+U"", ""Euro"",  ""no1"", JLong.valueOf(118L), localDateTime(6L)),
    changelogRow(""+U"", ""US Dollar"", ""no1"", JLong.valueOf(104L), localDateTime(4L)),
    changelogRow(""-D"", ""RMB"", ""no1"", JLong.valueOf(702L), localDateTime(4L)))

  val upsertSourceDataId = registerData(upsertSourceCurrencyData)
  tEnv.executeSql(
    s""""""
       |CREATE TABLE upsert_currency (
       |  currency STRING,
       |  currency_no STRING,
       |  rate  BIGINT,
       |  currency_time TIMESTAMP(3),
       |  WATERMARK FOR currency_time AS currency_time - interval '5' SECOND,
       |  PRIMARY KEY(currency) NOT ENFORCED
       |) WITH (
       |  'connector' = 'values',
       |  'changelog-mode' = 'UA,D',
       |  'data-id' = '$upsertSourceDataId'
       |)
       |"""""".stripMargin)
  val sql =
    """"""
      |SELECT
      |TUMBLE_START(currency_time, INTERVAL '5' SECOND) as w_start,
      |TUMBLE_END(currency_time, INTERVAL '5' SECOND) as w_end,
      |MAX(rate) AS max_rate
      |FROM upsert_currency
      |GROUP BY TUMBLE(currency_time, INTERVAL '5' SECOND)
      |"""""".stripMargin
  val sink = new TestingAppendSink
  tEnv.sqlQuery(sql).toAppendStream[Row].addSink(sink)
  env.execute()
}
{code}
I add. the above ITCase for window aggregate process upsert input stream when resolved [FLINK-20487|https://issues.apache.org/jira/browse/FLINK-20487],  an `ArrayIndexOutOfBoundsException` is thrown out when apply `WatermarkAssignerChangelogNormalizeTransposeRule`, the detail information is as following,
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 2java.lang.ArrayIndexOutOfBoundsException: 2
 at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:75) at org.apache.calcite.util.Util$TransformingList.get(Util.java:2732) at org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalExchange$$anonfun$2.apply(CommonPhysicalExchange.scala:108) at org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalExchange$$anonfun$2.apply(CommonPhysicalExchange.scala:108) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalExchange.distributionToString(CommonPhysicalExchange.scala:108) at org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalExchange.explainTerms(CommonPhysicalExchange.scala:94) at org.apache.calcite.rel.AbstractRelNode.getDigestItems(AbstractRelNode.java:409) at org.apache.calcite.rel.AbstractRelNode.deepHashCode(AbstractRelNode.java:391) at org.apache.calcite.rel.AbstractRelNode$InnerRelDigest.hashCode(AbstractRelNode.java:443) at java.util.HashMap.hash(HashMap.java:339) at java.util.HashMap.get(HashMap.java:557) at org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:815) at org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:799) at org.apache.calcite.plan.hep.HepPlanner.applyTransformationResults(HepPlanner.java:734) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:545) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271) at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:279) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163) at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toStreamInternal(StreamTableEnvironmentImpl.scala:291) at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.scala:325) at org.apache.flink.table.api.bridge.scala.TableConversions.toAppendStream(TableConversions.scala:78) at org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.testUnResolvedWindowAggregateOnUpsertSource(GroupWindowITCase.scala:470) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}
The root cause is after transpose `Calc` and `Exchange` in the following `RelNode` Tree, the distribution keys of `Exchange` should be adjusted because the `Calc` already projects unuseless columns.

!image-2021-05-17-16-46-00-789.png!",,fsk119,godfreyhe,jark,jingzhang,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20487,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/21 08:49;jingzhang;image-2021-05-17-16-46-00-789.png;https://issues.apache.org/jira/secure/attachment/13025528/image-2021-05-17-16-46-00-789.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 05 07:14:09 UTC 2021,,,,,,,,,,"0|z0r31s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/21 09:17;jingzhang;[~godfrey] Could I take this issue? ;;;","17/May/21 09:33;godfreyhe;Thanks for reporting this issue [~qingru zhang], assign to you;;;","25/May/21 10:07;jingzhang;[~jark] I'm very sorry for late response. Does this fix have time to enter the 1.13.1 version? Or alter to 1.13.2?;;;","25/May/21 10:11;jingzhang;[~jark] [~godfreyhe] I moved the issue to 1.13.2 temporarily.;;;","03/Jun/21 15:55;godfreyhe;Fixed in 1.14.0: a364daa37202dc4d7a60c613f547cdcd6893ecd2;;;","04/Jun/21 07:00;chesnay;The commit was reverted because it broke the master branch.;;;","05/Jun/21 07:14;godfreyhe;Fixed in 1.14.0: 73d97fb4c3ffd60b9657de734d0293eb5b14e4f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint fail,FLINK-22662,13378413,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,maguowei,maguowei,14/May/21 04:34,23/Sep/21 18:00,13/Jul/23 08:07,09/Jul/21 01:43,1.13.1,1.14.0,,,,,,,1.13.2,1.14.0,,,,,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,,"
{code:java}
2021-05-14T00:24:57.8487649Z May 14 00:24:57 [ERROR] testKillYarnSessionClusterEntrypoint(org.apache.flink.yarn.YARNHighAvailabilityITCase)  Time elapsed: 34.667 s  <<< ERROR!
2021-05-14T00:24:57.8488567Z May 14 00:24:57 java.util.concurrent.ExecutionException: 
2021-05-14T00:24:57.8489301Z May 14 00:24:57 org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.handler.RestHandlerException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (610ed4b159ece04c8ee2ec40e7d0c143)
2021-05-14T00:24:57.8493142Z May 14 00:24:57 	at org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.propagateException(JobExecutionResultHandler.java:94)
2021-05-14T00:24:57.8495823Z May 14 00:24:57 	at org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.lambda$handleRequest$1(JobExecutionResultHandler.java:84)
2021-05-14T00:24:57.8496733Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
2021-05-14T00:24:57.8497640Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
2021-05-14T00:24:57.8498491Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-14T00:24:57.8499222Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-05-14T00:24:57.8500003Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)
2021-05-14T00:24:57.8500872Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-05-14T00:24:57.8501702Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-05-14T00:24:57.8502662Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-14T00:24:57.8503472Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-05-14T00:24:57.8504269Z May 14 00:24:57 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079)
2021-05-14T00:24:57.8504892Z May 14 00:24:57 	at akka.dispatch.OnComplete.internal(Future.scala:263)
2021-05-14T00:24:57.8505565Z May 14 00:24:57 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-05-14T00:24:57.8506062Z May 14 00:24:57 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-05-14T00:24:57.8506819Z May 14 00:24:57 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-05-14T00:24:57.8507418Z May 14 00:24:57 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-05-14T00:24:57.8508373Z May 14 00:24:57 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-05-14T00:24:57.8509144Z May 14 00:24:57 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-05-14T00:24:57.8509972Z May 14 00:24:57 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-05-14T00:24:57.8510675Z May 14 00:24:57 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-05-14T00:24:57.8511376Z May 14 00:24:57 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2021-05-14T00:24:57.8512222Z May 14 00:24:57 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-05-14T00:24:57.8513090Z May 14 00:24:57 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-05-14T00:24:57.8513835Z May 14 00:24:57 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-05-14T00:24:57.8514576Z May 14 00:24:57 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-05-14T00:24:57.8515344Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-05-14T00:24:57.8516317Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-05-14T00:24:57.8517537Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-05-14T00:24:57.8518525Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-05-14T00:24:57.8519372Z May 14 00:24:57 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-05-14T00:24:57.8520060Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-05-14T00:24:57.8520845Z May 14 00:24:57 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-05-14T00:24:57.8521684Z May 14 00:24:57 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-05-14T00:24:57.8522646Z May 14 00:24:57 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-05-14T00:24:57.8523285Z May 14 00:24:57 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-05-14T00:24:57.8524046Z May 14 00:24:57 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-05-14T00:24:57.8524892Z May 14 00:24:57 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-05-14T00:24:57.8525798Z May 14 00:24:57 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (610ed4b159ece04c8ee2ec40e7d0c143)
2021-05-14T00:24:57.8526988Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2021-05-14T00:24:57.8527951Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2021-05-14T00:24:57.8528731Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:957)
2021-05-14T00:24:57.8529606Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2021-05-14T00:24:57.8530207Z May 14 00:24:57 	... 34 more
2021-05-14T00:24:57.8530805Z May 14 00:24:57 Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (610ed4b159ece04c8ee2ec40e7d0c143)
2021-05-14T00:24:57.8531746Z May 14 00:24:57 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$requestJobStatus$14(Dispatcher.java:596)
2021-05-14T00:24:57.8532553Z May 14 00:24:57 	at java.util.Optional.orElseGet(Optional.java:267)
2021-05-14T00:24:57.8533222Z May 14 00:24:57 	at org.apache.flink.runtime.dispatcher.Dispatcher.requestJobStatus(Dispatcher.java:590)
2021-05-14T00:24:57.8533857Z May 14 00:24:57 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-14T00:24:57.8534597Z May 14 00:24:57 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-14T00:24:57.8535203Z May 14 00:24:57 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-14T00:24:57.8535733Z May 14 00:24:57 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-14T00:24:57.8536250Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
2021-05-14T00:24:57.8536861Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
2021-05-14T00:24:57.8537578Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-05-14T00:24:57.8538242Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-05-14T00:24:57.8538791Z May 14 00:24:57 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-05-14T00:24:57.8539269Z May 14 00:24:57 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-05-14T00:24:57.8539781Z May 14 00:24:57 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-05-14T00:24:57.8540296Z May 14 00:24:57 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-05-14T00:24:57.8541002Z May 14 00:24:57 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-05-14T00:24:57.8541519Z May 14 00:24:57 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-14T00:24:57.8542125Z May 14 00:24:57 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-14T00:24:57.8542696Z May 14 00:24:57 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-05-14T00:24:57.8543188Z May 14 00:24:57 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-05-14T00:24:57.8543673Z May 14 00:24:57 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-05-14T00:24:57.8544141Z May 14 00:24:57 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-05-14T00:24:57.8544612Z May 14 00:24:57 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-05-14T00:24:57.8545040Z May 14 00:24:57 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-05-14T00:24:57.8545475Z May 14 00:24:57 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-05-14T00:24:57.8545802Z May 14 00:24:57 	... 4 more
2021-05-14T00:24:57.8546046Z May 14 00:24:57 ]
2021-05-14T00:24:57.8546439Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-05-14T00:24:57.8546964Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-05-14T00:24:57.8547666Z May 14 00:24:57 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.waitForJobTermination(YARNHighAvailabilityITCase.java:324)
2021-05-14T00:24:57.8548439Z May 14 00:24:57 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$testKillYarnSessionClusterEntrypoint$0(YARNHighAvailabilityITCase.java:180)
2021-05-14T00:24:57.8549084Z May 14 00:24:57 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:287)
2021-05-14T00:24:57.8549712Z May 14 00:24:57 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint(YARNHighAvailabilityITCase.java:156)
2021-05-14T00:24:57.8550288Z May 14 00:24:57 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-14T00:24:57.8550789Z May 14 00:24:57 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-14T00:24:57.8551369Z May 14 00:24:57 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-14T00:24:57.8551872Z May 14 00:24:57 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-14T00:24:57.8552476Z May 14 00:24:57 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-14T00:24:57.8553062Z May 14 00:24:57 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-14T00:24:57.8553631Z May 14 00:24:57 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-14T00:24:57.8554204Z May 14 00:24:57 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-14T00:24:57.8554798Z May 14 00:24:57 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-05-14T00:24:57.8555446Z May 14 00:24:57 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-05-14T00:24:57.8556007Z May 14 00:24:57 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-05-14T00:24:57.8556423Z May 14 00:24:57 	at java.lang.Thread.run(Thread.java:748)
2021-05-14T00:24:57.8557001Z May 14 00:24:57 	Suppressed: java.lang.AssertionError: There is at least one application on the cluster that is not finished.[App application_1620949990638_0002 is in state RUNNING.]
2021-05-14T00:24:57.8557683Z May 14 00:24:57 		at org.junit.Assert.fail(Assert.java:88)
2021-05-14T00:24:57.8558224Z May 14 00:24:57 		at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:324)
2021-05-14T00:24:57.8558785Z May 14 00:24:57 		at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:288)
2021-05-14T00:24:57.8559259Z May 14 00:24:57 		... 13 more
2021-05-14T00:24:57.8560003Z May 14 00:24:57 Caused by: org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.handler.RestHandlerException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (610ed4b159ece04c8ee2ec40e7d0c143)
2021-05-14T00:24:57.8561068Z May 14 00:24:57 	at org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.propagateException(JobExecutionResultHandler.java:94)
2021-05-14T00:24:57.8561817Z May 14 00:24:57 	at org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.lambda$handleRequest$1(JobExecutionResultHandler.java:84)
2021-05-14T00:24:57.8562552Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
2021-05-14T00:24:57.8563157Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
2021-05-14T00:24:57.8563754Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-14T00:24:57.8564321Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-05-14T00:24:57.8564967Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)
2021-05-14T00:24:57.8565600Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-05-14T00:24:57.8566171Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-05-14T00:24:57.8566756Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-14T00:24:57.8567389Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-05-14T00:24:57.8567983Z May 14 00:24:57 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079)
2021-05-14T00:24:57.8568577Z May 14 00:24:57 	at akka.dispatch.OnComplete.internal(Future.scala:263)
2021-05-14T00:24:57.8569023Z May 14 00:24:57 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-05-14T00:24:57.8569488Z May 14 00:24:57 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-05-14T00:24:57.8569963Z May 14 00:24:57 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-05-14T00:24:57.8570430Z May 14 00:24:57 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-05-14T00:24:57.8570995Z May 14 00:24:57 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-05-14T00:24:57.8571561Z May 14 00:24:57 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-05-14T00:24:57.8572108Z May 14 00:24:57 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-05-14T00:24:57.8572705Z May 14 00:24:57 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-05-14T00:24:57.8573248Z May 14 00:24:57 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2021-05-14T00:24:57.8573879Z May 14 00:24:57 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-05-14T00:24:57.8574448Z May 14 00:24:57 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-05-14T00:24:57.8574949Z May 14 00:24:57 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-05-14T00:24:57.8575451Z May 14 00:24:57 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-05-14T00:24:57.8575978Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-05-14T00:24:57.8576611Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-05-14T00:24:57.8577417Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-05-14T00:24:57.8578029Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-05-14T00:24:57.8578753Z May 14 00:24:57 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-05-14T00:24:57.8579296Z May 14 00:24:57 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-05-14T00:24:57.8579809Z May 14 00:24:57 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-05-14T00:24:57.8580396Z May 14 00:24:57 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-05-14T00:24:57.8580980Z May 14 00:24:57 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-05-14T00:24:57.8581481Z May 14 00:24:57 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-05-14T00:24:57.8582006Z May 14 00:24:57 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-05-14T00:24:57.8582606Z May 14 00:24:57 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-05-14T00:24:57.8583333Z May 14 00:24:57 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (610ed4b159ece04c8ee2ec40e7d0c143)
2021-05-14T00:24:57.8584065Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2021-05-14T00:24:57.8584634Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2021-05-14T00:24:57.8585207Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:957)
2021-05-14T00:24:57.8585775Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2021-05-14T00:24:57.8586183Z May 14 00:24:57 	... 34 more
2021-05-14T00:24:57.8586682Z May 14 00:24:57 Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (610ed4b159ece04c8ee2ec40e7d0c143)
2021-05-14T00:24:57.8587424Z May 14 00:24:57 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$requestJobStatus$14(Dispatcher.java:596)
2021-05-14T00:24:57.8587950Z May 14 00:24:57 	at java.util.Optional.orElseGet(Optional.java:267)
2021-05-14T00:24:57.8588522Z May 14 00:24:57 	at org.apache.flink.runtime.dispatcher.Dispatcher.requestJobStatus(Dispatcher.java:590)
2021-05-14T00:24:57.8589039Z May 14 00:24:57 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-14T00:24:57.8589522Z May 14 00:24:57 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-14T00:24:57.8590107Z May 14 00:24:57 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-14T00:24:57.8590624Z May 14 00:24:57 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-14T00:24:57.8591147Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
2021-05-14T00:24:57.8591755Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
2021-05-14T00:24:57.8592437Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-05-14T00:24:57.8593065Z May 14 00:24:57 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-05-14T00:24:57.8593604Z May 14 00:24:57 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-05-14T00:24:57.8594079Z May 14 00:24:57 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-05-14T00:24:57.8594581Z May 14 00:24:57 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-05-14T00:24:57.8595090Z May 14 00:24:57 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-05-14T00:24:57.8595684Z May 14 00:24:57 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-05-14T00:24:57.8596201Z May 14 00:24:57 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-14T00:24:57.8596780Z May 14 00:24:57 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-14T00:24:57.8597354Z May 14 00:24:57 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-05-14T00:24:57.8597844Z May 14 00:24:57 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-05-14T00:24:57.8598405Z May 14 00:24:57 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-05-14T00:24:57.8598872Z May 14 00:24:57 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-05-14T00:24:57.8599329Z May 14 00:24:57 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-05-14T00:24:57.8599760Z May 14 00:24:57 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-05-14T00:24:57.8600187Z May 14 00:24:57 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-05-14T00:24:57.8600519Z May 14 00:24:57 	... 4 more
2021-05-14T00:24:57.8600763Z May 14 00:24:57 ]
2021-05-14T00:24:57.8601150Z May 14 00:24:57 	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:486)
2021-05-14T00:24:57.8601707Z May 14 00:24:57 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:466)
2021-05-14T00:24:57.8602346Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
2021-05-14T00:24:57.8603013Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2021-05-14T00:24:57.8603578Z May 14 00:24:57 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2021-05-14T00:24:57.8604123Z May 14 00:24:57 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-05-14T00:24:57.8604648Z May 14 00:24:57 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-05-14T00:24:57.8605135Z May 14 00:24:57 	... 1 more
2021-05-14T00:24:57.8605372Z May 14 00:24:57 
2021-05-14T00:24:57.8605812Z May 14 00:24:57 [ERROR] testClusterClientRetrieval(org.apache.flink.yarn.YARNHighAvailabilityITCase)  Time elapsed: 1,800.422 s  <<< ERROR!
2021-05-14T00:24:57.8606429Z May 14 00:24:57 org.junit.runners.model.TestTimedOutException: test timed out after 1800000 milliseconds
2021-05-14T00:24:57.8606847Z May 14 00:24:57 	at java.lang.Thread.sleep(Native Method)
2021-05-14T00:24:57.8607394Z May 14 00:24:57 	at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1223)
2021-05-14T00:24:57.8607988Z May 14 00:24:57 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:593)
2021-05-14T00:24:57.8608620Z May 14 00:24:57 	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:418)
2021-05-14T00:24:57.8609254Z May 14 00:24:57 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.deploySessionCluster(YARNHighAvailabilityITCase.java:356)
2021-05-14T00:24:57.8609932Z May 14 00:24:57 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$testClusterClientRetrieval$2(YARNHighAvailabilityITCase.java:224)
2021-05-14T00:24:57.8610525Z May 14 00:24:57 	at org.apache.flink.yarn.YARNHighAvailabilityITCase$$Lambda$250/2101231496.run(Unknown Source)
2021-05-14T00:24:57.8611024Z May 14 00:24:57 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:287)
2021-05-14T00:24:57.8611610Z May 14 00:24:57 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.testClusterClientRetrieval(YARNHighAvailabilityITCase.java:219)
2021-05-14T00:24:57.8612136Z May 14 00:24:57 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-14T00:24:57.8612692Z May 14 00:24:57 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-14T00:24:57.8613238Z May 14 00:24:57 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-14T00:24:57.8613831Z May 14 00:24:57 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-14T00:24:57.8614333Z May 14 00:24:57 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-14T00:24:57.8614959Z May 14 00:24:57 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-14T00:24:57.8615521Z May 14 00:24:57 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-14T00:24:57.8616071Z May 14 00:24:57 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-14T00:24:57.8616633Z May 14 00:24:57 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-05-14T00:24:57.8617325Z May 14 00:24:57 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-05-14T00:24:57.8617864Z May 14 00:24:57 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-05-14T00:24:57.8618360Z May 14 00:24:57 	at java.lang.Thread.run(Thread.java:748)

{code}
",,dwysakowicz,maguowei,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21608,FLINK-22195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 09 01:43:35 UTC 2021,,,,,,,,,,"0|z0r0yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/21 14:39;rmetzger;[~maguowei] is it possible to provide the link to the CI run? (Otherwise, we can also just wait for the next occurrence of this). I'd like to know which profile caused the issue + access the logs.;;;","24/May/21 22:50;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Jun/21 23:29;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","03/Jun/21 03:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18586&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=29119;;;","04/Jun/21 03:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18653&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=29118;;;","14/Jun/21 07:17;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18940&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=ea63c80c-957f-50d1-8f67-3671c14686b9;;;","17/Jun/21 03:59;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19037&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=29524;;;","17/Jun/21 07:47;xtsong;[~fly_in_gis] and I have looked into this instability. Here are our findings.

The expected behaviors of {{testKillYarnSessionClusterEntrypoint}} are as follow:
1) AM_1 running
2) Kill AM
3) Yarn brings up AM_2
4) Signal the job to finish (via temp file)

The problem is that, it is possible that 3) & 4) happen before 2) is finished. This is because a Yarn container runs as a process tree, where the Flink java process is brought up by a wrapping {{launch_container.sh}} process. Yarn can detect the termination of AM_1 and start bring up AM_2 as soon as the wrapping process is terminated, while the Flink process might be still running. Consequently, the signal from 4) is received by AM_1 and the job finishes before AM_1 is completely shutdown. When AM_2 is started, there's no job to be recovered, thus the ""could not find job"" exception.

To fix this, we need to make sure AM_1 is completely terminated before proceeding 4). This can be achieved by looking for the PID changes.

Besides, a ZK outage is occasionally observed right after the AM failover. Due to absence of ZK logs, we do not find the cause of this outage. However, given that the outage is only observed together with the above described problem, we tend to see them as related.;;;","18/Jun/21 09:51;xtsong;Unfortunately, I'm not able to confirm our hypothesis locally.

I tried to reproduce the problem by adding a sleep time in the shutdown hook of {{ClusterEntrypoin}}. However, it never happen to me that a new AM is brought up before the old one is completely shutdown.

I'm adding a bit more logs for now, see if it can help us understand what's going on when this fails again.;;;","18/Jun/21 13:50;xtsong;Add logs in
- master (1.14): c056a954f0eaf7651335ad17b49afd33e7c24471
- release-1.13: 4dffdb6e1b2d509a27d3db6d07e4167656c5581d;;;","02/Jul/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","05/Jul/21 04:43;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19867&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=29490;;;","09/Jul/21 01:43;xtsong;Fixed via
- master (1.14): 928b6897b7a0c609ab27f679b31852f18fa40684
- release-1.13: 76edcdade12c3ebc2157debf80f4fe398c541a12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveInputFormatPartitionReader can return invalid data,FLINK-22661,13378409,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,14/May/21 03:22,23/Sep/21 17:21,13/Jul/23 08:07,20/May/21 08:18,,,,,,,,,1.13.1,1.14.0,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"This happens when there're multiple splits to read, and an extra record is returned when we switch splits.",,lirui,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 08:18:29 UTC 2021,,,,,,,,,,"0|z0r0xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/21 08:18;lirui;Fixed in master: 74f4b7fc61a8222716723f308119da287a3af953
Fixed in release-1.13: c88c0c98198b061e351d7574a5c5b891ba38116b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'execution.checkpointing.interval'  missing in Flink doc,FLINK-22659,13378341,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rainieli,wangqinghuan,wangqinghuan,13/May/21 17:08,23/Sep/21 17:21,13/Jul/23 08:07,24/May/21 08:14,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Documentation,,,,,0,pull-request-available,,,,," Flink deployment configuration describe how to configure checkpointing in flink-conf.yaml[https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/config/]
{quote}*Checkpointing*

You can configure checkpointing directly in code within your Flink job or application. Putting these values here in the configuration defines them as defaults in case the application does not configure anything.
{quote}
*

 
{quote}{{state.backend}}: The state backend to use. This defines the data structure mechanism for taking snapshots. Common values are {{filesystem}} or {{rocksdb}}.
{quote} * 
{quote}{{state.checkpoints.dir}}: The directory to write checkpoints to. This takes a path URI like _s3://mybucket/flink-app/checkpoints_ or _hdfs://namenode:port/flink/checkpoints_.
{quote}
 * 
{quote}{{state.savepoints.dir}}: The default directory for savepoints. Takes a path URI, similar to {{state.checkpoints.dir}}.
{quote}

In my test for Flink-1.13.0, however,Flink checkpointing was not enabled without 'execution.checkpointing.interval' value in flink-conf.yaml. In order to enable checkpointing in case the application does not configure anything, we need to configure these values in flink-conf.yaml.
 * {{state.backend}}:
 * {{state.checkpoints.dir:}}
 * {{state.savepoints.dir:}}
 * execution.checkpointing.interval:

{{'execution.checkpointing.interval' value missing in document.}}

 ",,rainieli,wangqinghuan,yunta,,,,,,,,,,,,,,";19/May/21 04:12;rainieli;3600",,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 24 08:13:58 UTC 2021,,,,,,,,,,"0|z0r0io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/21 18:19;rainieli; can you assign this one to me? :);;;","14/May/21 01:21;wangqinghuan;I don't have permission to assign issue. How to deal with?[~rainieli];;;","14/May/21 04:16;yunta;[~rainieli], already assigned to you, please go ahead.;;;","14/May/21 06:33;rainieli;Thanks Yun.;;;","18/May/21 07:38;rainieli;Please review [https://github.com/apache/flink/pull/15979]

 ;;;","24/May/21 08:13;yunta;Merged 
master: 21c44688e982caf106e5fc509409717c6014f990
release-1.13: 31ee0cb0c4425c4101fe398325d86a0251c49971;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlCreateTable  toString()/unparse() lose CONSTRAINTS  and watermarks,FLINK-22654,13378260,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Terry1897,qin1040392240,qin1040392240,13/May/21 07:27,28/Aug/21 12:08,13/Jul/23 08:07,17/May/21 02:32,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"create a SqlCreateTable using like clause   and then toString() or unparse()  will lose watermark 

if  no column. 
{code:java}
public static SqlParser getSqlParser(String sql) {
    SourceStringReader sqlReader = new SourceStringReader(sql);

    return SqlParser.create(sqlReader,
            SqlParser.configBuilder()
                    .setParserFactory(FlinkSqlParserImpl.FACTORY)
                    .setLex(Lex.JAVA)
                    .setIdentifierMaxLength(256)
                    .setConformance(FlinkSqlConformance.DEFAULT)
                    .build());
}

public static void main(String[] args) throws Exception {


    SqlParser sqlParser = getSqlParser("""" +
            ""create TEMPORARY table t_order_course (\n"" +
            ""   WATERMARK FOR last_update_time AS last_update_time - INTERVAL '5' SECOND\n"" +
            "") with (\n"" +
            ""  'scan.startup.mode' = 'specific-offsets',\n"" +
            ""  'scan.startup.specific-offsets' = 'partition:0,offset:1169129'\n"" +
            "") like cdc.`qq_data(sh-backend-tst:3306)`.t_order_course (\n"" +
            ""   OVERWRITING  WATERMARKS\n"" +
            ""   OVERWRITING OPTIONS\n"" +
            ""   EXCLUDING CONSTRAINTS\n"" +
            "" \n"" +
            "")"");
    SqlNode sqlNode = sqlParser.parseStmt();

    System.out.println(sqlNode.toString());

}
{code}
output:

CREATE TEMPORARY TABLE `t_order_course` WITH ( 'scan.startup.mode' = 'specific-offsets', 'scan.startup.specific-offsets' = 'partition:0,offset:1169129' ) LIKE `cdc`.`qq_data(sh-backend-tst:3306)`.`t_order_course` ( OVERWRITING WATERMARKS OVERWRITING OPTIONS EXCLUDING CONSTRAINTS )",,godfreyhe,libenchao,qin1040392240,Terry1897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 17 02:32:13 UTC 2021,,,,,,,,,,"0|z0r00w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/21 03:27;Terry1897;I reproduce this problem in my local environment, and it's a bug of `SqlCreateTable#unparse()`.
I will open a pr to fix it soon.;;;","17/May/21 02:32;godfreyhe;Fixed in 1.14.0: cb4b4d37870fba22af122d1c31e49ce7f3ed096c
Fixed in 1.13.1: 6f4a49ce7c79a8f820c22bd797c6db13f5d0d177;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataGen SQL Connector does not support defining fields min/max option of decimal type field,FLINK-22640,13378001,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,12/May/21 04:40,28/Aug/21 11:21,13/Jul/23 08:07,13/May/21 06:26,1.13.0,,,,,,,,1.14.0,,,,,,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,,,When defining fields' min/max option of decimal type field will fail to create DataGen SQL Connector.,,lincoln.86xy,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 13 06:26:38 UTC 2021,,,,,,,,,,"0|z0qyfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/21 06:26;lzljs3620320;Fixed via master: e5435bb2ca00f81ca0cf9543c47c56157d431e40;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CatalogFunctionImpl.isGeneric should use ContextClassLoader,FLINK-22632,13377851,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Adrian Z,Adrian Z,11/May/21 11:53,12/May/21 04:29,13/Jul/23 08:07,11/May/21 14:02,1.12.2,1.12.3,,,,,,,1.13.0,,,,,,,Connectors / Hive,,,,,0,easyfix,,,,,"Hello, community.

I'm using Hive catalog, I'm trying to load UDF through HTTP.

I registered a UDF, then I submited a FlinkSQL job with command line:
{code:java}
flink run ... -C ""http://someHost/plusTwoFunc.jar"" ...{code}
When the job started, it throws an Exception:(:
{code:java}
Caused by: java.lang.ClassNotFoundException: com.slankka.flink.udf.PlusTwoFunc
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.flink.table.catalog.CatalogFunctionImpl.isGeneric(CatalogFunctionImpl.java:72)
{code}
I know that user code is loaded by {{SafetyNetWrapperClassLoader}} . and {{FlinkUserCodeClassLoader}} , {{SafetyNetWrapperClassLoader}} is a facade classloader, {{FlinkUserCodeClassLoader}} is the real classloader whose classpath contains the jar dependency, but it failed to load udf in {{CatalogFunctionImpl}}
{code:java}
@Override
public boolean isGeneric() {
    if (functionLanguage == FunctionLanguage.PYTHON) {
        return true;
    }
    try {
        Class c = Class.forName(className); //ClassNotFound
        if (UserDefinedFunction.class.isAssignableFrom(c)) {
            return true;
        }
    } catch (ClassNotFoundException e) {
        throw new RuntimeException(String.format(""Can't resolve udf class %s"", className), e);
    }
    return false;
}
{code}
 When I change to:
{code:java}
Thread.currentThread().getContextClassLoader().loadClass(className);
{code}
Then it works fine.

My proposal is CatalogFunctionImpl.isGeneric should using ContextClassloader, that is {{FlinkUserCodeClassLoader}} proxied by {{SafetyNetWrapperClassLoader}}",,Adrian Z,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,FLINK-20606,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-05-11 11:53:02.0,,,,,,,,,,"0|z0qxi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix web page display bug,FLINK-22628,13377796,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,quyanghaoren,quyanghaoren,quyanghaoren,11/May/21 07:40,28/Aug/21 11:21,13/Jul/23 08:07,12/May/21 14:11,1.13.0,,,,,,,,1.13.0,1.14.0,,,,,,API / Core,Documentation,,,,0,pull-request-available,,,,,"The character style of the official website is abnormal.

[https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/libs/state_processor_api/]

!flink-web-bug.png!","Product environment.

Stable 1.13",quyanghaoren,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/21 07:37;quyanghaoren;flink-web-bug.png;https://issues.apache.org/jira/secure/attachment/13025299/flink-web-bug.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 12 14:11:42 UTC 2021,,,,,,,,,,"0|z0qx5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/21 14:11;sjwiesman;fixed in master: ad86adf515fdec7174f3e19abc6ac2e6f8217a20

fixed in 1.13: c32022bd1b0de8c4d14168558add86aeb0c62d6e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSinkMigrationITCase unstable,FLINK-22625,13377773,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,dwysakowicz,dwysakowicz,11/May/21 06:53,18/Jun/21 09:18,13/Jul/23 08:07,18/Jun/21 08:15,1.14.0,,,,,,,,1.14.0,,,,,,,Connectors / FileSystem,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17817&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91&l=22179

{code}
May 11 00:43:40 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Sink: Unnamed (1/3) of job 733a4777cca170f86724832642e2a8b1 has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
May 11 00:43:40 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:152)
May 11 00:43:40 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:114)
May 11 00:43:40 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
May 11 00:43:40 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
May 11 00:43:40 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
May 11 00:43:40 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
May 11 00:43:40 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
May 11 00:43:40 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
May 11 00:43:40 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
May 11 00:43:40 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
May 11 00:43:40 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
May 11 00:43:40 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
May 11 00:43:40 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
May 11 00:43:40 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
May 11 00:43:40 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
May 11 00:43:40 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
May 11 00:43:40 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
May 11 00:43:40 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
May 11 00:43:40 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
May 11 00:43:40 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
May 11 00:43:40 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
May 11 00:43:40 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
May 11 00:43:40 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
May 11 00:43:40 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
May 11 00:43:40 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

{code}",,akalashnikov,dwysakowicz,gaoyunhaii,mapohl,pnowojski,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22593,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 18 09:18:31 UTC 2021,,,,,,,,,,"0|z0qx0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/21 20:30;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=429&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=ebffacc3-9693-5a7f-11b7-68b343827cf3&l=22155;;;","17/May/21 06:03;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=429&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=ebffacc3-9693-5a7f-11b7-68b343827cf3&l=22142;;;","06/Jun/21 18:04;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18690&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91&l=21836;;;","07/Jun/21 02:54;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18707&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91&l=21736;;;","08/Jun/21 02:09;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18754&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91&l=21739;;;","08/Jun/21 02:18;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18754&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699&l=22092;;;","11/Jun/21 02:05;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18890&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91&l=21739;;;","11/Jun/21 02:08;xtsong;[~gaoyunhaii], could you please take a look into this test instability?;;;","11/Jun/21 02:31;gaoyunhaii;[~xintongsong] sure, I'll have a look.;;;","11/Jun/21 02:45;xtsong;Thank, [~gaoyunhaii]. I've assigned you to the ticket.;;;","17/Jun/21 10:36;akalashnikov;[~gaoyunhaii], I think I found the problem and I will fix along with the same problem in another test. Please, check this PR [https://github.com/apache/flink/pull/16151/files#diff-5df9eea57ea9542415f7e58a7c7051a1c6156f42423ba69f4323e697bb2c9cb4,] and if you agree that it will resolve the problem can you close this ticket as duplicate of https://issues.apache.org/jira/browse/FLINK-22593;;;","18/Jun/21 08:15;pnowojski;Probable fix merged to master as 3ae6801f6e3;;;","18/Jun/21 09:08;gaoyunhaii;Hi [~akalashnikov], does the original issue happens due to a single sink task might count down `savepointLatch` multiple times ? Very thanks for the fix and it also looks good to me, and sorry for the late reply.;;;","18/Jun/21 09:14;akalashnikov;??a single sink task might count down `savepointLatch` multiple times??

Yes, it was exactly the reason.;;;","18/Jun/21 09:18;gaoyunhaii;Got it, very thanks for the fix and the explanation~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default resource allocation strategy will allocate more pending task managers than demand,FLINK-22624,13377714,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,11/May/21 02:22,28/Aug/21 11:20,13/Jul/23 08:07,11/May/21 07:48,1.14.0,,,,,,,,1.14.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"When the {{DefaultResourceAllocationStrategy}} try to fulfill a requirement with allocating new pending task managers. The remaining resource of those task managers will never be used to fulfill other requirement, which hurt resource utilization.",,guoyangze,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 11 07:48:07 UTC 2021,,,,,,,,,,"0|z0qwns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/21 07:48;xtsong;Fixed via
- master (1.14): f667629ece8ed59a1dc439da7d49e6298880f320;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect free resource metrics of task managers,FLINK-22618,13377590,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paul8263,guoyangze,guoyangze,10/May/21 09:26,30/Nov/21 20:37,13/Jul/23 08:07,13/May/21 09:43,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,starter,,,,"In FLINK-21177, the {{FineGrainedSlotManager#getFreeResourceOf}} wrongly return the total resource.",,guoyangze,mapohl,paul8263,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 13 09:43:24 UTC 2021,,,,,,,,,,"0|z0qvw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/21 10:00;mapohl;[~guoyangze] just to be sure: You're referring to {{FineGrainedSlotManager.getFreeResourceOf}}, aren't you? May you update the Jira issue description if I understood you correctly? ...just to be more explicit here and to avoid confusion.;;;","10/May/21 10:32;guoyangze;[~mapohl] Thanks for the notice. Description updated.;;;","12/May/21 00:59;paul8263;Sorry I forgot to tell you yesterday. Could you assign this to me pls?;;;","13/May/21 09:43;xtsong;Fixed via
- master (1.14): e510b3a6d1d9528065e7b84061415017740ac88f
- release-1.13: cfdf0013a640bac2be13285f8a1492b5d4781f6c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on bundle close when task failover after a failed task open,FLINK-22604,13377357,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,08/May/21 05:45,28/Aug/21 11:19,13/Jul/23 08:07,10/May/21 12:56,1.12.3,1.13.0,,,,,,,1.14.0,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"We observed a NullPointerException when job failover after a failed task open(e.g., stuck on initializing), this should be avoid.

A simulated case will reproduce the exception:

{code}

@Test
def testCloseWithoutOpen(): Unit = {
    val (testHarness, outputType) = createAggregationWithDistinct  // see detail in the patch
    testHarness.setup(new RowDataSerializer(outputType: _*))
    // simulate a failover after a failed task open(e.g., stuck on initializing)
    // expect no exception happens
    testHarness.close()
}

{code}

exception:

{code}

java.lang.NullPointerExceptionjava.lang.NullPointerException at org.apache.flink.table.runtime.operators.bundle.AbstractMapBundleOperator.finishBundle(AbstractMapBundleOperator.java:134) at org.apache.flink.table.runtime.operators.bundle.AbstractMapBundleOperator.close(AbstractMapBundleOperator.java:156) at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.close(AbstractStreamOperatorTestHarness.java:691)

{code}

 

 ",,huwh,libenchao,lincoln.86xy,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 12:56:28 UTC 2021,,,,,,,,,,"0|z0qugg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/21 12:56;lzljs3620320;Fixed via master:

6daa30f555b14c2c9cd8d8109dfd32bf7161dc7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Parquet to 1.12.2,FLINK-22602,13377353,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Bo Cui,Bo Cui,Bo Cui,08/May/21 03:46,11/Jan/22 15:12,13/Jul/23 08:07,11/Jan/22 15:12,1.12.3,1.13.0,1.13.1,,,,,,1.15.0,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,auto-deprioritized-major,pull-request-available,,,,"We scanned jackson-databind&core for some vulnerabilities (CVE-2020-24616/CVE-2021-20190/CVE-2020-36184/CVE-2020-36183/CVE-2020-9546...). Apache Parquet depends on them, so we upgraded parquet to version 1.12.0.

 

parquet code link：https://github.com/apache/parquet-mr/blob/db75a6815f2ba1d1ee89d1a90aeb296f1f3a8f20/pom.xml#L78",,Bo Cui,fpaul,martijnvisser,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 11 15:12:07 UTC 2022,,,,,,,,,,"0|z0qufk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","05/Jul/21 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","10/Jan/22 09:31;trohrmann;Should we upgrade to 1.12.2?

cc [~MartijnVisser] and [~fpaul];;;","10/Jan/22 10:13;martijnvisser;We should at least upgrade to the latest version to deal with https://issues.apache.org/jira/browse/PARQUET-2094 yes. I'll ask the contributor if he can do the upgrade. ;;;","11/Jan/22 15:12;fpaul;Merged in master: d7cf2c10f8d4fba81173854cbd8be27c657c7c7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMaster cannot be restarted,FLINK-22597,13377258,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,07/May/21 15:07,10/May/21 10:19,13/Jul/23 08:07,10/May/21 10:19,1.11.3,1.12.3,,,,,,,1.11.4,1.12.4,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Since we are reusing the {{DefaultLeaderRetrievalService}} for finding the resource manager in the {{JobMaster}} and since the leader retrieval service cannot be reused, it is not possible to restart the {{JobMaster}}. This causes Flink to fail in case that the {{JobMaster}} regains the leadership in Flink versions <= 1.12. The problem no longer occurs for newer versions because of FLINK-11719.",,huwh,klion26,Thesharing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 10:19:45 UTC 2021,,,,,,,,,,"0|z0qtug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/21 10:19;trohrmann;Fixed via

1.12.4: daaa01d17c62036d59504878a2f5e3d8812094f7
1.11.4: 0c8a0bf0eeaa1e915841975b6402a67bedb6bb56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Active timeout is not triggered if there were no barriers,FLINK-22596,13377231,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,07/May/21 12:50,28/Aug/21 11:19,13/Jul/23 08:07,10/May/21 09:22,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"The condition in the active timeout timer is often incorrect, because we do not reset the {{allBarriersReceivedFuture}} on barrier announcements.",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 09:22:15 UTC 2021,,,,,,,,,,"0|z0qtog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/21 09:22;dwysakowicz;Fixed in:
* master
** e0193ce3ff1918ef3e2f5dd09762f86c052519cb
* 1.13.1
** 3a9d14457f886937b8604607dee10d1046323dc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase.testShouldAddEntropyToSavepointPath unstable,FLINK-22593,13377214,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,akalashnikov,rmetzger,rmetzger,07/May/21 11:24,23/Sep/21 17:54,13/Jul/23 08:07,30/Jun/21 09:44,1.14.0,,,,,,,,1.14.0,,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,stale-blocker,stale-critical,test-stability,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9072&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85

{code}
2021-05-07T10:56:20.9429367Z May 07 10:56:20 [ERROR] Tests run: 13, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 33.441 s <<< FAILURE! - in org.apache.flink.test.checkpointing.SavepointITCase
2021-05-07T10:56:20.9445862Z May 07 10:56:20 [ERROR] testShouldAddEntropyToSavepointPath(org.apache.flink.test.checkpointing.SavepointITCase)  Time elapsed: 2.083 s  <<< ERROR!
2021-05-07T10:56:20.9447106Z May 07 10:56:20 java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Sink: Unnamed (3/4) of job 4e155a20f0a7895043661a6446caf1cb has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2021-05-07T10:56:20.9448194Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-05-07T10:56:20.9448797Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-05-07T10:56:20.9449428Z May 07 10:56:20 	at org.apache.flink.test.checkpointing.SavepointITCase.submitJobAndTakeSavepoint(SavepointITCase.java:305)
2021-05-07T10:56:20.9450160Z May 07 10:56:20 	at org.apache.flink.test.checkpointing.SavepointITCase.testShouldAddEntropyToSavepointPath(SavepointITCase.java:273)
2021-05-07T10:56:20.9450785Z May 07 10:56:20 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-07T10:56:20.9451331Z May 07 10:56:20 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-07T10:56:20.9451940Z May 07 10:56:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-07T10:56:20.9452498Z May 07 10:56:20 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-07T10:56:20.9453247Z May 07 10:56:20 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-07T10:56:20.9454007Z May 07 10:56:20 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-07T10:56:20.9454687Z May 07 10:56:20 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-07T10:56:20.9455302Z May 07 10:56:20 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-07T10:56:20.9455909Z May 07 10:56:20 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-05-07T10:56:20.9456493Z May 07 10:56:20 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-07T10:56:20.9457074Z May 07 10:56:20 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-07T10:56:20.9457636Z May 07 10:56:20 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-07T10:56:20.9458157Z May 07 10:56:20 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-07T10:56:20.9458678Z May 07 10:56:20 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-07T10:56:20.9459252Z May 07 10:56:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-07T10:56:20.9459865Z May 07 10:56:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-07T10:56:20.9460433Z May 07 10:56:20 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-07T10:56:20.9461058Z May 07 10:56:20 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-07T10:56:20.9461607Z May 07 10:56:20 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-07T10:56:20.9462159Z May 07 10:56:20 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-07T10:56:20.9462705Z May 07 10:56:20 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-07T10:56:20.9463243Z May 07 10:56:20 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-07T10:56:20.9463812Z May 07 10:56:20 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-05-07T10:56:20.9464436Z May 07 10:56:20 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-05-07T10:56:20.9465073Z May 07 10:56:20 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-05-07T10:56:20.9465688Z May 07 10:56:20 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-05-07T10:56:20.9466316Z May 07 10:56:20 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-07T10:56:20.9466969Z May 07 10:56:20 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-07T10:56:20.9467579Z May 07 10:56:20 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-07T10:56:20.9468169Z May 07 10:56:20 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-05-07T10:56:20.9469189Z May 07 10:56:20 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Sink: Unnamed (3/4) of job 4e155a20f0a7895043661a6446caf1cb has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2021-05-07T10:56:20.9470242Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2021-05-07T10:56:20.9470862Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2021-05-07T10:56:20.9471473Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2021-05-07T10:56:20.9472078Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-05-07T10:56:20.9472743Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-07T10:56:20.9473360Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-05-07T10:56:20.9474022Z May 07 10:56:20 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$null$0(CheckpointCoordinator.java:482)
2021-05-07T10:56:20.9474672Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-05-07T10:56:20.9475283Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-05-07T10:56:20.9475901Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-07T10:56:20.9476514Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-05-07T10:56:20.9477247Z May 07 10:56:20 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointTriggerRequest.completeExceptionally(CheckpointCoordinator.java:2047)
2021-05-07T10:56:20.9478016Z May 07 10:56:20 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:853)
2021-05-07T10:56:20.9478753Z May 07 10:56:20 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$7(CheckpointCoordinator.java:608)
2021-05-07T10:56:20.9479430Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2021-05-07T10:56:20.9480080Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2021-05-07T10:56:20.9480691Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2021-05-07T10:56:20.9481278Z May 07 10:56:20 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-05-07T10:56:20.9481820Z May 07 10:56:20 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-05-07T10:56:20.9482457Z May 07 10:56:20 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2021-05-07T10:56:20.9483190Z May 07 10:56:20 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2021-05-07T10:56:20.9483845Z May 07 10:56:20 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-05-07T10:56:20.9484440Z May 07 10:56:20 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-05-07T10:56:20.9484963Z May 07 10:56:20 	at java.lang.Thread.run(Thread.java:748)
2021-05-07T10:56:20.9485836Z May 07 10:56:20 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Sink: Unnamed (3/4) of job 4e155a20f0a7895043661a6446caf1cb has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2021-05-07T10:56:20.9486869Z May 07 10:56:20 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:152)
2021-05-07T10:56:20.9487681Z May 07 10:56:20 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:114)
2021-05-07T10:56:20.9488413Z May 07 10:56:20 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2021-05-07T10:56:20.9489056Z May 07 10:56:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-05-07T10:56:20.9489678Z May 07 10:56:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-05-07T10:56:20.9490336Z May 07 10:56:20 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-05-07T10:56:20.9490985Z May 07 10:56:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-05-07T10:56:20.9491615Z May 07 10:56:20 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-05-07T10:56:20.9492156Z May 07 10:56:20 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-05-07T10:56:20.9492706Z May 07 10:56:20 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-05-07T10:56:20.9493267Z May 07 10:56:20 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-05-07T10:56:20.9493925Z May 07 10:56:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-05-07T10:56:20.9494489Z May 07 10:56:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-07T10:56:20.9495050Z May 07 10:56:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-07T10:56:20.9495583Z May 07 10:56:20 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-05-07T10:56:20.9496120Z May 07 10:56:20 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-05-07T10:56:20.9496662Z May 07 10:56:20 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-05-07T10:56:20.9497176Z May 07 10:56:20 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-05-07T10:56:20.9497684Z May 07 10:56:20 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-05-07T10:56:20.9498178Z May 07 10:56:20 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-05-07T10:56:20.9498656Z May 07 10:56:20 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-05-07T10:56:20.9499226Z May 07 10:56:20 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-05-07T10:56:20.9499796Z May 07 10:56:20 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-05-07T10:56:20.9500370Z May 07 10:56:20 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-05-07T10:56:20.9500950Z May 07 10:56:20 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,dwysakowicz,klion26,maguowei,mapohl,pnowojski,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22703,,,,,,,,,,,FLINK-22625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 30 09:44:11 UTC 2021,,,,,,,,,,"0|z0qtko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/21 06:29;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17758&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9759;;;","10/May/21 15:05;mapohl;I have a build where this failure not only occurred for [testShouldAddEntropyToSavepointPath |https://dev.azure.com/mapohl/flink/_build/results?buildId=419&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4173] 
but also [testTriggerSavepointAndResumeWithFileBasedCheckpointsAndRelocateBasePath|https://dev.azure.com/mapohl/flink/_build/results?buildId=419&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4146]

{code}
// ...
May 10 11:09:20 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Map (2/4) of job 4bb523f101d1e478338d76a671c24564 has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
May 10 11:09:20 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:152)
May 10 11:09:20 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:114)
May 10 11:09:20 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
May 10 11:09:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
May 10 11:09:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
May 10 11:09:20 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
May 10 11:09:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
May 10 11:09:20 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
May 10 11:09:20 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
May 10 11:09:20 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
May 10 11:09:20 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
May 10 11:09:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
May 10 11:09:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
May 10 11:09:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
May 10 11:09:20 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
May 10 11:09:20 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
May 10 11:09:20 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
May 10 11:09:20 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
May 10 11:09:20 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
May 10 11:09:20 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
May 10 11:09:20 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
May 10 11:09:20 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
May 10 11:09:20 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
May 10 11:09:20 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
May 10 11:09:20 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code};;;","11/May/21 06:51;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17817&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4479;;;","11/May/21 15:42;mapohl;Different (Savepoint-related) test but same failure: [This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=423&view=logs&j=d0dc8a09-802e-543a-1851-c31096d61b33&t=5952d6c2-ad1d-5ad4-5bfc-48bb7f31ebd9&l=8878] failed (not exclusively) due to {{MemoryStateBackendWindowITCase.SavepointWindowReaderITCase}}.

{code}
May 11 11:57:22 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Window(org.apache.flink.state.api.utils.WaitingWindowAssigner@688a2c09, EventTimeTrigger, AggregateSum, PassThroughWindowFunction) -> Sink: Unnamed (4/4) of job 7f4a91324e73baa6434c62fad6449a58 has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
May 11 11:57:22 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:152)
May 11 11:57:22 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:114)
May 11 11:57:22 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
May 11 11:57:22 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
May 11 11:57:22 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
May 11 11:57:22 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
May 11 11:57:22 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
May 11 11:57:22 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
May 11 11:57:22 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
May 11 11:57:22 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
May 11 11:57:22 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
May 11 11:57:22 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
May 11 11:57:22 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
May 11 11:57:22 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
May 11 11:57:22 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
May 11 11:57:22 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
May 11 11:57:22 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
May 11 11:57:22 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
May 11 11:57:22 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
May 11 11:57:22 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
May 11 11:57:22 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
May 11 11:57:22 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
May 11 11:57:22 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
May 11 11:57:22 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
May 11 11:57:22 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code};;;","11/May/21 15:59;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=422&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=9756;;;","14/May/21 02:31;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17956&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4537


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17956&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4479;;;","20/May/21 04:35;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18151&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4481;;;","22/May/21 14:03;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18224&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=14321;;;","25/May/21 07:42;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18292&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107;;;","26/May/21 02:18;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18328&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4481;;;","02/Jun/21 23:30;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Jun/21 22:41;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Jun/21 07:22;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18940&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&l=4470;;;","17/Jun/21 05:48;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9136&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87;;;","18/Jun/21 08:15;pnowojski;Merged to master as 3fa69829517;;;","23/Jun/21 02:19;xtsong;Reopening.

Another instance on master, after the fix being merged.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19303&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4099

cc [~akalashnikov][~pnowojski];;;","30/Jun/21 09:44;dwysakowicz;Fixed in 3b92d67017173547268673d71652a6bc98167abb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
numBuffersInLocal is always zero when using unaligned checkpoints,FLINK-22592,13377203,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,pnowojski,pnowojski,07/May/21 10:21,28/Aug/21 12:08,13/Jul/23 08:07,17/May/21 03:18,1.11.3,1.12.3,1.13.0,,,,,,1.12.5,1.13.1,1.14.0,,,,,Runtime / Network,,,,,0,beginner-friendly,pull-request-available,,,,"This is because {{LocalRecoveredInputChannel#toInputChannelInternal}} is passing wrong parameter to {{LocalInputChannel}}'s constructor (twice {{numBytesIn}}):
{code:java}
    protected InputChannel toInputChannelInternal() {
        return new LocalInputChannel(
                inputGate,
                getChannelIndex(),
                partitionId,
                partitionManager,
                taskEventPublisher,
                initialBackoff,
                maxBackoff,
                numBytesIn,
                numBytesIn,
                channelStateWriter);
    }
{code}",,klion26,pnowojski,wind_ljy,xmarker,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 17 03:18:02 UTC 2021,,,,,,,,,,"0|z0qti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/21 03:18;xtsong;Fixed via
- master (1.14): 6c6256130583a246309d1585029ad30d243941b3
- release-1.13: 9b2f97d07dfa39c58572655f0a30d76ad9156546
- release-1.12: 60093e4c51fd0e566e44c8ec7585a67c9376e164
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keyword 'CATALOG' is missing in sql client doc,FLINK-22581,13376953,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,warwithin,warwithin,warwithin,06/May/21 06:35,28/May/21 11:09,13/Jul/23 08:07,06/May/21 08:02,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"Excerpt from sql client doc:
```
CREATE CATALOG MyCatalog
  WITH (
    'type' = 'hive'
  );

USE MyCatalog;

```

The statement `USE MyCatalog' should be 'USE CATALOG MyCatalog'",,jark,warwithin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 08:02:17 UTC 2021,,,,,,,,,,"0|z0qryw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/21 07:29;jark;Feel free to open pull requests [~warwithin].;;;","06/May/21 07:37;warwithin;[~jark], 
Sent a PR for this. https://github.com/apache/flink/pull/15844;;;","06/May/21 08:02;jark;Fixed in
 - master: cb4efe6b2ad67fe9f155f64d133cc7ddea19333f
 - release-1.13: 9eeecf3684591cba1b01f481a3d5c0a7600641cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesLeaderElectionAndRetrievalITCase is failing,FLINK-22577,13376825,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,mapohl,mapohl,05/May/21 16:00,28/Aug/21 11:19,13/Jul/23 08:07,10/May/21 10:12,1.12.3,1.13.0,1.14.0,,,,,,1.12.4,1.13.1,1.14.0,,,,,Deployment / Kubernetes,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,,"{{KubernetesLeaderElectionAndRetrievalITCase}} is failing constantly. Running it locally results in an {{AssertionError}}:
{code}
3069 [KubernetesLeaderElector-ExecutorService-thread-1] DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Loop thread interrupted
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998) ~[?:1.8.0_265]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) ~[?:1.8.0_265]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231) ~[?:1.8.0_265]
	at io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector.loop(LeaderElector.java:200) ~[kubernetes-client-4.9.2.jar:?]
	at io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector.renewWithTimeout(LeaderElector.java:100) ~[kubernetes-client-4.9.2.jar:?]
	at io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector.run(LeaderElector.java:71) ~[kubernetes-client-4.9.2.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_265]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_265]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_265]
3078 [KubernetesLeaderElector-ExecutorService-thread-1] ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler [] - FATAL: Thread 'KubernetesLeaderElector-ExecutorService-thread-1' produced an uncaught exception. Stopping the process...
java.lang.AssertionError: null
	at org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver.writeLeaderInformation(KubernetesLeaderElectionDriver.java:130) ~[classes/:?]
	at org.apache.flink.runtime.leaderelection.TestingLeaderElectionEventHandler.lambda$onRevokeLeadership$1(TestingLeaderElectionEventHandler.java:69) ~[test-classes/:?]
	at org.apache.flink.runtime.leaderelection.TestingLeaderElectionEventHandler.waitForInitialization(TestingLeaderElectionEventHandler.java:93) ~[test-classes/:?]
	at org.apache.flink.runtime.leaderelection.TestingLeaderElectionEventHandler.onRevokeLeadership(TestingLeaderElectionEventHandler.java:66) ~[test-classes/:?]
	at org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver$LeaderCallbackHandlerImpl.notLeader(KubernetesLeaderElectionDriver.java:202) ~[classes/:?]
	at io.fabric8.kubernetes.client.extended.leaderelection.LeaderCallbacks.onStopLeading(LeaderCallbacks.java:38) ~[kubernetes-client-4.9.2.jar:?]
	at io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector.run(LeaderElector.java:72) ~[kubernetes-client-4.9.2.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_265]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_265]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_265]
{code}

The failure never popped up due to FLINK-22564

* [1.12 release branch|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17554&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=2701]
* [1.13 release branch|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17558&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=2760]
* [master|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17560&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=2757]",,mapohl,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22564,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 10:12:12 UTC 2021,,,,,,,,,,"0|z0qr6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/21 16:00;mapohl;CC [~fly_in_gis];;;","06/May/21 11:30;wangyang0918;[~mapohl] Could you share how to reproduce this issue locally? I have run more 100 times and all of them passed without the FATAL error.;;;","06/May/21 12:45;mapohl;I started minikube locally and set {{ITCASE_KUBECONFIG}} pointing to my Kubernetes config file within Intellij's Run Configuration. This enabled the test locally.;;;","06/May/21 13:14;trohrmann;I think this is a problem of the {{TestingLeaderEelctionEventHandler}} which still accepts callbacks after we close the {{KubernetesLeaderElectionDriver}}.;;;","07/May/21 06:49;wangyang0918;I know why I could not reproduce this issue locally. Because I did not add the {{-ea}} VM options when running unit test.

 

I think [~trohrmann] is right. The callback in {{TestingLeaderEelctionEventHandler}} could happen even after the {{KubernetesLeaderElectionDriver}} is closed. It only happens on the unit test and production codes are not affected. ;;;","10/May/21 10:12;trohrmann;Fixed via

1.14.0: fbf84acf63102db455c89cb8e497cda423a1c4d5
1.13.1: 3ff9eb7029784349fb135e6849b745ba82c7b8c0
1.12.4: ed9965c33853ab95e0a3264b772f82fd8404239a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jira bot does not set the status to Open if it unassigns people from tickets,FLINK-22576,13376824,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,trohrmann,trohrmann,05/May/21 15:57,10/May/21 14:39,13/Jul/23 08:07,10/May/21 14:39,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,The Jira bot does not set the status to {{Open}} if it unassigns people from a ticket.,,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22026,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 14:39:54 UTC 2021,,,,,,,,,,"0|z0qr6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/21 14:39;chesnay;master: f185d8654881b1a14ac11f112848aa2b7be75de7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive Scheduler: Can not cancel restarting job,FLINK-22574,13376787,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,05/May/21 12:04,28/Aug/21 11:21,13/Jul/23 08:07,13/May/21 14:51,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"I have a job in state RESTARTING. When I now issue a cancel RPC call, I get the following exception:

Relevant trace:
{code}
Caused by: java.lang.IllegalStateException: Assuming running execution graph 
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) 
 at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.(StateWithExecutionGraph.java:94) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling.(Canceling.java:41) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:98) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:72) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1128) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCanceling(AdaptiveScheduler.java:802) 
 at org.apache.flink.runtime.scheduler.adaptive.Restarting.cancel(Restarting.java:74) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.cancel(AdaptiveScheduler.java:453) 
 at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:417) 
{code}

Full trace as reported in the UI:
{code}
org.apache.flink.runtime.rest.handler.RestHandlerException: Job cancellation failed: Cancellation failed. 
 at org.apache.flink.runtime.rest.handler.job.JobCancellationHandler.lambda$handleRequest$0(JobCancellationHandler.java:127) 
 at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) 
 at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) 
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) 
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079) 
 at akka.dispatch.OnComplete.internal(Future.scala:263) 
 at akka.dispatch.OnComplete.internal(Future.scala:261) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) 
 at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) 
 at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:573) 
 at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) 
 at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) 
 at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532) 
 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29) 
 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) 
 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91) 
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) 
 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) 
 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91) 
 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) 
 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) 
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) 
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: org.apache.flink.util.FlinkException: Cancellation failed. 
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$cancel$3(JobMasterServiceLeadershipRunner.java:197) 
 at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) 
 at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) 
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) 
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079) 
 at akka.dispatch.OnComplete.internal(Future.scala:263) 
 at akka.dispatch.OnComplete.internal(Future.scala:261) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) 
 at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) 
 at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:573) 
 at akka.actor.ActorRef.tell(ActorRef.scala:126) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:311) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) 
 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) 
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) 
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) 
 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) 
 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) 
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
 at akka.actor.Actor.aroundReceive(Actor.scala:517) 
 at akka.actor.Actor.aroundReceive$(Actor.scala:515) 
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) 
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) 
 at akka.actor.ActorCell.invoke(ActorCell.scala:561) 
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) 
 at akka.dispatch.Mailbox.run(Mailbox.scala:225) 
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ... 4 more Caused by: java.lang.IllegalStateException: Assuming running execution graph 
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) 
 at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.(StateWithExecutionGraph.java:94) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling.(Canceling.java:41) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:98) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:72) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1128) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCanceling(AdaptiveScheduler.java:802) 
 at org.apache.flink.runtime.scheduler.adaptive.Restarting.cancel(Restarting.java:74) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.cancel(AdaptiveScheduler.java:453) 
 at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:417) 
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
 at java.lang.reflect.Method.invoke(Method.java:498) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) ... 23 more
{code}
(Sorry for the poor stack trace formatting, I coped the stack trace from the UI)

It seems that the Canceling state assumes we only transition into it from Executing (ExecutionGraph = RUNNING).

In my opinion a job should be cancellable at all times, for example when a job is stuck in a restart loop, cancelling is the only way out (unless retries are exhausted).",,klion26,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 12 18:58:30 UTC 2021,,,,,,,,,,"0|z0qqy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/21 14:10;trohrmann;Thanks for tackling this ticket [~rmetzger]. Please set it into progress when you start working on it.;;;","12/May/21 18:58;rmetzger;Merged to master in https://github.com/apache/flink/commit/02d30ace69dc18555a5085eccf70ee884e73a16e
Merged to release-1.13 in https://github.com/apache/flink/commit/25489aa0cc536f1551a0a887bdfc690824a24bd9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncIO can timeout elements after completion,FLINK-22573,13376768,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,arvid,05/May/21 09:53,28/May/21 11:09,13/Jul/23 08:07,06/May/21 06:05,1.11.3,1.12.3,1.13.0,1.14.0,,,,,1.12.4,1.13.1,1.14.0,,,,,API / DataStream,,,,,0,pull-request-available,,,,,"AsyncIO emits completed elements over the mailbox at which any timer is also canceled. However, if the mailbox cannot process (heavy backpressure), it may be that the timer still triggers on a completed element.",,klion26,libenchao,qinjunjerry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 06:04:53 UTC 2021,,,,,,,,,,"0|z0qqu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/21 19:16;arvid;Merged the fix into master as 56e292b92ec0dce383becf5150a2dd92f3fcae7a..0790768167f1a43d90df89ca4ceb2248841d91d3.;;;","06/May/21 06:04;arvid;Merged into 1.12 as 41dc5871d76e913bad9be59d65e2bb26d67fec15 and into 1.13 as b146cb8928026b2a4c7aae9fce492951fd6d18a7.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes-related ITCases do not fail even in case of failure,FLINK-22564,13376726,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,05/May/21 06:33,18/May/21 07:29,13/Jul/23 08:07,18/May/21 07:26,1.12.3,1.13.0,,,,,,,1.12.5,1.13.1,,,,,,Deployment / Kubernetes,Test Infrastructure,,,,0,test-stability,,,,,"-It appears that the k8s ITCases located in {{test_kubernetes_itcases.sh}} are not running (e.g. [here|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17412&view=ms.vss-test-web.build-test-results-tab&runId=494592&resultId=100770&paneView=debug] and [here|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17556&view=ms.vss-test-web.build-test-results-tab&runId=505566&resultId=100765&paneView=debug]) due to {{ITCASE_KUBECONFIG}} not being available within the JVM.-

New Findings: The ITCases actually run as part of the e2e tests. But they do not seem to fail in case of a failure. {{Run Kubernetes IT test}} has passed [in this build #17556|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17556&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=2913] even though {{KubernetesStateHandleStoreITCase}} failed (see [here|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17556&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=2805]).",,mapohl,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22577,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 18 07:29:36 UTC 2021,,,,,,,,,,"0|z0qqko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/21 06:34;mapohl;CC [~autophagy];;;","05/May/21 08:31;trohrmann;[~mapohl] I couldn't really find the connection of the linked reports with a specific AZP pipeline job. Can't it be the case that the K8s ITCases are run when the tests for {{flink-kubernetes}} are executed? In this case, it is expected that they should not run because they are e2e tests which need to be run with a {{Minikube}} instance running.;;;","05/May/21 10:50;mapohl;Thanks for pointing that out. [~chesnay] gave me some clarity on the issue. The Kubernetes-related ITCases are triggered as part of the {{misc}} profile but skipped due to the missing {{ITCASE_KUBECONFIG}} variable. But they are also triggered as part of the e2e tests (in the [run-nightly-tests.sh:136|https://github.com/apache/flink/blob/42b9d730f50fe55ab9781739e952fff817fe1312/flink-end-to-end-tests/run-nightly-tests.sh#L136).

We can verify that the tests get triggered in the e2e stage as shown in [this build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17556&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=2761]. What worries me is that the {{KubernetesStateHandleStoreITCase}} failed but the corresponding testcase was tagged as successful (see [log line 2913|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17556&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=2913]).

I'm going to rephrase the issue to cover the findings.;;;","05/May/21 14:20;trohrmann;cc [~fly_in_gis];;;","05/May/21 16:02;mapohl;FYI: {{KubernetesLeaderElectioNAndRetrievalITCase}} (FLINK-22577) seems to be constantly failing but is masked by this issue.;;;","06/May/21 07:36;wangyang0918;[~mapohl] I think you mean the Kubernetes-related ITCases failed, but the azure pipeline did not stop with failure. Right?;;;","06/May/21 12:44;mapohl;Yes, that's what I meant.;;;","13/May/21 06:19;wangyang0918;[~mapohl] I think this ticket has been fixed via [https://github.com/apache/flink/commit/12010db73dd342ad8a854b60d5e53638f31c48a1.] So could I close this one?;;;","17/May/21 06:58;mapohl;[~fly_in_gis] the link you shared returns a 404. I guess, you meant [12010db|https://github.com/apache/flink/commit/12010db73dd342ad8a854b60d5e53638f31c48a1]? Can you point me to a build which includes the fix but has a failure in Kubernetes ITCases?;;;","17/May/21 14:37;wangyang0918;[~mapohl] I pushed a test commit to verify this fix. You could find the failed builder here[1].

 

[1]. https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=561&view=logs&j=9401bf33-03c4-5a24-83fe-e51d75db73ef&t=72901ab2-7cd0-57be-82b1-bca51de20fba;;;","18/May/21 07:26;mapohl;Looks good to me. I'm closing this issue due to the fix in [12010db|https://github.com/apache/flink/commit/12010db] which was verified through [this forced build failure|https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=561&view=logs&j=9401bf33-03c4-5a24-83fe-e51d75db73ef&t=72901ab2-7cd0-57be-82b1-bca51de20fba&l=2661]. Thanks [~fly_in_gis];;;","18/May/21 07:29;mapohl;Just for completeness:
{{master}} - {{12010db73dd342ad8a854b60d5e53638f31c48a1}}
{{1.13}} - {{207d0fdb4b0b3a3780dbffbb5bb9d43b8f8f1fd4}}
{{1.12}} - {{8c7a1c8ec758eaa7706845b41979e35f6f52efb2}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate recent updates to backpressure and checkpointing docs,FLINK-22562,13376537,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,pnowojski,pnowojski,04/May/21 10:25,29/Sep/21 07:49,13/Jul/23 08:07,29/Sep/21 07:49,1.13.1,1.14.0,,,,,,,1.14.0,,,,,,,chinese-translation,,,,,0,auto-unassigned,pull-request-available,,,,"https://issues.apache.org/jira/browse/FLINK-22253
https://github.com/apache/flink/pull/15811",,jark,JZ Chen,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 14 22:39:44 UTC 2021,,,,,,,,,,"0|z0qpeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/21 02:29;JZ Chen;Hi [~pnowojski], can I take this task?;;;","06/May/21 07:14;pnowojski;Sure [~JZ Chen], that would be great :);;;","17/May/21 02:22;JZ Chen;Please review: https://github.com/apache/flink/pull/15866;;;","01/Jun/21 23:27;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","07/Jun/21 09:44;jark;Translated backpressure docs in master: e9fe3b46b6a079727efd2a74b8d3aeedd7088d75;;;","14/Jun/21 22:39;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertKafkaTableITCase.testKafkaSourceSinkWithKeyAndFullValue fails with output mismatch,FLINK-22559,13376501,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,dwysakowicz,dwysakowicz,04/May/21 07:43,28/Aug/21 11:19,13/Jul/23 08:07,10/May/21 07:24,1.13.1,1.14.0,,,,,,,1.13.1,1.14.0,,,,,,Table SQL / Ecosystem,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17527&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7009

{code}
May 03 23:12:46 Expected: <1,name 1,2020-03-08T13:12:11.123,100,payload 1>
May 03 23:12:46 <2,name 2,2020-03-09T13:12:11.123,101,payload 2>
May 03 23:12:46 <3,name 3,2020-03-10T13:12:11.123,102,payload 3>
May 03 23:12:46 <1,name 1,2020-03-08T13:12:11.123,100,payload 1>
May 03 23:12:46 <1,name 1,2020-03-11T13:12:11.123,100,payload>
May 03 23:12:46      but: was <1,name 1,2020-03-11T13:12:11.123,100,payload>
May 03 23:12:46 <1,name 1,2020-03-11T13:12:11.123,100,payload>
May 03 23:12:46 <1,name 1,2020-03-08T13:12:11.123,100,payload 1>
May 03 23:12:46 <3,name 3,2020-03-10T13:12:11.123,102,payload 3>
May 03 23:12:46 <2,name 2,2020-03-09T13:12:11.123,101,payload 2>
{code}",,dwysakowicz,fsk119,jark,leonard,maguowei,rmetzger,roman,trohrmann,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22546,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22637,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 07:24:44 UTC 2021,,,,,,,,,,"0|z0qp6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/21 07:45;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17527&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121&l=6685;;;","04/May/21 07:48;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17530&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121&l=6757;;;","04/May/21 07:50;dwysakowicz;This test fails all over the place in the linked runs. cc [~jark] [~twalthr];;;","04/May/21 17:20;roman;https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=844&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=18a8058f-13c3-5857-f389-32cb0ee00ed2&l=6682;;;","05/May/21 06:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17535&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6686;;;","05/May/21 07:08;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17558&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7081;;;","05/May/21 07:10;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17552&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6758;;;","05/May/21 09:11;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17540&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6686;;;","06/May/21 12:54;chesnay;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17630&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","07/May/21 08:33;rmetzger;I'll disable this test now on master.;;;","07/May/21 09:50;rmetzger;https://github.com/apache/flink/commit/aca4328554505072c8872cf77ce37c8300e370b9;;;","07/May/21 09:51;rmetzger;[~fsk119] can you take a look at the test?;;;","07/May/21 11:28;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17675&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6686;;;","07/May/21 12:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17683&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7015;;;","07/May/21 12:55;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17679&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6690;;;","07/May/21 13:03;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17682&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7070;;;","08/May/21 07:09;leonard;The reason is the primary key indices is wrong which lead to the data order changed unexpectedly. 

It's should be imported by FLINK-22426, cc [~twalthr]

CommonExecSink
{code:java}

private RowType getConsumedRowType(ResolvedSchema schema) {
    return (RowType) schema.toSinkRowDataType().getLogicalType();
}

which should use

private RowType getConsumedRowType(ResolvedSchema schema) {
    return (RowType) schema.toPhysicalRowDataType().getLogicalType();
}{code};;;","08/May/21 11:57;twalthr;Thanks for looking into this [~Leonard Xu]. Sorry for introducing this bug. Actually I still have to understand why physical data type is needed at this location. I guess we are making some implicit assumptions that are not documented.;;;","10/May/21 06:19;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17761&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6653;;;","10/May/21 06:20;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17761&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6988;;;","10/May/21 07:02;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17749&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7009;;;","10/May/21 07:09;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17749&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7;;;","10/May/21 07:24;twalthr;Fixed in 1.14.0: 1ef78435ddd08c7743bb065981a7f7fadde1795a
Fixed in 1.13.1: cfa9a398ccb73ce38f9df0478bcbe547013c756b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Japicmp fails on 1.12 branch,FLINK-22557,13376496,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,dwysakowicz,dwysakowicz,04/May/21 07:33,28/May/21 11:09,13/Jul/23 08:07,06/May/21 09:37,1.12.3,,,,,,,,1.12.4,,,,,,,Build System,,,,,0,pull-request-available,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17529&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065

{code}
2021-05-03T21:26:13.4153009Z [ERROR] Failed to execute goal com.github.siom79.japicmp:japicmp-maven-plugin:0.11.0:cmp (default) on project flink-scala_2.12: Breaking the build because there is at least one incompatibility: org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2.CoGroupDataSet$$anonfun$apply$2(org.apache.flink.api.scala.CoGroupDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2:CLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3.CoGroupDataSet$$anonfun$apply$3(org.apache.flink.api.scala.CoGroupDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3:CLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4.CoGroupDataSet$$anonfun$apply$4(org.apache.flink.api.scala.CoGroupDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4:CLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.apply(scala.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.CoGroupDataSet$$anonfun$buildGroupSortList$1(org.apache.flink.api.scala.CoGroupDataSet,org.apache.flink.api.common.typeinfo.TypeInformation,java.util.ArrayList):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.result$1:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1:CLASS_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1.CrossDataSet$$anonfun$apply$1(org.apache.flink.api.scala.CrossDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1:CLASS_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2.CrossDataSet$$anonfun$apply$2(org.apache.flink.api.scala.CrossDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2:CLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1.DataSet$$anonfun$1(org.apache.flink.api.scala.DataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1:CLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1.DataSet$$anonfun$write$1(org.apache.flink.api.scala.DataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1:CLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2.DataSet$$anonfun$write$2(org.apache.flink.api.scala.DataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2:CLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1.DataSet$$anonfun$writeAsCsv$1(org.apache.flink.api.scala.DataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1.ExecutionEnvironment$$anonfun$createInput$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1.ExecutionEnvironment$$anonfun$fromCollection$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2.ExecutionEnvironment$$anonfun$fromCollection$2(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1.ExecutionEnvironment$$anonfun$fromElements$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1.ExecutionEnvironment$$anonfun$readFile$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2.ExecutionEnvironment$$anonfun$readFile$2(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1.ExecutionEnvironment$$anonfun$readTextFile$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1.ExecutionEnvironment$$anonfun$readTextFileWithValue$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1.apply(org.apache.flink.api.scala.DataSet,org.apache.flink.api.scala.DataSet):METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1.apply(java.lang.Object,java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1.ExecutionEnvironment$$anonfun$union$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function2[scala.Function2]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet.org$apache$flink$api$scala$GroupedDataSet$$groupSortKeyPositions():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet.org$apache$flink$api$scala$GroupedDataSet$$groupSortOrders():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1.GroupedDataSet$$anonfun$combineGroup$1(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2.GroupedDataSet$$anonfun$combineGroup$2(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1.apply(int):METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1(org.apache.flink.api.scala.GroupedDataSet,org.apache.flink.api.java.operators.SortedGrouping):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1.GroupedDataSet$$anonfun$reduce$1(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2.GroupedDataSet$$anonfun$reduce$2(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1.GroupedDataSet$$anonfun$reduceGroup$1(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2.GroupedDataSet$$anonfun$reduceGroup$2(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3.GroupedDataSet$$anonfun$reduceGroup$3(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3:CLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1.JoinDataSet$$anonfun$apply$1(org.apache.flink.api.scala.JoinDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1:CLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2.JoinDataSet$$anonfun$apply$2(org.apache.flink.api.scala.JoinDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2:CLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3.JoinDataSet$$anonfun$apply$3(org.apache.flink.api.scala.JoinDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3:SUPERCLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3:CLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4.JoinDataSet$$anonfun$apply$4(org.apache.flink.api.scala.JoinDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4:SUPERCLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.org$apache$flink$api$scala$typeutils$CaseClassTypeInfo$$fieldTypes:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1.apply(java.lang.String):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1.CaseClassTypeInfo$$anonfun$getFieldIndices$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1.apply(org.apache.flink.api.common.typeinfo.TypeInformation):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1.CaseClassTypeInfo$$anonfun$getFlatFields$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo,int,java.util.List,scala.runtime.IntRef):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1.apply(scala.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1.CaseClassTypeInfo$$anonfun$getGenericParameters$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.apply(int):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.apply$mcVI$sp(int):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.CaseClassTypeInfo$$anonfun$getTypeAt$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo,scala.runtime.ObjectRef,java.lang.String,java.lang.Object):CONSTRUCTOR_REMOVED,scala.Function1$mcVI$sp[scala.Function1$mcVI$sp]:INTERFACE_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1.apply(scala.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1.CaseClassTypeInfo$$anonfun$toString$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1:CLASS_REMOVED,org.apache.flink.api.scala.UnfinishedJoinOperation.createJoinFunctionAssigner(org.apache.flink.api.common.operators.Keys,org.apache.flink.api.common.operators.Keys):METHOD_REMOVED,org.apache.flink.api.scala.utils.package.DataSetUtils(org.apache.flink.api.scala.DataSet,org.apache.flink.api.common.typeinfo.TypeInformation,scala.reflect.ClassTag):METHOD_REMOVED,org.apache.flink.api.scala.utils.package:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package:CLASS_REMOVED,org.apache.flink.api.scala.utils.package$.DataSetUtils(org.apache.flink.api.scala.DataSet,org.apache.flink.api.common.typeinfo.TypeInformation,scala.reflect.ClassTag):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package$:CLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1.apply(org.apache.flink.api.java.tuple.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1.package$DataSetUtils$$anonfun$countElementsPerPartition$1(org.apache.flink.api.scala.utils.package$DataSetUtils):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1:CLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1.apply(org.apache.flink.api.java.tuple.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1.package$DataSetUtils$$anonfun$zipWithIndex$1(org.apache.flink.api.scala.utils.package$DataSetUtils):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1:CLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1.apply(org.apache.flink.api.java.tuple.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1.package$DataSetUtils$$anonfun$zipWithUniqueId$1(org.apache.flink.api.scala.utils.package$DataSetUtils):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1:CLASS_REMOVED -> [Help 1]
{code}",,dwysakowicz,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 09:37:55 UTC 2021,,,,,,,,,,"0|z0qp5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/21 07:34;dwysakowicz;It happens for scala 2.12 only.;;;","04/May/21 07:35;dwysakowicz;Could you take a look [~AHeise] [~chesnay]? You upgraded the reference version in japicmp.;;;","05/May/21 12:00;chesnay;It appears the scala 2.12 artifacts actually contain scala 2.11.

The file sizes of the flink-scala 2.12 jars has changed significantly between 1.12.2 and 1.12.3, and in the latter it not matches the 2.11 jars.

1.12.2:
 * 2.11: 848KB
 * 2.12: 722KB

1.12.3:
 * 2.11: 848KB
 * 2.12: 848KB;;;","05/May/21 12:49;chesnay;Let's revert the japicmp configuration to again test against 1.12.2, and bump it straight to 1.12.4 once it was released.;;;","05/May/21 12:53;chesnay;For clarity, this happened because a shortcut was taken during the deployment of the jars, where the release scripts were not used but {{mvn deploy}} was called directly, without {{clean}}.

This took the classes previously compiled against Scala 2.11, and just repackaged them into differently-labeled jars.;;;","06/May/21 09:37;chesnay;master: 2691b8d2f6dc536a96c5448aee64f086053a26ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LGPL-2.1 files in flink-python jars,FLINK-22555,13376438,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,bayard,bayard,03/May/21 20:12,28/Aug/21 11:18,13/Jul/23 08:07,09/May/21 18:25,1.12.3,1.13.0,,,,,,,1.12.4,1.13.1,,,,,,API / Python,,,,,0,pull-request-available,,,,,"Looking at, for example, [https://repo1.maven.org/maven2/org/apache/flink/flink-python_2.11/1.13.0/] the jar file contains three LGPL-2.1 source files:
 * flink-python_2.11-1.13.0/META-INF/maven/org.jboss.modules/jboss-modules/pom.xml
 * flink-python_2.11-1.13.0/schema/module-1_1.xsd
 * flink-python_2.11-1.13.0/schema/module-1_0.xsd

There's nothing in the DEPENDENCIES or licenses directory on the topic. It looks like Netty brings in the LGPL-2.1 JBoss dependency (depending on the Apache-2.0 JBoss Marshalling, which depends on the LGPL-2.1 JBoss Modules).

The xsd files are also appear to be coming from JBoss Modules. Given Apache's position on LGPL-2.1 dependency inclusion in ASF projects, this seems like an issue.",,bayard,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 09 18:25:27 UTC 2021,,,,,,,,,,"0|z0qoso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/21 23:54;chesnay;This seems to be bundled in beam-vendor-grpc-1_26_0-0.3; maybe we can just exclude these files. [~bayard] could you open an issue with the Beam project to remove said files from their releases?;;;","04/May/21 00:01;chesnay;Out of curiosity, how did you discover this?;;;","04/May/21 06:20;rmetzger;I will look into extending the license checker for catching this case automatically in the future: FLINK-22556;;;","04/May/21 10:28;trohrmann;cc [~dianfu].;;;","09/May/21 18:25;chesnay;master:
8f3d483d7432f2857035be07d8f54612cde18b57 
1.13:
60b604425b97e759c0d3d8f62b3a24d656dd6e4c 
1.12:
87555d815077fac83f2ee0216ecb4bcb258e7e03;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-docker dev-master works against 1.11-SNAPSHOT,FLINK-22550,13376356,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,03/May/21 11:00,05/May/21 09:17,13/Jul/23 08:07,05/May/21 09:17,,,,,,,,,,,,,,,,flink-docker,,,,,0,pull-request-available,,,,,"The dev-master branch is supposed to work against the latest snapshot version of Flink, but it currently points to 1.11.

We also need to update the release guide to ensure this is updated when a release branch is cut.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 05 09:17:18 UTC 2021,,,,,,,,,,"0|z0qoao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/21 09:17;chesnay;dev-master: 6facb38d0da864dc44e0989963ef0630421fa887;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint fail due to IllegalReferenceCountException,FLINK-22548,13376322,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,maguowei,maguowei,03/May/21 04:33,28/May/21 11:08,13/Jul/23 08:07,04/May/21 13:15,1.14.0,,,,,,,,1.14.0,,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17499&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9615


{code:java}
2021-05-02T23:18:10.5449890Z May 02 23:18:10 [ERROR] shouldRescaleUnalignedCheckpoint[upscale pipeline from 1 to 2](org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase)  Time elapsed: 2.645 s  <<< ERROR!
2021-05-02T23:18:10.5451234Z May 02 23:18:10 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-05-02T23:18:10.5452049Z May 02 23:18:10 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-05-02T23:18:10.5453118Z May 02 23:18:10 	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:167)
2021-05-02T23:18:10.5454315Z May 02 23:18:10 	at org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint(UnalignedCheckpointRescaleITCase.java:368)
2021-05-02T23:18:10.5455228Z May 02 23:18:10 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-02T23:18:10.5499169Z May 02 23:18:10 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-02T23:18:10.5500258Z May 02 23:18:10 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-02T23:18:10.5501465Z May 02 23:18:10 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-02T23:18:10.5502547Z May 02 23:18:10 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-02T23:18:10.5503965Z May 02 23:18:10 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-02T23:18:10.5504867Z May 02 23:18:10 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-02T23:18:10.5505729Z May 02 23:18:10 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-02T23:18:10.5506527Z May 02 23:18:10 	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
2021-05-02T23:18:10.5507319Z May 02 23:18:10 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-02T23:18:10.5508119Z May 02 23:18:10 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-02T23:18:10.5508896Z May 02 23:18:10 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-02T23:18:10.5509592Z May 02 23:18:10 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-02T23:18:10.5510318Z May 02 23:18:10 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-02T23:18:10.5517041Z May 02 23:18:10 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-02T23:18:10.5518247Z May 02 23:18:10 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-02T23:18:10.5520181Z May 02 23:18:10 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-02T23:18:10.5520928Z May 02 23:18:10 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-02T23:18:10.5521660Z May 02 23:18:10 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-02T23:18:10.5522323Z May 02 23:18:10 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-02T23:18:10.5523524Z May 02 23:18:10 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-02T23:18:10.5524309Z May 02 23:18:10 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-02T23:18:10.5524934Z May 02 23:18:10 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-05-02T23:18:10.5525581Z May 02 23:18:10 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-05-02T23:18:10.5526293Z May 02 23:18:10 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-02T23:18:10.5526966Z May 02 23:18:10 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-02T23:18:10.5527688Z May 02 23:18:10 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-02T23:18:10.5535712Z May 02 23:18:10 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-02T23:18:10.5536510Z May 02 23:18:10 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-02T23:18:10.5537216Z May 02 23:18:10 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-02T23:18:10.5538023Z May 02 23:18:10 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-02T23:18:10.5538639Z May 02 23:18:10 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-02T23:18:10.5539313Z May 02 23:18:10 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-05-02T23:18:10.5540069Z May 02 23:18:10 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-05-02T23:18:10.5540838Z May 02 23:18:10 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-05-02T23:18:10.5541565Z May 02 23:18:10 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-05-02T23:18:10.5543081Z May 02 23:18:10 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-02T23:18:10.5544143Z May 02 23:18:10 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-02T23:18:10.5544872Z May 02 23:18:10 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-02T23:18:10.5545584Z May 02 23:18:10 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-05-02T23:18:10.5546418Z May 02 23:18:10 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=100)
2021-05-02T23:18:10.5547515Z May 02 23:18:10 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-05-02T23:18:10.5548618Z May 02 23:18:10 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-05-02T23:18:10.5549630Z May 02 23:18:10 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:207)
2021-05-02T23:18:10.5550454Z May 02 23:18:10 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:197)
2021-05-02T23:18:10.5551323Z May 02 23:18:10 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:188)
2021-05-02T23:18:10.5552179Z May 02 23:18:10 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:677)
2021-05-02T23:18:10.5553067Z May 02 23:18:10 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-05-02T23:18:10.5553934Z May 02 23:18:10 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
2021-05-02T23:18:10.5554583Z May 02 23:18:10 	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
2021-05-02T23:18:10.5555251Z May 02 23:18:10 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-02T23:18:10.5556119Z May 02 23:18:10 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-02T23:18:10.5556779Z May 02 23:18:10 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
2021-05-02T23:18:10.5557555Z May 02 23:18:10 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
2021-05-02T23:18:10.5558345Z May 02 23:18:10 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-05-02T23:18:10.5559115Z May 02 23:18:10 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-05-02T23:18:10.5559825Z May 02 23:18:10 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-05-02T23:18:10.5560452Z May 02 23:18:10 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-05-02T23:18:10.5561113Z May 02 23:18:10 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-05-02T23:18:10.5561772Z May 02 23:18:10 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-05-02T23:18:10.5562411Z May 02 23:18:10 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-05-02T23:18:10.5563194Z May 02 23:18:10 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-02T23:18:10.5563943Z May 02 23:18:10 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-02T23:18:10.5564550Z May 02 23:18:10 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-05-02T23:18:10.5565175Z May 02 23:18:10 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-05-02T23:18:10.5565795Z May 02 23:18:10 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-05-02T23:18:10.5566542Z May 02 23:18:10 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-05-02T23:18:10.5567147Z May 02 23:18:10 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-05-02T23:18:10.5567711Z May 02 23:18:10 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-05-02T23:18:10.5568274Z May 02 23:18:10 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-05-02T23:18:10.5568937Z May 02 23:18:10 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-05-02T23:18:10.5569616Z May 02 23:18:10 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-05-02T23:18:10.5570312Z May 02 23:18:10 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-05-02T23:18:10.5570987Z May 02 23:18:10 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-05-02T23:18:10.5571793Z May 02 23:18:10 Caused by: org.apache.flink.shaded.netty4.io.netty.util.IllegalReferenceCountException: refCnt: 0, increment: 1
2021-05-02T23:18:10.5572863Z May 02 23:18:10 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.retain0(ReferenceCountUpdater.java:123)
2021-05-02T23:18:10.5573769Z May 02 23:18:10 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.retain(ReferenceCountUpdater.java:110)
2021-05-02T23:18:10.5574789Z May 02 23:18:10 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.retain(AbstractReferenceCountedByteBuf.java:80)
2021-05-02T23:18:10.5575672Z May 02 23:18:10 	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.retainBuffer(NetworkBuffer.java:166)
2021-05-02T23:18:10.5576462Z May 02 23:18:10 	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.retainBuffer(NetworkBuffer.java:47)
2021-05-02T23:18:10.5577241Z May 02 23:18:10 	at org.apache.flink.runtime.io.network.buffer.BufferConsumer.copy(BufferConsumer.java:143)
2021-05-02T23:18:10.5578035Z May 02 23:18:10 	at org.apache.flink.runtime.io.network.buffer.BufferConsumer.toDebugString(BufferConsumer.java:202)
2021-05-02T23:18:10.5578861Z May 02 23:18:10 	at org.apache.flink.runtime.io.network.logger.NetworkActionsLogger.traceRecover(NetworkActionsLogger.java:94)
2021-05-02T23:18:10.5579757Z May 02 23:18:10 	at org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.addRecovered(PipelinedSubpartition.java:142)
2021-05-02T23:18:10.5580850Z May 02 23:18:10 	at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.recover(RecoveredChannelStateHandler.java:195)
2021-05-02T23:18:10.5581946Z May 02 23:18:10 	at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.recover(RecoveredChannelStateHandler.java:144)
2021-05-02T23:18:10.5583033Z May 02 23:18:10 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateChunkReader.readChunk(SequentialChannelStateReaderImpl.java:207)
2021-05-02T23:18:10.5584104Z May 02 23:18:10 	at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readSequentially(SequentialChannelStateReaderImpl.java:107)
2021-05-02T23:18:10.5585071Z May 02 23:18:10 	at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.read(SequentialChannelStateReaderImpl.java:93)
2021-05-02T23:18:10.5586048Z May 02 23:18:10 	at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readOutputData(SequentialChannelStateReaderImpl.java:79)
2021-05-02T23:18:10.5586930Z May 02 23:18:10 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:571)
2021-05-02T23:18:10.5587723Z May 02 23:18:10 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
2021-05-02T23:18:10.5588521Z May 02 23:18:10 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:554)
2021-05-02T23:18:10.5589195Z May 02 23:18:10 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:756)
2021-05-02T23:18:10.5589848Z May 02 23:18:10 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
2021-05-02T23:18:10.5590546Z May 02 23:18:10 	at java.lang.Thread.run(Thread.java:748)

{code}
",,dwysakowicz,maguowei,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 04 13:15:45 UTC 2021,,,,,,,,,,"0|z0qo34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/21 05:38;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17422&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4313;;;","03/May/21 07:20;trohrmann;cc [~pnowojski];;;","04/May/21 07:47;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17527&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9693;;;","04/May/21 10:38;pnowojski;This is just a TRACE logging issue. There was indeed a potential illegal access to buffer when printing some TRACE level logs, so it shouldn't affect any users. And it wouldn't be causing any data corruptions even user enabled TRACE logs. (fix is also trivial an will be merged soon);;;","04/May/21 13:15;pnowojski;merged commit 1554292 into apache:master

I think this issue can not occur on the release-1.13 branch, as the extra logging has never been merged there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinatorHolderTest. verifyCheckpointEventOrderWhenCheckpointFutureCompletesLate fail,FLINK-22547,13376321,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,maguowei,maguowei,03/May/21 04:28,23/Sep/21 18:08,13/Jul/23 08:07,14/Jul/21 14:36,1.13.0,1.14.0,,,,,,,1.12.5,1.13.2,1.14.0,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17499&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=7502


{noformat}
2021-05-02T22:45:49.7343556Z May 02 22:45:49 java.lang.AssertionError
2021-05-02T22:45:49.7344688Z May 02 22:45:49 	at org.junit.Assert.fail(Assert.java:86)
2021-05-02T22:45:49.7345646Z May 02 22:45:49 	at org.junit.Assert.assertTrue(Assert.java:41)
2021-05-02T22:45:49.7346698Z May 02 22:45:49 	at org.junit.Assert.assertTrue(Assert.java:52)
2021-05-02T22:45:49.7353570Z May 02 22:45:49 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.checkpointEventValueAtomicity(OperatorCoordinatorHolderTest.java:363)
2021-05-02T22:45:49.7355384Z May 02 22:45:49 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.verifyCheckpointEventOrderWhenCheckpointFutureCompletesLate(OperatorCoordinatorHolderTest.java:331)
2021-05-02T22:45:49.7356826Z May 02 22:45:49 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-02T22:45:49.7904883Z May 02 22:45:49 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-02T22:45:49.7905443Z May 02 22:45:49 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-02T22:45:49.7905918Z May 02 22:45:49 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-02T22:45:49.7906402Z May 02 22:45:49 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-02T22:45:49.7907018Z May 02 22:45:49 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-02T22:45:49.7907555Z May 02 22:45:49 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-02T22:45:49.7909318Z May 02 22:45:49 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-02T22:45:49.7910078Z May 02 22:45:49 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-02T22:45:49.7910869Z May 02 22:45:49 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-02T22:45:49.7911597Z May 02 22:45:49 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-02T22:45:49.7912383Z May 02 22:45:49 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-02T22:45:49.7914058Z May 02 22:45:49 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-02T22:45:49.7915214Z May 02 22:45:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-02T22:45:49.7916058Z May 02 22:45:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-02T22:45:49.7916852Z May 02 22:45:49 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-02T22:45:49.7917550Z May 02 22:45:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-02T22:45:49.7919076Z May 02 22:45:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-02T22:45:49.7920292Z May 02 22:45:49 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-02T22:45:49.7921041Z May 02 22:45:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-02T22:45:49.7921788Z May 02 22:45:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-02T22:45:49.7922652Z May 02 22:45:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-05-02T22:45:49.7923564Z May 02 22:45:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-05-02T22:45:49.7924834Z May 02 22:45:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-05-02T22:45:49.7925709Z May 02 22:45:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-05-02T22:45:49.7926617Z May 02 22:45:49 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-02T22:45:49.7927661Z May 02 22:45:49 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-02T22:45:49.7928497Z May 02 22:45:49 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-02T22:45:49.7929783Z May 02 22:45:49 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}
",,kevin.cyj,maguowei,pnowojski,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 14 14:36:06 UTC 2021,,,,,,,,,,"0|z0qo2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/21 07:21;trohrmann;cc [~sewen];;;","13/Jul/21 14:59;sewen;I found the problem, it is a test instability (thread race in test setup).
Will open a PR to make this robust...;;;","14/Jul/21 14:36;sewen;Fixed in
  - master (1.14.0) via 2c1284bfe272f33e9a98a3ea41a6ed37292eef65
  - release-1.13 (1.13.3) via 950ea33cb6ec5716ff26750eddf8a7bd4b869d0e
  - release-1.12 (1.12.5) via 8f0e13ced6ee64e796f68c8d470de94e7ab9e47d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVM crashes when running OperatorEventSendingCheckpointITCase.testOperatorEventAckLost,FLINK-22545,13376319,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,maguowei,maguowei,03/May/21 04:16,15/Dec/21 01:40,13/Jul/23 08:07,25/Aug/21 12:02,1.12.3,,,,,,,,1.12.8,1.13.3,1.14.0,,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17501&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4287
",,dwysakowicz,kevin.cyj,leonard,maguowei,rmetzger,sewen,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22464,FLINK-24855,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 25 11:24:14 UTC 2021,,,,,,,,,,"0|z0qo2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/21 07:21;trohrmann;cc [~sewen];;;","19/May/21 10:52;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/May/21 03:00;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18329&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185&l=4346;;;","02/Jun/21 23:30;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","04/Jun/21 01:55;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18652&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4249;;;","21/Jun/21 02:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19153&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4243;;;","24/Jun/21 04:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19416&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4240;;;","28/Jun/21 02:22;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19595&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4243;;;","28/Jun/21 14:25;rmetzger;The issue has only happened in the release-1.12 branch.

The JVM is stopping because of this
{code}
23:53:46,730 [SourceCoordinator-Source: numbers -> Map -> Sink: Data stream collect sink] ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'SourceCoordinator-Source: numbers -> Map -> Sink: Data stream collect sink' produced an uncaught exception. Stopping the process...
java.lang.Error: This indicates that a fatal error has happened and caused the coordinator executor thread to exit. Check the earlier logsto see the root cause of the problem.
	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider$CoordinatorExecutorThreadFactory.newThread(SourceCoordinatorProvider.java:114) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619) ~[?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932) ~[?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025) ~[?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) ~[?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_282]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{code};;;","28/Jun/21 14:56;trohrmann;Did you check whether the logs contain any errors as the error message indicates [~rmetzger]?;;;","28/Jun/21 15:15;rmetzger;Yes, I did not find anything suspicious (yet). 
I'll post again once I have more findings ;) ;;;","29/Jun/21 18:20;rmetzger;I've unassigned myself from the ticket for now, because I'll be on vacation for a few days.;;;","13/Jul/21 12:25;sewen;The exception that causes this crash is from a guard that aims to check that not more than one Coordinator Thread is spawned and working on the mailbox. The Coordinator Thread is using a Single Threaded Executor, so there should never be more than one thread.

However, I think it can happen that the thread is terminated (if it was idle for long) and then another thread gets spawned again.
In that case, the error we see would be thrown.

To fix that, we would need to perform the check for a previous thread in a different way. I'll open a PR with a suggestion.

What puzzles me a bit is that this only occurs in Flink 1.12 and not in newer versions. Maybe this is because of different timings?
;;;","13/Jul/21 12:41;rmetzger;Thanks a lot for looking into this!

From the exception message, it seems that the intention was rather to catch fatal errors (exceptions thrown out of the thread). However since we register an uncaught exception handler, this additional check doesn't seem necessary?
;;;","13/Jul/21 12:54;sewen;The adjusted check is reasonable in any case, so I will apply it to 1.12 / 1.13 / master.;;;","13/Jul/21 13:01;sewen;> From the exception message, it seems that the intention was rather to catch fatal errors (exceptions thrown out of the thread). However since we register an uncaught exception handler, this additional check doesn't seem necessary?

Probably not in this case, but the code allows plugging in other exception handlers. I'll try to keep the original author's intention here...;;;","13/Jul/21 22:34;sewen;Fixed in
  - 1.12.5 (release-1.12) via 331ae2cdfa7462b7cc5ed018484cb3eda257a174
  - 1.13.3 (release-1.13) via 17a294daacdc67b1939607a41e67e56af2fa6888
  - 1.14.0 (master) via 4b867d8a6c187d6bef45294a6e6a524254410167;;;","18/Jul/21 08:32;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20620&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4501;;;","18/Jul/21 21:55;sewen;The stack trace looks like the executor is creating a second thread after the original thread terminated.

Looking at this more, the whole business of this thread factory that guards the thread creation just looks like it doesn't really work. 
I would vote to remove this altogether.
We either should just use a normal thread factory, or just manually create a thread with a blocking queue of Runnables to pull from.

Before doing that, I would like to understand better why the executor thread terminates - just to see if there is an issue in some other part of the system that propagates a failure. Will push a hotfix to extend the logging.;;;","06/Aug/21 03:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21642&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7166e71c-cad6-5ec9-ae14-15891ce68128&l=5198;;;","06/Aug/21 03:18;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21643&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4486;;;","16/Aug/21 07:56;sewen;Thanks for reporting the logs, I am taking a look here...;;;","17/Aug/21 17:02;sewen;The original exception raised and causing the test instability is the following:

The testing enumerator delays split assignment until after the next checkpoint is triggered. By the time the assignment operation is executed (in the mailbox), however, the target task might not be running any more, which gives us below exception.

{code}
22:32:15,333 [SourceCoordinator-Source: numbers -> Map -> Sink: Data stream collect sink] ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext [] - Uncaught Exception in Source Coordinator Executor
java.lang.IllegalArgumentException: Cannot assign splits null to subtask 0 because the subtask is not registered.
	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.lambda$assignSplits$3(SourceCoordinatorContext.java:182) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.callInCoordinatorThread(SourceCoordinatorContext.java:397) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.assignSplits(SourceCoordinatorContext.java:176) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.api.connector.source.SplitEnumeratorContext.assignSplit(SplitEnumeratorContext.java:82) ~[flink-core-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.api.connector.source.lib.util.IteratorSourceEnumerator.handleSplitRequest(IteratorSourceEnumerator.java:63) ~[flink-core-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase$AssignAfterCheckpointEnumerator.fullFillPendingRequests(OperatorEventSendingCheckpointITCase.java:311) ~[test-classes/:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code}

I am fixing the test in this issue and opening a separate issue to downgrade the handling of the asynchronous enumerator exceptions to a global failure, not a process kill: FLINK-23843;;;","18/Aug/21 08:48;sewen;Fixed in
  - 1.14.0 via d081d9a7b32d5fae74eddcfa3b2d1c1318d3c44b
  - 1.13.3 via 6fbdb15391fd26334841b540b82080e312066f82;;;","25/Aug/21 03:33;xtsong;Instance on 1.12:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22771&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4485;;;","25/Aug/21 03:33;xtsong;Reopen to backport the fix for release-1.12.

[~sewen], before we do this, could you confirm the instance on the 1.12 branch is the same problem?;;;","25/Aug/21 11:24;sewen;The cause seems to be the same. I am backporting the fix to the {{release-1.12} branch...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource leak would happen if exception thrown during AbstractInvokable#restore of task life,FLINK-22535,13375859,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,yunta,yunta,29/Apr/21 14:49,28/May/21 11:08,13/Jul/23 08:07,03/May/21 13:22,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,,"FLINK-17012 introduced new initialization phase such as {{AbstractInvokable.restore}}, however, if [invokable.restore()|https://github.com/apache/flink/blob/79a521e08df550d96f97bb6915191d8496bb29ea/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L754-L759] throws exception out, no more {{StreamTask#cleanUpInvoke}} would be called, leading to resource leak.

We internally leveraged another way to use managed memory by registering specific operator identifier in memory manager, forgetting to call the stream task cleanup would let stream operator not be disposed and we have to face critical resource leak.",,akalashnikov,gaoyunhaii,guoyangze,kevin.cyj,klion26,maguowei,pnowojski,trohrmann,wind_ljy,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17012,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 03 13:22:57 UTC 2021,,,,,,,,,,"0|z0ql8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/21 16:21;gaoyunhaii;Also another related possible issue is that if StreamTask#invoke throws exception, the _cleanUpInvoke_ might be called twice, but we do not has a flag to skip the second execution. Maybe we could also move cleanUpInvoke to finally.;;;","29/Apr/21 17:08;akalashnikov;[~gaoyunhaii] thanks for the observation. But how is it possible to call cleanUpInvoke twice? As I see if the exception happens, only one cleanUpInvoke will be invoked which is in catch block and then the exception will be rethrown which allows avoiding the second invocation.

 

According to the original problem, it looks like we indeed should finish StreamTask#restore with _cleanUpInvoke._ My fault. I will fix it.;;;","29/Apr/21 17:18;gaoyunhaii;Hi  [~akalashnikov], sorry I mixed the order of braces, you are right that the exception would be thrown at the last of the catch block. Sorry for the wrong information.;;;","30/Apr/21 05:35;gaoyunhaii;Hi [~akalashnikov] sorry I think if there is always the re-thrown, there should be still problems since that if the invoke() throws exception and enter the catch block, then the try block throws the second exception (like in the cancelTask() method), then cleanupInvoke() seems would not be called before re-thrown the exception.;;;","03/May/21 13:22;pnowojski;Merged to master 5926c4ac22b^..5926c4ac22b
Merged to release-1.13 as ae37cde62b3^..ae37cde62b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The zone id in exception message should be GMT+08:00 instead of GMT+8:00,FLINK-22525,13375790,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,leonard,leonard,leonard,29/Apr/21 09:04,28/Aug/21 11:20,13/Jul/23 08:07,11/May/21 04:23,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"{code:java}
Flink SQL> SET table.local-time-zone=UTC+3;
Flink SQL> select current_row_timestamp();
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: The supported Zone ID is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-8:00', but configured Zone ID is 'UTC+3'.
{code}
The valid zoned should  be 'GMT-08:00'",,frank wang,jark,jingzhang,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 07:02:35 UTC 2021,,,,,,,,,,"0|z0qktc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/21 11:24;frank wang;i check the code, maybe we should remove some validate code 

 
{code:java}
//flink code
Instant instant = null;
if (timestampField instanceof java.time.Instant) {
    instant = ((Instant) timestampField);
} else if (timestampField instanceof java.sql.Timestamp) {
    Timestamp timestamp = ((Timestamp) timestampField);
    // conversion between java.sql.Timestamp and TIMESTAMP_WITH_LOCAL_TIME_ZONE
    instant =
            TimestampData.fromEpochMillis(
                            timestamp.getTime(), timestamp.getNanos() % 1000_000)
                    .toInstant();
} else if (timestampField instanceof TimestampData) {
    instant = ((TimestampData) timestampField).toInstant();
} else if (timestampField instanceof Integer) {
    instant = Instant.ofEpochSecond((Integer) timestampField);
} else if (timestampField instanceof Long) {
    instant = Instant.ofEpochMilli((Long) timestampField);
}
if (instant != null) {
    return timestampToString(
            instant.atZone(sessionTimeZone).toLocalDateTime(),
            getPrecision(fieldType));
} else {
    return timestampField;
}{code}
and i test this code, i found it can print the right time that i need
{code:java}
//test code
Instant instant=Instant.now();
System.out.println(instant.atZone(ZoneId.of(""UTC+8"")).toLocalDateTime().toString());
{code}
so can we remove some of code as follow
{code:java}
//flink validate time zone code
private void validateTimeZone(String zone) {
    final String zoneId = zone.toUpperCase();
    if (zoneId.startsWith(""UTC+"")
            || zoneId.startsWith(""UTC-"")
            || SHORT_IDS.containsKey(zoneId)) {
        throw new IllegalArgumentException(
                String.format(
                        ""The supported Zone ID is either a full name such as 'America/Los_Angeles',""
                                + "" or a custom timezone id such as 'GMT-8:00', but configured Zone ID is '%s'."",
                        zone));
    }
}
{code}
 ;;;","30/Apr/21 07:48;leonard;[~frank wang], we check the zone Id because the *java.util.TimeZone* can not parse zone Id format like 'UTC+8', we used this conversion internal, the *java.time.ZoneId* actually support this format.
{code:java}
@Test
public void testTimeZoneParse() {
    System.out.println(ZoneId.of(""UTC+8"")     + "" "" + TimeZone.getTimeZone(""UTC+8""));
    System.out.println(ZoneId.of(""GMT+08:00"") + "" "" + TimeZone.getTimeZone(""GMT+08:00""));
}

UTC+08:00 sun.util.calendar.ZoneInfo[id=""GMT"",offset=0,dstSavings=0,useDaylight=false,transitions=0,lastRule=null]
GMT+08:00 sun.util.calendar.ZoneInfo[id=""GMT+08:00"",offset=28800000,dstSavings=0,useDaylight=false,transitions=0,lastRule=null]

{code};;;","10/May/21 07:02;jark;Fixed in 
 - master: 92512869889ff8e77e1e8030e7014cf7c5119ce4
 - release-1.13: b5f375dee021fb7a1294908e6a70553adfb3a81a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TUMBLE TVF should throw helpful exception when specifying second interval parameter,FLINK-22523,13375757,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,29/Apr/21 06:56,28/Aug/21 11:20,13/Jul/23 08:07,11/May/21 02:21,,,,,,,,,1.13.1,1.14.0,,,,,,Table SQL / API,Table SQL / Planner,,,,0,pull-request-available,,,,,"Currently, the following query can run and no exception is thrown. 

However, the second interval parameter (i.e. the offset parameter) is not supported yet. We should throw a exception for this. 


{code:sql}
select 
  date_format(window_end, 'yyyy-MM-dd') as date_str,
  date_format(window_end, 'HH:mm') as time_str,
  count(distinct user_id) as uv
from table(tumble(table user_behavior, descriptor(ts), interval '10' minute, interval '1' day))
group by window_start, window_end;
{code}",,godfreyhe,jark,jingzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 11 02:21:19 UTC 2021,,,,,,,,,,"0|z0qkm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/21 02:21;jark;Fixed in 
 - master: 3d201442cee372856f2c919dee38d29bf08b77f4
 - release-1.13: 1088b8726732d5121a40a88f38e2fe0bcefffb37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BytesHashMap has many verbose logs,FLINK-22522,13375756,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,29/Apr/21 06:54,28/May/21 11:07,13/Jul/23 08:07,30/Apr/21 11:29,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"I ran a query which contains operators using BytesHashMap, and the logs file contain many following logs

{code}
2021-04-29 14:00:47,420 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 758, 24838144 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:48,322 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 672, 22020096 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:48,345 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 764, 25034752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:49,611 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 634, 20774912 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:49,703 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 757, 24805376 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:50,582 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 658, 21561344 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:50,621 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 762, 24969216 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:51,837 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 636, 20840448 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:51,862 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 758, 24838144 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:52,836 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 649, 21266432 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:52,899 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 761, 24936448 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:54,064 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 617, 20217856 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:54,148 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 754, 24707072 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:54,871 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 673, 22052864 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:54,959 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 765, 25067520 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:55,912 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 641, 21004288 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:56,017 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 761, 24936448 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:56,924 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 639, 20938752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:57,047 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 759, 24870912 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:57,727 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 666, 21823488 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:57,768 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 764, 25034752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:58,541 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 665, 21790720 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:58,589 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 766, 25100288 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:59,373 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 657, 21528576 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:59,422 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 764, 25034752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:00,201 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 673, 22052864 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:00,248 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 766, 25100288 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:01,015 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 665, 21790720 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:01,080 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 764, 25034752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:01,626 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 689, 22577152 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:01,698 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 767, 25133056 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:02,017 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 721, 23625728 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:02,108 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 773, 25329664 in bytes, init allocating 32 for bucket area.
{code}",,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 30 11:29:23 UTC 2021,,,,,,,,,,"0|z0qkls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/21 11:29;jark;Fixed in 
 - master: bf40fa5b8cd5b3a8876f37ab4aa7bf06e8b9f666
 - release-1.13: a299ae28802c147ceee656e0bccd8a9656a24b1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't call current_timestamp with hive dialect for hive-3.1,FLINK-22512,13375596,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,lirui,lirui,28/Apr/21 12:54,28/May/21 11:09,13/Jul/23 08:07,06/May/21 03:35,,,,,,,,,1.13.1,1.14.0,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,libenchao,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 03:35:34 UTC 2021,,,,,,,,,,"0|z0qjm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/21 03:35;lirui;Fixed in master: c688bf3c83e72155ccf5d04fe397b7c0a1274fd1
Fixed in release-1.13: 2f71141639826cc29006ae0e114a6219bdb4f0fa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of non-composite result type in Python TableAggregateFunction,FLINK-22511,13375591,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,28/Apr/21 12:16,28/Aug/21 12:08,13/Jul/23 08:07,15/May/21 07:38,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 15 07:38:28 UTC 2021,,,,,,,,,,"0|z0qjl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/21 07:38;hxbks2ks;Merged into master via b83e64ed1402713955a630d4f635ab7068450347
Merged into release-1.13 via 7253525f6d43a54bff7336768506974a60aa6754;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit the precision of Resource,FLINK-22505,13375549,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,28/Apr/21 08:53,28/Aug/21 11:18,13/Jul/23 08:07,08/May/21 02:30,1.13.0,,,,,,,,1.14.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"In our internal deployment, we found that a high precision {{CPUResource}} may cause the required resource never to be fulfilled. Think about the following scenario:
- The {{SlotManager}} receives a slot request with 1.000000000000001 CPU and decides to allocate a pending task manager with that resource spec.
- The resource manager starts a task manager and sets the CPU by dynamic config. In this step, we cast the {{CPUResource}} to a double value, where the precision loss happens.
The task manager will finally register with 1.0 CPU and thus can not deduct any pending task manager or fulfill the slot request.

To solve that issue, we proposed to limit the precision of Resource to a safe value, e.g. 8, to prevent the precision loss when cast to double.
- For {{CPUResource}}, the supported scale for the CPU is 3 in k8s while in Yarn, the CPU should be an integer.
- For {{ExternalResource}}, the value will always be treated as an integer.",,guoyangze,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 08 02:30:07 UTC 2021,,,,,,,,,,"0|z0qjbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/21 02:30;xtsong;Fixed via
- master (1.14): ce9d6420365842da6859f1e37f02e09d2e4010c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultCompletedCheckpointStore drops unrecoverable checkpoints silently,FLINK-22502,13375535,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,trohrmann,trohrmann,28/Apr/21 07:47,28/Aug/21 12:09,13/Jul/23 08:07,18/May/21 20:20,1.11.3,1.12.2,1.13.0,1.14.0,,,,,1.12.5,1.13.1,1.14.0,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,,,,,"The {{DefaultCompletedCheckpointStore.recover()}} tries to be resilient if it cannot recover a checkpoint (e.g. due to a transient storage outage or a checkpoint being corrupted). This behaviour was introduced with FLINK-7783.

The problem is that this behaviour might cause us to ignore the latest valid checkpoint if there is a transient problem when restoring it. This might be ok for at least once processing guarantees, but it clearly violates exactly once processing guarantees. On top of it, it is very hard to spot.

I propose to change this behaviour so that {{DefaultCompletedCheckpointStore.recover()}} fails if it cannot read the checkpoints it is supposed to read. If the {{recover}} method fails during a recovery, it will kill the process. This will usually restart the process which will retry the checkpoint recover operation. If the problem is of transient nature, then it should eventually succeed. In case that this problem occurs during an initial job submission, then the job will directly transition to a {{FAILED}} state.

The proposed behaviour entails that if there is a permanent problem with the checkpoint (e.g. corrupted checkpoint), then Flink won't be able to recover without the intervention of the user. I believe that this is the right decision because Flink can no longer give exactly once guarantees in this situation and a user needs to explicitly resolve this situation.",,jingzhang,klion26,pnowojski,rmetzger,roman,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-7783,,FLINK-22692,,,FLINK-22494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 18 20:20:15 UTC 2021,,,,,,,,,,"0|z0qj8o:",9223372036854775807,"On recovery, if a failure occurs during retrieval of a checkpoint, the job is restarted (instead of skipping the checkpoint in some circumstances). This prevents potential consistency violations.",,,,,,,,,,,,,,,,,,,"06/May/21 14:40;trohrmann;[~roman_khachatryan] thanks for tackling this issue. Please move it to in progress if you start working on it.;;;","06/May/21 14:56;roman;Thanks for reminding, I've moved the ticket in progress.;;;","18/May/21 20:20;roman;Merged int master as 4579be715da1ad3040eb45a273137c859404887a

into 1.13 as 32f41f861ef082cc1154937124f66ff0c03dc32c

into 1.12 as a153976c629ab73b5ee507807c91145a2536436d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink HiveCatalog ParquetTable join Dim table error,FLINK-22501,13375525,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,zhanglw,zhanglw,28/Apr/21 07:32,12/May/21 05:55,13/Jul/23 08:07,12/May/21 05:55,1.12.0,,,,,,,,,,,,,,,,,,,,0,,,,,,"Env: flink1.12.0,  Flink Hive Catalog access parquet Table occurs bottom erros 

Caused by: java.lang.IllegalArgumentException
 at java.nio.Buffer.position(Buffer.java:244)
 at org.apache.flink.hive.shaded.parquet.io.api.Binary$ByteBufferBackedBinary.getBytes(Binary.java:424)
 at org.apache.flink.hive.shaded.formats.parquet.vector.ParquetDictionary.decodeToBinary(ParquetDictionary.java:59)
 at org.apache.flink.table.data.vector.heap.HeapBytesVector.getBytes(HeapBytesVector.java:124)
 at org.apache.flink.table.data.vector.VectorizedColumnBatch.getByteArray(VectorizedColumnBatch.java:97)
 at org.apache.flink.table.data.ColumnarRowData.getString(ColumnarRowData.java:113)
 at BatchCalc$12.processElement(Unknown Source)
 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:112)
 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:93)
 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
 at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:163)
 at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)
 at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101)
 at org.apache.flink.connector.file.src.impl.FileSourceRecordEmitter.emitRecord(FileSourceRecordEmitter.java:45)
 at org.apache.flink.connector.file.src.impl.FileSourceRecordEmitter.emitRecord(FileSourceRecordEmitter.java:35)
 at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:126)
 at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:251)
 at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:65)
 at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
 at java.lang.Thread.run(Thread.java:748)",,Nieal-Yang,zhanglw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-28 07:32:07.0,,,,,,,,,,"0|z0qj6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterEntrypointTest.testCloseAsyncShouldBeExecutedInShutdownHook failed,FLINK-22496,13375476,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,trohrmann,maguowei,maguowei,28/Apr/21 02:44,23/Sep/21 17:24,13/Jul/23 08:07,08/Jun/21 16:03,1.14.0,,,,,,,,1.12.5,1.13.2,1.14.0,,,,,Runtime / Coordination,,,,,0,auto-deprioritized-major,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17313&view=logs&j=21408240-6569-5a01-c099-3adfe83ce651&t=b2761bb8-3852-5a0d-bc43-6a1d327b63cb&l=6207


{code:java}
52 [ERROR] testCloseAsyncShouldBeExecutedInShutdownHook(org.apache.flink.runtime.entrypoint.ClusterEntrypointTest)  Time elapsed: 9.83 s  <<< FAILURE!
Apr 27 21:20:52 java.lang.AssertionError: 
Apr 27 21:20:52 Process 843 does not exit within 3000 ms
Apr 27 21:20:52 Expected: is <true>
Apr 27 21:20:52      but: was <false>
Apr 27 21:20:52 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Apr 27 21:20:52 	at org.junit.Assert.assertThat(Assert.java:956)
Apr 27 21:20:52 	at org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.testCloseAsyncShouldBeExecutedInShutdownHook(ClusterEntrypointTest.java:224)
Apr 27 21:20:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 27 21:20:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 27 21:20:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 27 21:20:52 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 27 21:20:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 27 21:20:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 27 21:20:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 27 21:20:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 27 21:20:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 27 21:20:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 27 21:20:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 27 21:20:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 27 21:20:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 27 21:20:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 27 21:20:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 21:20:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 21:20:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)

{code}
",,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 16:03:33 UTC 2021,,,,,,,,,,"0|z0qivk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/21 23:09;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","05/Jun/21 22:48;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/21 10:39;trohrmann;I suspect that the timeout is too tight. The logs do not contain any information because we don't upload the logs of the external process.;;;","08/Jun/21 16:03;trohrmann;Fixed via

master:
18ba8eabdb5a5c046274c3234395aa2ef3b75ab3
946be3676e356a7508a03b4f82bbe40bdd503834

1.13.2:
1a55cae23b3c1e943298ca4a93cad2704994e29b
3660748b252e8de1553305dc326fc38065756951

1.12.5:
b7b324d0a5f23e1e0790ef01b6406446f1aef71c
4c054566615082033f41f67e1fc80b2065aa011d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid discarding checkpoints in case of failure,FLINK-22494,13375384,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,27/Apr/21 16:24,05/Jul/22 07:03,13/Jul/23 08:07,18/May/21 10:43,1.12.3,1.13.0,1.14.0,,,,,,1.12.5,1.13.1,1.14.0,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,,,,,"Both {{StateHandleStore}} implementations (i.e. [KubernetesStateHandleStore:157|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesStateHandleStore.java#L157] and [ZooKeeperStateHandleStore:170|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L170]) discard checkpoints if the checkpoint metadata wasn't written to the backend. 

This does not cover the cases where the data was actually written to the backend but the call failed anyway (e.g. due to network issues). In such a case, we might end up having a pointer in the backend pointing to a checkpoint that was discarded.

Instead of discarding the checkpoint data in this case, we might want to keep it for this specific use case. Otherwise, we might run into Exceptions when recovering from the Checkpoint later on. We might want to add a warning to the user pointing to the possibly orphaned checkpoint data.",,klion26,mapohl,pnowojski,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25265,FLINK-25098,,,,,,,,,,,,,,,FLINK-24543,FLINK-22502,FLINK-22704,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 18 10:43:19 UTC 2021,,,,,,,,,,"0|z0qib4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/21 16:27;mapohl;[~fly_in_gis]: I had the discussion with [~fpaul] and [~trohrmann] about this issue and we came to the conclusion that having an orphaned Checkpoint pointer in the ConfigMap isn't the best solution. Hence, I created this ticket to handle the this case.

-We might only remove the discard in case of failure since we cannot be sure whether the data was actually written to the backend or not in case of failure.-

In case of failure, we might only trigger the discard of the checkpoint if we can make sure that the ConfigMap wasn't updated.;;;","27/Apr/21 16:43;trohrmann;If the exception allows to say that we did not write anything to ZK or K8s, then the checkpoint blobs can be removed.;;;","28/Apr/21 07:42;wangyang0918;[~mapohl] I agree with you that it is not a good behavior to have such orphaned checkpoint pointer in the ZNode or ConfigMap.

Given that it could happen the ZK/K8s client failed with exception but the data was actually written to the ZNode/ConfigMap. I am not sure about how to guarantee that the data on the DFS is only discarded when writing ConfigMap/ZK has true failure. Do you mean we need to check the existence of ZNode / ConfigMap key before discarding the state on DFS?;;;","28/Apr/21 07:58;mapohl;[~fly_in_gis] I updated my statement above because I realized that it was not really clear what I meant. Anyway, you understood me correctly: Either we do a loop checking whether the entry made it into the ConfigMap/ZNode or we skip the discard entirely.;;;","28/Apr/21 08:16;mapohl;I analyzed the code a bit further and realized that the discard is not only triggered in the {{StateHandleStore}} but also initiated in the [CheckpointCoordinator:1208|https://github.com/apache/flink/blob/577113f0c339df844f2cc32b1d4a09d3da28085a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1208]. 

{{CheckpointCoordinator}} also triggers a cleanup in case of a {{CompletedCheckpointStore.addCheckpoint}} failure. We would end up in this cleanup since we're still trying to decline the processed checkpoint by forwarding the Exception from {{StateHandleStore.addAndLock}}.

Hence, it's not enough to not discard in case of failure in the {{StateHandleStore}} implementations but also on the {{CheckpointCoordinator}} level.;;;","29/Apr/21 09:09;trohrmann;cc [~pnowojski];;;","18/May/21 10:43;trohrmann;Fixed via

1.14.0:
9d2e2d980e
cc59ad5e62
417cf78fd0
e632591623
fa0d1dc3d5

1.13.1:
a24b0d86d7
6ce9cc900c
b5c49ef484
a6c13e79b6
13bc663802

1.12.5:
a53a1f3e99
95bd043f0a
b81887e647
5aec1bb949
6b53f8b0e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond found unexpected files,FLINK-22493,13375357,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,dwysakowicz,dwysakowicz,27/Apr/21 14:09,28/May/21 11:07,13/Jul/23 08:07,29/Apr/21 18:35,1.14.0,,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17285&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=9340

{code}
Apr 27 11:10:07 [INFO] Running org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Apr 27 11:10:24 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 17.177 s <<< FAILURE! - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Apr 27 11:10:24 [ERROR] testStopWithSavepointFailOnFirstSavepointSucceedOnSecond(org.apache.flink.test.scheduling.AdaptiveSchedulerITCase)  Time elapsed: 0.305 s  <<< FAILURE!
Apr 27 11:10:24 java.lang.AssertionError: Found unexpected files: /tmp/junit3745203124457058148/savepoint/savepoint-8596b1-b3046c9bcf40
Apr 27 11:10:24 	at org.junit.Assert.fail(Assert.java:88)
Apr 27 11:10:24 	at org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond(AdaptiveSchedulerITCase.java:226)
Apr 27 11:10:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 27 11:10:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 27 11:10:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 27 11:10:24 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 27 11:10:24 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 27 11:10:24 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 27 11:10:24 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 27 11:10:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 11:10:24 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 27 11:10:24 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 27 11:10:24 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 27 11:10:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 27 11:10:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 27 11:10:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 11:10:24 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Apr 27 11:10:24 
{code}",,dwysakowicz,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/21 07:36;rmetzger;AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond.log;https://issues.apache.org/jira/secure/attachment/13024790/AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 18:35:07 UTC 2021,,,,,,,,,,"0|z0qi54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/21 14:25;trohrmann;cc [~rmetzger] could you validate whether this is a problem with the adaptive scheduler?;;;","28/Apr/21 11:15;rmetzger;Yes, I'll take a look.;;;","29/Apr/21 09:06;trohrmann;Why did you upgrade the priority [~rmetzger]? Did you have found something?;;;","29/Apr/21 09:45;rmetzger;I upgraded the priority because we usually mark test instabilities as critical issues. I have not found anything. I have no reason to believe that AdaptiveScheduler or the stop with savepoint logic is doing anything wrong here, it's rather the cleanup of failed savepoints in the checkpoint coordinator. I will post my findings here once I understood the problem.;;;","29/Apr/21 10:29;rmetzger;I believe the problem is the following here:

Once all tasks are running, the test triggers a savepoint, which intentionally fails, because of a test exception in a Task's checkpointing method. The test then waits for the savepoint future to fail, and the scheduler to restart the tasks. Once they are running again, it performs a sanity check whether the savepoint directory has been properly removed. In the reported run, there was still the savepoint directory around.

The savepoint directory is removed via the PendingCheckpoint.discard() method. This method is executed using the i/o executor pool of the CheckpointCoordinator. There is no guarantee that this discard method has been executed when the job is running again (and the executor shuts down with the dispatcher, hence it is not bound to job restarts).

I'll open a small PR to harden the test.;;;","29/Apr/21 18:35;rmetzger;master https://github.com/apache/flink/commit/00584d3180890e91c6dc8457bd45d98ec11c6652
release-1.13 https://github.com/apache/flink/commit/229669cf16883b80c00c2b3aad75effc962d16d6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisTableApiITCase with wrong results,FLINK-22492,13375336,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,dwysakowicz,dwysakowicz,27/Apr/21 12:47,23/Sep/21 17:52,13/Jul/23 08:07,26/Jun/21 06:00,1.13.0,,,,,,,,1.13.2,1.14.0,,,,,,Connectors / Kinesis,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17280&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27178

{code}
Apr 27 12:26:04 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 59.289 s <<< FAILURE! - in org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase
Apr 27 12:26:04 [ERROR] testTableApiSourceAndSink(org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase)  Time elapsed: 59.283 s  <<< FAILURE!
Apr 27 12:26:04 java.lang.AssertionError: expected:<3> but was:<0>
Apr 27 12:26:04 	at org.junit.Assert.fail(Assert.java:88)
Apr 27 12:26:04 	at org.junit.Assert.failNotEquals(Assert.java:834)
Apr 27 12:26:04 	at org.junit.Assert.assertEquals(Assert.java:645)
Apr 27 12:26:04 	at org.junit.Assert.assertEquals(Assert.java:631)
Apr 27 12:26:04 	at org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.testTableApiSourceAndSink(KinesisTableApiITCase.java:121)
Apr 27 12:26:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 27 12:26:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 27 12:26:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 27 12:26:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 27 12:26:04 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 27 12:26:04 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 27 12:26:04 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 27 12:26:04 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 27 12:26:04 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
Apr 27 12:26:04 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
Apr 27 12:26:04 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Apr 27 12:26:04 	at java.lang.Thread.run(Thread.java:748)

{code}",,dwysakowicz,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 26 05:59:59 UTC 2021,,,,,,,,,,"0|z0qi0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/21 23:04;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Jun/21 23:21;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/21 20:41;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18797&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27740;;;","09/Jun/21 04:54;xtsong;[~AHeise], could you help take a look at this issue?;;;","24/Jun/21 12:42;dwysakowicz;{code}
Jun 24 12:36:04 [ERROR] testTableApiSourceAndSink(org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase)  Time elapsed: 21.093 s  <<< FAILURE!
Jun 24 12:36:04 java.lang.AssertionError: expected:<[org.apache.flink.streaming.kinesis.test.model.Order@bed, org.apache.flink.streaming.kinesis.test.model.Order@c11, org.apache.flink.streaming.kinesis.test.model.Order@c35]> but was:<[]>
Jun 24 12:36:04 	at org.junit.Assert.fail(Assert.java:88)
Jun 24 12:36:04 	at org.junit.Assert.failNotEquals(Assert.java:834)
Jun 24 12:36:04 	at org.junit.Assert.assertEquals(Assert.java:118)
Jun 24 12:36:04 	at org.junit.Assert.assertEquals(Assert.java:144)
Jun 24 12:36:04 	at org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.testTableApiSourceAndSink(KinesisTableApiITCase.java:111)
Jun 24 12:36:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 24 12:36:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 24 12:36:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 24 12:36:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Jun 24 12:36:04 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Jun 24 12:36:04 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jun 24 12:36:04 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Jun 24 12:36:04 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jun 24 12:36:04 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jun 24 12:36:04 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jun 24 12:36:04 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Jun 24 12:36:04 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jun 24 12:36:04 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Jun 24 12:36:04 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Jun 24 12:36:04 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Jun 24 12:36:04 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Jun 24 12:36:04 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Jun 24 12:36:04 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Jun 24 12:36:04 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Jun 24 12:36:04 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Jun 24 12:36:04 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Jun 24 12:36:04 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Jun 24 12:36:04 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
Jun 24 12:36:04 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
Jun 24 12:36:04 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jun 24 12:36:04 	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19464&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27340;;;","24/Jun/21 15:49;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19480&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=28029;;;","25/Jun/21 07:02;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19506&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27469;;;","25/Jun/21 21:42;arvid;Merged into master as 66120f98066c690d10bcca46c9e78767be5bec1b.;;;","26/Jun/21 05:58;arvid;Merged into 1.13 as b06862333119359afba6a8f43ed08a55a7c7e57b.;;;","26/Jun/21 05:59;arvid;See for FLINK-23009 for different issue with the same test.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
subtask backpressure indicator shows value for entire job,FLINK-22489,13375284,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,alpinegizmo,alpinegizmo,27/Apr/21 09:48,18/Jun/21 09:16,13/Jul/23 08:07,28/Apr/21 13:23,1.10.3,1.11.3,1.12.2,1.13.0,1.9.3,,,,1.11.4,1.12.4,1.13.1,1.14.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,"In the backpressure tab of the web UI, the OK/LOW/HIGH indication is displaying the job-level backpressure for every subtask, rather than the individual subtask values (effectively showing max back pressure from all of the subtasks of the given task for every subtask, instead of the individual values). 

 !backPressureTab.png! ",,alpinegizmo,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15752,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/21 09:47;alpinegizmo;backPressureTab.png;https://issues.apache.org/jira/secure/attachment/13024656/backPressureTab.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 28 13:23:40 UTC 2021,,,,,,,,,,"0|z0qhow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/21 13:23;pnowojski;Merged to master as 778483a0a38
Merged to release-1.13 as b808f9199a1
Merged to release-1.12 as 00d3356d865
Merged to release-1.11 as 7a899941f48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Kinesis][Consumer] Potential lock-up under error condition,FLINK-22479,13375061,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dannycranmer,dannycranmer,dannycranmer,26/Apr/21 12:59,28/May/21 11:06,13/Jul/23 08:07,10/May/21 13:34,1.12.0,1.12.1,1.12.2,1.12.3,1.13.0,,,,1.12.4,1.13.1,1.14.0,,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,,"*Background*
This connector has been [forked|https://github.com/awslabs/amazon-kinesis-connector-flink] by AWS for use on KDA with Flink 1.11. Bugs have been encountered:
- Under high backpressure scenarios
- When an error is thrown during tear down

*Scope*
Pull in the following fixes from AWS fork:
* Fix issue where {{KinesisDataFetcher.shutdownFetcher()}} hangs ([issue|https://github.com/awslabs/amazon-kinesis-connector-flink/issues/23], [pull request|https://github.com/awslabs/amazon-kinesis-connector-flink/pull/24])
  
* Log error when shutting down Kinesis Data Fetcher ([issue|https://github.com/awslabs/amazon-kinesis-connector-flink/issues/22], [pull request|https://github.com/awslabs/amazon-kinesis-connector-flink/pull/25])
  
* Treating TimeoutException as Recoverable Exception ([issue|https://github.com/awslabs/amazon-kinesis-connector-flink/pull/28], [pull request|https://github.com/awslabs/amazon-kinesis-connector-flink/issues/21])
  
* Add time-out for acquiring subscription and passing events from network to source thread to prevent deadlock ([pull request|https://github.com/awslabs/amazon-kinesis-connector-flink/pull/18]) ",,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-26 12:59:50.0,,,,,,,,,,"0|z0qgbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer starts with NoSuchFileException,FLINK-22469,13374981,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gsomogyi,gsomogyi,gsomogyi,26/Apr/21 05:52,18/Oct/21 07:49,13/Jul/23 08:07,27/Apr/21 13:33,1.12.2,1.14.0,,,,,,,1.14.0,,,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,"When history server started initially it throws the following exception:
{code:java}
2021-04-23 23:25:17,487 ERROR org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher [] - Failed to update job overview.
java.nio.file.NoSuchFileException: /var/folders/jd/35_sh46s7zq0qc6khfw8hc800000gn/T/flink-web-history-35a77053-0e52-4cb3-8d22-626e3ce3cbd7/jobs/overview.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[?:1.8.0_282]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.8.0_282]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.8.0_282]
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) ~[?:1.8.0_282]
	at java.nio.file.Files.newByteChannel(Files.java:361) ~[?:1.8.0_282]
	at java.nio.file.Files.createFile(Files.java:632) ~[?:1.8.0_282]
	at org.apache.flink.runtime.webmonitor.history.HistoryServer.createOrGetFile(HistoryServer.java:324) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.updateJobOverview(HistoryServerArchiveFetcher.java:464) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.access$000(HistoryServerArchiveFetcher.java:74) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher$JobArchiveFetcherTask.<init>(HistoryServerArchiveFetcher.java:199) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.<init>(HistoryServerArchiveFetcher.java:124) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServer.<init>(HistoryServer.java:230) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServer.<init>(HistoryServer.java:146) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServer$1.call(HistoryServer.java:130) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServer$1.call(HistoryServer.java:127) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.history.HistoryServer.main(HistoryServer.java:126) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
{code}
The issue is that ""webDir"" not yet created by ""HistoryServer"" when ""HistoryServerArchiveFetcher"" tries to reach ""jobs/overview.json"" file in it.
",,gsomogyi,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 27 13:33:34 UTC 2021,,,,,,,,,,"0|z0qftk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/21 13:33;gyfora;Merged to master: fa19ba2acbe878b0c454f1ad93d0c412ab6ed380;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure hangs with `AdaptiveScheduler`,FLINK-22464,13374970,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,maguowei,maguowei,26/Apr/21 03:57,23/Sep/21 17:29,13/Jul/23 08:07,25/Jun/21 14:20,1.13.1,1.14.0,,,,,,,1.13.2,1.14.0,,,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17178&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=8171


{code:java}

	2021-05-10T02:56:09.3603584Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f677000b800 nid=0x40e4 waiting on condition [0x00007f6776cc8000]
2021-05-10T02:56:09.3604176Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2021-05-10T02:56:09.3604468Z 	at java.lang.Thread.sleep(Native Method)
2021-05-10T02:56:09.3604925Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:237)
2021-05-10T02:56:09.3605582Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:113)
2021-05-10T02:56:09.3606205Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2021-05-10T02:56:09.3606924Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-05-10T02:56:09.3607469Z 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1320)
2021-05-10T02:56:09.3607996Z 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1303)
2021-05-10T02:56:09.3608616Z 	at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.runTest(OperatorEventSendingCheckpointITCase.java:223)
2021-05-10T02:56:09.3609378Z 	at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure(OperatorEventSendingCheckpointITCase.java:135)
2021-05-10T02:56:09.3609968Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-10T02:56:09.3610386Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-10T02:56:09.3610858Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-10T02:56:09.3611295Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-10T02:56:09.3611703Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-10T02:56:09.3612207Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-10T02:56:09.3612774Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-10T02:56:09.3613470Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-10T02:56:09.3613930Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-10T02:56:09.3614401Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-10T02:56:09.3614770Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-10T02:56:09.3615138Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-10T02:56:09.3615584Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-10T02:56:09.3616070Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-10T02:56:09.3616487Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-10T02:56:09.3616962Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-10T02:56:09.3617361Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-10T02:56:09.3617785Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-10T02:56:09.3618209Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-10T02:56:09.3618635Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-05-10T02:56:09.3619101Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-10T02:56:09.3619507Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-10T02:56:09.3619879Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-05-10T02:56:09.3620239Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-05-10T02:56:09.3620596Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-10T02:56:09.3621009Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-10T02:56:09.3621406Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-10T02:56:09.3621906Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-10T02:56:09.3622319Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-10T02:56:09.3622843Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-10T02:56:09.3623263Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-05-10T02:56:09.3623739Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-05-10T02:56:09.3624332Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-05-10T02:56:09.3624854Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-05-10T02:56:09.3625342Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-05-10T02:56:09.3625851Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-05-10T02:56:09.3626385Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-10T02:56:09.3626978Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-10T02:56:09.3627473Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-10T02:56:09.3627912Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}
",,dwysakowicz,maguowei,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 25 14:20:22 UTC 2021,,,,,,,,,,"0|z0qfr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/21 04:54;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17491&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=8151;;;","10/May/21 06:22;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17761&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=7709;;;","10/May/21 13:45;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9076&view=logs&j=39259e64-fab3-5f14-656b-9c348703dab5&t=2f155df2-a089-542d-f56b-bb9d11f597f3;;;","19/May/21 10:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","25/May/21 07:43;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18292&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228;;;","25/May/21 08:12;rmetzger;The issue is reproducible 100% of the time locally with current master.
The job is in a restart loop due to this error:

{code}
22:48:45,251 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2 (type=CHECKPOINT) @ 1621896525250 for job 566fd0f105339e3cecdfb994df6e1cf9.
22:48:45,251 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder [] - Coordinator checkpoint 2 for coordinator cbc357ccb763df2852fee8c4fc7d55f2 is awaiting 1 pending events
22:48:45,704 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint 2 for job 566fd0f105339e3cecdfb994df6e1cf9. (1 consecutive failed attempts so far)
org.apache.flink.util.FlinkException: Failing OperatorCoordinator checkpoint because some OperatorEvents before this checkpoint barrier were not received by the target tasks.
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$completeCheckpointOnceEventsAreDone$4(OperatorCoordinatorHolder.java:344) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:?]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
	at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:905) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$23(FutureUtils.java:1356) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
	at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1255) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:217) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$orTimeout$15(FutureUtils.java:582) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_282]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_282]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_282]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_282]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
22:48:45,706 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: numbers -> Map -> Sink: Data stream collect sink (1/1) (fe486f6c9a46e270c73a9832d6e2aab5) switched from RUNNING to FAILED on 1072e91c-cd0e-4c5a-99fe-46fa9f65f579 @ localhost (dataPort=-1).
org.apache.flink.util.FlinkException: An OperatorEvent from an OperatorCoordinator to a task was lost. Triggering task failover to ensure consistency. Event: 'AddSplitEvents[[[B@57ec5d1f]]', targetTask: Source: numbers -> Map -> Sink: Data stream collect sink (1/1) - execution #0
	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.lambda$sendEvent$0(SubtaskGatewayImpl.java:81) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:?]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_282]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
Caused by: java.util.concurrent.TimeoutException
	at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1255) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:217) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$orTimeout$15(FutureUtils.java:582) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_282]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_282]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_282]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_282]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_282]
22:48:45,707 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution fe486f6c9a46e270c73a9832d6e2aab5.
22:48:45,707 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0 (fe486f6c9a46e270c73a9832d6e2aab5).
22:48:45,707 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0 (fe486f6c9a46e270c73a9832d6e2aab5) switched from RUNNING to CANCELING.
22:48:45,707 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0 (fe486f6c9a46e270c73a9832d6e2aab5).
22:48:45,707 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Restarting job.
{code};;;","25/May/21 11:11;rmetzger;Actually, the test seems to pass regularly on CI. Yesterday, it finished after 9 minutes. 
With the regular scheduler, the test finishes in under 5 seconds on CI.

Even if the test is not always failing with Adaptive Scheduler, the test should not take significantly more time with Adaptive Scheduler.

In AS, it seems that {{AssignAfterCheckpointEnumerator.snapshotState()}} doesn't get called anymore after the first exception occurred.

[~sewen] could you take a look at the test failure?;;;","25/May/21 12:43;rmetzger;Disabled test for AS in https://github.com/apache/flink/commit/80ad5b3b511a68cce19a53291000c9936e10db17;;;","01/Jun/21 07:47;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18467&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=7992;;;","01/Jun/21 10:18;trohrmann;The test now seems to also fail with the {{DefaultScheduler}}.;;;","01/Jun/21 11:53;rmetzger;It is still only failing with AS, but I only disabled it in master, not in the release-1.13 branch. I've now deactivated it there as well: feac87eb5a2ab66c04182e9b0d866571678a3535;;;","01/Jun/21 11:55;rmetzger;Once I have time I'll look into this failure again.;;;","01/Jun/21 15:40;trohrmann;Ah ok, thanks for the clarification [~rmetzger].;;;","08/Jun/21 22:43;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","25/Jun/21 14:20;rmetzger;Resolved in master: https://github.com/apache/flink/commit/be15eb0f01175acc8cfa1c146caa6f12778941cf
Resolved in release-1.13: https://github.com/apache/flink/commit/cd54b584371b67e67015d104d34c2f19a446e2b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalArgumentException is thrown in WindowAttachedWindowingStrategy when two phase is enabled for distinct agg,FLINK-22463,13374969,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,26/Apr/21 03:53,28/May/21 11:03,13/Jul/23 08:07,26/Apr/21 09:16,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Caused by: java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
	at org.apache.flink.table.planner.plan.logical.WindowAttachedWindowingStrategy.<init>(WindowAttachedWindowingStrategy.java:51)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)


The reason is the {{windowStart}} may be {{-1}} when two phase is enabled for distinct agg, see [TwoStageOptimizedWindowAggregateRule.java#L143|https://github.com/apache/flink/blob/a3363b91b144edfbae5ab114984ded622d3f8fbc/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/physical/stream/TwoStageOptimizedWindowAggregateRule.java#L143]",,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 26 09:16:25 UTC 2021,,,,,,,,,,"0|z0qfqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/21 09:16;godfreyhe;Fixed in 1.13.0: b813e7289b013fe077f048562ef5f4118bae4a3c
Fixed in 1.14.0: 1f31505a7ab1cca31a99282f1ccc4703a102abcb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcExactlyOnceSinkE2eTest.testInsert failed because of too many clients.,FLINK-22462,13374968,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,maguowei,maguowei,26/Apr/21 03:48,23/Sep/21 17:54,13/Jul/23 08:07,30/Jun/21 09:07,1.13.1,1.14.0,,,,,,,1.13.2,1.14.0,,,,,,Connectors / JDBC,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17178&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=13514


{code:java}
Apr 25 23:05:31 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 138.743 s <<< FAILURE! - in org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest
Apr 25 23:05:31 [ERROR] testInsert(org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest)  Time elapsed: 137.267 s  <<< ERROR!
Apr 25 23:05:31 org.postgresql.util.PSQLException: FATAL: sorry, too many clients already
Apr 25 23:05:31 	at org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:524)
Apr 25 23:05:31 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:145)
Apr 25 23:05:31 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:196)
Apr 25 23:05:31 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
Apr 25 23:05:31 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:211)
Apr 25 23:05:31 	at org.postgresql.Driver.makeConnection(Driver.java:459)
Apr 25 23:05:31 	at org.postgresql.Driver.connect(Driver.java:261)
Apr 25 23:05:31 	at java.sql.DriverManager.getConnection(DriverManager.java:664)
Apr 25 23:05:31 	at java.sql.DriverManager.getConnection(DriverManager.java:247)
Apr 25 23:05:31 	at org.apache.flink.connector.jdbc.xa.JdbcXaFacadeTestHelper.getInsertedIds(JdbcXaFacadeTestHelper.java:81)
Apr 25 23:05:31 	at org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.testInsert(JdbcExactlyOnceSinkE2eTest.java:119)
Apr 25 23:05:31 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

{code}
",,maguowei,maver1ck,roman,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22889,,,,,,,,FLINK-24209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 20 14:16:20 UTC 2021,,,,,,,,,,"0|z0qfqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/21 07:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17749&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407&l=13119;;;","20/May/21 08:14;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18130&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13457;;;","22/Jun/21 07:45;roman;[~AHeise] could you please review the PR (https://github.com/apache/flink/pull/15823)?;;;","22/Jun/21 22:35;roman;Merged
into 1.13 as d14b5490badf31af64e0763c9d1111a728111a54..b2779de1536e38c369142b9fd4f66886d1af6d0e
into master as 95230f56fe2c5f320c4bd87860fdcc1ee753d24b..33ff046c79218897ab567dbe5aa85452d7d5dbdf

Thanks [~ym] and [~arvid] for reviewing.;;;","24/Jun/21 03:06;xtsong;Seems the problem still exist.

Instance on master, with the previous fix included.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19415&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13199;;;","24/Jun/21 09:23;roman;Reposting from FLINK-22889:

1.13: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19436&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=16533];;;","30/Jun/21 09:07;roman;Test fix merged into master as e72f4d06b20b2dca4bd090091ee663cb9fa90630

and into 1.13 as 46fdc5fb0207cd5671d3404a551bd27bdedb554d.;;;","20/Jul/21 12:16;maver1ck;[~roman_khachatryan] 
Could this bug be related ? I'm facing similar issues with Oracle and fix from this PR doesn't work.
https://issues.apache.org/jira/browse/FLINK-23437

 ;;;","20/Jul/21 13:53;roman;[~maver1ck]

Yes, it seems related actually. Could you try to reproduce it in 1.13.2 (or newer)?;;;","20/Jul/21 14:16;maver1ck;I can reproduce this on 1.13.2-rc2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to translate Lookup Join when join on a CAST expression on dimention table column,FLINK-22454,13374887,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wenlong.lwl,fsk119,fsk119,25/Apr/21 09:27,23/Sep/21 17:25,13/Jul/23 08:07,10/Jun/21 02:26,1.13.0,,,,,,,,1.14.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Please add test in {{LookupJoinTest}}
{code:java}
 def before(): Unit ={
    util.addDataStream[(Int, String, Long)](
      ""MyTable"", 'a, 'b, 'c, 'proctime.proctime, 'rowtime.rowtime)

    if (legacyTableSource) {
      TestTemporalTable.createTemporaryTable(util.tableEnv, ""LookupTable"")
    } else {
      util.addTable(
        """"""
          |CREATE TABLE LookupTable (
          |  `id` DECIMAL(38, 10),
          |  `to_qty` DECIMAL(38, 10),
          |  `name` STRING,
          |  `age` INT,
          |  `id_int` as CAST(`id` AS INT)
          |) WITH (
          |  'connector' = 'values'
          |)
          |"""""".stripMargin)
  }
{code}
{code:java}
@Test
  def test(): Unit = {
    val sql =
    """"""
    |SELECT MyTable.b, LookupTable.`to_qty`
    |FROM MyTable
    |LEFT JOIN LookupTable FOR SYSTEM_TIME AS OF MyTable.`proctime`
    |ON MyTable.a = CAST(LookupTable.`id` as INT)
    |"""""".stripMargin

    util.tableEnv.sqlQuery(sql).explain()
  }

{code}


The exception stack is 

{code}
org.apache.flink.table.api.TableException: Temporal table join requires equivalent condition of the same type, but the condition is a[INT]=id[DECIMAL(38, 10)]

	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.validateLookupKeyType(CommonExecLookupJoin.java:303)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.translateToPlanInternal(CommonExecLookupJoin.java:222)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:247)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:88)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:70)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:69)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:69)
	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:104)
	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:46)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:691)
	at org.apache.flink.table.api.internal.TableImpl.explain(TableImpl.java:582)
	at org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.test(LookupJoinTest.scala:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)


{code}",,fsk119,godfreyhe,jark,jingzhang,leonard,wenlong.lwl,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 10 02:26:58 UTC 2021,,,,,,,,,,"0|z0qf8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/21 05:59;wenlong.lwl;Hi, [~fsk119], I think is error is by designed. Lookup join must have look up keys in join condition, we ignored casting on field ref while extracting lookup keys(see CommonPhysicalLookupJoin#getIdenticalSourceField), in order to support lookup keys which have interoperable types(see PlannerTypeUtils#isInteroperable), and added  a check on the type of lookup keys to make sure that the types is really interoperable. 

In your case, the cast can not be ignored(int and decimal are not interoperable), so planner throws error about type not compatible on lookup key.;;;","10/Jun/21 02:26;godfreyhe;Fixed in 1.14.0: f3936b20f488acf1b2592e4f0d5a5da02f444864;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkRexUtil create Sarg String array elemet supplement space,FLINK-22448,13374875,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Akihito Liang,Akihito Liang,25/Apr/21 08:08,12/May/21 02:46,13/Jul/23 08:07,12/May/21 02:46,,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,,,,,,"As we know, the new version of Calcite introduces the {{SEARCH}} rex call to express range conditions. But when i used string array to express range in the StreamSQL, i found that some string in the array had problems with the completion length by using space.

the following query:

 
{code:java}
create view tempView as
  select * from sourceTable where action in ('systemnotifyv2', 'session_auth', 'create_session', 'close_single_chat'){code}
after Sarg operator created, the result is :

 

 
{code:java}
create view tempView as
 select * from sourceTable where action in ('systemnotifyv2', 'session_auth  ', 'create_session', 'close_single_chat')
{code}
I debuged to see why dose the happans. After calling  rexBuilder.makeLiteral in

FlinkRexUtil#expandSearchOperands, the string 'session_auth' became 'session_auth  '.And i also found that the type and length of the string array were determined by the first string in the array.Just like my example above, the type of the array was Char and the length of the array was 14.the length of 'session_auth' string was 12 so that calcite would supplement  2 space to make it meet the length of 14.

Now, i All I can think of is adding trim parameter to remove the space。do you have a better way to fix or avoid the problem happens?

 ",,Akihito Liang,godfreyhe,jark,wenlong.lwl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22486,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 12 02:25:13 UTC 2021,,,,,,,,,,"0|z0qf68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/21 02:25;wenlong.lwl;hi, [~Akihito Liang], as tested in Flink-22486, I think this issue didn't existed now, in both master and release-1.13, the result type of in would be correct:  varchar(max of the literal);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop async checkpoint description of state backends in Chinese docs,FLINK-22444,13374851,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,25/Apr/21 02:45,28/May/21 11:03,13/Jul/23 08:07,26/Apr/21 07:00,1.13.0,,,,,,,,1.13.0,1.14.0,,,,,,Documentation,,,,,0,pull-request-available,,,,,"We should remove async checkpoint description of state backends in Chinese documentation  as FLINK-21935 had removed ""state.backend.async"" option. As no one had taken the ticket FLINK-22146, we should at least remove documentation of not-supported features.",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21935,,,,,,,,,,,,,,,,,,,,,FLINK-22146,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 26 07:00:30 UTC 2021,,,,,,,,,,"0|z0qf0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/21 07:00;yunta;Merged:
master: f9d3245ade6f0ffb1be633148bd97c4c2aabc2a2

release-1.13: 50e74bc86a7df3f669878af9c1facff8503b098e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can not be execute an extreme long sql under batch mode,FLINK-22443,13374849,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,macdoor615,macdoor615,25/Apr/21 02:33,15/Jul/21 02:01,13/Jul/23 08:07,08/Jul/21 06:36,1.12.2,,,,,,,,1.12.5,1.13.2,1.14.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,stale-blocker,stale-critical,,,"1. execute an extreme long sql under batch mode

 
{code:java}
select
'CD' product_name,
r.code business_platform,
5 statisticperiod,
cast('2021-03-24 00:00:00' as timestamp) coltime,
cast(r1.indicatorvalue as double) as YWPT_ZHQI_CD_038_GZ_00002,
cast(r2.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00007,
cast(r3.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00005,
cast(r4.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00006,
cast(r5.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00029,
cast(r6.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00028,
cast(r7.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00015,
cast(r8.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00014,
cast(r9.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00011,
cast(r10.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00010,
cast(r11.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00013,
cast(r12.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00012,
cast(r13.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00027,
cast(r14.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00026,
cast(r15.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00046,
cast(r16.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00047,
cast(r17.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00049,
cast(r18.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00048,
cast(r19.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00024,
cast(r20.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00025,
cast(r21.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00022,
cast(r22.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00023,
cast(r23.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00054,
cast(r24.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00055,
cast(r25.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00033,
cast(r26.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00032,
cast(r27.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00053,
cast(r28.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00052,
cast(r29.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00051,
cast(r30.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00050,
cast(r31.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00043,
cast(r32.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00042,
cast(r33.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00017,
cast(r34.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00016,
cast(r35.indicatorvalue as double) as YWPT_ZHQI_CD_038_GZ_00003,
cast(r36.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00045,
cast(r37.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00044,
cast(r38.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00038,
cast(r39.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00039,
cast(r40.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00037,
cast(r41.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00036,
cast(r42.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00040,
cast(r43.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00041,
cast(r44.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00034,
cast(r45.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00035,
cast(r46.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00030,
cast(r47.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00031,
cast(r48.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00020,
cast(r49.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00021,
cast(r50.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00018,
cast(r51.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00019,
cast(r52.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00004,
cast(r53.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00008,
cast(r54.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00061,
cast(r55.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00009,
localtimestamp as crtime,
'2021-03-24' as dt
from prod_mysql_bnpmp.r_biz_product r
left join raw_restapi_load.p_hcd r1 on r1.dt='2021-03-24' and r1.coltime =cast('2021-03-24 00:00:00' as timestamp) and r1.businessplatform=r.code and r1.indicatornumber='YWPT-ZHQI-CD-038-GZ-00002'
left join raw_restapi_load.p_hcd r2 on r2.dt='2021-03-24' and r2.coltime =cast('2021-03-24 00:00:00' as timestamp) and r2.businessplatform=r.code and r2.indicatornumber='YWPT-ZHQI-CD-038-YW-00007'
left join raw_restapi_load.p_hcd r3 on r3.dt='2021-03-24' and r3.coltime =cast('2021-03-24 00:00:00' as timestamp) and r3.businessplatform=r.code and r3.indicatornumber='YWPT-ZHQI-CD-038-YW-00005'
left join raw_restapi_load.p_hcd r4 on r4.dt='2021-03-24' and r4.coltime =cast('2021-03-24 00:00:00' as timestamp) and r4.businessplatform=r.code and r4.indicatornumber='YWPT-ZHQI-CD-038-YW-00006'
left join raw_restapi_load.p_hcd r5 on r5.dt='2021-03-24' and r5.coltime =cast('2021-03-24 00:00:00' as timestamp) and r5.businessplatform=r.code and r5.indicatornumber='YWPT-ZHQI-CD-038-XT-00029'
left join raw_restapi_load.p_hcd r6 on r6.dt='2021-03-24' and r6.coltime =cast('2021-03-24 00:00:00' as timestamp) and r6.businessplatform=r.code and r6.indicatornumber='YWPT-ZHQI-CD-038-XT-00028'
left join raw_restapi_load.p_hcd r7 on r7.dt='2021-03-24' and r7.coltime =cast('2021-03-24 00:00:00' as timestamp) and r7.businessplatform=r.code and r7.indicatornumber='YWPT-ZHQI-CD-038-XT-00015'
left join raw_restapi_load.p_hcd r8 on r8.dt='2021-03-24' and r8.coltime =cast('2021-03-24 00:00:00' as timestamp) and r8.businessplatform=r.code and r8.indicatornumber='YWPT-ZHQI-CD-038-XT-00014'
left join raw_restapi_load.p_hcd r9 on r9.dt='2021-03-24' and r9.coltime =cast('2021-03-24 00:00:00' as timestamp) and r9.businessplatform=r.code and r9.indicatornumber='YWPT-ZHQI-CD-038-XT-00011'
left join raw_restapi_load.p_hcd r10 on r10.dt='2021-03-24' and r10.coltime =cast('2021-03-24 00:00:00' as timestamp) and r10.businessplatform=r.code and r10.indicatornumber='YWPT-ZHQI-CD-038-XT-00010'
left join raw_restapi_load.p_hcd r11 on r11.dt='2021-03-24' and r11.coltime =cast('2021-03-24 00:00:00' as timestamp) and r11.businessplatform=r.code and r11.indicatornumber='YWPT-ZHQI-CD-038-XT-00013'
left join raw_restapi_load.p_hcd r12 on r12.dt='2021-03-24' and r12.coltime =cast('2021-03-24 00:00:00' as timestamp) and r12.businessplatform=r.code and r12.indicatornumber='YWPT-ZHQI-CD-038-XT-00012'
left join raw_restapi_load.p_hcd r13 on r13.dt='2021-03-24' and r13.coltime =cast('2021-03-24 00:00:00' as timestamp) and r13.businessplatform=r.code and r13.indicatornumber='YWPT-ZHQI-CD-038-XT-00027'
left join raw_restapi_load.p_hcd r14 on r14.dt='2021-03-24' and r14.coltime =cast('2021-03-24 00:00:00' as timestamp) and r14.businessplatform=r.code and r14.indicatornumber='YWPT-ZHQI-CD-038-XT-00026'
left join raw_restapi_load.p_hcd r15 on r15.dt='2021-03-24' and r15.coltime =cast('2021-03-24 00:00:00' as timestamp) and r15.businessplatform=r.code and r15.indicatornumber='YWPT-ZHQI-CD-038-XT-00046'
left join raw_restapi_load.p_hcd r16 on r16.dt='2021-03-24' and r16.coltime =cast('2021-03-24 00:00:00' as timestamp) and r16.businessplatform=r.code and r16.indicatornumber='YWPT-ZHQI-CD-038-XT-00047'
left join raw_restapi_load.p_hcd r17 on r17.dt='2021-03-24' and r17.coltime =cast('2021-03-24 00:00:00' as timestamp) and r17.businessplatform=r.code and r17.indicatornumber='YWPT-ZHQI-CD-038-XT-00049'
left join raw_restapi_load.p_hcd r18 on r18.dt='2021-03-24' and r18.coltime =cast('2021-03-24 00:00:00' as timestamp) and r18.businessplatform=r.code and r18.indicatornumber='YWPT-ZHQI-CD-038-XT-00048'
left join raw_restapi_load.p_hcd r19 on r19.dt='2021-03-24' and r19.coltime =cast('2021-03-24 00:00:00' as timestamp) and r19.businessplatform=r.code and r19.indicatornumber='YWPT-ZHQI-CD-038-XT-00024'
left join raw_restapi_load.p_hcd r20 on r20.dt='2021-03-24' and r20.coltime =cast('2021-03-24 00:00:00' as timestamp) and r20.businessplatform=r.code and r20.indicatornumber='YWPT-ZHQI-CD-038-XT-00025'
left join raw_restapi_load.p_hcd r21 on r21.dt='2021-03-24' and r21.coltime =cast('2021-03-24 00:00:00' as timestamp) and r21.businessplatform=r.code and r21.indicatornumber='YWPT-ZHQI-CD-038-XT-00022'
left join raw_restapi_load.p_hcd r22 on r22.dt='2021-03-24' and r22.coltime =cast('2021-03-24 00:00:00' as timestamp) and r22.businessplatform=r.code and r22.indicatornumber='YWPT-ZHQI-CD-038-XT-00023'
left join raw_restapi_load.p_hcd r23 on r23.dt='2021-03-24' and r23.coltime =cast('2021-03-24 00:00:00' as timestamp) and r23.businessplatform=r.code and r23.indicatornumber='YWPT-ZHQI-CD-038-XT-00054'
left join raw_restapi_load.p_hcd r24 on r24.dt='2021-03-24' and r24.coltime =cast('2021-03-24 00:00:00' as timestamp) and r24.businessplatform=r.code and r24.indicatornumber='YWPT-ZHQI-CD-038-XT-00055'
left join raw_restapi_load.p_hcd r25 on r25.dt='2021-03-24' and r25.coltime =cast('2021-03-24 00:00:00' as timestamp) and r25.businessplatform=r.code and r25.indicatornumber='YWPT-ZHQI-CD-038-XT-00033'
left join raw_restapi_load.p_hcd r26 on r26.dt='2021-03-24' and r26.coltime =cast('2021-03-24 00:00:00' as timestamp) and r26.businessplatform=r.code and r26.indicatornumber='YWPT-ZHQI-CD-038-XT-00032'
left join raw_restapi_load.p_hcd r27 on r27.dt='2021-03-24' and r27.coltime =cast('2021-03-24 00:00:00' as timestamp) and r27.businessplatform=r.code and r27.indicatornumber='YWPT-ZHQI-CD-038-XT-00053'
left join raw_restapi_load.p_hcd r28 on r28.dt='2021-03-24' and r28.coltime =cast('2021-03-24 00:00:00' as timestamp) and r28.businessplatform=r.code and r28.indicatornumber='YWPT-ZHQI-CD-038-XT-00052'
left join raw_restapi_load.p_hcd r29 on r29.dt='2021-03-24' and r29.coltime =cast('2021-03-24 00:00:00' as timestamp) and r29.businessplatform=r.code and r29.indicatornumber='YWPT-ZHQI-CD-038-XT-00051'
left join raw_restapi_load.p_hcd r30 on r30.dt='2021-03-24' and r30.coltime =cast('2021-03-24 00:00:00' as timestamp) and r30.businessplatform=r.code and r30.indicatornumber='YWPT-ZHQI-CD-038-XT-00050'
left join raw_restapi_load.p_hcd r31 on r31.dt='2021-03-24' and r31.coltime =cast('2021-03-24 00:00:00' as timestamp) and r31.businessplatform=r.code and r31.indicatornumber='YWPT-ZHQI-CD-038-XT-00043'
left join raw_restapi_load.p_hcd r32 on r32.dt='2021-03-24' and r32.coltime =cast('2021-03-24 00:00:00' as timestamp) and r32.businessplatform=r.code and r32.indicatornumber='YWPT-ZHQI-CD-038-XT-00042'
left join raw_restapi_load.p_hcd r33 on r33.dt='2021-03-24' and r33.coltime =cast('2021-03-24 00:00:00' as timestamp) and r33.businessplatform=r.code and r33.indicatornumber='YWPT-ZHQI-CD-038-XT-00017'
left join raw_restapi_load.p_hcd r34 on r34.dt='2021-03-24' and r34.coltime =cast('2021-03-24 00:00:00' as timestamp) and r34.businessplatform=r.code and r34.indicatornumber='YWPT-ZHQI-CD-038-XT-00016'
left join raw_restapi_load.p_hcd r35 on r35.dt='2021-03-24' and r35.coltime =cast('2021-03-24 00:00:00' as timestamp) and r35.businessplatform=r.code and r35.indicatornumber='YWPT-ZHQI-CD-038-GZ-00003'
left join raw_restapi_load.p_hcd r36 on r36.dt='2021-03-24' and r36.coltime =cast('2021-03-24 00:00:00' as timestamp) and r36.businessplatform=r.code and r36.indicatornumber='YWPT-ZHQI-CD-038-XT-00045'
left join raw_restapi_load.p_hcd r37 on r37.dt='2021-03-24' and r37.coltime =cast('2021-03-24 00:00:00' as timestamp) and r37.businessplatform=r.code and r37.indicatornumber='YWPT-ZHQI-CD-038-XT-00044'
left join raw_restapi_load.p_hcd r38 on r38.dt='2021-03-24' and r38.coltime =cast('2021-03-24 00:00:00' as timestamp) and r38.businessplatform=r.code and r38.indicatornumber='YWPT-ZHQI-CD-038-XT-00038'
left join raw_restapi_load.p_hcd r39 on r39.dt='2021-03-24' and r39.coltime =cast('2021-03-24 00:00:00' as timestamp) and r39.businessplatform=r.code and r39.indicatornumber='YWPT-ZHQI-CD-038-XT-00039'
left join raw_restapi_load.p_hcd r40 on r40.dt='2021-03-24' and r40.coltime =cast('2021-03-24 00:00:00' as timestamp) and r40.businessplatform=r.code and r40.indicatornumber='YWPT-ZHQI-CD-038-XT-00037'
left join raw_restapi_load.p_hcd r41 on r41.dt='2021-03-24' and r41.coltime =cast('2021-03-24 00:00:00' as timestamp) and r41.businessplatform=r.code and r41.indicatornumber='YWPT-ZHQI-CD-038-XT-00036'
left join raw_restapi_load.p_hcd r42 on r42.dt='2021-03-24' and r42.coltime =cast('2021-03-24 00:00:00' as timestamp) and r42.businessplatform=r.code and r42.indicatornumber='YWPT-ZHQI-CD-038-XT-00040'
left join raw_restapi_load.p_hcd r43 on r43.dt='2021-03-24' and r43.coltime =cast('2021-03-24 00:00:00' as timestamp) and r43.businessplatform=r.code and r43.indicatornumber='YWPT-ZHQI-CD-038-XT-00041'
left join raw_restapi_load.p_hcd r44 on r44.dt='2021-03-24' and r44.coltime =cast('2021-03-24 00:00:00' as timestamp) and r44.businessplatform=r.code and r44.indicatornumber='YWPT-ZHQI-CD-038-XT-00034'
left join raw_restapi_load.p_hcd r45 on r45.dt='2021-03-24' and r45.coltime =cast('2021-03-24 00:00:00' as timestamp) and r45.businessplatform=r.code and r45.indicatornumber='YWPT-ZHQI-CD-038-XT-00035'
left join raw_restapi_load.p_hcd r46 on r46.dt='2021-03-24' and r46.coltime =cast('2021-03-24 00:00:00' as timestamp) and r46.businessplatform=r.code and r46.indicatornumber='YWPT-ZHQI-CD-038-XT-00030'
left join raw_restapi_load.p_hcd r47 on r47.dt='2021-03-24' and r47.coltime =cast('2021-03-24 00:00:00' as timestamp) and r47.businessplatform=r.code and r47.indicatornumber='YWPT-ZHQI-CD-038-XT-00031'
left join raw_restapi_load.p_hcd r48 on r48.dt='2021-03-24' and r48.coltime =cast('2021-03-24 00:00:00' as timestamp) and r48.businessplatform=r.code and r48.indicatornumber='YWPT-ZHQI-CD-038-XT-00020'
left join raw_restapi_load.p_hcd r49 on r49.dt='2021-03-24' and r49.coltime =cast('2021-03-24 00:00:00' as timestamp) and r49.businessplatform=r.code and r49.indicatornumber='YWPT-ZHQI-CD-038-XT-00021'
left join raw_restapi_load.p_hcd r50 on r50.dt='2021-03-24' and r50.coltime =cast('2021-03-24 00:00:00' as timestamp) and r50.businessplatform=r.code and r50.indicatornumber='YWPT-ZHQI-CD-038-XT-00018'
left join raw_restapi_load.p_hcd r51 on r51.dt='2021-03-24' and r51.coltime =cast('2021-03-24 00:00:00' as timestamp) and r51.businessplatform=r.code and r51.indicatornumber='YWPT-ZHQI-CD-038-XT-00019'
left join raw_restapi_load.p_hcd r52 on r52.dt='2021-03-24' and r52.coltime =cast('2021-03-24 00:00:00' as timestamp) and r52.businessplatform=r.code and r52.indicatornumber='YWPT-ZHQI-CD-038-YW-00004'
left join raw_restapi_load.p_hcd r53 on r53.dt='2021-03-24' and r53.coltime =cast('2021-03-24 00:00:00' as timestamp) and r53.businessplatform=r.code and r53.indicatornumber='YWPT-ZHQI-CD-038-YW-00008'
left join raw_restapi_load.p_hcd r54 on r54.dt='2021-03-24' and r54.coltime =cast('2021-03-24 00:00:00' as timestamp) and r54.businessplatform=r.code and r54.indicatornumber='YWPT-ZHQI-CD-038-XT-00061'
left join raw_restapi_load.p_hcd r55 on r55.dt='2021-03-24' and r55.coltime =cast('2021-03-24 00:00:00' as timestamp) and r55.businessplatform=r.code and r55.indicatornumber='YWPT-ZHQI-CD-038-YW-00009'
where 1=1
and r.code='YWPT-ZHQI-CD-038'
and r.type='biz';{code}
 

2. get error 

 
{code:java}
[ERROR] Could not execute SQL statement. Reason:
java.io.IOException: Can not make a progress: all selected inputs are already finished
{code}
3. execute same sql under streaming mode and get expected output

 ","execute command

 
{code:java}
bin/sql-client.sh embedded -d conf/sql-client-batch.yaml 
{code}
content of conf/sql-client-batch.yaml

 
{code:java}
catalogs:
- name: bnpmphive
  type: hive
  hive-conf-dir: /home/gum/hive/conf
  hive-version: 3.1.2
execution:
  planner: blink
  type: batch
  #type: streaming
  result-mode: table
  parallelism: 4
  max-parallelism: 2000
  current-catalog: bnpmphive
  #current-database: snmpprobe 
#configuration:
#  table.sql-dialect: hivemodules:
   - name: core
     type: core
   - name: myhive
     type: hivedeployment:
  # general cluster communication timeout in ms
  response-timeout: 5000
  # (optional) address from cluster to gateway
  gateway-address: """"
  # (optional) port from cluster to gateway
  gateway-port: 0
{code}
 ",frank wang,hackergin,jark,libenchao,macdoor615,pnowojski,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/21 15:30;macdoor615;flink-gum-taskexecutor-8-hb3-prod-hadoop-002.log.4.zip;https://issues.apache.org/jira/secure/attachment/13024564/flink-gum-taskexecutor-8-hb3-prod-hadoop-002.log.4.zip","22/May/21 15:04;macdoor615;raw_p_restapi_hcd.csv.zip;https://issues.apache.org/jira/secure/attachment/13025802/raw_p_restapi_hcd.csv.zip",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 08 06:36:57 UTC 2021,,,,,,,,,,"0|z0qf0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/21 14:59;jark;I think there should be more detailed error/exception information in {{logs/sql-client.log}} or in {{taskmanager.log}}. 
[~macdoor615] could you check them or upload them (if possible) ?;;;","25/Apr/21 15:31;macdoor615;I uploaded taskmanager log, flink-gum-taskexecutor-8-hb3-prod-hadoop-002.log.4.zip

please search for ""all selected inputs are already finished"";;;","26/Apr/21 02:34;jark;Thanks [~macdoor615], I found the exception in the log:

{code}
java.io.IOException: Can not make a progress: all selected inputs are already finished
	at org.apache.flink.streaming.runtime.io.MultipleInputSelectionHandler.calculateOverallStatus(MultipleInputSelectionHandler.java:95) ~[flink-dist_2.11-1.12.2.jar:1.12.2]
	at org.apache.flink.streaming.runtime.io.MultipleInputSelectionHandler.updateStatus(MultipleInputSelectionHandler.java:81) ~[flink-dist_2.11-1.12.2.jar:1.12.2]
	at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:88) ~[flink-dist_2.11-1.12.2.jar:1.12.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:396) ~[flink-dist_2.11-1.12.2.jar:1.12.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191) ~[flink-dist_2.11-1.12.2.jar:1.12.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:617) ~[flink-dist_2.11-1.12.2.jar:1.12.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:581) ~[flink-dist_2.11-1.12.2.jar:1.12.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) [flink-dist_2.11-1.12.2.jar:1.12.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) [flink-dist_2.11-1.12.2.jar:1.12.2]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{code}

It seems the are optimized into multi-input transformation, and there may be a bug in the multi-input. [~godfreyhe], could you also help to have a look?;;;","19/May/21 10:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/May/21 00:54;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","22/May/21 04:14;frank wang;can you give your table schema, maybe it can help me mock test to solve this problem, thks,[~macdoor615];;;","22/May/21 09:50;macdoor615;{code:java}
drop table if exists raw_restapi_rotated.hcd_biz;
create table raw_restapi_rotated.hcd_biz
(
product_name varchar(20),
business_platform varchar(50),
statisticperiod  bigint,
coltime timestamp,
YWPT_ZHQI_CD_038_GZ_00002 double,
YWPT_ZHQI_CD_038_YW_00007 double,
YWPT_ZHQI_CD_038_YW_00005 double,
YWPT_ZHQI_CD_038_YW_00006 double,
YWPT_ZHQI_CD_038_XT_00029 double,
YWPT_ZHQI_CD_038_XT_00028 double,
YWPT_ZHQI_CD_038_XT_00015 double,
YWPT_ZHQI_CD_038_XT_00014 double,
YWPT_ZHQI_CD_038_XT_00011 double,
YWPT_ZHQI_CD_038_XT_00010 double,
YWPT_ZHQI_CD_038_XT_00013 double,
YWPT_ZHQI_CD_038_XT_00012 double,
YWPT_ZHQI_CD_038_XT_00027 double,
YWPT_ZHQI_CD_038_XT_00026 double,
YWPT_ZHQI_CD_038_XT_00046 double,
YWPT_ZHQI_CD_038_XT_00047 double,
YWPT_ZHQI_CD_038_XT_00049 double,
YWPT_ZHQI_CD_038_XT_00048 double,
YWPT_ZHQI_CD_038_XT_00024 double,
YWPT_ZHQI_CD_038_XT_00025 double,
YWPT_ZHQI_CD_038_XT_00022 double,
YWPT_ZHQI_CD_038_XT_00023 double,
YWPT_ZHQI_CD_038_XT_00054 double,
YWPT_ZHQI_CD_038_XT_00055 double,
YWPT_ZHQI_CD_038_XT_00033 double,
YWPT_ZHQI_CD_038_XT_00032 double,
YWPT_ZHQI_CD_038_XT_00053 double,
YWPT_ZHQI_CD_038_XT_00052 double,
YWPT_ZHQI_CD_038_XT_00051 double,
YWPT_ZHQI_CD_038_XT_00050 double,
YWPT_ZHQI_CD_038_XT_00043 double,
YWPT_ZHQI_CD_038_XT_00042 double,
YWPT_ZHQI_CD_038_XT_00017 double,
YWPT_ZHQI_CD_038_XT_00016 double,
YWPT_ZHQI_CD_038_GZ_00003 double,
YWPT_ZHQI_CD_038_XT_00045 double,
YWPT_ZHQI_CD_038_XT_00044 double,
YWPT_ZHQI_CD_038_XT_00038 double,
YWPT_ZHQI_CD_038_XT_00039 double,
YWPT_ZHQI_CD_038_XT_00037 double,
YWPT_ZHQI_CD_038_XT_00036 double,
YWPT_ZHQI_CD_038_XT_00040 double,
YWPT_ZHQI_CD_038_XT_00041 double,
YWPT_ZHQI_CD_038_XT_00034 double,
YWPT_ZHQI_CD_038_XT_00035 double,
YWPT_ZHQI_CD_038_XT_00030 double,
YWPT_ZHQI_CD_038_XT_00031 double,
YWPT_ZHQI_CD_038_XT_00020 double,
YWPT_ZHQI_CD_038_XT_00021 double,
YWPT_ZHQI_CD_038_XT_00018 double,
YWPT_ZHQI_CD_038_XT_00019 double,
YWPT_ZHQI_CD_038_YW_00004 double,
YWPT_ZHQI_CD_038_YW_00008 double,
YWPT_ZHQI_CD_038_XT_00061 bigint,
YWPT_ZHQI_CD_038_YW_00009 double,
crtime timestamp,
primary key (coltime,product_name,business_platform,statisticperiod) not enforced
)
comment ''
partitioned by (dt string)
stored as orc
tblproperties (
  'table.exec.hive.fallback-mapred-writer' = 'false',
  'auto-compaction'='true',
  'compaction.file-size'='256MB'
);
{code};;;","22/May/21 11:14;frank wang;do you give your source table schema, tks,[~macdoor615];;;","22/May/21 11:23;frank wang;i have init some table schema ,does this ok?[~macdoor615]
{code:java}
//source
create table raw_restapi_rotated.r_biz_product
( 
 code varchar(50), 
 type varchar(20)
)
create table raw_restapi_rotated.p_hcd
( 
    dt timestamp, 
    businessplatform archar(50), 
    coltime date, 
    indicatornumber varchar(50), 
    indicatorvalue bigint
)
{code};;;","22/May/21 14:43;macdoor615;I think your source table schema is okay.

Here is mine.

 
{code:java}
drop table if exists raw_restapi_load.p_hcd;
create table raw_restapi_load.p_hcd
(
 gwid string,
 id string,
 performanceID string,
 productName string,
 businessPlatform string,
 hostName string,
 hostIp string,
 statisticPeriod string,
 beginTime timestamp,
 endTime timestamp,
 indicatorObject string,
 indicatorDimension string,
 indicatorLevel string,
 indicatorNumber string,
 content string,
 indicatorValue string,
 indicatorUnit string,
 indicatorNumeratorValue string,
 indicatorNumeratorUnit string,
 indicatorDenominatorValue string,
 indicatorDenominatorUnit string,
 coltime timestamp,
 crtime timestamp
)
comment ''
PARTITIONED BY (dt string)
STORED AS orc
TBLPROPERTIES (
 'partition.time-extractor.timestamp-pattern'='$dt 00:00:00',
 'sink.rolling-policy.rollover-interval'='15 min',
 'sink.partition-commit.trigger'='process-time',
 'sink.partition-commit.delay'='0 min',
 'sink.partition-commit.policy.kind'='metastore,success-file'
);

drop table if exists prod_mysql_bnpmp.r_biz_product;
create table prod_mysql_bnpmp.r_biz_product
(
id varchar(36),
type varchar(20),
name varchar(100),
abbr varchar(20),
code  varchar(50),
alias varchar(100),
class  varchar(30),
updtime timestamp,
primary key(id) not enforced
)
with
(
'connector' = 'jdbc',
'url' = 'jdbc:mysql://hb3-prod-mysql:32487/gem_bnpmp?rewriteBatchedStatements=true',
'driver'= 'com.mysql.cj.jdbc.Driver',
'username' = 'service1',
'password' = '',
'table-name' = 'r_biz_product'
);

{code};;;","22/May/21 15:07;macdoor615;I uploaded some sample data of table raw_restapi_load.p_hcd (only necessary columns). 

raw_p_restapi_hcd.csv.zip;;;","23/May/21 22:51;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","25/May/21 22:56;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","27/May/21 23:05;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","29/May/21 23:11;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","31/May/21 23:25;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","02/Jun/21 23:30;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Jun/21 23:23;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Jun/21 22:49;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","08/Jun/21 22:44;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Jun/21 22:41;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","12/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Jun/21 22:41;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","16/Jun/21 22:41;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","22/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","24/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","28/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","30/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","02/Jul/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Jul/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","05/Jul/21 05:03;TsReaper;Hi dear Flink community.

I've looked into this issue and found that this is caused by the overflow in the initialization of {{MultipleInputSelectionHandler}}.

Currently line 54 is {{this.allSelectedMask = (1 << inputCount) - 1;}}, however the maximum value of {{inputCount}} is 64 causing the overflow. I'll fix this shortly.;;;","08/Jul/21 06:36;pnowojski;Fixed on master edb3000a7e9
on release-1.13 8057026bb6a
on release-1.12 4812b42d100;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using scala api to change the TimeCharacteristic of the PatternStream is invalid,FLINK-22442,13374819,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wydhcws,wydhcws,wydhcws,24/Apr/21 13:32,28/May/21 11:08,13/Jul/23 08:07,05/May/21 19:12,1.12.0,1.12.1,1.12.2,,,,,,1.12.4,1.13.1,1.14.0,,,,,Library / CEP,,,,,0,pull-request-available,,,,,"Using scala api to change the TimeCharacteristic of the PatternStream is invalid
you can only use the eventTime for PatternStream

the bug is ：
in the code in   org.apache.flink.cep.scala.PatternStream
when we called function like inProcessingTime()
the real JPatternStream in the object not be updated
{code:java}
// org.apache.flink.cep.scala.PatternStream
class PatternStream[T](jPatternStream: JPatternStream[T]) {

  private[flink] def wrappedPatternStream = jPatternStream
 ...... 
 def sideOutputLateData(lateDataOutputTag: OutputTag[T]): PatternStream[T] = {
   jPatternStream.sideOutputLateData(lateDataOutputTag)
   this
 }
  def inProcessingTime(): PatternStream[T] = {
    jPatternStream.inProcessingTime()
    this
  }
  def inEventTime(): PatternStream[T] = {
    jPatternStream.inEventTime()
    this
  }
｝
}
{code}



",,dwysakowicz,wydhcws,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 05 19:12:55 UTC 2021,,,,,,,,,,"0|z0qets:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/21 13:35;wydhcws;I want to fix this bug ，Please assign this fix to me, thanks.[~dwysakowicz]
and 
Test case is :


{code:java}
public class TestCepJava {

    public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException {
        testJPatternStreamChangeTimeCharacteristic(getJPatternStream());
/*
    * the result  is
    * EventTime
    * ProcessingTime
    *
    * the  JPatternStream's  TimeCharacteristic  is be changed
    * */
  }
    }


    public static PatternStream<Event> getJPatternStream(){
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Event> input =
                env.fromElements(
                        new Event(1, ""barfoo"", 1.0),
                        new Event(8, ""end"", 1.0));

        Pattern<Event, ?> pattern =
                Pattern.<Event>begin(""start"")
                        .where(
                                new SimpleCondition<Event>() {

                                    @Override
                                    public boolean filter(Event value) throws Exception {
                                        return value.getName().equals(""start"");
                                    }
                                });
        PatternStream<Event> eventPatternStream = CEP.pattern(input, pattern);
        return eventPatternStream;
    }

    public static <T> void testJPatternStreamChangeTimeCharacteristic(PatternStream<T> estream) throws NoSuchFieldException, IllegalAccessException {
        getTimeBehaviour(estream);
        PatternStream<T> pStream = estream.inProcessingTime();
        getTimeBehaviour(pStream);
    }

    public static <T> void getTimeBehaviour(PatternStream pstream) throws NoSuchFieldException, IllegalAccessException {
        Field builder = pstream.getClass().getDeclaredField(""builder"");
        builder.setAccessible(true);
        Object o = builder.get(pstream);
        Field timeBehaviour = o.getClass().getDeclaredField(""timeBehaviour"");
        timeBehaviour.setAccessible(true);
        System.out.println(timeBehaviour.get(o));
    }
}
{code}



{code:scala}
// Some comments here
//    org.apache.flink.cep.scala.PatternStream
//    org.apache.flink.cep.PatternStream
object TestCepScala {
  def main(args: Array[String]): Unit = {
    updateCepTimeCharacteristicByScalaApi()

    /*
    * the result  is
    * EventTime
    * EventTime
    *
    * the real JPatternStream's  TimeCharacteristic  is not be changed
    * */
  }

  def updateCepTimeCharacteristicByScalaApi(): Unit = {

    //  get  org.apache.flink.cep.PatternStream
    val jestream: org.apache.flink.cep.PatternStream[Event] = TestCepJava.getJPatternStream

    //get org.apache.flink.cep.scala.PatternStream
    val sePstream = new PatternStream[Event](jestream)

    //get and print TimeBehaviour
    getTimeBehaviourFromScalaPatternStream(sePstream)
    //change TimeCharacteristic use scala api
    val sPstream: PatternStream[Event] = sePstream.inProcessingTime()

    //get and print TimeBehaviour
    getTimeBehaviourFromScalaPatternStream(sPstream)


  }

  def getTimeBehaviourFromScalaPatternStream(scPstream: org.apache.flink.cep.scala.PatternStream[Event]): Unit = {
    val field: Field = scPstream.getClass.getDeclaredField(""jPatternStream"")
    field.setAccessible(true)
    val JPattern: AnyRef = field.get(scPstream)
    val stream: cep.PatternStream[Event] = JPattern.asInstanceOf[cep.PatternStream[Event]]
    TestCepJava.getTimeBehaviour(stream)
  }
{code}

;;;","25/Apr/21 00:56;wydhcws;fix  and  add the test in CEPScalaApiPatternStreamTest.scala;;;","05/May/21 19:12;dwysakowicz;Fixed in:
* master
** cb0d8eb85048cfb08be54e75f2f90b1d590b7ae0
* 1.13.1
** 32c64bb4d30499f602079cd34c73371106fe09c3
* 1.12.4
** c9b51568d56b57a905d28b898408166db1f66a15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add numRecordsOut metric for Async IO,FLINK-22438,13374707,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Tony Giao,Tony Giao,Tony Giao,23/Apr/21 17:07,28/May/21 11:06,13/Jul/23 08:07,29/Apr/21 09:29,1.11.3,1.12.2,1.13.0,,,,,,1.12.4,1.13.1,1.14.0,,,,,Runtime / Metrics,Runtime / Task,,,,0,pull-request-available,,,,,"In Flink WebUI,there is no numRecordsOut metric,and the class AsyncWaitOperator did't have this metric in fact.Other operators have this metric, which makes it difficult to monitor Async IO operator and can cause confusion for users.

I think we can directly  use the wrapping output class CountingOutput to update numRecordsOut metric.CountingOutput is used in super class of AsyncWaitOperator(AbstractStreamOperator).

Here is my commit,And I have run a test, it work.

[my  commit|https://github.com/onyourhead/flink/commit/58a8ac27b292280696639caa2e311637cd631a00]",,knaufk,Tony Giao,trohrmann,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,FLINK-14044,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/21 17:07;Tony Giao;QQ截图20210424004201.png;https://issues.apache.org/jira/secure/attachment/13024533/QQ%E6%88%AA%E5%9B%BE20210424004201.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 09:29:43 UTC 2021,,,,,,,,,,"0|z0qe4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/21 08:59;knaufk;[~Tony Giao] Thank you for opening this ticket. I would actually consider this is bug. You're solution looks good at first glance, but I am not too familiar with the code. 

[~chesnay] Could you quickly confirm that it is indeed such an easy?;;;","26/Apr/21 12:22;Tony Giao;[~knaufk] In Flink 1.9,there is no problem with AsyncIO's numRecordsOut metric, and it use the wrapping output class CountingOutput.After Flink 1.10,because the new code is submitted, the class AsyncWaitOperator directly uses the output parameter passed in the Setup method to construct a TimestampedCollector instance, but the original output field does not manage the NumRecordsOut Metric.

The output field is only used in the TimestampedCollector. CountingOutput proxies the output field and adds the NumRecordsOut metric, A call to output in a TimestampedCollector will take effect through a method with the same name as countingOutput.

So it seems that our use of CountingOutput to construct a TimestampedCollector will not affect the existing code.;;;","27/Apr/21 16:11;trohrmann;Thanks for reporting this issue [~Tony Giao]. I think your analysis is correct and it is a bug in the {{AsyncWaitOperator}}. Do you want to open a PR against Flink's repository?;;;","28/Apr/21 02:40;Tony Giao;Yes,I'll submit a PR later;;;","29/Apr/21 09:29;trohrmann;Fixed via

1.14.0: 56c61dd58b26596331575cac85812d36b3c4a383
1.13.1: 87b1831df9135382057b887f58d3f6ebf6e05f6b
1.12.4: 67d4ea659622c50f6d150e2d023cbf029d5be6b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dispatcher does not store suspended jobs in execution graph store,FLINK-22434,13374688,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,23/Apr/21 15:35,09/Dec/21 11:28,13/Jul/23 08:07,21/May/21 12:53,,,,,,,,,1.12.5,1.13.1,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Only globally terminated jobs are currently stored in the execution graph store after termination. In case the JobManager is shutdown and jobs are still running, these jobs will be suspended which is a non-globally terminated state.

The problem surfaces when a user tries to access information about the job during termination, leading to a job not found response. By storing all terminated jobs in the execution graph store this should be fixed.",,fpaul,jingzhang,klion26,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22820,FLINK-24232,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 12:53:14 UTC 2021,,,,,,,,,,"0|z0qe0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/21 12:53;trohrmann;Fixed via

1.14.0:
5fd02d28503b9bc52821533b76865e3ef365c0ce
36bd6091c1a2cf6f3b353cb55aa51964437e8f33

1.13.1:
cfdb34a5caa44d448005c01ca93dde2f020911ae
cd449e0378e41197ff1ee1f0784cefa3efdb579b

1.12.5:
083fe40c2426e04b2f22326fea0c97ab739b92b2
5052dc3fc9081d82cbd39bb4370e473a2aa255cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatorEventsExactlyOnceITCase stalls on Adaptive Scheduler,FLINK-22433,13374684,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,sewen,sewen,23/Apr/21 15:19,28/May/21 09:13,13/Jul/23 08:07,25/Apr/21 18:13,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Tests,,,,,0,pull-request-available,test-stability,,,,"Logs of the test failure:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17077&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11113

Steps to reproduce: Adjust the {{CoordinatorEventsExactlyOnceITCase}} and extend the MiniCluster configuration:
{code}
@BeforeClass
public static void startMiniCluster() throws Exception {
    final Configuration config = new Configuration();
    config.setString(RestOptions.BIND_PORT, ""0"");
    config.set(JobManagerOptions.SCHEDULER, JobManagerOptions.SchedulerType.Adaptive);
    config.set(ClusterOptions.ENABLE_DECLARATIVE_RESOURCE_MANAGEMENT, true);
{code}
",,maguowei,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 25 18:13:15 UTC 2021,,,,,,,,,,"0|z0qdzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/21 03:29;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17129&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11288

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17132&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11149;;;","25/Apr/21 09:38;maguowei;OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure fails with `AdaptiveScheduler`
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17132&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=7832;;;","25/Apr/21 09:51;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17143&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11279;;;","25/Apr/21 18:13;sewen;Fixed in
  - 1.14.0 ({{master}}) via e676442b9faa1ec0b668e8394dd2353ac2de01c6
  - 1.13.1 ({{release-1.13}}) via 0dc632681defaa1d66d3b2e884f311121467d894;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveScheduler does not log failure cause when recovering,FLINK-22431,13374658,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,23/Apr/21 13:01,28/May/21 11:03,13/Jul/23 08:07,26/Apr/21 09:49,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,The {{AdaptiveScheduler}} does not log the failure cause when recovering.,,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 26 09:49:33 UTC 2021,,,,,,,,,,"0|z0qdu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/21 09:49;trohrmann;Fixed via

1.14.0: 6be966884bb6b0482bb4f019e644f5f9225e628f
1.13.1: 72fc287e06a3bc124d850df9c53bdea3b3c13393;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exclude Sub-Tasks in all bot ""stale-unassigned"" rule of Jira Bot",FLINK-22429,13374653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,knaufk,knaufk,knaufk,23/Apr/21 12:26,23/Apr/21 14:10,13/Jul/23 08:07,23/Apr/21 14:10,,,,,,,,,1.14.0,,,,,,,,,,,,0,pull-request-available,,,,,,,knaufk,v_ganeshraju,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-23 12:26:52.0,,,,,,,,,,"0|z0qdsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Writing to already released buffers potentially causing data corruption during job failover/cancellation,FLINK-22424,13374581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,23/Apr/21 07:38,28/May/21 11:06,13/Jul/23 08:07,30/Apr/21 14:42,1.10.3,1.11.3,1.12.2,1.13.0,1.6.4,1.7.2,1.8.3,1.9.3,1.11.4,1.12.4,1.13.1,1.14.0,,,,Runtime / Network,,,,,1,pull-request-available,,,,,"I modified the code to not re-use the same memory segments, but on recycling always free up the segment. And what I have observed is a similar problem as reported in FLINK-21181 ticket, but even more severe:

{noformat}
Caused by: java.lang.RuntimeException: segment has been freed
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:109)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:93)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:50)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase$ReEmitAll.process(UnalignedCheckpointStressITCase.java:477)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase$ReEmitAll.process(UnalignedCheckpointStressITCase.java:468)
	at org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:57)
	at org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:32)
	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.emitWindowContents(WindowOperator.java:577)
	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onProcessingTime(WindowOperator.java:533)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:284)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1395)
	... 11 more
Caused by: java.lang.IllegalStateException: segment has been freed
	at org.apache.flink.core.memory.MemorySegment.put(MemorySegment.java:483)
	at org.apache.flink.core.memory.MemorySegment.put(MemorySegment.java:1398)
	at org.apache.flink.runtime.io.network.buffer.BufferBuilder.append(BufferBuilder.java:100)
	at org.apache.flink.runtime.io.network.buffer.BufferBuilder.appendAndCommit(BufferBuilder.java:82)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:250)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:142)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:104)
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:107)
	... 24 more
{noformat}
That's happening also during cancellation/job failover. It's failing when trying to write to already `free`'ed up buffer. Without my changes, this code would silently write some data to a buffer that has already been recycled/returned to the pool. If someone else would pick up this buffer, it would easily lead to the data corruption.

As far as I can tell, the exact reason behind this is that the buffer to which timer attempts to write to, has been released from `ResultSubpartition#onConsumedSubpartition`, causing `BufferConsumer` to be closed (which recycles/frees underlying memory segment ), while matching `BufferBuilder` is still being used...",,f.pompermaier,kevin.cyj,pnowojski,serge1994,Thesharing,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21181,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 30 14:42:54 UTC 2021,,,,,,,,,,"0|z0qdd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/21 07:42;pnowojski;As of now, I don't understand why is this issue popping up only from triggering processing timers. If that's the case, it would mean this issue is probably younger, and not affecting versions before 1.10.x or so.

Also I think the impact might be only for setups when there is more than one job running on the same cluster.;;;","23/Apr/21 07:58;trohrmann;Why would the problem only occur if multiple jobs run on the same cluster [~pnowojski]?;;;","23/Apr/21 08:01;pnowojski;Because writing to those released memory segments happens only while the job is already being cancelled/failing. For this data corruption to be visible, another task, from another job (or different failover region?) would had to be present and keep running. And also that 2nd task would need to immediately acquire this just released buffer. So impact is limited.;;;","23/Apr/21 10:20;trohrmann;Sorry for asking these stupid questions but are you sure about the fact that it needs a terminating job? What about a second {{Task}} from the same job which is deployed to the TM where the first {{Task}} just finished (e.g. when processing a bounded stream)?;;;","23/Apr/21 12:22;pnowojski;Because if the first task is finished, it wouldn't be able to write anything and wouldn't be able to corrupt anything. So it can only happen during cancelation/failing over. For the bug to happen, downstream task must fail/cancel first and release subpartition view, while upstream task is keep writing more data.;;;","23/Apr/21 12:44;pnowojski;One more finding. It appears that this bug affects only {{PipelinedResultPartition}}.;;;","26/Apr/21 11:01;pnowojski;merged commit 19ca330 into apache:master
merged commit 2b7bb05a4db into release-1.12
merged commit 0c1af6c into apache:release-1.11;;;","30/Apr/21 14:42;pnowojski;merged to release-1.13 as da26733e484 and 65319b256c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testScheduleRunAsync fail,FLINK-22419,13374533,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,maguowei,maguowei,23/Apr/21 02:42,15/Dec/21 01:44,13/Jul/23 08:07,16/Nov/21 21:12,1.14.0,1.15.0,,,,,,,1.12.5,1.12.8,1.13.1,1.13.6,1.14.0,1.14.3,1.15.0,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17077&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=6833


{code:java}
Apr 22 22:56:40 [ERROR] testScheduleRunAsync(org.apache.flink.runtime.rpc.RpcEndpointTest)  Time elapsed: 0.404 s  <<< FAILURE!
Apr 22 22:56:40 java.lang.AssertionError
Apr 22 22:56:40 	at org.junit.Assert.fail(Assert.java:86)
Apr 22 22:56:40 	at org.junit.Assert.assertTrue(Assert.java:41)
Apr 22 22:56:40 	at org.junit.Assert.assertTrue(Assert.java:52)
Apr 22 22:56:40 	at org.apache.flink.runtime.rpc.RpcEndpointTest.testScheduleRunAsync(RpcEndpointTest.java:318)
Apr 22 22:56:40 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 22 22:56:40 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 22 22:56:40 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 22 22:56:40 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 22 22:56:40 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 22 22:56:40 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 22 22:56:40 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 22 22:56:40 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 22 22:56:40 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 22 22:56:40 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 22 22:56:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 22 22:56:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)

{code}
",,maguowei,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 16 21:12:07 UTC 2021,,,,,,,,,,"0|z0qd2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/21 05:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17477&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865&l=6811;;;","07/May/21 08:23;chesnay;master: 8199f1e4a3ed0f633a0cb7d9fd187afd74f2b4fb

1.13: 0d75adb0bb46fcaba9377d1b8c078a587063ee5d;;;","11/Jun/21 02:10;xtsong;Reopen to port the fix to 1.12.

Instance on 1.12:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18891&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=7950;;;","11/Jun/21 07:27;xtsong;1.12: e9e8d1c630f714b802b08ed33d2062c098e40290;;;","12/Nov/21 11:38;roman;Happened again on private branch (1.15 with 8199f1e4a3ed0f633a0cb7d9fd187afd74f2b4fb):
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26413&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7737]

 

It seems like there is no hard guarantee that the test will not fail. When I change 2x timeouts to 1.1x timeout then it fails reliably locally.;;;","16/Nov/21 21:12;roman;Merged 
master: 815bc2a5d4dfc926d076cf3425de87a5f1c0f1a4..75eb8d50edb7eea12d20fd97650e5e6cce123d4d 
1.14: 99bb454485153a4943bf987c1e35a4dfa8cbbd98..5a0609aa55c3ea69746a27a19622d8c97da772f9
1.13: 220086780254692b7fcaefa06f54e34360305d65..695a9a31da12d1b10ff40f0c889b759dd38b867e
1.12: 902516864d6e031c2aa71e89852c5e72308d22c1..c1f8e731c8cb35c2e8a8b1294adc30312eac5349;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertKafkaTableITCase hangs when collecting results,FLINK-22416,13374419,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,dwysakowicz,dwysakowicz,22/Apr/21 14:09,25/Aug/21 02:36,13/Jul/23 08:07,25/Aug/21 02:36,1.13.0,,,,,,,,1.14.0,,,,,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,stale-assigned,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17037&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7002

{code}
2021-04-22T11:16:35.6812919Z Apr 22 11:16:35 [ERROR] testSourceSinkWithKeyAndPartialValue[format = csv](org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase)  Time elapsed: 30.01 s  <<< ERROR!
2021-04-22T11:16:35.6814151Z Apr 22 11:16:35 org.junit.runners.model.TestTimedOutException: test timed out after 30 seconds
2021-04-22T11:16:35.6814781Z Apr 22 11:16:35 	at java.lang.Thread.sleep(Native Method)
2021-04-22T11:16:35.6815444Z Apr 22 11:16:35 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:237)
2021-04-22T11:16:35.6816250Z Apr 22 11:16:35 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:113)
2021-04-22T11:16:35.6817033Z Apr 22 11:16:35 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2021-04-22T11:16:35.6817719Z Apr 22 11:16:35 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-04-22T11:16:35.6818351Z Apr 22 11:16:35 	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
2021-04-22T11:16:35.6818980Z Apr 22 11:16:35 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.collectRows(KafkaTableTestUtils.java:52)
2021-04-22T11:16:35.6819978Z Apr 22 11:16:35 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.testSourceSinkWithKeyAndPartialValue(UpsertKafkaTableITCase.java:147)
2021-04-22T11:16:35.6820803Z Apr 22 11:16:35 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-04-22T11:16:35.6821365Z Apr 22 11:16:35 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-04-22T11:16:35.6822072Z Apr 22 11:16:35 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-04-22T11:16:35.6822656Z Apr 22 11:16:35 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-04-22T11:16:35.6823124Z Apr 22 11:16:35 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-04-22T11:16:35.6823672Z Apr 22 11:16:35 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-04-22T11:16:35.6824202Z Apr 22 11:16:35 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-04-22T11:16:35.6824709Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-04-22T11:16:35.6825230Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-04-22T11:16:35.6825716Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-04-22T11:16:35.6826204Z Apr 22 11:16:35 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-04-22T11:16:35.6826807Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-04-22T11:16:35.6827378Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-04-22T11:16:35.6827926Z Apr 22 11:16:35 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-04-22T11:16:35.6828331Z Apr 22 11:16:35 	at java.lang.Thread.run(Thread.java:748)
2021-04-22T11:16:35.6828600Z Apr 22 11:16:35 
2021-04-22T11:16:35.6829073Z Apr 22 11:16:35 [ERROR] testAggregate[format = csv](org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase)  Time elapsed: 30.001 s  <<< ERROR!
2021-04-22T11:16:35.6829689Z Apr 22 11:16:35 org.junit.runners.model.TestTimedOutException: test timed out after 30 seconds
2021-04-22T11:16:35.6830073Z Apr 22 11:16:35 	at sun.misc.Unsafe.park(Native Method)
2021-04-22T11:16:35.6830468Z Apr 22 11:16:35 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2021-04-22T11:16:35.6831165Z Apr 22 11:16:35 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2021-04-22T11:16:35.6832071Z Apr 22 11:16:35 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2021-04-22T11:16:35.6832927Z Apr 22 11:16:35 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2021-04-22T11:16:35.6833427Z Apr 22 11:16:35 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-04-22T11:16:35.6833930Z Apr 22 11:16:35 	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:129)
2021-04-22T11:16:35.6834497Z Apr 22 11:16:35 	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:92)
2021-04-22T11:16:35.6835331Z Apr 22 11:16:35 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.wordCountToUpsertKafka(UpsertKafkaTableITCase.java:340)
2021-04-22T11:16:35.6836104Z Apr 22 11:16:35 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.testAggregate(UpsertKafkaTableITCase.java:72)
2021-04-22T11:16:35.6836728Z Apr 22 11:16:35 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-04-22T11:16:35.6837269Z Apr 22 11:16:35 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-04-22T11:16:35.6837837Z Apr 22 11:16:35 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-04-22T11:16:35.6838311Z Apr 22 11:16:35 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-04-22T11:16:35.6838945Z Apr 22 11:16:35 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-04-22T11:16:35.6839507Z Apr 22 11:16:35 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-04-22T11:16:35.6840092Z Apr 22 11:16:35 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-04-22T11:16:35.6840595Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-04-22T11:16:35.6841105Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-04-22T11:16:35.6841738Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-04-22T11:16:35.6842236Z Apr 22 11:16:35 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-04-22T11:16:35.6842861Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-04-22T11:16:35.6843436Z Apr 22 11:16:35 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-04-22T11:16:35.6843939Z Apr 22 11:16:35 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-04-22T11:16:35.6844335Z Apr 22 11:16:35 	at java.lang.Thread.run(Thread.java:748)
{code}",,dwysakowicz,jark,leonard,maguowei,renqs,sewen,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22085,,,,,,,,,,,,,,,,FLINK-22198,FLINK-23095,FLINK-22890,,,,,,,,"26/Apr/21 09:27;TsReaper;idea-test.png;https://issues.apache.org/jira/secure/attachment/13024583/idea-test.png","26/Apr/21 09:27;TsReaper;threads_report.txt;https://issues.apache.org/jira/secure/attachment/13024584/threads_report.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 25 02:35:42 UTC 2021,,,,,,,,,,"0|z0qcd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 14:10;dwysakowicz;[~TsReaper] Could you check if it is a new issue or could it be related to https://issues.apache.org/jira/browse/FLINK-21996?focusedCommentId=17326449&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17326449;;;","22/Apr/21 15:34;sewen;I am also taking a look here.

As a side note: I think here shows why I am personally not a fan of timeouts on JUnit tests. We now have no thread dump and stack traces of the system as it hangs. If we were not using a timeout, the CI runner would have caught the timeout and created a thread dump which often is a great starting point for debugging.;;;","22/Apr/21 16:35;sewen;At a first glance, I could not correlate this to any Operator Coordinator changes. They might be either legitimate timeouts in the test, or an issue with result fetching.

Everything switches properly to RUNNING and then the test times out.
{code}
11:15:25,353 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - ChangelogNormalize(key=[k_event_id, k_user_id]) -> Calc(select=[k_user_id, name, CAST(timestamp) AS timestamp, k_event_id, user_id, payload]) -> NotNullEnforcer(fields=[k_user_id, k_event_id]) (2/4) (e76de9f0b5f935f2830579334640a391) switched from INITIALIZING to RUNNING.
11:15:25,355 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-64, groupId=null] Resetting offset for partition key_partial_value_topic_csv-0 to offset 4.
11:15:25,355 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - ChangelogNormalize(key=[k_event_id, k_user_id]) -> Calc(select=[k_user_id, name, CAST(timestamp) AS timestamp, k_event_id, user_id, payload]) -> NotNullEnforcer(fields=[k_user_id, k_event_id]) (1/4)#0 (fd516fced016ff6800bf67393a8f1caf) switched from INITIALIZING to RUNNING.
11:15:25,357 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - ChangelogNormalize(key=[k_event_id, k_user_id]) -> Calc(select=[k_user_id, name, CAST(timestamp) AS timestamp, k_event_id, user_id, payload]) -> NotNullEnforcer(fields=[k_user_id, k_event_id]) (1/4) (fd516fced016ff6800bf67393a8f1caf) switched from INITIALIZING to RUNNING.
11:15:54,449 [                main] ERROR org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase [] - 
Test testSourceSinkWithKeyAndPartialValue[format = csv](org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase) failed with:
org.junit.runners.model.TestTimedOutException: test timed out after 30 seconds
{code}

The only striking thing relating to the result fetcher is the line below, but it looks like this regularly happens on startup of the fetcher and is not indicative of an error.

{code}
11:15:25,296 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Invalid request. Received version = , offset = 0, while expected version = bde5cdc9-94e3-403c-8f10-827a65bd5c74, offset = 0
{code}

The second test failure looks like it is timing timing out while the job is waiting for slots. For some reason, the ""after test cancel pending jobs"" action doesn't seem to work properly after the first test timeout (maybe related to the execution with timeout?).

*Action:* I would suggest to remove the timeout rule from that test to make sure we have a thread dump on the next failure.;;;","24/Apr/21 03:27;maguowei;Another test cases failed because of timeouts

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17129&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121&l=6680;;;","26/Apr/21 09:28;TsReaper;I've reproduced this issue by running the tests repeatedly in IDE. It seems that the test is stuck in the ""@After"" phase, not in the test itself (note the green tick beside the test case and the spinning wheel above).

 !idea-test.png! 

The thread report is as follows.

 [^threads_report.txt] ;;;","26/Apr/21 10:06;dwysakowicz;Honestly, [~TsReaper] I think what you reported is something completely different and actually might be a problem in your local setup. Notice that the test in the issue times out in the test method on collecting results. Whereas as you said yourself, in your IDE it finishes in the tear down method. Moreover you don't see the Collector sink in any of the stacktraces.;;;","27/Apr/21 07:13;jark;Timeout is removed in 14ae6fee5c835871ea0145b94fbc1e8585fe78b3;;;","27/Apr/21 07:18;jark;Removing timeout is used helping locating problems. Let's observe it for a while.

Just a side note, if the problem is related to this: https://issues.apache.org/jira/browse/FLINK-21996?focusedCommentId=17326449&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17326449
, we may need to allow failover restart in {{org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase#setup}}.;;;","10/May/21 07:18;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17746&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=10070;;;","19/May/21 10:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/May/21 23:01;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","01/Jun/21 07:14;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18467&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=10171;;;","02/Jun/21 23:30;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Jun/21 23:23;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Jun/21 22:49;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Jun/21 07:40;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18953&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=11801;;;","21/Jun/21 10:36;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19196&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=11100;;;","22/Jun/21 11:38;sewen;Here is a thread dump (excluding the auxiliary threads, like waiting I/O pool threads, blob server, REST handler, etc.).

One confusing thing is to see unspilling threads from the channel state.

{code}
""collect-sink-operator-coordinator-executor-thread-pool-thread-1"" #1436 daemon prio=5 os_prio=0 tid=0x00007fda30073000 nid=0x4683 waiting on condition [0x00007fd7da6e5000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f865ca78> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""channel-state-unspilling-thread-1"" #1435 daemon prio=5 os_prio=0 tid=0x00007fd878002800 nid=0x4682 waiting on condition [0x00007fd7dbdfc000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8b012c0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""Kafka Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (4/4)#0"" #1434 daemon prio=5 os_prio=0 tid=0x00007fd8c0003000 nid=0x4681 waiting on condition [0x00007fd8187c6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8c34308> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at org.apache.flink.streaming.connectors.kafka.internals.ClosableBlockingQueue.getBatchBlocking(ClosableBlockingQueue.java:396)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:240)

""Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (4/4)#0"" #1410 prio=5 os_prio=0 tid=0x00007fd84c002000 nid=0x4680 in Object.wait() [0x00007fd8188c7000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.streaming.connectors.kafka.internals.Handover.pollNext(Handover.java:73)
	- locked <0x00000000f8c498c8> (a java.lang.Object)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.runFetchLoop(KafkaFetcher.java:133)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:826)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:104)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:62)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)

""channel-state-unspilling-thread-1"" #1431 daemon prio=5 os_prio=0 tid=0x00007fd84c004800 nid=0x467f waiting on condition [0x00007fd7886c7000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8c905e0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""Kafka Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (2/4)#0"" #1433 daemon prio=5 os_prio=0 tid=0x00007fd8a4001000 nid=0x467e waiting on condition [0x00007fd78aceb000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8ceac68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at org.apache.flink.streaming.connectors.kafka.internals.ClosableBlockingQueue.getBatchBlocking(ClosableBlockingQueue.java:396)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:240)

""channel-state-unspilling-thread-1"" #1432 daemon prio=5 os_prio=0 tid=0x00007fd874003000 nid=0x467d waiting on condition [0x00007fd7db5f4000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8a78200> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""Kafka Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (3/4)#0"" #1430 daemon prio=5 os_prio=0 tid=0x00007fd898001800 nid=0x467c runnable [0x00007fd7893d4000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x00000000f88f9b40> (a sun.nio.ch.Util$3)
	- locked <0x00000000f88f9b30> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000f88f7308> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.apache.kafka.common.network.Selector.select(Selector.java:794)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:467)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:547)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1300)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:258)

""channel-state-unspilling-thread-1"" #1429 daemon prio=5 os_prio=0 tid=0x00007fd888002800 nid=0x467b waiting on condition [0x00007fd78a6e5000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8c3a188> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""channel-state-unspilling-thread-1"" #1428 daemon prio=5 os_prio=0 tid=0x00007fd884001000 nid=0x467a waiting on condition [0x00007fd7d8dce000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8cfd688> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (2/4)#0"" #1414 prio=5 os_prio=0 tid=0x00007fd844002000 nid=0x4678 in Object.wait() [0x00007fd7898d9000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.streaming.connectors.kafka.internals.Handover.pollNext(Handover.java:73)
	- locked <0x00000000f8cc1da8> (a java.lang.Object)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.runFetchLoop(KafkaFetcher.java:133)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:826)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:104)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:62)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)

""channel-state-unspilling-thread-1"" #1426 daemon prio=5 os_prio=0 tid=0x00007fd844001000 nid=0x4677 waiting on condition [0x00007fd7885c6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8cfdc08> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (3/4)#0"" #1412 prio=5 os_prio=0 tid=0x00007fd850004800 nid=0x4676 in Object.wait() [0x00007fd7894d5000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.streaming.connectors.kafka.internals.Handover.pollNext(Handover.java:73)
	- locked <0x00000000f8d17488> (a java.lang.Object)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.runFetchLoop(KafkaFetcher.java:133)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:826)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:104)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:62)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)

""channel-state-unspilling-thread-1"" #1425 daemon prio=5 os_prio=0 tid=0x00007fd850003800 nid=0x4675 waiting on condition [0x00007fd78a7e6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8d36660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""Thread-93"" #1424 prio=5 os_prio=0 tid=0x00007fd884003800 nid=0x4674 runnable [0x00007fd7d99d8000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.net.SocketInputStream.read(SocketInputStream.java:224)
	at java.io.DataInputStream.readUnsignedByte(DataInputStream.java:288)
	at org.apache.flink.types.StringValue.readString(StringValue.java:753)
	at org.apache.flink.api.common.typeutils.base.StringSerializer.deserialize(StringSerializer.java:73)
	at org.apache.flink.api.common.typeutils.base.StringSerializer.deserialize(StringSerializer.java:31)
	at org.apache.flink.streaming.api.operators.collect.CollectCoordinationRequest.<init>(CollectCoordinationRequest.java:52)
	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunction$ServerThread.run(CollectSinkFunction.java:391)

""channel-state-unspilling-thread-1"" #1423 daemon prio=5 os_prio=0 tid=0x00007fd858001000 nid=0x4673 waiting on condition [0x00007fd78a5e4000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8b79648> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""Sink: Collect table sink (1/1)#0"" #1422 prio=5 os_prio=0 tid=0x00007fda48099000 nid=0x4672 waiting on condition [0x00007fd789ddc000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f898d8c0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)

""ChangelogNormalize(key=[k_event_id, k_user_id]) -> Calc(select=[k_user_id, name, CAST(timestamp) AS timestamp, k_event_id, user_id, payload]) -> NotNullEnforcer(fields=[k_user_id, k_event_id]) (1/4)#0"" #1421 prio=5 os_prio=0 tid=0x00007fda480b2800 nid=0x4671 waiting on condition [0x00007fd818ccb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8b39290> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)

""ChangelogNormalize(key=[k_event_id, k_user_id]) -> Calc(select=[k_user_id, name, CAST(timestamp) AS timestamp, k_event_id, user_id, payload]) -> NotNullEnforcer(fields=[k_user_id, k_event_id]) (4/4)#0"" #1418 prio=5 os_prio=0 tid=0x00007fda480b1800 nid=0x466e waiting on condition [0x00007fd7883c4000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8918bf0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)

""ChangelogNormalize(key=[k_event_id, k_user_id]) -> Calc(select=[k_user_id, name, CAST(timestamp) AS timestamp, k_event_id, user_id, payload]) -> NotNullEnforcer(fields=[k_user_id, k_event_id]) (3/4)#0"" #1417 prio=5 os_prio=0 tid=0x00007fda480b0800 nid=0x466d waiting on condition [0x00007fd7da9e8000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8a15bc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)

""Kafka Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (1/4)#0"" #1416 daemon prio=5 os_prio=0 tid=0x00007fd870001000 nid=0x466c waiting on condition [0x00007fd7884c5000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8d0d0e0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at org.apache.flink.streaming.connectors.kafka.internals.ClosableBlockingQueue.getBatchBlocking(ClosableBlockingQueue.java:396)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:240)

""Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (1/4)#0"" #1403 prio=5 os_prio=0 tid=0x00007fd840005000 nid=0x466b in Object.wait() [0x00007fd788fd0000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.streaming.connectors.kafka.internals.Handover.pollNext(Handover.java:73)
	- locked <0x00000000f8d72e58> (a java.lang.Object)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.runFetchLoop(KafkaFetcher.java:133)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:826)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:104)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:62)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)

""channel-state-unspilling-thread-1"" #1415 daemon prio=5 os_prio=0 tid=0x00007fd840004000 nid=0x466a waiting on condition [0x00007fd78bbfa000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8d45b28> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""ChangelogNormalize(key=[k_event_id, k_user_id]) -> Calc(select=[k_user_id, name, CAST(timestamp) AS timestamp, k_event_id, user_id, payload]) -> NotNullEnforcer(fields=[k_user_id, k_event_id]) (2/4)#0"" #1407 prio=5 os_prio=0 tid=0x00007fda480b0000 nid=0x4665 waiting on condition [0x00007fd7db9f8000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8ad7b68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)

""Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (4/4)#0"" #1406 prio=5 os_prio=0 tid=0x00007fda480af000 nid=0x4664 waiting on condition [0x00007fd7899da000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8bf2718> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)

""Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (3/4)#0"" #1405 prio=5 os_prio=0 tid=0x00007fda48084800 nid=0x4663 waiting on condition [0x00007fd788acb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8ce0488> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)

""Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (2/4)#0"" #1404 prio=5 os_prio=0 tid=0x00007fda48084000 nid=0x4662 waiting on condition [0x00007fd788ccd000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8d624c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)


""Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (1/4)#0"" #1400 prio=5 os_prio=0 tid=0x00007fda480b9000 nid=0x465f waiting on condition [0x00007fd7d96d5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8d88820> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1699/155725233.run(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)
{code};;;","22/Jun/21 12:48;dwysakowicz;As far as I can tell the unspilling threads are there, because they're not closed after restoring the gates. They're being closed along with the {{StreamTask}}. I think we could close the thread pool after restore.;;;","22/Jun/21 14:22;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19306&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=10718;;;","23/Jun/21 21:04;sewen;At a first glance it seems that all stuck cases are cases where there was a failover (there are always trailing unspilling threads).
That gives me two theories:

(1) Either there is a bug in the collect failover logic 

(2) Or the failures are triggered because of operator event loss and this has the same root cause as FLINK-23046. There seem to be cases where the event loss in in fact not compensated properly even after failover. ;;;","30/Jun/21 16:39;arvid;Looking at the stack traces: 
- all subtasks are waiting for data. 
- Legacy thread is waiting for data to arrive. 
- Kafka consumer thread is waiting for partition assignments. 

So it seems as if there is just no data after recovery... I guess we need to trace why {{hasAssignedPartitions == false}}.


{noformat}
""Kafka Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, upsert_kafka]], fields=[k_user_id, name, k_event_id, user_id, payload, timestamp]) (4/4)#0"" #1434 daemon prio=5 os_prio=0 tid=0x00007fd8c0003000 nid=0x4681 waiting on condition [0x00007fd8187c6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f8c34308> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at org.apache.flink.streaming.connectors.kafka.internals.ClosableBlockingQueue.getBatchBlocking(ClosableBlockingQueue.java:396)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:240)
{noformat}

It may be similar to FLINK-21996, but afaik this ticket was for new sources only and here we have a legacy source. [~sewen] would it be possible to have the same issue for old sources?;;;","01/Jul/21 09:32;renqs;I checked logs of recent 3 failure instances, and seems that the starting offset is not seek to 0 even though the offset reset strategy is EARLIEST:

For [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19306&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=10718] :
{noformat}
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-103, groupId=null] Seeking to EARLIEST offset of partition key_full_value_topic_avro-0
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-103, groupId=null] Resetting offset for partition key_full_value_topic_avro-0 to offset 4.{noformat}
For [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19196&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=11100] :
{noformat}
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-112, groupId=null] Seeking to EARLIEST offset of partition key_partial_value_topic_avro-0
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-112, groupId=null] Resetting offset for partition key_partial_value_topic_avro-0 to offset 4.
{noformat}
For [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18953&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=11801] :
{noformat}
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-94, groupId=null] Seeking to EARLIEST offset of partition users_avro-0
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-94, groupId=null] Resetting offset for partition users_avro-1 to offset 5.
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-94, groupId=null] Resetting offset for partition users_avro-0 to offset 6.
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-94, groupId=null] Seeking to EARLIEST offset of partition users_avro-1
INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=consumer-94, groupId=null] Resetting offset for partition users_avro-1 to offset 5.{noformat}
This might explain why collectRow timed out: Kafka Consumer didn't fetch enough messages because it didn't start reading from earliest offset, and the collector just waited forever. 

Since each test run creates a new topic, the earliest offset of partition is supposed to be zero. I'm still investigating why Kafka brokers return a non-zero offset for EARLIEST reset strategy. This might happen if Kafka log passes retention time and being truncated, but this should not be the reason because the default retention time is 7 days and the test will not run for that long. 

Another point I'd like to mention is that all three failure instances mentioned above failed with the last format option (avro). This could be just coincidence, just providing more information for debugging~;;;","02/Jul/21 13:15;dwysakowicz;[~gaoyunhaii] might have found the issue in a similar task: https://issues.apache.org/jira/browse/FLINK-22085?focusedCommentId=17372420&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17372420;;;","19/Jul/21 20:53;dwysakowicz;I'll deprioritize the issue as it has not happened in a while.;;;","18/Aug/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","25/Aug/21 02:35;renqs;This issue shares the same root cause with FLINK-22198 and should be fixed on master by 7814ee257b526d52f80a17143e228fb936b03ff5. Please reopen the ticket if it happens again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hide Checkpointing page in the UI for batch jobs,FLINK-22413,13374386,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,rmetzger,rmetzger,22/Apr/21 11:33,28/Aug/21 11:20,13/Jul/23 08:07,11/May/21 09:47,,,,,,,,,1.14.0,,,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,"This is a follow up to https://github.com/apache/flink/commit/8dca9fa852c72984ac873eae9a96bbd739e502f3#commitcomment-49744255

Batch jobs don't need a Checkpointing page, because there is no information for these jobs available there.",,nicholasjiang,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 11 09:47:35 UTC 2021,,,,,,,,,,"0|z0qc5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/21 11:51;nicholasjiang;[~rmetzger], I'm working for hiding the checkpointing page and I found that there is no any rest api to expose 'execution.runtime-mode', which could judge whether the job is batch or streaming job. IMO, this should firstly expose the runtime mode of the job, then hide the checkpointing page for the batch job. What do you think about?;;;","06/May/21 11:55;rmetzger;Thanks a lot for working on the ticket. I assigned you to it.

execution.runtime-mode is maybe not the right property to check (because it just determines batch / streaming execution in the DataStream API). I would go with {{JobGraph.jobType}}. Probably it is not exposed to the REST Api, so we need to do that first.;;;","10/May/21 06:20;nicholasjiang;[~rmetzger], I have added the JobGraph.jobType to the REST Api and hided the checkpointing page. Please help to review the pull request.;;;","11/May/21 09:47;chesnay;master: dc4565600487f7141d42c8fef039d1a46dca5291;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Table Parsr Hive Drop Partitions Syntax unparse is Error,FLINK-22408,13374345,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aidenma,aidenma,aidenma,22/Apr/21 09:20,28/May/21 11:05,13/Jul/23 08:07,12/May/21 09:07,1.11.3,,,,,,,,1.12.5,1.13.1,1.14.0,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Flink Table Parser is error：

*Synopsis:*

 

*SQL：*
{code:java}
alter table tbl drop partition (p1='a',p2=1), partition(p1='b',p2=2);{code}
*hive muit partition unparse toSqlString is :*
{code:java}
ALTER TABLE `TBL`\n"" +
      ""DROP\n"" +
      ""PARTITION (`P1` = 'a', `P2` = 1)\n"" +
      ""PARTITION (`P1` = 'b', `P2` = 2)
{code}
Missing comma in Partition SqlNodeList 

 Hive syntax：
{code:java}
ALTER TABLE table_name DROP [IF EXISTS] PARTITION (partition_spec) [, PARTITION (partition_spec)];
{code}
 ",,aidenma,jark,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 11 13:03:34 UTC 2021,,,,,,,,,,"0|z0qbwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/21 04:08;jark;This is an unsupported feature and we should discuss the synax API first before moving to pull request. ;;;","23/Apr/21 05:00;aidenma;We mainly use hive in the table environment. We find this syntax problem in the production environment. At present, there is a difference between hive and hive syntax, which leads to Hive SQL failure.;;;","23/Apr/21 05:07;aidenma;I think the final Unparse result：
{code:java}
ALTER TABLE tbl DROP PARTITION (p1='a',p2=1), PARTITION (p1='b',p2=2);
{code};;;","23/Apr/21 06:20;lirui;Hi [~aidenma], thanks for reporting the issue. I think it's a valid one. But I don't see why it can lead to Hive SQL failure. Are you somehow relying on the unparse result in your code?

And btw, in flink 1.13 we have added {{HiveParser}} to support hive dialect. The old calcite parser (for hive) is deprecated and will eventually be removed.;;;","23/Apr/21 06:23;lirui;Hi [~jark], the feature is already supported: https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_dialect.html#drop-partitions
The issue here is the unparse result of the SqlNode is incorrect.;;;","23/Apr/21 06:30;jark;Sorry, I was thinking this is used in default dialect. ;;;","27/Apr/21 03:52;aidenma;Hi [~lirui] Thank！I got it! I know that if there is a subsequent release of version 1.13, I will switch the version of the existing business. I have a business requirement here: I use the module SQL translation in Flink to do some front-end SQL verification and formatting, because I found that the syntax of hive is inconsistent when converting dialects. There may be misunderstandings among users. So I think we should keep the syntax consistent with hive.;;;","27/Apr/21 05:44;lirui;[~aidenma] OK. I think we can fix this issue in release-1.12 branch. We don't need to fix it in 1.13 and later, since the calcite parser is deprecated. So please submit a PR for 1.12.;;;","07/May/21 06:23;aidenma;[~lirui] The branch has been pulled according to realease-1.12. See the link for details.;;;","11/May/21 10:13;lirui;Hi [~aidenma], I realized the fix is in {{SqlDropPartitions}} which is not only used by hive. So please also submit a PR for master.;;;","11/May/21 10:14;lirui;Fixed in master: 3b5f39996abdc808a6703bb235523d43871f07c5
Fixed in release-1.13: 21866fd1b0af732289ea22336c45e1eefcec009e
Fixed in release-1.12: 9e5b0d408b110bbc56987c383435be9c3da394ac;;;","11/May/21 12:47;aidenma;Hi [~lirui] , The branch release-1.13 need to submit?;;;","11/May/21 13:03;lirui;Yes, we should also fix it in 1.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test ReactiveModeITCase.testScaleDownOnTaskManagerLoss(),FLINK-22406,13374328,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,sewen,sewen,22/Apr/21 08:22,28/Aug/21 11:18,13/Jul/23 08:07,07/May/21 07:58,1.13.0,1.14.0,,,,,,,1.13.1,1.14.0,,,,,,Tests,,,,,0,pull-request-available,test-stability,,,,"The test is stalling on Azure CI.

https://dev.azure.com/sewen0794/Flink/_build/results?buildId=292&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4865",,dwysakowicz,maguowei,rmetzger,roman,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23524,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 07 07:58:27 UTC 2021,,,,,,,,,,"0|z0qbsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 10:51;chesnay;There are a few odd things in the logs. It seems like the JM is prematurely moving tasks into a canceled state.
{code:java}
23:47:20,274 INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (2/2) (0d91787b2ba65cd0f259be619b293b96) switched from CREATED to DEPLOYING.
23:47:20,274 INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (2/2) (0d91787b2ba65cd0f259be619b293b96) switched from DEPLOYING to CANCELING.
23:47:20,277 INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (2/2) (0d91787b2ba65cd0f259be619b293b96) switched from CANCELING to CANCELED.
23:47:20,282 INFO  o.a.f.r.taskexecutor.TaskExecutor           [] - Received task Source: Custom Source -> Sink: Unnamed (2/2)#0 (0d91787b2ba65cd0f259be619b293b96), deploy into slot with allocation id 
23:47:20,287 INFO  o.a.f.r.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (2/2)#0 (0d91787b2ba65cd0f259be619b293b96) switched from CREATED to DEPLOYING.48a192cd6be4f34599cac87ad5d8caba.
23:47:20,296 INFO  o.a.f.r.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (2/2)#0 (0d91787b2ba65cd0f259be619b293b96) switched from DEPLOYING to INITIALIZING.
23:47:20,327 WARN  o.a.f.r.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (2/2)#0 (0d91787b2ba65cd0f259be619b293b96) switched from INITIALIZING to FAILED with failure cause: org.apache.flink.runtime.jobmaster.ExecutionGraphException: The execution attempt 0d91787b2ba65cd0f259be619b293b96 was not found. {code}
This doesn't necessarily explain the issue, but with a stray task hanging around for longer than we expect it to there is now the possibility that, after the downscaling has concluded, the number of active instances is 3. If the test thread enters the waiting loop at this time it will never exit, because we don't notify the thread if instances are shutting down. This is entirely theoretical though, but it is the only explanation I can come up with that could cause the test to get stuck.;;;","23/Apr/21 13:51;chesnay;I managed to get the debug logs for a failed run, confirming my theory. The test starts out with 2 TMs, each with 2 slots. The test then in total runs 3 jobs; after the first 2 slots arrive with p=2, then with p=4 after the next 2 slots arrive, and one final with with p=2 again after we shutdown a TM.

The problem is caused by the first job, coupled with how test counts active subtasks.

The first job effectively is immediately canceled after having been deployed, which can cause the cancellation message to be processed before the task deployment, which fails because nothing is deployed (yet), and the JM marks the task is canceled and proceeds with a restart. The task deployment later arrives, and is processed as usual. This task sticks around until the task initialization is complete and the task transitions into a running state, with the corresponding state update being rejected by the JM.

After an offline discussion with [~trohrmann], we concluded that we could fix this by not checking what is actually being deployed, but instead poll the JM for the current parallelism.;;;","28/Apr/21 04:13;maguowei;another case
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17316&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=10889;;;","05/May/21 06:14;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17544&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","05/May/21 06:56;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17539&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=5316;;;","05/May/21 06:57;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17538&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=10541;;;","05/May/21 06:58;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17543&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=5667;;;","05/May/21 07:09;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17558&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=5673;;;","05/May/21 07:11;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17552&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=5327;;;","05/May/21 07:11;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17552&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=10983;;;","05/May/21 09:12;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17540&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a;;;","07/May/21 06:39;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17625&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","07/May/21 07:58;chesnay;master: 271ac1b8bac100b613c313e11773109db15015e5

1.13: 5b7e87f7c4c39bbc13c459be97b7d7b4102d0f45;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE problem when call HiveInspectors.toFlinkObject for hive-exec 2.0.0,FLINK-22400,13374167,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hehuiyuan,hehuiyuan,hehuiyuan,21/Apr/21 15:20,28/Aug/21 12:08,13/Jul/23 08:07,17/May/21 06:35,,,,,,,,,1.14.0,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"ENV:

flink 1.12   hive 2.0.0

ERROR LOG:
{code:java}
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:199)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:154)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:116)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:275)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:67)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:399)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:620)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:584)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:844)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:636)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:146)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:101)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: org.apache.flink.connectors.hive.FlinkHiveException: java.lang.NullPointerException
    at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.nextRecord(HiveMapredSplitReader.java:190)
    at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter$HiveReader.nextRecord(HiveBulkFormatAdapter.java:336)
    at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter$HiveReader.readBatch(HiveBulkFormatAdapter.java:319)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:56)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:138)
    ... 6 more
Caused by: java.lang.NullPointerException
    at org.apache.flink.table.functions.hive.conversion.HiveInspectors.toFlinkObject(HiveInspectors.java:335)
    at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.nextRecord(HiveMapredSplitReader.java:180)
    ... 11 more

{code}
{code:java}
Map<?, ?> map = mapInspector.getMap(data);

hive-exec 2.0.0

{code}
 AbstractParquetMapInspector Class

!image-2021-04-21-23-18-56-107.png|width=372,height=191!

 

Null may be returned here，then throw NPE when  call `map.size`

(map == null)

 ",,hehuiyuan,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/21 15:18;hehuiyuan;image-2021-04-21-23-18-56-107.png;https://issues.apache.org/jira/secure/attachment/13024400/image-2021-04-21-23-18-56-107.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 17 06:35:00 UTC 2021,,,,,,,,,,"0|z0qat4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 15:44;hehuiyuan;Hi   [~lirui]  ,  please take a look;;;","17/May/21 06:35;lirui;Fixed in master: d76d50c7b0df9063233690c40d7fc5cb6ea51367;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertKafkaTableITCase hangs when setting up kafka,FLINK-22387,13374035,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,dwysakowicz,dwysakowicz,21/Apr/21 07:03,07/Sep/21 10:29,13/Jul/23 08:07,25/Aug/21 02:25,1.12.4,1.13.1,1.14.0,,,,,,1.14.0,,,,,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16901&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6932
{code}
2021-04-20T20:01:32.2276988Z Apr 20 20:01:32 ""main"" #1 prio=5 os_prio=0 tid=0x00007fe87400b000 nid=0x4028 runnable [0x00007fe87df22000]
2021-04-20T20:01:32.2277666Z Apr 20 20:01:32    java.lang.Thread.State: RUNNABLE
2021-04-20T20:01:32.2278338Z Apr 20 20:01:32 	at org.testcontainers.shaded.okio.Buffer.getByte(Buffer.java:312)
2021-04-20T20:01:32.2279325Z Apr 20 20:01:32 	at org.testcontainers.shaded.okio.RealBufferedSource.readHexadecimalUnsignedLong(RealBufferedSource.java:310)
2021-04-20T20:01:32.2280656Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.readChunkSize(Http1ExchangeCodec.java:492)
2021-04-20T20:01:32.2281603Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.read(Http1ExchangeCodec.java:471)
2021-04-20T20:01:32.2282163Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.internal.Util.skipAll(Util.java:204)
2021-04-20T20:01:32.2282870Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.internal.Util.discard(Util.java:186)
2021-04-20T20:01:32.2283494Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.close(Http1ExchangeCodec.java:511)
2021-04-20T20:01:32.2284460Z Apr 20 20:01:32 	at org.testcontainers.shaded.okio.ForwardingSource.close(ForwardingSource.java:43)
2021-04-20T20:01:32.2285183Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.internal.connection.Exchange$ResponseBodySource.close(Exchange.java:313)
2021-04-20T20:01:32.2285756Z Apr 20 20:01:32 	at org.testcontainers.shaded.okio.RealBufferedSource.close(RealBufferedSource.java:476)
2021-04-20T20:01:32.2286287Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.internal.Util.closeQuietly(Util.java:139)
2021-04-20T20:01:32.2286795Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.ResponseBody.close(ResponseBody.java:192)
2021-04-20T20:01:32.2287270Z Apr 20 20:01:32 	at org.testcontainers.shaded.okhttp3.Response.close(Response.java:290)
2021-04-20T20:01:32.2287913Z Apr 20 20:01:32 	at org.testcontainers.shaded.com.github.dockerjava.okhttp.OkDockerHttpClient$OkResponse.close(OkDockerHttpClient.java:285)
2021-04-20T20:01:32.2288606Z Apr 20 20:01:32 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$null$0(DefaultInvocationBuilder.java:272)
2021-04-20T20:01:32.2289295Z Apr 20 20:01:32 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder$$Lambda$340/2058508175.close(Unknown Source)
2021-04-20T20:01:32.2289886Z Apr 20 20:01:32 	at com.github.dockerjava.api.async.ResultCallbackTemplate.close(ResultCallbackTemplate.java:77)
2021-04-20T20:01:32.2290567Z Apr 20 20:01:32 	at org.testcontainers.utility.ResourceReaper.start(ResourceReaper.java:202)
2021-04-20T20:01:32.2291051Z Apr 20 20:01:32 	at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:205)
2021-04-20T20:01:32.2291879Z Apr 20 20:01:32 	- locked <0x00000000e9cd50f8> (a [Ljava.lang.Object;)
2021-04-20T20:01:32.2292313Z Apr 20 20:01:32 	at org.testcontainers.LazyDockerClient.getDockerClient(LazyDockerClient.java:14)
2021-04-20T20:01:32.2292870Z Apr 20 20:01:32 	at org.testcontainers.LazyDockerClient.authConfig(LazyDockerClient.java:12)
2021-04-20T20:01:32.2293383Z Apr 20 20:01:32 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:310)
2021-04-20T20:01:32.2293890Z Apr 20 20:01:32 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1029)
2021-04-20T20:01:32.2294578Z Apr 20 20:01:32 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
2021-04-20T20:01:32.2295157Z Apr 20 20:01:32 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-04-20T20:01:32.2295622Z Apr 20 20:01:32 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-04-20T20:01:32.2296067Z Apr 20 20:01:32 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-04-20T20:01:32.2296472Z Apr 20 20:01:32 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-04-20T20:01:32.2296957Z Apr 20 20:01:32 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-04-20T20:01:32.2297490Z Apr 20 20:01:32 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-04-20T20:01:32.2298021Z Apr 20 20:01:32 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-04-20T20:01:32.2298552Z Apr 20 20:01:32 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-04-20T20:01:32.2299266Z Apr 20 20:01:32 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-04-20T20:01:32.2299822Z Apr 20 20:01:32 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-04-20T20:01:32.2300349Z Apr 20 20:01:32 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-04-20T20:01:32.2300855Z Apr 20 20:01:32 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,fsk119,jark,joemoe,leonard,maguowei,mapohl,xtsong,,,,,,,,,,,,,,,,,,,,,,FLINK-22971,,,,,,,,,,,,,,,,,,FLINK-22386,,,,,,,,,,,,,,,,FLINK-22198,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 25 02:25:30 UTC 2021,,,,,,,,,,"0|z0qa00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 09:02;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16924&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6611;;;","21/Apr/21 15:30;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=404&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=18a8058f-13c3-5857-f389-32cb0ee00ed2&l=6614;;;","29/Apr/21 05:53;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17371&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6899;;;","19/May/21 10:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/May/21 23:01;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","04/Jun/21 01:48;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18651&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6392;;;","04/Jun/21 02:56;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18653&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6664;;;","08/Jun/21 10:09;joemoe;hi [~fsk119], can you have a look at this?;;;","11/Jun/21 03:02;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18892&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6974;;;","15/Jun/21 03:07;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18959&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6609;;;","16/Jun/21 03:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19005&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6987;;;","29/Jun/21 01:54;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19644&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6230;;;","30/Jun/21 03:31;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19698&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6682;;;","02/Jul/21 01:40;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19796&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6543;;;","02/Jul/21 01:41;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19796&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7817;;;","05/Jul/21 02:53;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19856&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6186;;;","05/Jul/21 04:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19866&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6638;;;","05/Jul/21 05:18;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19873&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6274;;;","06/Jul/21 03:06;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19944&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=6890;;;","09/Jul/21 03:20;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20197&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6857;;;","12/Jul/21 04:01;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20286&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6550;;;","21/Jul/21 02:57;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20750&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6638;;;","06/Aug/21 03:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21643&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6551;;;","09/Aug/21 03:33;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21739&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7545;;;","12/Aug/21 09:38;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21952&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7223;;;","13/Aug/21 02:34;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21994&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=6962;;;","13/Aug/21 03:03;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22022&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6890;;;","16/Aug/21 03:23;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22153&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=7121;;;","16/Aug/21 03:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22155&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7863;;;","16/Aug/21 03:49;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22196&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7374;;;","16/Aug/21 03:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22196&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=7121;;;","16/Aug/21 07:59;xtsong;cc [~jark] [~Leonard Xu] [~fsk119]
Could any of you help look into this?;;;","16/Aug/21 11:38;jark;[~xtsong] I think this is caused by FLINK-22971 and is hard to be fixed by a single test. We should come up with a more stable container infrastructure.;;;","17/Aug/21 04:57;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22329&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7115;;;","17/Aug/21 05:06;xtsong;[~jark],

I'm not sure about this. Issues caused by FLINK-22971 typically have stacks stuck at the phase downloading the test container. However, the stack of recent instances of this issue shows that the flink operator has start fetching data, which IIUC is after the test container being downloaded.

Another observation is that, while caching the docker images does not completely solve the problem, it does reduce the frequency of occurrence for most test container related instabilities. I suspect there's other reasons that makes this issue stand out and remain occurring with a high frequency.

{code}
Aug 16 22:05:48 ""main"" #1 prio=5 os_prio=0 tid=0x00007fac9800b000 nid=0x4d63 waiting on condition [0x00007fac9e817000]
Aug 16 22:05:48    java.lang.Thread.State: TIMED_WAITING (sleeping)
Aug 16 22:05:48 	at java.lang.Thread.sleep(Native Method)
Aug 16 22:05:48 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:237)
Aug 16 22:05:48 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:113)
Aug 16 22:05:48 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Aug 16 22:05:48 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Aug 16 22:05:48 	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
Aug 16 22:05:48 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.collectRows(KafkaTableTestUtils.java:52)
Aug 16 22:05:48 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.writeChangelogToUpsertKafkaWithMetadata(UpsertKafkaTableITCase.java:570)
Aug 16 22:05:48 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.testTemporalJoin(UpsertKafkaTableITCase.java:94)
Aug 16 22:05:48 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 16 22:05:48 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 16 22:05:48 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 16 22:05:48 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 16 22:05:48 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 16 22:05:48 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 16 22:05:48 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 16 22:05:48 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 16 22:05:48 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 16 22:05:48 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 16 22:05:48 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 16 22:05:48 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
{code};;;","17/Aug/21 05:12;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22329&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=7084;;;","17/Aug/21 06:36;jark;[~xtsong], it seems recent stack is different from the original stack. 
[~fsk119], what about add a timeout on the {{KafkaTableTestUtils.collectRows}}, so that CI not block on this tests.
We can further checks why there is no enough records in Kafka topic later. ;;;","17/Aug/21 13:10;jark;Seems related to FLINK-22198. ;;;","18/Aug/21 03:55;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22374&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7123;;;","19/Aug/21 02:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22431&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7245;;;","19/Aug/21 04:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22463&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=7180;;;","23/Aug/21 02:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22577&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=7092;;;","23/Aug/21 02:43;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22591&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7125;;;","23/Aug/21 02:45;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22605&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7384;;;","23/Aug/21 03:37;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22618&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=7131;;;","24/Aug/21 03:18;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22684&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=7180;;;","25/Aug/21 02:25;jark;Fixed by FLINK-22198.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type mismatch in NetworkBufferPool,FLINK-22385,13374005,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sharkd,sharkd,sharkd,21/Apr/21 04:04,28/May/21 09:12,13/Jul/23 08:07,22/Apr/21 15:51,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Metrics,,,,,0,pull-request-available,,,,,"'Network Metrics' in flink ui  display error when network memory is large. 

!flink-ui-bug.png!",,Feifan Wang,kevin.cyj,mapohl,sharkd,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/21 03:57;sharkd;flink-ui-bug.png;https://issues.apache.org/jira/secure/attachment/13024362/flink-ui-bug.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 15:51:04 UTC 2021,,,,,,,,,,"0|z0q9tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 04:34;kevin.cyj;cc [~pnowojski].;;;","21/Apr/21 08:40;mapohl;I'm gonna pick that one up. That is a bug that is solely affecting the metrics. It was introduced in FLINK-14422. Thanks for addressing it [~sharkd];;;","22/Apr/21 15:51;trohrmann;Fixed via 

master: 4ab8ceac00bd8a501ed1acb01ba911d0e18333b9
1.13.0: d18bba54308703d1b89074063025bfa1ca4e66c9
1.12.3: f01727dfe0372d9ec118cbe75c7f4b155ea301c1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDBStateBackendTests::test_get_set_number_of_transfering_threads fails on azure,FLINK-22381,13373939,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,roman,roman,20/Apr/21 20:11,21/Apr/21 08:02,13/Jul/23 08:07,20/Apr/21 20:41,1.13.0,1.14.0,,,,,,,1.13.0,1.14.0,,,,,,Runtime / State Backends,,,,,0,,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16900&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=21206

{code}
 =================================== FAILURES ===================================
 _____ RocksDBStateBackendTests.test_get_set_number_of_transfering_threads ______

 self = <pyflink.datastream.tests.test_state_backend.RocksDBStateBackendTests testMethod=test_get_set_ number_of_transfering_threads>

     def test_get_set_number_of_transfering_threads(self):

         state_backend = RocksDBStateBackend(""file://var/checkpoints/"")

 >       self.assertEqual(state_backend.get_number_of_transfering_threads(), 1)
 E       AssertionError: 4 != 1

 pyflink/datastream/tests/test_state_backend.py:185: AssertionError
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16900&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=34196

{code}
[ERROR] Failures:
[ERROR]   ConfigOptionsDocsCompletenessITCase.testCompleteness:76->compareDocumentedAndExistingOption s:220 Documentation is outdated, please regenerate it according to the instructions in flink-docs/README.md.
   Problems:
           Documentation of state.backend.rocksdb.checkpoint.transfer.thread.num in RocksDBOptions is  outdated. Expected: default=(4) description=(The number of threads (per stateful operator) used to transfer (download and upload) files in RocksD BStateBackend.).
           Documented option state.backend.rocksdb.checkpoint.transfer.thread.num does not exist.
           Documented option state.backend.rocksdb.checkpoint.transfer.thread.num does not exist.
{code}",,dwysakowicz,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21694,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 07:57:38 UTC 2021,,,,,,,,,,"0|z0q9eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/21 20:41;dwysakowicz;This has been fixed in 80c1e713c56ef8d0455e508c1b45f61b878493c8;;;","21/Apr/21 07:54;roman;Shouldn't it be also backported to 1.13?;;;","21/Apr/21 07:57;dwysakowicz;Fixed via:
* master
** 80c1e713c56ef8d0455e508c1b45f61b878493c8
* 1.13 
** af204951497648fe2ed992de7bacd7304dd6808b
** 749faf692bca82d3b4ff325cd23a3566ccd4b77f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SequentialChannelStateReaderImpl may recycle buffer twice,FLINK-22376,13373865,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,roman,roman,20/Apr/21 14:22,23/Sep/21 17:23,13/Jul/23 08:07,07/Jun/21 09:03,1.13.0,,,,,,,,1.13.2,1.14.0,,,,,,Runtime / Network,Runtime / Task,,,,0,pull-request-available,,,,,"In ChannelStateChunkReader.readChunk in case of error buffer is recycled in the catch block. However, it might already have been recycled in stateHandler.recover().

Using minor priority, as this only affects already failing path.
 ",,akalashnikov,pnowojski,roman,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 07 09:03:34 UTC 2021,,,,,,,,,,"0|z0q8y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/21 11:56;akalashnikov;In my opinion, the pattern of using the Buffer should be logically something like this:
{noformat}
Buffer buf = getOrCreate();
try {
 ....
 buf.retain();
 try {
 .....
 } finally {
 buf.recycle();
 }
} finally {
 buf.recycle();
}
{noformat}
or
{noformat}
Buffer buf = get();
try {
 ....
 list.add(buf.retain());
} finally {
 buf.recycle();
}

//otherThread/method
Buffer buf =list.get();
try {
 .....
 } finally {
 buf.recycle();
 }
{noformat}
In fact, in most cases, it indeed uses in such a way. But unfortunately when BufferBuilder is used this pattern is broken. For example:
{noformat}
BufferBuilder buff = createBufferBuilder();
try{
 BufferConsumer consumer = buff.createBufferConsumer();
 try{
 } finally {
 consumer.recycle();
 }
} finally {
 buff.recycle();//error - this buffer is already recycled when consumer.recycle()
}
{noformat}
and
{noformat}
BufferBuilder buff = createBufferBuilder();
try{
 list.add(buff.createBufferConsumer());
} finally {
 buff.recycle();
}

BufferConsumer consumer = list.get();
try{
 //error - it is impossible to use consumer because it is already recycled in buff.recycle();
 } finally {
 consumer.recycle();//error - this buffer is already recycled when buff.recycle()
 }
{noformat}
This happens because BufferBuilder writes directly to MemorySegment missing the Buffer. But the reference count is stored in the Buffer so it is impossible to recycle BufferBuilder correctly.

My proposal is to change the implementation of BufferBuilder in such a way that it writes in Buffer instead of MemorySegment and during the creation of the new consumer it is just 'retain' this buffer. So in this case the Buffer will be recycled only when all consumers and source BufferBuilder invoke the recycle.

P.S. I also don't see a lot of sense having the BufferConsumer#CachedPositionMarker. So I would want to delete it.;;;","07/May/21 12:15;pnowojski;{quote}
P.S. I also don't see a lot of sense having the BufferConsumer#CachedPositionMarker. So I would want to delete it.
{quote}
AFAIK it's required for correctness. For example a race condition between two threads.
Thread 1:
{code}
bufferBuilder.appendAndCommit(...);
bufferBuilder.appendAndCommit(...);
bufferBuilder.finish();
{code}
Thread2:
{code}
bufferConsumer.build();
if (bufferConsumer.isFinished()) {
  bufferConsumer.close();
}
{code}
Without separated positions, 2nd thread can build a buffer with just the data from the first {{appendAndCommit}} call, but {{isFinished()}} would already return true.

Secondly, it would cause more {{volatile}} reads.

About the recycling problem. I agree ideally it would be better for {{BufferBuilder}} to do it's own reference counting. However I'm not sure if that would be for free. That would have to be benchmarked carefully. So far we were maintaining an implicit invariant, that {{BufferBuilder}} can not be accessed after closing all of the consumers. It's not very clean but it was working well. I'm not sure why did we have to add {{BufferBuilder#recycle}} method.;;;","07/May/21 13:27;akalashnikov;Yes, I understand that it is needed for caching of isFinished. But I just tried to say that one local flag isFinished would be enough. Right now, the methods CachedPositionMarker#update and CachedPositionMarker#getCached are called always one by one so it means we never read the cached value but we need it only for isFinished.

I was just confused when saw this code. In my head, I expected something like this:
  
{noformat}
boolean isFinished;
int writerPosition(){
   int position = positionMarker.get();
   isFinished = PositionMarker.isFinished(position);
   return position;
}
{noformat}
Just in case, I don't have a target to remove this code, and maybe saving a couple of code lines also doesn't make sense.
----
According to the original problem. of course, I don't like the idea that we just assume some guarantees(like BufferBuilder can not be accessed after closing all of the consumers). But if the direct writing to MemorySegment has better performance, I can propose pretty easy solution.
 BufferBuilder:
{noformat}
public void recycle() {
        if(!bufferConsumerCreated)
            recycler.recycle(memorySegment);
    }
{noformat}
Pattern of usage:
{noformat}
BufferBuilder builder = getOrCreateBuilder();
try{
} catch(Exception ex) {
 builder.recycle();
}
{noformat}
In this case, if BufferBuilder has the consumers already, then these consumers responsible for recycling but if BufferBuilder doesn't have the consumers yet and something bad happens, then BufferBuilder recycles the buffer by itself.;;;","07/Jun/21 09:03;pnowojski;Merged to master as 39746870694..2293362f0a1
Merged to release-1.13 as 8511ecbc33e..6a2f80605b1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB state backend might occur ClassNotFoundException when deserializing on TM side,FLINK-22369,13373773,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sjwiesman,yunta,yunta,20/Apr/21 07:19,28/Aug/21 11:17,13/Jul/23 08:07,21/Apr/21 09:30,1.13.0,,,,,,,,1.13.0,,,,,,,,,,,,0,pull-request-available,,,,,"FLINK-19467 introduced new {{EmbeddedRocksDBStateBackend}} and added new interface {{[EmbeddedRocksDBStateBackend#setLogger|https://github.com/apache/flink/blob/24031e55e4cf35a5818db2e927e65b290a9b2aed/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/EmbeddedRocksDBStateBackend.java#L287]}} to ensures users of the legacy {{RocksDBStateBackend}} see consistent logging.

However, this change introduce another non transient {{[logger|https://github.com/apache/flink/blob/24031e55e4cf35a5818db2e927e65b290a9b2aed/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/EmbeddedRocksDBStateBackend.java#L115]}} and it would be deserialized on TM side first. If the client has different log4j implementation from TM side, we might meet ClassNotFoundException:
 !image-2021-04-20-15-18-49-706.png!",,dwysakowicz,sjwiesman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19467,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/21 07:18;yunta;image-2021-04-20-15-18-49-706.png;https://issues.apache.org/jira/secure/attachment/13024307/image-2021-04-20-15-18-49-706.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 09:30:37 UTC 2021,,,,,,,,,,"0|z0q8ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/21 07:23;yunta;[~sjwiesman] , I wonder do we really need this feature to ensure users of the legacy {{RocksDBStateBackend}} see consistent logging since we need to introduce other more code and test to just make such a small feature work. If we could just remove such small feature, things could be much simpler.;;;","20/Apr/21 12:36;sjwiesman;Let’s drop it, not worth the trouble;;;","21/Apr/21 09:30;dwysakowicz;Fixed in:
* master
** 60e17f4ff8e58c43f75b40b3f9c139694311b305
* 1.13.0
** 61007c9ff7d1619bbf0b2c1f72bfb3c39aca505d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase hangs on azure,FLINK-22368,13373768,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,dwysakowicz,dwysakowicz,20/Apr/21 06:57,22/Jun/21 13:55,13/Jul/23 08:07,03/May/21 12:56,1.13.0,,,,,,,,1.12.4,1.13.1,1.14.0,,,,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16818&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&l=10144
",,AHeise,dwysakowicz,kevin.cyj,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22019,,,FLINK-22439,,,,,,,,FLINK-17726,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 03 12:56:32 UTC 2021,,,,,,,,,,"0|z0q8co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/21 09:27;arvid;The test doesn't finish as checkpointing gets stuck in the last execution attempt (5):


{noformat}
23:02:26,104 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Streaming Job (5d70bcb288d90589845e39c2953b27c3) switched from state RESTARTING to RUNNING.
23:02:26,118 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: source (2/20) (2d3357d530b11041d123bde87da7584b) switched from INITIALIZING to RUNNING.
... (in total all 100 tasks are RUNNING)
23:02:26,347 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - failing-map (10/20) (23870b8b94e5ea774ca3da72a7ca7251) switched from INITIALIZING to RUNNING.
...
23:02:27,165 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 5d70bcb288d90589845e39c2953b27c3 since some tasks of job 5d70bcb288d90589845e39c2953b27c3 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
23:02:28,165 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 5d70bcb288d90589845e39c2953b27c3 since some tasks of job 5d70bcb288d90589845e39c2953b27c3 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
... (in total 10k failed to trigger...)
01:55:56,165 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 5d70bcb288d90589845e39c2953b27c3 since some tasks of job 5d70bcb288d90589845e39c2953b27c3 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
{noformat}
;;;","20/Apr/21 09:32;arvid;Okay, it makes sense that it doesn't checkpoint as

{noformat}
23:02:26,166 [Source: source (3/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (3/20)#5 (f45d32db48b407a34edc6dc048c5e0c2) switched from RUNNING to FINISHED.
{noformat}

I'm investigating further.;;;","20/Apr/21 09:41;arvid;All source tasks are finished, but the job is not finishing for some reason

{noformat}
23:02:26,166 [Source: source (3/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (3/20)#5 (f45d32db48b407a34edc6dc048c5e0c2) switched from RUNNING to FINISHED.
23:02:26,167 [Source: source (8/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (8/20)#5 (53d62fcb80e6b2e5ee7657033a555d6f) switched from RUNNING to FINISHED.
23:02:26,166 [Source: source (7/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (7/20)#5 (1147339526bf7fadcd47ef579c4c4130) switched from RUNNING to FINISHED.
23:02:26,168 [Source: source (2/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (2/20)#5 (2d3357d530b11041d123bde87da7584b) switched from RUNNING to FINISHED.
23:02:26,170 [Source: source (9/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (9/20)#5 (426fbb3a5b561a61affed4c40b0a8f8a) switched from RUNNING to FINISHED.
23:02:26,171 [Source: source (1/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (1/20)#5 (a5585ddf277047a6d2b67d8d3cf2cd0e) switched from RUNNING to FINISHED.
23:02:26,213 [Source: source (5/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (5/20)#5 (9ad44b9747f266816750e98063306fc4) switched from RUNNING to FINISHED.
23:02:26,216 [Source: source (20/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (20/20)#5 (d92ff5256f0c3e1e2a43c7413f6ce71f) switched from RUNNING to FINISHED.
23:02:26,215 [Source: source (13/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (13/20)#5 (327096e9fc8562fdc0ea12e031f98749) switched from RUNNING to FINISHED.
23:02:26,215 [Source: source (10/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (10/20)#5 (456858f8cd5fca51c09519f29b21641e) switched from RUNNING to FINISHED.
23:02:26,223 [Source: source (18/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (18/20)#5 (8a75c4257e592a008bca8f0a29c5e856) switched from RUNNING to FINISHED.
23:02:26,223 [Source: source (16/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (16/20)#5 (c17c62b90c66f7bd11debe913916fd89) switched from RUNNING to FINISHED.
23:02:26,225 [Source: source (11/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (11/20)#5 (dd463d97b55ee56bcb6e2853760e3daf) switched from RUNNING to FINISHED.
23:02:26,239 [Source: source (19/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (19/20)#5 (202827bf688aa5c206bd3b900ad3beb2) switched from RUNNING to FINISHED.
23:02:26,240 [Source: source (4/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (4/20)#5 (475c75d73150c1d20eaaa185f96c81e1) switched from RUNNING to FINISHED.
23:02:26,245 [Source: source (17/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (17/20)#5 (afb774c210e06bd3e62d42e749c22417) switched from RUNNING to FINISHED.
23:02:26,245 [Source: source (6/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (6/20)#5 (776d580fcfd5ddf38643822311884d70) switched from RUNNING to FINISHED.
23:02:26,246 [Source: source (14/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (14/20)#5 (7eccee9beab3b7e6b5977d5f726e9c9e) switched from RUNNING to FINISHED.
23:02:26,245 [Source: source (15/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (15/20)#5 (45df84d24e611dfd7d972795548a7f33) switched from RUNNING to FINISHED.
23:02:26,252 [Source: source (12/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (12/20)#5 (65cad528fa7a6e0fe6822c4677d8797a) switched from RUNNING to FINISHED.
{noformat}

{{failing-map (8/20)#5}} and none of the sinks (naturally) are not finishing.

I found
{noformat}
23:02:26,422 [failing-map (8/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - failing-map (8/20)#5 (77221af17305e76caafacbf7bc696af7) switched from RUNNING to CANCELED.
23:02:26,425 [failing-map (8/20)#5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for failing-map (8/20)#5 (77221af17305e76caafacbf7bc696af7).
23:02:26,425 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task failing-map (8/20)#5 77221af17305e76caafacbf7bc696af7.
{noformat}

But I haven't found any reason why it's in CANCELED. There is no trace of any FAILING/FAILED in the 5. attempt.;;;","20/Apr/21 14:53;dwysakowicz;Is this the same issue? It fails in the UCRescaleITCase: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16865&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=25088;;;","21/Apr/21 07:43;dwysakowicz;Again rescaling IT case hanged: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16915&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=18487;;;","21/Apr/21 14:48;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16949&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=17374;;;","26/Apr/21 16:20;roman;I also see that the job is being stuck because one of the tasks transitions from RUNNING to CANCELLED (instead of FINISHED). 
 This happens because RemoteInputChannel is polled after it was released. In such a case RemoteInputChannel [throws|https://github.com/apache/flink/blob/e905db9e20950fc605350fad007b1c3e4f09de91/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java#L217] CancelTaskException which triggers task cancellation. Cancellation prevents EndOfPartition event from being propagated and therefore downstream tasks keep running.

 

I've published a PR to prevent enqueing ""released"" channel (on receive); and to validate that the channel hasn't receive EoP (on poll).;;;","03/May/21 12:56;roman;Merged into 1.12 as afc0a9002c425a95c032a94750a5e9ae45fae8d3.
Merged into 1.13 as b41c5fab3e3eebb04b891ec2ac0688bedc2d94eb.
Merged into master as 6e3ccd5a9613a2de06c8ec410ba41e6e0c6959be.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
syncSavepointId should only be reset if it is equal to the checkpoint id,FLINK-22367,13373765,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,dwysakowicz,dwysakowicz,20/Apr/21 06:30,15/Dec/21 01:40,13/Jul/23 08:07,03/Sep/21 15:00,1.13.0,,,,,,,,1.12.8,1.13.3,,,,,,Runtime / Checkpointing,,,,,0,auto-deprioritized-critical,auto-unassigned,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16818&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&l=3844

{code}
[ERROR] Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 13.135 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase
Apr 19 22:28:44 [ERROR] terminateWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished(org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase)  Time elapsed: 10.237 s  <<< ERROR!
Apr 19 22:28:44 java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.stopWithSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
Apr 19 22:28:44 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
Apr 19 22:28:44 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
Apr 19 22:28:44 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.stopWithSavepointNormalExecutionHelper(JobMasterStopWithSavepointITCase.java:123)
Apr 19 22:28:44 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.terminateWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished(JobMasterStopWithSavepointITCase.java:111)
Apr 19 22:28:44 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 19 22:28:44 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 19 22:28:44 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 19 22:28:44 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Apr 19 22:28:44 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 19 22:28:44 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 19 22:28:44 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 19 22:28:44 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 19 22:28:44 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 19 22:28:44 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 19 22:28:44 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 19 22:28:44 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 19 22:28:44 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 19 22:28:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 19 22:28:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 19 22:28:44 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 19 22:28:44 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 19 22:28:44 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Apr 19 22:28:44 	at org.junit.runners.Suite.runChild(Suite.java:128)
Apr 19 22:28:44 	at org.junit.runners.Suite.runChild(Suite.java:27)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 19 22:28:44 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Apr 19 22:28:44 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
Apr 19 22:28:44 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
Apr 19 22:28:44 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
Apr 19 22:28:44 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
Apr 19 22:28:44 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
Apr 19 22:28:44 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
Apr 19 22:28:44 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Apr 19 22:28:44 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Apr 19 22:28:44 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Apr 19 22:28:44 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,akalashnikov,dwysakowicz,pnowojski,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 03 15:00:50 UTC 2021,,,,,,,,,,"0|z0q8c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/21 09:47;chesnay;hmm... it seems like a task isn't shutting down after a stopWithSavepoint operation. The savepoint operation itself seems to work fine, but only 1 subtask shuts down afterwards. After 10 seconds the test is timing out:
{code:java}
22:28:32,329 [  testVertex (2/2)#0] INFO  o.a.f.r.taskmanager.Task                    [] - testVertex (2/2)#0 (043f6b5353f19eee95021c9988dc2907) switched from DEPLOYING to INITIALIZING.
22:28:32,330 [  testVertex (2/2)#0] INFO  o.a.f.r.taskmanager.Task                    [] - testVertex (2/2)#0 (043f6b5353f19eee95021c9988dc2907) switched from INITIALIZING to RUNNING.
22:28:32,331 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - testVertex (2/2) (043f6b5353f19eee95021c9988dc2907) switched from DEPLOYING to INITIALIZING.
22:28:32,331 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - testVertex (2/2) (043f6b5353f19eee95021c9988dc2907) switched from INITIALIZING to RUNNING.
22:28:32,343 [    Checkpoint Timer] INFO  o.a.f.r.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1618871312339 for job 15f6c6d1527f3cbd07771ec998fbb38f.
22:28:32,344 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.jobmaster.JobMaster                 [] - Triggering stop-with-savepoint for job 15f6c6d1527f3cbd07771ec998fbb38f.
22:28:32,351 [jobmanager-future-thread-10] WARN  o.a.f.r.checkpoint.CheckpointCoordinator    [] - Received late message for now expired checkpoint attempt 1 from task 0febd0493e4d0151092e5d9f1d11f8d7 of job 15f6c6d1527f3cbd07771ec998fbb38f at 0acad546-425d-4c54-81c6-318541f6fc12 @ localhost (dataPort=-1).
22:28:32,352 [jobmanager-future-thread-11] WARN  o.a.f.r.checkpoint.CheckpointCoordinator    [] - Received late message for now expired checkpoint attempt 1 from task 043f6b5353f19eee95021c9988dc2907 of job 15f6c6d1527f3cbd07771ec998fbb38f at 0acad546-425d-4c54-81c6-318541f6fc12 @ localhost (dataPort=-1).
22:28:32,358 [    Checkpoint Timer] INFO  o.a.f.r.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2 (type=SAVEPOINT_TERMINATE) @ 1618871312356 for job 15f6c6d1527f3cbd07771ec998fbb38f.
22:28:32,405 [jobmanager-future-thread-7] INFO  o.a.f.r.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 2 for job 15f6c6d1527f3cbd07771ec998fbb38f (0 bytes in 48 ms).
22:28:32,413 [  testVertex (1/2)#0] INFO  o.a.f.r.taskmanager.Task                    [] - testVertex (1/2)#0 (0febd0493e4d0151092e5d9f1d11f8d7) switched from RUNNING to FINISHED.
22:28:32,413 [  testVertex (1/2)#0] INFO  o.a.f.r.taskmanager.Task                    [] - Freeing task resources for testVertex (1/2)#0 (0febd0493e4d0151092e5d9f1d11f8d7).
22:28:32,414 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task testVertex (1/2)#0 0febd0493e4d0151092e5d9f1d11f8d7.
22:28:32,423 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - testVertex (1/2) (0febd0493e4d0151092e5d9f1d11f8d7) switched from RUNNING to FINISHED.22:28:42,376 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - Job Unnamed job (15f6c6d1527f3cbd07771ec998fbb38f) switched from state RUNNING to CANCELLING.


22:28:42,377 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 0febd0493e4d0151092e5d9f1d11f8d7.
22:28:42,378 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.executiongraph.ExecutionGraph       [] - testVertex (2/2) (043f6b5353f19eee95021c9988dc2907) switched from RUNNING to CANCELING.
22:28:42,382 [flink-akka.actor.default-dispatcher-6] INFO  o.a.f.r.taskmanager.Task                    [] - Attempting to cancel task testVertex (2/2)#0 (043f6b5353f19eee95021c9988dc2907).
22:28:42,383 [flink-akka.actor.default-dispatcher-6] INFO  o.a.f.r.taskmanager.Task                    [] - testVertex (2/2)#0 (043f6b5353f19eee95021c9988dc2907) switched from RUNNING to CANCELING.
22:28:42,383 [flink-akka.actor.default-dispatcher-6] INFO  o.a.f.r.taskmanager.Task                    [] - Triggering cancellation of task code testVertex (2/2)#0 (043f6b5353f19eee95021c9988dc2907).
22:28:42,385 [  testVertex (2/2)#0] INFO  o.a.f.r.taskmanager.Task                    [] - testVertex (2/2)#0 (043f6b5353f19eee95021c9988dc2907) switched from CANCELING to CANCELED.{code};;;","20/Apr/21 10:41;chesnay;I couldn't reproduce the issue locally.;;;","22/Apr/21 09:51;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16996&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4651;;;","19/May/21 10:50;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","26/May/21 22:59;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","03/Jun/21 23:33;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","11/Jun/21 22:41;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","30/Jun/21 08:07;roman;1.13

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19704&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a];;;","13/Jul/21 03:07;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20348&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0&l=4913;;;","27/Jul/21 13:51;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20998&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=10063;;;","06/Aug/21 03:17;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21642&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=4670;;;","06/Aug/21 04:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21644&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc&l=4084;;;","16/Aug/21 11:22;akalashnikov;[~dwysakowicz] this ticket does still make sense? As I see this test(terminateWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished) was removed by you in FLINK-23408 and as I understand since that we don't have such error(in other tests);;;","17/Aug/21 08:21;xtsong;I think this still makes sense for the release 1.13 series. I've updated the fix version.;;;","03/Sep/21 15:00;dwysakowicz;Fixed in:
* 1.13.3
** 9f60a51f9460d8bfba42377303f9d79d7e1e9ad6
* 1.12.6
** 87ce3ac3c9a9c30eab0cb378575592e9a857665f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Images are not displayed in ""Data Sources"" documentation page",FLINK-22365,13373746,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jark,jark,20/Apr/21 04:47,08/Jun/21 10:22,13/Jul/23 08:07,08/Jun/21 10:22,,,,,,,,,,,,,,,,Documentation,,,,,0,auto-deprioritized-critical,,,,,See https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/dev/datastream/sources/ ,,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 10:22:13 UTC 2021,,,,,,,,,,"0|z0q87s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/21 04:48;jark;cc [~sjwiesman];;;","27/Apr/21 23:30;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","19/May/21 10:52;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/21 10:22;chesnay;I believe this has been fixed in the meantime.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python tests do not pass on 1.x branch if it has not been released,FLINK-22359,13373660,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,dwysakowicz,dwysakowicz,19/Apr/21 16:45,28/Aug/21 11:16,13/Jul/23 08:07,20/Apr/21 06:19,1.13.0,,,,,,,,1.13.0,,,,,,,API / Python,Test Infrastructure,,,,0,pull-request-available,,,,,"The python tests fail because they try to download something for 0.0 version.

{code}
Apr 19 11:39:38   Downloading https://files.pythonhosted.org/packages/6c/38/ff06cec1b32e796c8422153d6d29a6c8c6dab962436779e34b0d72df0f2f/grpcio-tools-1.14.2.tar.gz (1.9MB)
Apr 19 11:39:38 Collecting apache-flink-libraries
Apr 19 11:39:38   Downloading https://files.pythonhosted.org/packages/6c/b1/78dcaec55a437c3b8b3eb479b169d7fac15e86ffe9bd7340767934ceab2f/apache_flink_libraries-0.0.tar.gz (3.5MB)
Apr 19 11:39:38     Complete output from command python setup.py egg_info:
Apr 19 11:39:38     The flink core files are not found. Please make sure your installation package is complete, or do this in the flink-python directory of the flink source directory.

{code}",,dian.fu,dwysakowicz,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22363,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 10:28:49 UTC 2021,,,,,,,,,,"0|z0q7oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 16:45;dwysakowicz;cc [~dian.fu] [~hxbks2ks];;;","20/Apr/21 02:30;hxbks2ks;The problem comes in updating the version in version.py when we cut a new branch. This will lead to the wrong version number, which will cause building failure of the apache-flink-libraries from source code. It will download apache-flink-libraries from PyPi if it failed to build and install apache-flink-libraries from source code when testing in Azure Pipeline.;;;","20/Apr/21 06:19;hxbks2ks;Merged into release-1.13 via adeb0dbe30618b1d9e3856fcb6071fe88ba15811;;;","20/Apr/21 08:33;chesnay;Shouldn't the change to the scripts be merged to master? Now we're gonna run into the same issue again in 1.14.;;;","20/Apr/21 08:56;hxbks2ks;Merged the commit 9e11078f483800776e9b0e12c0a9adb955bf49e9 of updating create_snapshot_branch.sh into master.
[~chesnay] Thanks a lot for the reminding.;;;","20/Apr/21 10:28;dwysakowicz;For reporting it also caused e2e kubernetes tests to fail: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16822&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=4278;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem/Hive partition file is not committed when watermark is applied on rowtime of TIMESTAMP_LTZ type,FLINK-22356,13373625,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,jark,jark,19/Apr/21 13:53,28/May/21 09:13,13/Jul/23 08:07,25/Apr/21 14:52,,,,,,,,,1.13.1,1.14.0,,,,,,Connectors / FileSystem,Connectors / Hive,Table SQL / API,,,0,pull-request-available,,,,,"

{code:sql}
set execution.checkpointing.interval = 10s;
set table.local-time-zone = 'Asia/Shanghai';

create table mysource (
  ms bigint,
  ts as to_timestamp_ltz(ms, 3),
  watermark for ts as ts - interval '0.001' second
) with (
  'connector' = 'socket',
  'format' = 'json',
  'hostname' = '127.0.0.1',
  'port' = '9999'
);

CREATE TABLE fs_table2 (
    ms bigint,
  dt STRING,
  `hour` STRING,
  `mm` string
) PARTITIONED BY (dt, `hour`, `mm`) WITH (
  'connector'='filesystem',
  'path'='/Users/wuchong/Downloads/fs_table2',
  'format'='csv',
  'sink.partition-commit.delay'='1min',
  'sink.partition-commit.policy.kind'='success-file',
  'sink.rolling-policy.rollover-interval' = '30s',
  'sink.rolling-policy.check-interval' = '30s',
  'sink.partition-commit.trigger'='partition-time',
  'partition.time-extractor.timestamp-pattern' = '$dt $hour:$mm:00'
);

insert into  fs_table2
SELECT ms,
DATE_FORMAT(ts, 'yyyy-MM-dd'), DATE_FORMAT(ts, 'HH'), DATE_FORMAT(ts, 'mm')
FROM mysource;
{code}


Enther some data in socket:

{code}
> nc -lk 9999
{""ms"": 1618839600000}
{""ms"": 1618839600123}
{""ms"": 1618839600456}
{""ms"": 1618839600789}
{""ms"": 1618839660000}
{""ms"": 1618839660123}
{""ms"": 1618839660456}
{""ms"": 1618839660789}
{""ms"": 1618839720000}
{""ms"": 1618839780000}
{""ms"": 1618839840000}
{""ms"": 1618839900000}
{""ms"": 1618839960000}
{""ms"": 1618840020000}
{code}

However, all the files are not committed (not {{_SUCCESS}} file):
{code}
➜  hour=21 tree
.
├── mm=40
│   └── part-cf06c6da-d301-4623-832c-9e0f356f6fb4-0-0
├── mm=41
│   └── part-cf06c6da-d301-4623-832c-9e0f356f6fb4-0-1
├── mm=42
│   └── part-cf06c6da-d301-4623-832c-9e0f356f6fb4-0-2
├── mm=43
│   └── part-cf06c6da-d301-4623-832c-9e0f356f6fb4-0-3
├── mm=44
│   └── part-cf06c6da-d301-4623-832c-9e0f356f6fb4-0-4
├── mm=45
│   └── part-cf06c6da-d301-4623-832c-9e0f356f6fb4-0-5
├── mm=46
│   └── part-cf06c6da-d301-4623-832c-9e0f356f6fb4-0-6
└── mm=47
    └── part-cf06c6da-d301-4623-832c-9e0f356f6fb4-0-7

8 directories, 8 files
{code}
",,hackergin,jark,leonard,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 25 14:52:40 UTC 2021,,,,,,,,,,"0|z0q7gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 13:57;jark;You can also reproduce this problem by using a simple datagen source 

{code:sql}
create table datagen_source (
    order_number BIGINT,
    price        int,
    buyer        string,
    ts as cast(current_timestamp as timestamp_ltz(3)),
    proctime as proctime(),
    watermark for ts as ts - interval '0.001' second
) with (
    'connector' = 'datagen',
    'rows-per-second' = '1'
);
{code};;;","21/Apr/21 14:08;leonard;The root cause of this issue is the the extracted partition time is a TIMESTAMP literal from hive partition field, the `PartitionTimeCommitter` use watermark to compare the TIMESTAMP value, if the input rowtime field of TableSink is TIMESTAMP_LTZ, we need to shift the TIMESTAMP literal and then compare with watermark.
 But currently the rowtime field Datatype information is unreachable for TableSink, what I can image is introducing a method in DynamicTableSink.Context
{code:java}
public interface DynamicTableSink {

    ChangelogMode getChangelogMode(ChangelogMode requestedMode);
    
    SinkRuntimeProvider getSinkRuntimeProvider(Context context);

    DynamicTableSink copy();

    interface Context {

        boolean isBounded();
 
       /**
         * Returns the DataType of the input time attribute field.
         *
         * <p>This information can be used to compare a table partition value with watermark in
         * partition commit scenario.
         */
        Optional<DataType> getTimeAttributeDataType();

....{code}
 

Do you have any suggestion?  [~twalthr] [~jark];;;","21/Apr/21 14:23;leonard;I create an initial PR based on above comment, I'll add tests if we can reach an agreement or we have another idea to fix this issue.;;;","21/Apr/21 15:22;twalthr;[~Leonard Xu] I would like to avoid making {{DynamicTableSink.Context}} more complicated. It took a long time to make the interfaces as concise as possible. Can't we fix the issue on the Hive side only? The sink factory can simply access the resolved schema and check the watermark type and pass it to the sink, no? Btw isn't any usage of {{org.apache.flink.table.filesystem.DefaultPartTimeExtractor#toMills}} invalid, not only for {{PartitionTimeCommitter}}.;;;","22/Apr/21 02:41;jark;> Can't we fix the issue on the Hive side only? The sink factory can simply access the resolved schema and check the watermark type and pass it to the sink, no?
We can't. Because the watermark information is on source table, not sink table, so Hive sink factory can't access to the watermark information. 

> Btw isn't any usage of org.apache.flink.table.filesystem.DefaultPartTimeExtractor#toMills invalid, not only for PartitionTimeCommitter.
The another usage of {{#toMillis}} is used for comparing ordering of partitions, so it's used validly. ;;;","22/Apr/21 08:49;twalthr;I see. How about we expose the rowtime index through the context instead? I guess this could be useful for a couple of use cases, not only this scenario. We also did it in {{TransformationSinkProvider}} and it could help for this use case as well?;;;","22/Apr/21 09:06;jark;Sounds good to me [~twalthr]. ;;;","22/Apr/21 09:31;leonard;Thanks [~jark] and [~twalthr] ,  but maybe rowtime index is not enough?
 The *consumeDataType* is not the query output DataType, but the schema of TableSink, it doesn't contains row time attribute.
{code:java}
<T> TypeInformation<T> createTypeInformation(DataType consumedDataType);
{code}
And I also consider if we extract the time attribute type from input *DataStream<RowData*>'s TypeInformation in *SinkRuntimeProvider*, but it's pretty weak.;;;","22/Apr/21 10:14;twalthr;It doesn't contain the ""attribute"" information anymore but it contains the information whether the field is TIMESTAMP or TIMESTAMP_LTZ, no? Regarding physical columns, {{consumedDataType}} and {{outputDataType}} should be equal at the beginning of the row modulo metadata. ;;;","22/Apr/21 11:53;leonard;I found two bad cases :( after rethink our solution,  1. the query may not contain row time field  2. user may use implicit cast eg cast TIMESTAMP_LTZ  field to TIMESTAMP field. I think we need a watermark metadataHandler to infer the watermark output(time attribute field) type and then expose to sink.;;;","22/Apr/21 13:37;twalthr;Yes, the query may not contain a row time field. I'm wondering when we decided that a watermark can be both TIMESTAMP or TIMESTAMP_LTZ, this makes the design pretty complicated. I thought we wanted to make watermarks always TIMESTAMP_LTZ (at least internally), even though the schema might return a different type for the expression. Given that watermarks are always TIMESTAMP_LTZ, all we need to do is converting the partition timestamp using the session time zone in the sink. ;;;","22/Apr/21 13:50;twalthr;Also, how do you derive a watermark type from a join between a table with rowtime TIMESTAMP and a table with rowtime TIMESTAMP_LTZ?;;;","22/Apr/21 14:14;twalthr;I have to admit that I haven't looked into this time topic for quite some time after the FLIP has been accepted. Why do we even allow TIMESTAMP for rowtime? I thought we agreed on hard breaking pipelines for the greater good and FLIP-162 mentions only rowtime as TIMESTAMP_LTZ similar to PROCTIME(). This restriction would also solve FLINK-22378 easily.;;;","22/Apr/21 16:48;leonard;I think what we want to achieve is that if users' data source used long time, we recommend him/her use TIMESTAMP_LTZ, if users' data source used timestamp string time, we recommend him/her use TIMESTAMP,  and the later way is the most common way before we enhance TIMESTAMP_LTZ as row/proc time, it's too radical to disable TIMESTAMP as row time from my understanding.;;;","23/Apr/21 06:14;twalthr;I understand the reasoning behind it but this is also achievable by casting the TIMESTAMP to TIMESTAMP_LTZ under the hood. We could make it possible that WATERMARK is always TIMESTAMP_LTZ by inserting a CAST(watermark_expression AS TIMESTAMP_LTZ) in the watermark generator. In this case, we only need to worry about casting TIMESTAMP to TIMESTAMP_LTZ in window operators etc.;;;","23/Apr/21 15:50;leonard;After offline discussion with Timo and Jark,  we reach a consensus for he quick fix.
 we have two user case will meet this issue： # user reported in mail list that they defined a long({{Sytem.currentTimeMillis}}, e.g. 0 ) in DataStream and used java.sql.Timestamp(e.g {{1970-01-01 08:00:00}} in Shanghai) to define the row time and then convert to Table, the watermark in table is extended from DataStream, if the partition time field is from event time field, when comparing the watermark with the partition time field, this issue happens. I admit this is a wrong usage but some users used.

  2. This issue described, user defined a watermark on TIMESTAMP_LTZ column, and the partition-time field is aligned with local timestamp, this issue happens.

we think the simplest way is to add a configuration for hive connector that used in comparing watermark long value and partition timestamp string. 
  
 Thus, we finally proposed a configuration to fix this case. *`sink.partition-commit.watermark-time-zone`,* it means the time zone to parse the long watermark value to TIMESTAMP value, the parsed watermark timestamp is used to compare with partition time to decide the partition should commit or not. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is user configured time zone, the the value should be the user configured local time zone.
  ;;;","25/Apr/21 14:52;jark;Fixed in 
 - master: bddcbc4f93a8d473eafc0f9a1f976d43aa3af4ca
 - release-1.13: 740ad3df3ff6a33cb33223a71e174f989b5b04aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple Task Manager Memory Model image does not show up,FLINK-22355,13373618,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ShawnHx,dwysakowicz,dwysakowicz,19/Apr/21 13:29,23/Sep/21 17:20,13/Jul/23 08:07,08/May/21 06:19,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Documentation,,,,,0,pull-request-available,stale-critical,starter,,,An image does not show up in https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/ after migrating to Hugo.,,dwysakowicz,ShawnHx,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/21 13:29;dwysakowicz;gnome-shell-screenshot-AMVR10.png;https://issues.apache.org/jira/secure/attachment/13024293/gnome-shell-screenshot-AMVR10.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 08 06:19:02 UTC 2021,,,,,,,,,,"0|z0q7fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/21 23:30;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","08/May/21 02:48;ShawnHx;Hi, [~dwysakowicz].

I'm willing to fix this. Could you assign it to me ?;;;","08/May/21 03:12;xtsong;[~ShawnHx], you're assigned.;;;","08/May/21 06:19;xtsong;Fixed via
- master (1.14): e6ef269718340a94ecc7d6c6a8d6555ebdda16df
- release-1.13: 908752919e2f7518eaa5e38f4b36e796caa57baf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to define watermark on computed column of CURRENT_TIMESTAMP and LOCALTIMESTAMP,FLINK-22354,13373611,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,jark,jark,19/Apr/21 12:35,28/Aug/21 11:17,13/Jul/23 08:07,21/Apr/21 14:30,,,,,,,,,1.13.0,1.14.0,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"It is very common to define watermarks on {{localtimestamp}} and {{current_tiemstamp}} to support *ingestion time*.  However, the following DDLs failed in v1.13. I also tested the following DDLs can pass in v1.12.1, so this is a regression. 

The root cause may be introduced by FLINK-21435 which adds a [strict check to only allow  precision = 3|https://github.com/apache/flink/blob/f51168041512b0473decabb2088c1ff4fa4f34bc/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/DefaultSchemaResolver.java#L225] (precision = 0 should also can be time attribute), however the precision of return type of {{current_timestamp}} and {{localtimestamp}} are 0 (another long-lived bug). We should fix them both. 

{code:sql}
Flink SQL> create table datagen_source (
>     order_number BIGINT,
>     price        int,
>     buyer        string,
>     ts as current_timestamp,
>     proctime as proctime(),
>     watermark for ts as ts - interval '0.001' second
> ) with (
>     'connector' = 'datagen',
>     'rows-per-second' = '1'
> );
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Invalid data type of time field for watermark definition. The field must be of type TIMESTAMP(3) or TIMESTAMP_LTZ(3).

Flink SQL> create table datagen_source (
>     order_number BIGINT,
>     price        int,
>     buyer        string,
>     ts as cast(current_timestamp as timestamp_ltz(3)),
>     proctime as proctime(),
>     watermark for ts as ts - interval '0.001' second
> ) with (
>     'connector' = 'datagen',
>     'rows-per-second' = '1'
> );
[INFO] Execute statement succeed.

Flink SQL> create table datagen_source2 (
>     order_number BIGINT,
>     price        int,
>     buyer        string,
>     ts as localtimestamp,
>     proctime as proctime(),
>     watermark for ts as ts - interval '0.001' second
> ) with (
>     'connector' = 'datagen',
>     'rows-per-second' = '1'
> );
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Invalid data type of time field for watermark definition. The field must be of type TIMESTAMP(3) or TIMESTAMP_LTZ(3).
{code}",,dwysakowicz,jark,joemoe,leonard,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 14:30:55 UTC 2021,,,,,,,,,,"0|z0q7ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 12:37;jark;cc [~dwysakowicz], [~twalthr], personally, I think this can be a blocker. But I also want to hear your opinions.;;;","20/Apr/21 10:05;twalthr;I'm wondering whether we should actually use {{CURRENT_TIMESTAMP}} and {{LOCALTIMESTAMP}} to model ingestion time. Because for batch those functions will be constant and I guess even in batch you actually want {{CURRENT_ROW_TIMESTAMP}} semantics. Shouldn't we just advertise `PROCTIME()` for this purpose? But it is true that the check might be too strict it should be {{precision <= 3}}.;;;","20/Apr/21 12:30;leonard;There're are some user feedback from the user mail list that users are using {{CURRENT_TIMESTAMP, or NOW() as rowtime, I think the backward compatibility is Jark's concern. The recommended way should be CURRENT_ROW_TIMESTAMP() and PROCTIME() as Timo described.}}

I also found some function precision is 0  like CURRENT_TIMESTAMP but the underlying data is in milliseconds, I'd like to fix them to 3 at the same time. ;;;","20/Apr/21 12:51;jark;[~twalthr], I think we can't provide batch&streaming unified/consistent result for ingestion-time and processing-time. So I think {{CURRENT_ROW_TIMESTAMP}} or {{CURRENT_TIMESTAMP}} doesn't make big difference if using batch mode. 

Besides, we can't use {{PROCTIME()}} for this purpose, because it will be a processing-time window. The core differences between ingestion-time and processing-time is where the time is materialized. Ingestion-time is useful in some cases, for example, cascading window aggreation, because the second processing-time windows will re-calculate assigned windows according to wall-clock instead of the window rowtime/proctime. ;;;","20/Apr/21 12:57;twalthr;[~Leonard Xu] I thought the FLIP does cover the exact return types of these functions. The FLIP states {{CURRENT_TIMESTAMP Flink proposed changes 2020-12-28 23:52:52}}, so I thought we agreed on a precision of 0. We should perform some research how other vendors are handling this. For example, Oracle is 6 and MySQL is 0. In any case, the precision should be configurable per call like {{CURRENT_TIMESTAMP(6)}} like other vendors offer it.;;;","20/Apr/21 13:05;twalthr;[~jark] Maybe there was some misunderstanding. I totally agree that we need ingestion time semantics as well. And you are right that we can't use {{PROCTIME}} for this. We are also forbidding defining a watermark on a processing time column. So {{CURRENT_ROW_TIMESTAMP}} should be the way to go. {{CURRENT_TIMESTAMP}} could be locked on query start.;;;","20/Apr/21 14:05;leonard;[~twalthr] MySQL is 0, Oracle and postgresql is 6, Snowflake is 9, I think in big data area, milliseconds is more used, and our watermark alway support to milliseconds precision. In the past, many users wrongly using these functions but they can still work because the real value returned(code-gened) is in millisecond and the rowtime checks isn't strict to 3, it's strange to connector/UDF developer the underlying data value is in millisecond but the DataType precision is 0, that's why I want to fix the wrong behavior.

I think the configurable precision is right thing and we should improve it in the future. In the FLIP we didn't discuss the precision but focus on the Type and behavior, and I think it's not a part with the FLIP, should be an existed wrong behavior from my side. ;;;","20/Apr/21 14:14;twalthr;I'm fine with precision 3 if this is a well-chosen decision. Regarding the configurable precision, if the functions would use the new built-in function stack this change would be straight forward and consistent. We should aim to support the configurable precious ideally in this release. We should not block the release on this, but if we manage to get it in the story would definitely be rounder. Because users are used to the semantics of those functions.;;;","20/Apr/21 14:30;leonard;I used the new function stack and its very useful to add some new functions, but for this case maybe it's not a good solution,  calcite only supports the function `LOCALTIMETIMESTAMP` and doesn't support `LOCALTIMESTAMP()`, and the new function stack doesn't supports `CURRENT_ROW_TIMESTAMP` style function that's why I finally work around with `CURRENT_ROW_TIMESTAMP()`(see https://issues.apache.org/jira/browse/CALCITE-4545).  The configurable time function should be achieved, but I think it isn't current time point, and this should be a new feature? So I tend to just fix the wrong behavior in this issue and introduce configurable time function in the future. WDYT? [~twalthr];;;","20/Apr/21 15:02;twalthr;[~Leonard Xu] thanks for the background information, this sounds reasonable. Yes, let's fix the wrong default behavior first.

Afterwards, we can discuss if we want to consider the configurable time function a feature or not. Personally, I would be fine to merge it to 1.13 to finalize the time function story as quickly as possible.;;;","20/Apr/21 15:15;leonard;Thanks [~twalthr] , I'll open the PR to fix current behavior ASSP. ;;;","21/Apr/21 02:23;jark;We can open another issue to discuss the precision configurable time functions. ;;;","21/Apr/21 07:50;dwysakowicz;Thanks for the discussion. If I understood it correctly there are two aspects here:

1. Changing the default precision (this blocks the release)
2. Adding configurable functions (nice to have, sort of a new feature)

From my point of view, we are approaching a point when we can try to have a release. We have already postponed it quite a bit. Therefore I'd suggest to fix the point 1. as quickly as possible. For the point 2. I'd recommend merging it first only to the main branch. We can revisit the decision if we should include it in 1.13 if there are further delays because of other issues. WDYT?

BTW. [~Leonard Xu] Could you please move the ticket to ""in progress""?;;;","21/Apr/21 08:05;jark;[~dwysakowicz] sounds good to me. 
;;;","21/Apr/21 14:30;jark;Fixed in 
 - master: d3043735f50f52c783eb4c7777c9b3c6df4972d7...3a2d8251a7a41a5ef68db7c3b2151e670c9a966d
 - release-1.13: 74c6a062063e0dd7dbf5f9036fd4c810471da1a5...cdf65b690c79cc940ea1b42d0b8506540071900f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCALTIMESTAMP doesn't return correct result when setting session time zone to 'UTC-10:00',FLINK-22349,13373552,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,jark,jark,19/Apr/21 09:01,28/Aug/21 11:17,13/Jul/23 08:07,21/Apr/21 09:57,,,,,,,,,1.13.0,1.14.0,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"
{code:sql}
set execution.runtime-mode = batch;
set sql-client.execution.result-mode = tableau;
set table.local-time-zone = 'UTC-10:00';
select 
  LOCALTIME,
  CURRENT_DATE,
  LOCALTIMESTAMP,
  CURRENT_TIME,
  CURRENT_TIMESTAMP,
  NOW(),
  CURRENT_ROW_TIMESTAMP(),
  PROCTIME()
from (values (1), (2), (3), (4), (5)) as T(a);

+--------------+--------------+-------------------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+
|    LOCALTIME | CURRENT_DATE |          LOCALTIMESTAMP | CURRENT_TIME |       CURRENT_TIMESTAMP |                  EXPR$5 |                  EXPR$6 |                  EXPR$7 |
+--------------+--------------+-------------------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+
| 09:00:58.905 |   2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 |
| 09:00:58.905 |   2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 |
| 09:00:58.905 |   2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 |
| 09:00:58.905 |   2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 |
| 09:00:58.905 |   2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 |
+--------------+--------------+-------------------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+
5 rows in set

{code}",,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 09:57:11 UTC 2021,,,,,,,,,,"0|z0q70o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 09:57;jark;Fixed in 
 - master: 4be9aff3eccb3808df1f10ef7c30480ec11a9cb0
 - release-1.13: 5f666c3731e5c41e06e1cc759fb5c2eb550df6f0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the Python operators of Python DataStream API doesn't use managed memory in execute_and_collect,FLINK-22348,13373534,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,19/Apr/21 08:02,28/Aug/21 11:17,13/Jul/23 08:07,21/Apr/21 12:51,1.13.0,,,,,,,,1.13.0,,,,,,,API / Python,,,,,0,pull-request-available,,,,,,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 12:51:53 UTC 2021,,,,,,,,,,"0|z0q6wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 12:51;dian.fu;Fixed in
- master via 8b2fadaad8647704203c5faf011920d8fb7f7c09
- release-1.13 via 9f4025123d7f9da2d05cafea5b7cd0f186dab467;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql-client-defaults.yaml should be removed from dist,FLINK-22346,13373515,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,jark,jark,19/Apr/21 07:35,28/Aug/21 11:16,13/Jul/23 08:07,20/Apr/21 13:58,,,,,,,,,1.13.0,1.14.0,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"According to https://cwiki.apache.org/confluence/display/FLINK/FLIP-163%3A+SQL+Client+Improvements#FLIP163:SQLClientImprovements-Compatibility,Deprecation,andMigrationPlan, the {{sql-client-defaults.yaml}} will be removed since 1.13. However, it is still there. I think this will make users misunderstand that it is still encouraged to be used. Besides, the default result of {{SET;}} command will contain lots of DEPRECATE informations which is confusing. ",,hackergin,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 13:58:23 UTC 2021,,,,,,,,,,"0|z0q6sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 07:36;jark;cc [~fsk119];;;","20/Apr/21 13:58;jark;Fixed in
 - master: cd3415fe9de7fcf49831244445552011c8f4b950
 - release-1.13: 394822776d0d7080178f02a9df936026ac496f3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatorEventsExactlyOnceITCase hangs on azure,FLINK-22345,13373489,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,dwysakowicz,dwysakowicz,19/Apr/21 07:16,23/Apr/21 15:22,13/Jul/23 08:07,23/Apr/21 15:22,1.13.0,1.14.0,,,,,,,1.12.3,1.13.0,,,,,,API / DataStream,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16731&view=logs&j=02c4e775-43bf-5625-d1cc-542b5209e072&t=e5961b24-88d9-5c77-efd3-955422674c25&l=9896

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007fa8c800b800 nid=0x58b3 waiting on condition [0x00007fa8cfd1c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000008147a7e8> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:802)
	at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test(CoordinatorEventsExactlyOnceITCase.java:187)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,dwysakowicz,gaoyunhaii,kezhuw,maguowei,mapohl,rmetzger,sewen,Thesharing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21996,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/21 20:58;sewen;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13024409/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 23 15:20:00 UTC 2021,,,,,,,,,,"0|z0q6mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 07:17;dwysakowicz;Could you have a look [~sewen]? ;;;","19/Apr/21 07:18;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16731&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11303;;;","19/Apr/21 11:45;sewen;I will take a look, thanks for pinging me.;;;","19/Apr/21 14:30;dwysakowicz;Thanks!;;;","20/Apr/21 06:50;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16818&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11494;;;","20/Apr/21 08:30;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=402&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=7561;;;","21/Apr/21 07:18;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16904&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11673;;;","21/Apr/21 15:29;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=404&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b&l=9874;;;","21/Apr/21 21:02;sewen;I think I found the reason. In all jobs, the scheduler gets lock up over this assertion:
(it has been there for a while)

 !screenshot-1.png! 

This causes the scheduler to get stuck in a loop continuously throwing this exception:

{code}
22:44:02,804 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Trying to recover from a global failure.
java.lang.AssertionError: null
	at org.apache.flink.runtime.scheduler.SchedulerBase.restoreState(SchedulerBase.java:403) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.restartTasks(DefaultScheduler.java:290) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$null$1(DefaultScheduler.java:260) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:719) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:701) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_282]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[classes/:?]
{code}

I think the assertion is incorrect, because the global failure doesn't always reset all vertices, if failures overlap, and some vertices have been reset before and not yet been deployed. Please [~trohrmann] or [~mapohl] or [~rmetzger] or [~chesnay], if one of you could confirm this.

So the fix is probably to simply remove that assertion.;;;","21/Apr/21 21:29;sewen;I pushed a fix to remove this assertion (which in any case doesn't seem to hold) to have this fix in the 1.13 release.

Still, a confirmation whether the diagnosis is correct would be good to make sure there isn't yet another issue at work.

;;;","21/Apr/21 23:54;sewen;With that fix applied, the situation that would previously get the scheduler caught in the assertion failure loop now causes a strange situation:

The scheduler calls ""restoreState()"" for recovery from a global failure in a case where not all tasks had new execution attempts created (here actually none of the tasks seem to have gotten new execution attempts). So the state is restored to the CANCELED executions, not the new executions.

{code}
22:36:19,156 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - operator-2 (1/1) (563c234c87f6e32c414755d47a9bca0a) switched from CANCELING to CANCELED.
22:36:19,160 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 1deff4638a055a66c9a19fffcea6c44f: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
22:36:19,186 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - operator-1 (1/1) (49a2e08ca4abb6aaf844475a0fbfdb21) switched from CANCELING to CANCELED.
22:36:19,194 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 1deff4638a055a66c9a19fffcea6c44f
22:36:20,085 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
22:36:20,085 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Resetting the Operator Coordinators to an empty state.
{code}

That trips another assertion in the coordinator code (which could be removed), but I think it does indeed seem strange that this happens.
I am wondering if there isn't a bug in the scheduler that it gets inconsistent when dealing with multiple concurrent global failures. Or will it simply restore the state twice (once to the cancelled attempts, once again to the recovered attempts)?

In this test run, the problem occurs in multiple profiles:

https://dev.azure.com/sewen0794/Flink/_build/results?buildId=291&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b;;;","22/Apr/21 09:50;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16996&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11498;;;","22/Apr/21 09:52;sewen;Fixed in

1.14.0 (master) via
  - 02e7db875920b34099cfb570089da20da6a88cb1
  - e3cfa40bf67def24d4b528055bb479298241764d

1.13.0 (release-1.13) via
  - 41f013db8605ce47cbc9b1e7b69c3cb0359873ef
  - 9400f880699d38eb065917ba6e9f13ef5632654f

1.12.3 (release-1.12) via
  - 13160bb001849bd4d487f024972bf4091e82a8e6
  - a555c396167118c5f3510a09dadb3540d9ab773f
;;;","22/Apr/21 09:56;sewen;After a discussion with [~trohrmann], we concluded the following:

The situation where restores happen to the old (canceled/failed) execution attempts can happen right now, in the presence of overlapping global failures.

That is not ideal, but it is also not a big problem, because there will be another restore to the new execution attempts later.
So the effect of this corner case is mainly some extra work in the CheckpointCoordinator to assign state to tasks twice, and extra work in the OperatorCoordinator to execute the {{resetToCheckpoint()}} twice.

So we fix this for now by removing the assertions that guarded against this situation and catch the case in the {{OperatorCoordinatorHolder}} to avoid calling {{subtaskReset()}} on restore for outdated execution attempts. 
;;;","23/Apr/21 02:52;maguowei;The case failed in the master branch after the fixes merged
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17077&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11113;;;","23/Apr/21 06:53;dwysakowicz;I checked logs and it seems somehow the test goes into an infinite loop of restarts. A weird thing about it is that the job somehow gets cancelled...

The job is always being cancelled right after triggering the first checkpoint..;;;","23/Apr/21 07:39;dwysakowicz;I can actually reproduce this locally. This happens only when using the adaptive scheduler. You can reproduce the issue if you extend the MiniCluster configuration:

{code}
    @BeforeClass
    public static void startMiniCluster() throws Exception {
        final Configuration config = new Configuration();
        config.setString(RestOptions.BIND_PORT, ""0"");
        config.set(JobManagerOptions.SCHEDULER, JobManagerOptions.SchedulerType.Adaptive);
        config.set(ClusterOptions.ENABLE_DECLARATIVE_RESOURCE_MANAGEMENT, true);

....
{code};;;","23/Apr/21 08:14;trohrmann;I will try to take a look at the problem.;;;","23/Apr/21 08:18;dwysakowicz;I figured out the problem. Will give an update shortly.;;;","23/Apr/21 08:23;dwysakowicz;The problem is in the test. The {{EventSendingCoordinator}} uses a field {{failedBefore}} to check if it should throw an exception or not. This works under the assumption that an instance of the coordinator is created only once and survives restarts. That is not the case for the {{AdaptiveScheduler}}, which recreates the whole {{ExecutionGraph}} and creates a new coordinator with the {{failedBefore}} flag reset. Therefore we fail indefinitely.;;;","23/Apr/21 08:31;dwysakowicz;Or is it actually a production grade problem? Is it the contract of the {{OperatorCoordinator}} that it survives global failovers? Even with exceptions from the coordinator itself?;;;","23/Apr/21 11:16;sewen;There is no contract that the coordinator needs to survives global failures. Master re-election for example also re-created the coordinator.

This is just a specific behavior of this test, to cache the status to run the script of the testing scenario.
And indeed, such tests that test not contracts but script scenarios are sensitive to implementation changes, that's why they should be rare.

I'll create a patch for the test...;;;","23/Apr/21 15:06;sewen;Small Update: The fact that the job looked to be cancelling after the first checkpoint is missing logging. There is a global failure that triggers the cancellation, which is not logged. That issue is tracked in FLINK-22431. ;;;","23/Apr/21 15:20;sewen;Tracking the new issue under FLINK-22433, because it is a different problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe table doesn't work with hive dialect,FLINK-22341,13373452,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lirui,lirui,19/Apr/21 05:39,28/May/21 09:11,13/Jul/23 08:07,22/Apr/21 13:33,,,,,,,,,1.13.0,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 13:33:03 UTC 2021,,,,,,,,,,"0|z0q6eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 13:33;lirui;Fixed in 1.14.0: 0270244b571ae3892609c8a780a2f157682f7ce8
Fixed in 1.13.0: 1b9ad44217de50bb3febcf1d549c2ceacbdc9859;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concurrent access to ThresholdMeter may lead to NPE,FLINK-22340,13373451,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xtsong,xtsong,xtsong,19/Apr/21 05:31,28/Aug/21 11:14,13/Jul/23 08:07,19/Apr/21 11:37,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"It is reported on github [1] that NPE is thrown from ThreasholdMeter, which is likely caused by concurrent accesses.

[1] https://github.com/apache/flink/commit/b9e576fb845b817d804da3d68471ff8a4723dcf3#commitcomment-49681105",,jiamo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 19 11:37:34 UTC 2021,,,,,,,,,,"0|z0q6e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 06:01;jiamo;I copy the ThreasholdMeter for custom use. And got this NPE:

 
{code:java}
java.lang.NullPointerException at com.papayamobile.streaming.ThresholdMeter.getEventCountsRecentInterval(ThresholdMeter.java:67) at com.papayamobile.streaming.ThresholdMeter.checkAgainstThreshold(ThresholdMeter.java:56) at com.papayamobile.streaming.StreamingClickJob$ToClick.flatMap(StreamingClickJob.java:66) at com.papayamobile.streaming.StreamingClickJob$ToClick.flatMap(StreamingClickJob.java:42) at org.apache.flink.streaming.api.operators.StreamFlatMap.processElement(StreamFlatMap.java:47) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollectWithTimestamp(StreamSourceContexts.java:322) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collectWithTimestamp(StreamSourceContexts.java:426) at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:986) at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.access$000(KinesisDataFetcher.java:111) at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$AsyncKinesisRecordEmitter.emit(KinesisDataFetcher.java:314) at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$SyncKinesisRecordEmitter$1.put(KinesisDataFetcher.java:331) at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$SyncKinesisRecordEmitter$1.put(KinesisDataFetcher.java:328) at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:970) at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.deserializeRecordForCollectionAndUpdateState(ShardConsumer.java:209) at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.lambda$run$0(ShardConsumer.java:125) at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.AdaptivePollingRecordPublisher.lambda$run$0(AdaptivePollingRecordPublisher.java:77) at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:117) at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.AdaptivePollingRecordPublisher.run(AdaptivePollingRecordPublisher.java:75) at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:113) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
{code}
And jus use `ThreasholdMeter` in normal case:
{code:java}
   public static class ToClick extends RichFlatMapFunction<ObjectNode, Click> {
      public transient ThresholdMeter meter;
      public transient SimpleCounter counter;
      public MetricGroup group;

      @Override
      public void open(org.apache.flink.configuration.Configuration parameters) throws Exception {
         group = getRuntimeContext()
               .getMetricGroup().addGroup(""Click"");
         meter = group.meter(""1seconds"", new ThresholdMeter(30000, Duration.ofSeconds(1)));
         counter = group.counter(""totalCount"", new SimpleCounter());
      }


      @Override
      public void flatMap(ObjectNode object, Collector<Click> out) throws Exception {

          ObjectMapper mapper = new ObjectMapper();
          Click click = mapper.readValue(object.toString(), Click.class);
          meter.markEvent();
          counter.inc();
          meter.checkAgainstThreshold();
          out.collect(click);

      }
   }
{code}
 

As xintongsong said
"" ThresholdMeter is a Flink runtime internal class. It should not be used in a user defined function. ""

Is there a more better way to achieve the same effect?

 ;;;","19/Apr/21 06:50;xtsong;Thanks for the info, [~jiamo].

Despite the reported problem happens in a user defined function, which is not how {{ThresholdMeter}} is expected to be used, the same problem can also happen for the runtime internals. Thus, we need to fix this anyway. I've opened a PR to make {{ThresholdMeter}} thread safe.

In your case, I would suggest to use {{MeterView}}. One of the reasons we make {{ThresholdMeter}} an internal class is that, it is not optimized for performance critical scenarios. Notice that a user defined function can be invoked on every data record, which makes it extremely sensitive to the performance. {{MeterView}} updates the rate with a background thread, and reduces the computation overhead by maintaining a history of events count. However, that means you would need do the threshold checking yourselves.;;;","19/Apr/21 11:37;xtsong;Fixed via
* master (1.14): 6c58832852239cf7c04c72ac337dc12dbee0c178
* release-1.13: 021cf0b26dc1d0221ce07162f45c06b7854bb84b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some encoding exceptions were not thrown in cython coders,FLINK-22339,13373443,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,19/Apr/21 03:23,28/Aug/21 11:14,13/Jul/23 08:07,19/Apr/21 09:25,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 19 09:25:05 UTC 2021,,,,,,,,,,"0|z0q6cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 09:25;hxbks2ks;Merged into release-1.12 via f4ac5acab9c8831f44df8d6600ac00df9ad91e97
Merged into release-1.13 via b2d242a40db4620bc5d4c153ed84c1d45be5829d
Merged into master via ef8803afa95708b35cb2c35db21d78cc2f68edfa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clear watermark output when test finished for FromElementSourceFunctionWithWatermark,FLINK-22338,13373442,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,19/Apr/21 03:23,28/Aug/21 11:15,13/Jul/23 08:07,20/Apr/21 03:11,1.13.0,,,,,,,,1.13.0,1.14.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,fsk119,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 03:11:49 UTC 2021,,,,,,,,,,"0|z0q6c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 10:39;lzljs3620320;master (1.14): 5fbf0d7f820c39f9ac210ef624647236b22a9cf7;;;","20/Apr/21 03:11;lzljs3620320;release-1.13: 5d4b451c6146a8a0470f4c17c76caa0411dbeffd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing credentials in jobconf causes repeated authentication in Hive datasource,FLINK-22329,13373197,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zuston,zuston,zuston,17/Apr/21 10:07,28/Aug/21 12:17,13/Jul/23 08:07,16/Jun/21 06:12,,,,,,,,,1.14.0,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Related Flink code: [https://github.com/apache/flink/blob/577113f0c339df844f2cc32b1d4a09d3da28085a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceFileEnumerator.java#L107]

 

In this {{getSplits}} method, it will call hadoop {{FileInputFormat's getSplits}} method. related hadoop code is [here|https://github.com/apache/hadoop/blob/03cfc852791c14fad39db4e5b14104a276c08e59/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java#L426]. Simple code is as follows
{code:java}
// Hadoop FileInputFormat

public InputSplit[] getSplits(JobConf job, int numSplits)
  throws IOException {
  StopWatch sw = new StopWatch().start();
  FileStatus[] stats = listStatus(job);

 
  ......
}


protected FileStatus[] listStatus(JobConf job) throws IOException {
  Path[] dirs = getInputPaths(job);
  if (dirs.length == 0) {
    throw new IOException(""No input paths specified in job"");
  }

  // get tokens for all the required FileSystems..
  TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job);
  
  // Whether we need to recursive look into the directory structure

  ......
}
{code}
 

In {{listStatus}} method, it will obtain delegation tokens by calling  {{TokenCache.obtainTokensForNamenodes}} method. Howerver this method will give up to get delegation tokens when credentials in jobconf.

So it's neccessary to inject current ugi credentials into jobconf.

 

Besides, when Flink support delegation tokens directly without keytab([refer to this PR|https://issues.apache.org/jira/browse/FLINK-21700]), {{TokenCache.obtainTokensForNamenodes}} will failed  without this patch because of no corresponding credentials.

 

 

 

 ",,lirui,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 16 06:12:30 UTC 2021,,,,,,,,,,"0|z0q4ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/21 10:11;zuston;[~jark] [~lirui] Cloud you assign this task to me. Thanks;;;","25/Apr/21 08:52;zuston;Gentle ping [~lirui]. This PR will block Oozie integrate with Flink batch. Thanks ~;;;","14/May/21 09:37;zuston;Update it, please review it again. Thanks [~lirui];;;","16/Jun/21 06:12;lirui;Fixed in master: e461c7dd9615fa12fec8e158a9c201b17ab03ec3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE exception happens if it throws exception in finishBundle during job shutdown,FLINK-22327,13373121,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,16/Apr/21 22:12,28/May/21 09:10,13/Jul/23 08:07,19/Apr/21 01:58,,,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,"Currently, if it throws exceptions in finishBundle during job shutdown, NPE exception may happen if time-based finish bundle is scheduled. It caused the actual exception isn't propagate. This makes users very difficult to trouble shot the problem.

See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-called-already-closed-and-NullPointerException-td42997.html for more details.",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 19 01:58:40 UTC 2021,,,,,,,,,,"0|z0q4cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 01:58;dian.fu;Fixed in 
- master via 4f41e6dae85ce53d8dde432715b90789be719204
- release-1.12 via e64ed75a5f2dd89f8a23665a54ba3239bd362fdd
- release-1.13 via a428ed4b6ab7a469ed25ab77471d93d8c3f6867d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport FLINK-18071 for 1.12.x,FLINK-22324,13373045,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,16/Apr/21 15:04,20/Apr/21 12:10,13/Jul/23 08:07,20/Apr/21 12:10,1.12.2,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,,,See  FLINK-18071 - this issue only tracks the backport to allow closing the blocker issue for 1.13.0.,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 12:10:19 UTC 2021,,,,,,,,,,"0|z0q3w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/21 12:10;sewen;Backport tracked in FLINK-18071 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobEdges Typos,FLINK-22323,13373036,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,Jack-Lee,Jack-Lee,16/Apr/21 14:18,03/May/21 09:07,13/Jul/23 08:07,03/May/21 09:07,,,,,,,,,1.14.0,,,,,,,Runtime / Task,,,,,0,,,,,,,,Jack-Lee,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 03 09:07:03 UTC 2021,,,,,,,,,,"0|z0q3u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 15:52;Jack-Lee;https://github.com/apache/flink/pull/15647;;;","03/May/21 09:07;pnowojski;Thanks for the contribution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redundant CAST in plan when selecting window start and window end in window agg,FLINK-22313,13372979,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,TsReaper,TsReaper,16/Apr/21 10:41,28/Aug/21 11:20,13/Jul/23 08:07,11/May/21 02:18,1.13.0,,,,,,,,1.14.0,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"Add the following test case to {{org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest}} to reproduce this bug.

{code:scala}
@Test
def testSessionFunction(): Unit = {
  val sql =
    """"""
      |SELECT
      |    COUNT(*),
      |    SESSION_START(proctime, INTERVAL '15' MINUTE),
      |    SESSION_END(proctime, INTERVAL '15' MINUTE)
      |FROM MyTable
      |    GROUP BY SESSION(proctime, INTERVAL '15' MINUTE)
    """""".stripMargin
  util.verifyExecPlan(sql)
}
{code}

The produced plan is
{code}
Calc(select=[EXPR$0, CAST(w$start) AS EXPR$1, CAST(w$end) AS EXPR$2])
+- GroupWindowAggregate(window=[SessionGroupWindow('w$, proctime, 900000)], properties=[w$start, w$end, w$proctime], select=[COUNT(*) AS EXPR$0, start('w$) AS w$start, end('w$) AS w$end, proctime('w$) AS w$proctime])
   +- Exchange(distribution=[single])
      +- Calc(select=[proctime])
         +- WatermarkAssigner(rowtime=[rowtime], watermark=[(rowtime - 1000:INTERVAL SECOND)])
            +- Calc(select=[PROCTIME() AS proctime, rowtime])
               +- TableSourceScan(table=[[default_catalog, default_database, MyTable, project=[rowtime]]], fields=[rowtime])
{code}

This is because the nullability indicated by {{PlannerWindowStart#getResultType}} and {{SqlGroupedWindowFunction#WindowStartEndReturnTypeInference}} are different. Actually time attribute and window start / end should always be not null.",,jark,libenchao,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 11 02:18:50 UTC 2021,,,,,,,,,,"0|z0q3hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/21 02:18;jark;Fixed in master: 3c342c93ea56a326e13fd3caf48b6a46e6604de7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink JDBC XA connector need to set maxRetries to 0 to properly working,FLINK-22311,13372975,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,afedulov,maver1ck,maver1ck,16/Apr/21 10:33,30/Nov/22 20:35,13/Jul/23 08:07,08/Feb/22 21:16,1.13.0,,,,,,,,1.15.0,,,,,,,Connectors / JDBC,,,,,0,auto-deprioritized-major,pull-request-available,,,,"Hi,
We're using XA connector from Flink 1.13 in one of our projects and we were able to create duplicates of records during write to Oracle.

The reason was that default MAX_RETRIES in JdbcExecutionOptions is 3 and this can cause duplicates in DB.
I think we should at least mention this in docs or even validate this option when creating XA Sink.

In documentation we're using defaults.
https://github.com/apache/flink/pull/10847/files#diff-a585e56c997756bb7517ebd2424e5fab5813cee67d8dee3eab6ddd0780aff627R88",,maver1ck,roman,ym,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22239,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22141,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 08 21:16:06 UTC 2022,,,,,,,,,,"0|z0q3gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/21 05:26;maver1ck;This is the log.
 I have few duplicate records with 2021-04-15T22:15:42 timestamp.
{code:java}
1:052021-04-15T23:16:49.762722475+00:00 stdout F 2021-04-15 23:16:49,761 INFO org.apache.kafka.clients.FetchSessionHandler [] - [Consumer clientId=consumer-flink-ingestion-raw-2, groupId=flink-ingestion-raw] Error sending fetch request (sessionId=842995853, epoch=11339) to node 1: {}. 
2021-04-15T22:16:04.498583738+00:00 stdout F at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282] 2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) [flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) [flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:581) [flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:617) [flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191) [flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:396) ~[flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:174) ~[flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:204) ~[flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:191) ~[flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54) ~[flink-dist_2.12-1.12.2-stream1.jar:1.12.2-stream1] 
2021-04-15T22:16:04.498583738+00:00 stdout F at com.getindata.flink.ingestion.JdbcXaSinkDeserializationWrapper.invoke(JdbcXaSinkDeserializationWrapper.java:22) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at com.getindata.flink.ingestion.JdbcXaSinkDeserializationWrapper.invoke(JdbcXaSinkDeserializationWrapper.java:37) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.invoke(JdbcXaSinkFunction.java:287) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.writeRecord(JdbcBatchingOutputFormat.java:167) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:184) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:216) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at org.apache.flink.connector.jdbc.internal.executor.DynamicBatchStatementExecutor.executeBatch(DynamicBatchStatementExecutor.java:73) ~[ingestion-1.2.1.jar:?] 2021-04-15T22:16:04.498583738+00:00 stdout F at oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:237) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9487) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at oracle.jdbc.driver.T4CPreparedStatement.executeLargeBatch(T4CPreparedStatement.java:1447) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F at oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9711) ~[ingestion-1.2.1.jar:?] 
2021-04-15T22:16:04.498583738+00:00 stdout F 2021-04-15T22:16:04.498583738+00:00 stdout F java.sql.BatchUpdateException: ORA-04021: timeout occurred while waiting to lock object 
2021-04-15T22:16:04.498583738+00:00 stdout F 2021-04-15 22:16:04,490 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat [] - JDBC executeBatch error, retry times = 0
{code};;;","19/Apr/21 12:00;maver1ck;[~ym] 
I added PR for validating maxRetries.;;;","20/Apr/21 11:34;ym;[~maver1ck], Thanks for fixing this! reviewed, please take a look! ;;;","20/Apr/21 14:51;maver1ck;[~ym] I will take a look tomorrow.;;;","18/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Jun/21 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Feb/22 21:16;roman;Merged into master as 7e43674abc2281af51ad83b2e3972f7ffb3d2c7b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CliTableauResultView print results after cancel in STREAMING mode,FLINK-22308,13372968,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,16/Apr/21 10:07,28/May/21 09:09,13/Jul/23 08:07,17/Apr/21 01:37,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,,,fsk119,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 17 01:37:14 UTC 2021,,,,,,,,,,"0|z0q3ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/21 01:37;ykt836;fixed: e4a2738e8b9badf37eab4874109a3e49ad47a341;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkRelMdFilteredColumnInterval should remapping the columnIndex of the inputRel otherwise may cause IllegalArgumentException or get incorrectly metadata,FLINK-22303,13372825,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/Apr/21 07:23,28/May/21 09:10,13/Jul/23 08:07,18/Apr/21 12:40,1.12.2,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"FlinkRelMdFilteredColumnInterval should remapping the columnIndex of the inputRel otherwise may cause IllegalArgumentException or get incorrectly metadata.

The following case will get an `IllegalArgumentException`

{code}

@Test
 def testFilteredColumnIntervalValidation(): Unit = {
 util.verifyExecPlan(
 s""""""
 |select
 | sum(uv) filter (where c = 'all') as all_uv
 |from (
 | select
 | c, count(1) as uv
 | from T
 | group by c
 |) t
 |"""""".stripMargin)
 }

{code}

{code}

Caused by: java.lang.IllegalArgumentExceptionCaused by: java.lang.IllegalArgumentException at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122) at org.apache.flink.table.planner.plan.stats.ValueInterval$.compare(ValueInterval.scala:290) at org.apache.flink.table.planner.plan.stats.ValueInterval$.compareAndHandle(ValueInterval.scala:304) at org.apache.flink.table.planner.plan.stats.ValueInterval$.isIntersected(ValueInterval.scala:247) at org.apache.flink.table.planner.plan.stats.ValueInterval$.intersect(ValueInterval.scala:189) at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$$anonfun$5$$anonfun$8.apply(ColumnIntervalUtil.scala:226) at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$$anonfun$5$$anonfun$8.apply(ColumnIntervalUtil.scala:226) at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57) at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66) at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48) at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$$anonfun$5.apply(ColumnIntervalUtil.scala:226) at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$$anonfun$5.apply(ColumnIntervalUtil.scala:221) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$.getColumnIntervalWithFilter(ColumnIntervalUtil.scala:221) at org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnInterval.getFilteredColumnInterval(FlinkRelMdFilteredColumnInterval.scala:137)

{code}

 

 

 ",,godfreyhe,jark,libenchao,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 18 12:40:55 UTC 2021,,,,,,,,,,"0|z0q2j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/21 12:40;godfreyhe;Fixed in 1.13.0: 504880cfbf1fc80603eaf8401578e788a3a8335e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive reading fail when getting file numbers on different filesystem nameservices,FLINK-22294,13372615,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zuston,zuston,zuston,15/Apr/21 11:38,28/May/21 11:04,13/Jul/23 08:07,27/Apr/21 06:04,1.12.2,,,,,,,,1.13.1,1.14.0,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,The same problem like https://issues.apache.org/jira/browse/FLINK-20710,,lirui,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22293,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 25 08:48:40 UTC 2021,,,,,,,,,,"0|z0q18g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/21 11:50;zuston;Could you assign this task to me? [~lirui]  [~jark];;;","25/Apr/21 07:58;lirui;Hi [~zuston], please also submit a PR for release-1.13.;;;","25/Apr/21 07:59;lirui;Fixed in master: 6071f9686d2a7bfb154b1e7b1682b2cfee190922
Fixed in release-1.13: 7a4a18da90521fe3c0758d4579441c50907f368b;;;","25/Apr/21 08:48;zuston;Done. Cherry pick to branch release-1.13 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExceptionHistoryEntryExtractor throws fatal error when task failure,FLINK-22276,13372255,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,jinxing6042@126.com,jinxing6042@126.com,14/Apr/21 09:48,28/Aug/21 11:16,13/Jul/23 08:07,19/Apr/21 17:34,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"When running my batch job on Flink cluster, I got a fatal error as below and JM exits:

!image-2021-04-14-17-50-45-199.png!

Digging into the code,   when DefaultScheduler start archiving failure cause ([https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultScheduler.java#L259),] seems Execution#failureCause is not safely/correctly attached/updated.

I attached JM log, [~mapohl] Would you mind help verify on this ?

 

The job topology is like below:

!image-2021-04-15-14-46-42-660.png|width=519,height=308!
 # All edges are blocking – – parallelism is 2 and there are 6 pipelined regions for all;
 # Scheduled encountered fatal error when running ShuffleRead&Write

 ",,jinxing6042@126.com,kevin.cyj,maguowei,mapohl,tanyuxin,Thesharing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/21 09:50;jinxing6042@126.com;image-2021-04-14-17-50-45-199.png;https://issues.apache.org/jira/secure/attachment/13023846/image-2021-04-14-17-50-45-199.png","15/Apr/21 06:46;jinxing6042@126.com;image-2021-04-15-14-46-42-660.png;https://issues.apache.org/jira/secure/attachment/13023889/image-2021-04-15-14-46-42-660.png","14/Apr/21 09:59;jinxing6042@126.com;log;https://issues.apache.org/jira/secure/attachment/13023847/log",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 23 08:58:37 UTC 2021,,,,,,,,,,"0|z0pz0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/21 10:38;Thesharing;I've gone through the log and got some assumptions. In the log we can see there are two failovers that happen within 30 milliseconds:
 * failover 1 includes {{shuffleRead&Write (1/2)}}
 * failover 2 includes {{shuffleRead&Write (1/2) + shuffleRead&Write (2/2)}}

It seems that {{restartTasks}} of failover 2 runs first, resets the execution of {{shuffleRead&Write (1/2)}}. Then {{archiveFromFailureHandlingResult}} of failover 1 runs, finds that there's no failure cause in the new execution of {{shuffleRead&Write (1/2)}}.
  ;;;","14/Apr/21 18:21;mapohl;Thanks for bringing this up, [~jinxing6042@126.com]. On what revision of Flink's source code was this job executed? I'm puzzled a bit about the {{ExecutionState}} {{RECOVERING}}:
{code}
2021-04-14 17:06:34,566 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - shuffleRead&Write (1/2) (9fcfd71ad147a96527f080752b4732cd) switched from DEPLOYING to RECOVERING.
2021-04-14 17:06:34,569 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - shuffleRead&Write (2/2) (d8f382ae4087fe15368be9fae1bbf310) switched from DEPLOYING to RECOVERING.
2021-04-14 17:06:34,624 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - shuffleRead&Write (1/2) (9fcfd71ad147a96527f080752b4732cd) switched from RECOVERING to RUNNING.
2021-04-14 17:06:34,625 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - shuffleRead&Write (2/2) (d8f382ae4087fe15368be9fae1bbf310) switched from RECOVERING to RUNNING.
{code}

I haven't seen that {{ExecutionState}} ever and cannot find it in my version of the code, either?;;;","14/Apr/21 18:36;mapohl;I see, the {{RECOVERING}} was renamed into {{INITIALIZING}} in [b3879280|https://github.com/apache/flink/commit/b3879280]. Thanks [~chesnay] for pointing that out.;;;","14/Apr/21 20:42;mapohl;[~Thesharing] thanks for your analysis. I guess, you're right. The {{DefaultScheduler.delayExecutor}} is multi-threaded. That's where the race condition between restarting the two tasks and archiving the failure of the single task happens. We should have considered the {{ExecutionVertex's}} version when checking the tasks for failures. Already restarted tasks should not be considered for archiving as their failure should have been archived already.

One thing, that's probably unrelated, but maybe somebody can give me a reason for it: The {{RestartPipelinedRegionFailoverStrategy}} selects only one task for restart when the first failure occurs. For the second failure, the {{RestartPipelinedRegionFailoverStrategy}} selects two tasks for restart. Shouldn't the strategy always select the two tasks for restart considering that both belong to the same pipeline region? Or am I missing something here?

I'm gonna continue work on this tomorrow...;;;","15/Apr/21 06:28;Thesharing;Thank you for pointing this out, [~mapohl]. I go over the job with [~jinxing6042@126.com] and figure this out. When TaskExecutor updates the Execution state to FAILED, it will call {{JobMasterPartitionTracker#stopTrackingAndReleasePartitions}} for the result partition of {{shuffleRead&Write (1/2)}}.

Failover 1 is caused by {{shuffleRead&Write(1/2)}}. When failover 1 occurs, it only failover itself because {{shuffleRead&Write(2/2)}} is running normally at this moment.

Failover 2 is caused by {{shuffleRead&Write (2/2)}}. When failover 2 occurs, it will first iterate over all the downstream regions of {{shuffleRead&Write (2/2)}}, i.e. {{write (1/2)}} and {{write (2/2)}}. Then when iterating the region of {{write (1/2)}}, it will check if its needed input result partition is available or not, and then adds {{shuffleRead&Write (1/2)}} to the regions to restart. Therefore there are two tasks for restart in the failover 2.
  ;;;","15/Apr/21 07:53;mapohl;We were able to reproduce the issue. The issue is that we're working on {{ExecutionVertex}} instances instead of {{Executions}} for retrieving the {{failureInfo}}. I'm working on fixing it now.;;;","15/Apr/21 11:38;jinxing6042@126.com;Thanks a lot [~mapohl];;;","16/Apr/21 06:19;mapohl;[~jinxing6042@126.com] was the issue repeatably happening? If yes, it would be good to run the job with the fixes from the PR to verify that it got fixed.;;;","16/Apr/21 21:18;trohrmann;Temporary workaround merged via fe41a87d39cbebe14e8d5de1058874cb4f7e9550;;;","19/Apr/21 06:40;jinxing6042@126.com;Thanks a lot for quick fix ~ I will try the PR and come back later ~;;;","19/Apr/21 07:47;mapohl;FYI: There was a workaround merged on Friday ([PR #15648|https://github.com/apache/flink/pull/15648]) to enable building a new RC the past weekend. The actual resolution is still under progress in [PR #15640|https://github.com/apache/flink/pull/15640].;;;","19/Apr/21 17:34;trohrmann;Fixed via

master: 317687b20901151165db39365e3634b002c70d3e
1.13.0: f4105f65fb510de11ccb08c4b31d5e4c30744151;;;","23/Apr/21 03:27;jinxing6042@126.com;Thanks [~mapohl] ~ I tried the patch and it works very well :);;;","23/Apr/21 08:58;mapohl;Awesome, thanks for getting back to us [~jinxing6042@126.com] (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some scenes can't drop table by hive catalog,FLINK-22272,13372223,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangzeyu,wangzeyu,wangzeyu,14/Apr/21 07:25,23/Sep/21 17:22,13/Jul/23 08:07,04/Jun/21 10:00,1.13.1,,,,,,,,1.13.2,1.14.0,,,,,,Connectors / Hive,,,,,0,pull-request-available,stale-major,,,,"There are some scenes we can't drop table by hive catalog because of we execute error create table sql like ""caeate table tableName"". And then when we execute drop table sql ""drop table tableName"" will throw error ""org.apache.flink.table.catalog.exceptions.CatalogException: Failed to get table schema from properties for generic table"".
So, i think maybe we should remove the check when we drop table by third-party catalog and just drop table depend on the third-party catalog.",,jark,lirui,luoyuxia,wangzeyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 04 10:00:22 UTC 2021,,,,,,,,,,"0|z0pytc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/21 07:30;wangzeyu;So here's my solution , in class org.apache.flink.table.catalog.CatalogManager
{code:java}
private void dropTableInternal(
        ObjectIdentifier objectIdentifier, boolean ignoreIfNotExists, boolean isDropTable) {
    Predicate<CatalogBaseTable> filter =
            isDropTable
                    ? table -> table instanceof CatalogTable
                    : table -> table instanceof CatalogView;
    // Same name temporary table or view exists.
    if (filter.test(temporaryTables.get(objectIdentifier))) {
        String tableOrView = isDropTable ? ""table"" : ""view"";
        throw new ValidationException(
                String.format(
                        ""Temporary %s with identifier '%s' exists. ""
                                + ""Drop it first before removing the permanent %s."",
                        tableOrView, objectIdentifier, tableOrView));
    }
        execute(
                (catalog, path) -> catalog.dropTable(path, ignoreIfNotExists),
                objectIdentifier,
                ignoreIfNotExists,
                ""DropTable"");
}
{code}
And in order to support the issue [FLINK-17756]  , add some code in org.apache.flink.table.catalog.hive.HiveCatalog
{code:java}
@Override
public void dropTable(ObjectPath tablePath, boolean ignoreIfNotExists)
        throws TableNotExistException, CatalogException {
    checkNotNull(tablePath, ""tablePath cannot be null"");

    if ( TableType.valueOf(getHiveTable(tablePath).getTableType()) != TableType.VIRTUAL_VIEW) {
        try {
            client.dropTable(
                    tablePath.getDatabaseName(),
                    tablePath.getObjectName(),
                    // Indicate whether associated data should be deleted.
                    // Set to 'true' for now because Flink tables shouldn't have data in Hive. Can
                    // be changed later if necessary
                    true,
                    ignoreIfNotExists);
        } catch (NoSuchObjectException e) {
            if (!ignoreIfNotExists) {
                throw new TableNotExistException(getName(), tablePath);
            }
        } catch (TException e) {
            throw new CatalogException(
                    String.format(""Failed to drop table %s"", tablePath.getFullName()), e);
        }
    }
}
{code};;;","14/Apr/21 09:07;wangzeyu;I think we should give the catalog more freedom to drop table, to let user can drop because of they execute error create table sql.;;;","26/May/21 23:01;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","28/May/21 04:34;lirui;Hi [~wangzeyu], could you give an example to reproduce the issue you encountered?;;;","28/May/21 04:38;wangzeyu;execute sql ""caeate table tableName"".

And then when we execute drop table sql ""drop table tableName"" will throw error ""org.apache.flink.table.catalog.exceptions.CatalogException: Failed to get table schema from properties for generic table"".;;;","28/May/21 05:56;lirui;Do you mean {{tableName}} already exists in metastore?;;;","28/May/21 06:00;wangzeyu;This is my sence

Flink SQL> create table aaa;

Flink SQL> drop table aaa;

org.apache.flink.table.catalog.exceptions.CatalogException: Failed to get table schema from properties for generic table aaa;;;","28/May/21 06:08;wangzeyu;And i use hive catalog;;;","28/May/21 06:34;lirui;I see. When dropping a table, HiveCatalog doesn't perform any check. So I think the problem resides in {{getTable}}: user can create a generic table w/o schema in HiveCatalog but can't get it back. We can just let HiveCatalog return empty schema if it can't be retrieved from table properties.;;;","28/May/21 07:04;wangzeyu;It is a idea as you comment, but it maybe throw other error in other scenes. So i think wo should give the catalog more freedom to drop table.;;;","28/May/21 07:13;wangzeyu;And before this issue FLINK-17756 all actions of drop table is execute in catalog impl but not CatalogManager.;;;","28/May/21 11:56;lirui;What do you mean by ""give the catalog more freedom to drop table""? Do you mean to revert the changes made in FLINK-17756?;;;","28/May/21 12:24;wangzeyu;Yes, in this pr [GitHub Pull Request #15638|https://github.com/apache/flink/pull/15638] i recode  FLINK-17756;;;","28/May/21 13:04;lirui;These are actually two different problems:
# Whether we allow ""DROP TABLE"" to drop views, or ""DROP VIEW"" to drop tables
# If HiveCatalog allows creating generic tables with empty schema, whether it should also allow retrieving such tables. This has nothing to do with DROP TABLE

For #1, I think we should continue guard against such scenarios. That means we can't revert FLINK-17756.
For #2, I think the current behavior in HiveCatalog is inconsistent and it should support retrieving generic tables with empty schema.;;;","28/May/21 13:57;wangzeyu;For #1, I agree with you, so i just recode the code and also support the same feature in FLINK-17756.

For #2, I think you idea is ok for HiveCatalog, and, i just think the feature of drop table should be impl in catalog but not catalog manage;;;","31/May/21 03:15;lirui;Hi [~wangzeyu], could you explain what's the benefit to move the feature from CatalogManager to Catalog? As a SQL engine, Flink should provide consistent semantics for DROP TABLE/VIEW no matter what Catalog is being used. So moving it to Catalog means each implementation needs to repeat this logic, right? Besides, it requires public API change which has bigger impact on existing users and should be discussed on the mailing list.

As to #2, as I said it has noting to do with DROP TABLE/VIEW. We should fix it anyway.;;;","31/May/21 03:36;wangzeyu;Hi [Rui Li|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui], i used to think move the feature to Catalog has a more direct way to know what is the table attribute for TABLE/VIEW, and we can do everything in catalog isself when drop table/view because the different catalog maybe need deal different thing. But as you say Flink should provide consistent semantics for DROP TABLE/VIEW no matter what Catalog is being used. So i will submit a new PR for #2, could you help review?;;;","31/May/21 05:17;lirui;[~wangzeyu] Sure, assigning this to you;;;","31/May/21 09:54;wangzeyu;Hi [Rui Li|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui], i push a new commit in this [GitHub Pull Request #15638|https://github.com/apache/flink/pull/15638], and i update this PR document too. Please review thanks.;;;","01/Jun/21 11:51;wangzeyu;Hi [Rui Li|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui], is there any question in this PR?;;;","04/Jun/21 09:24;lirui;Fixed in master: 88f11e3fa52106115bf6f9a0f67a72f709242c69
Fixed in release-1.13: 70765dafb2af5a1ed2ff6ca6864e40ea71290006;;;","04/Jun/21 09:25;lirui;[~wangzeyu] Do you want to fix this in 1.12 as well? If so, please submit a PR for release-1.12 branch.;;;","04/Jun/21 09:47;wangzeyu;h4. [Rui Li |https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui]My fix is base on FLINK-21660, so maybe it should follow the version that FLINK-21660 effect.;;;","04/Jun/21 10:00;lirui;OK then I'll remove 1.12 from the fix version and close this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Harden JobMasterStopWithSavepointITCase,FLINK-22266,13372167,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,maguowei,maguowei,14/Apr/21 03:17,28/Aug/21 12:11,13/Jul/23 08:07,20/May/21 13:37,1.13.0,,,,,,,,1.13.1,1.14.0,,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16451&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=3884


{code:java}
[ERROR] throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate(org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase)  Time elapsed: 0.154 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithoutRestartsHelper(JobMasterStopWithSavepointITCase.java:154)
	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate(JobMasterStopWithSavepointITCase.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
",,dwysakowicz,maguowei,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22249,FLINK-22248,,FLINK-22269,FLINK-22268,FLINK-22717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 13:37:26 UTC 2021,,,,,,,,,,"0|z0pygw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/21 03:21;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16451&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=9177;;;","15/Apr/21 10:43;trohrmann;I think the problem of this test is that the test only waits on the JobStatus.RUNNING and then assumes that a synchronous savepoint succeeds. However, the individual tasks of the job could still be in a different state than ExecutionState.RUNNING and, hence, the savepoint can fail. Instead of waiting on JobStatus.RUNNING we should check that all tasks are RUNNING.

cc Dawid Wysakowicz because you seem to work on hardening these tests. It might make sense to group all the JobMasterStopWithSavepointITCase test instabilities since there exist quite some of them.;;;","15/Apr/21 11:43;dwysakowicz;Hey Till Rohrmann I've seen your comment Yes you are right. I am actually already working on a fix here: https://github.com/apache/flink/pull/15589/commits/0344f2d9a7d7f5e995dbd9c193d2c3dbd3fdddb8

I closed other issues regarding the test, as I will open a single PR to harden the whole test case.;;;","15/Apr/21 12:09;trohrmann;When running the tests in a loop, it can happen that you run into FLINK-22276. We will try to fix this problem asap. Just that you are not confused by it [~dwysakowicz].;;;","15/Apr/21 12:19;dwysakowicz;Thanks for the pointer!;;;","16/Apr/21 16:49;dwysakowicz;Hardened the tests via: 3ebf94051ea234b4bec41b36e802f6cbcf18521d...79eb5c79fd7f3b0b7518f70b7cb02c69e773aada;;;","18/Apr/21 02:42;maguowei;Just for reporting
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16710&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4589;;;","23/Apr/21 03:02;maguowei;I found that the case still failed on the master branch
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17077&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=3885

{code:java}
Apr 22 21:44:59 [ERROR] testRestartCheckpointCoordinatorIfStopWithSavepointFails(org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase)  Time elapsed: 60.366 s  <<< FAILURE!
Apr 22 21:44:59 java.lang.AssertionError
Apr 22 21:44:59 	at org.junit.Assert.fail(Assert.java:86)
Apr 22 21:44:59 	at org.junit.Assert.assertTrue(Assert.java:41)
Apr 22 21:44:59 	at org.junit.Assert.assertTrue(Assert.java:52)
Apr 22 21:44:59 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.testRestartCheckpointCoordinatorIfStopWithSavepointFails(JobMasterStopWithSavepointITCase.java:244)
Apr 22 21:44:59 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 22 21:44:59 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 22 21:44:59 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 22 21:44:59 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 22 21:44:59 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 22 21:44:59 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 22 21:44:59 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 22 21:44:59 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 22 21:44:59 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 22 21:44:59 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 22 21:44:59 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 22 21:44:59 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 22 21:44:59 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 22 21:44:59 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 22 21:44:59 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)


{code}



{code:java}

Apr 22 21:44:59 [ERROR] testRestartCheckpointCoordinatorIfStopWithSavepointFails(org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase)  Time elapsed: 60.366 s  <<< ERROR!
Apr 22 21:44:59 java.lang.IllegalStateException: MiniCluster is not yet running or has already been shut down.
Apr 22 21:44:59 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Apr 22 21:44:59 	at org.apache.flink.runtime.minicluster.MiniCluster.getDispatcherGatewayFuture(MiniCluster.java:850)
Apr 22 21:44:59 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:750)
Apr 22 21:44:59 	at org.apache.flink.runtime.minicluster.MiniCluster.listJobs(MiniCluster.java:683)
Apr 22 21:44:59 	at org.apache.flink.client.program.MiniClusterClient.listJobs(MiniClusterClient.java:111)
Apr 22 21:44:59 	at org.apache.flink.test.util.AbstractTestBase.cleanupRunningJobs(AbstractTestBase.java:73)
Apr 22 21:44:59 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 22 21:44:59 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 22 21:44:59 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 22 21:44:59 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 22 21:44:59 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 22 21:44:59 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 22 21:44:59 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 22 21:44:59 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
Apr 22 21:44:59 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 22 21:44:59 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 22 21:44:59 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 22 21:44:59 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 22 21:44:59 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 22 21:44:59 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 22 21:44:59 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 22 21:44:59 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 22 21:44:59 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr
{code}

;;;","24/Apr/21 03:31;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17129&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4648;;;","25/Apr/21 09:53;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17143&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4648;;;","26/Apr/21 02:24;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17140&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4648;;;","29/Apr/21 03:38;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17368&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4591;;;","29/Apr/21 09:04;trohrmann;[~dwysakowicz] can you take another look at this failing test case?;;;","03/May/21 04:23;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17502&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4511


{code:java}
suspendWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished(org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase)  Time elapsed: 10.077 s  <<< ERROR!
2021-05-02T22:50:16.1542494Z May 02 22:50:16 java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.stopWithSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2021-05-02T22:50:16.1544195Z May 02 22:50:16 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-05-02T22:50:16.1545389Z May 02 22:50:16 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-05-02T22:50:16.1581395Z May 02 22:50:16 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.stopWithSavepointNormalExecutionHelper(JobMasterStopWithSavepointITCase.java:123)
2021-05-02T22:50:16.1583108Z May 02 22:50:16 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.suspendWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished(JobMasterStopWithSavepointITCase.java:105)
2021-05-02T22:50:16.1584290Z May 02 22:50:16 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-02T22:50:16.1585178Z May 02 22:50:16 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-02T22:50:16.1586128Z May 02 22:50:16 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-02T22:50:16.1586983Z May 02 22:50:16 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-02T22:50:16.1587861Z May 02 22:50:16 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-02T22:50:16.1588812Z May 02 22:50:16 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-02T22:50:16.1589712Z May 02 22:50:16 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-02T22:50:16.1590653Z May 02 22:50:16 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-02T22:50:16.1591556Z May 02 22:50:16 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-02T22:50:16.1592354Z May 02 22:50:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-02T22:50:16.1593225Z May 02 22:50:16 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-02T22:50:16.1594370Z May 02 22:50:16 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-02T22:50:16.1595082Z May 02 22:50:16 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-02T22:50:16.1595862Z May 02 22:50:16 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-02T22:50:16.1596945Z May 02 22:50:16 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-02T22:50:16.1597960Z May 02 22:50:16 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-02T22:50:16.1598905Z May 02 22:50:16 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-02T22:50:16.1599640Z May 02 22:50:16 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-02T22:50:16.1600477Z May 02 22:50:16 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-02T22:50:16.1601289Z May 02 22:50:16 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-02T22:50:16.1602080Z May 02 22:50:16 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-02T22:50:16.1602907Z May 02 22:50:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-02T22:50:16.1603854Z May 02 22:50:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-02T22:50:16.1604594Z May 02 22:50:16 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-02T22:50:16.1605349Z May 02 22:50:16 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-02T22:50:16.1606128Z May 02 22:50:16 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-05-02T22:50:16.1607075Z May 02 22:50:16 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-05-02T22:50:16.1608111Z May 02 22:50:16 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-05-02T22:50:16.1608984Z May 02 22:50:16 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-05-02T22:50:16.1609976Z May 02 22:50:16 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-02T22:50:16.1640984Z May 02 22:50:16 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-02T22:50:16.1641875Z May 02 22:50:16 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-02T22:50:16.1657319Z May 02 22:50:16 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-05-02T22:50:16.1658668Z May 02 22:50:16 Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.stopWithSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2021-05-02T22:50:16.1659635Z May 02 22:50:16 	at com.sun.proxy.$Proxy32.stopWithSavepoint(Unknown Source)
2021-05-02T22:50:16.1660339Z May 02 22:50:16 	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$stopWithSavepoint$9(MiniCluster.java:724)
2021-05-02T22:50:16.1661430Z May 02 22:50:16 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-05-02T22:50:16.1662258Z May 02 22:50:16 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-05-02T22:50:16.1662973Z May 02 22:50:16 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-05-02T22:50:16.1663723Z May 02 22:50:16 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:751)
2021-05-02T22:50:16.1664283Z May 02 22:50:16 	at org.apache.flink.runtime.minicluster.MiniCluster.stopWithSavepoint(MiniCluster.java:722)
2021-05-02T22:50:16.1664965Z May 02 22:50:16 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.stopWithSavepoint(JobMasterStopWithSavepointITCase.java:250)
2021-05-02T22:50:16.1665756Z May 02 22:50:16 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.stopWithSavepointNormalExecutionHelper(JobMasterStopWithSavepointITCase.java:117)
2021-05-02T22:50:16.1666314Z May 02 22:50:16 	... 34 more
2021-05-02T22:50:16.1668114Z May 02 22:50:16 Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#544661845]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2021-05-02T22:50:16.1669462Z May 02 22:50:16 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2021-05-02T22:50:16.1670011Z May 02 22:50:16 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2021-05-02T22:50:16.1670622Z May 02 22:50:16 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2021-05-02T22:50:16.1671147Z May 02 22:50:16 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2021-05-02T22:50:16.1671721Z May 02 22:50:16 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
2021-05-02T22:50:16.1672244Z May 02 22:50:16 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
2021-05-02T22:50:16.1672975Z May 02 22:50:16 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
2021-05-02T22:50:16.1673767Z May 02 22:50:16 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2021-05-02T22:50:16.1674434Z May 02 22:50:16 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2021-05-02T22:50:16.1675060Z May 02 22:50:16 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
2021-05-02T22:50:16.1675597Z May 02 22:50:16 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2021-05-02T22:50:16.1676109Z May 02 22:50:16 	at java.lang.Thread.run(Thread.java:748)
{code}
;;;","03/May/21 05:16;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17477&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4650;;;","04/May/21 13:47;dwysakowicz;Yes, I will, sorry for the delay.;;;","04/May/21 15:32;dwysakowicz;There might actually be a problem in the AdaptiveScheduler. After investigating some logs, I find the following stacktrace:

{code}
21:54:12,518 [flink-akka.actor.default-dispatcher-4] WARN  org.apache.flink.runtime.minicluster.MiniCluster             [] - Error in MiniCluster. Shutting the MiniCluster down.
java.lang.IllegalStateException: The vertex must be in CREATED or SCHEDULED state to be deployed. Found state RUNNING
        at org.apache.flink.runtime.executiongraph.Execution.deploy(Execution.java:546) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.ExecutionVertex.deploy(ExecutionVertex.java:427) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.Executing.deploySafely(Executing.java:139) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.Executing.deploy(Executing.java:132) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.Executing.<init>(Executing.java:63) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.Executing$Factory.getState(Executing.java:363) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.Executing$Factory.getState(Executing.java:334) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1139) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToExecuting(AdaptiveScheduler.java:787) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.handleSavepointCompletion(StopWithSavepoint.java:106) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.lambda$null$0(StopWithSavepoint.java:89) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:1093) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$26(AdaptiveScheduler.java:1108) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_282]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_282]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
{code}

After the exception, the MiniCluster is being shut down, leading to random exceptions from other tests. Could you have some time to take a look [~trohrmann] [~rmetzger]?;;;","04/May/21 15:37;dwysakowicz;General comment. I think there are two issues in here. One happening more frequently occurs only with the adaptive scheduler and manifests with the exception above. The other one is when we get the {{AskTimeoutException}}.;;;","04/May/21 16:46;trohrmann;Thanks for the analysis [~dwysakowicz]. [~rmetzger] looking at the {{Executing}} state, I think the current code of the {{StopWithSavepoint}} cannot work. If the savepoint fails, we cannot go to the {{Executing}} state because the current contract is that when entering the {{Executing}} state we deploy the given {{ExecutionGraph}}. In this case here, the {{ExecutionGraph}} is already running.

So I see two solutions:

1. We go into {{Restarting}} in order to restart the {{ExecutionGraph}}
2. We move the {{ExecutionGraph.deploy}} call out of the {{Executing}} state.;;;","06/May/21 09:16;rmetzger;Thanks a lot for looking into the problem [~trohrmann]. I'll open a PR for the problem.
;;;","06/May/21 14:09;trohrmann;Great, thanks [~rmetzger]. Please set the issue in progress when you start working on it.;;;","10/May/21 06:33;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17758&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4551

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17746&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4550

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17719&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4549;;;","11/May/21 07:01;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17820&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4548;;;","17/May/21 06:47;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17993&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4548;;;","20/May/21 13:37;rmetzger;Merged to master in https://github.com/apache/flink/commit/11065420e03a95fc53930e981474b98abcabc1f7
Merged to release-1.13 in https://github.com/apache/flink/commit/040bd81a28adaeee70f6d878045632b8b3588fa9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using TIMESTAMPADD function with partition value has some problem  when push partition into  TableSource,FLINK-22263,13372076,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hehuiyuan,hehuiyuan,13/Apr/21 15:29,18/Apr/21 06:43,13/Jul/23 08:07,14/Apr/21 02:47,1.12.2,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,,,,,,"SQL (table api):
{code:java}
CREATE CATALOG myhive
WITH (
        'type' = 'hive',
        'default-database' = 'hhy'
);

INSERT INTO  default_catalog.default_database.table_sink select * from  myhive.hhy.tmp_flink_test where dt=CAST(TIMESTAMPADD(DAY, -1, CURRENT_DATE) as varchar);

{code}
 

Error log:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Data type 'INTERVAL SECOND(3) NOT NULL' with conversion class 'java.time.Duration' does not support a value literal of class 'java.math.BigDecimal'.Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Data type 'INTERVAL SECOND(3) NOT NULL' with conversion class 'java.time.Duration' does not support a value literal of class 'java.math.BigDecimal'. at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:286) at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:79) at org.apache.flink.table.expressions.ApiExpressionUtils.valueLiteral(ApiExpressionUtils.java:251) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:432) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:340) at org.apache.calcite.rex.RexLiteral.accept(RexLiteral.java:1173) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$7.apply(RexNodeExtractor.scala:440) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$7.apply(RexNodeExtractor.scala:440) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:439) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:340) at org.apache.calcite.rex.RexCall.accept(RexCall.java:174) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$7.apply(RexNodeExtractor.scala:440) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$7.apply(RexNodeExtractor.scala:440) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:439) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:340) at org.apache.calcite.rex.RexCall.accept(RexCall.java:174) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$7.apply(RexNodeExtractor.scala:440) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$7.apply(RexNodeExtractor.scala:440) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:439) at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:340) at org.apache.calcite.rex.RexCall.accept(RexCall.java:174) at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionFromCatalogAndPrune(PushPartitionIntoTableSourceScanRule.java:336) at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionsAndPrune(PushPartitionIntoTableSourceScanRule.java:293) at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.onMatch(PushPartitionIntoTableSourceScanRule.java:167) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:86) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:57) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock$1.apply(BatchCommonSubGraphBasedOptimizer.scala:52) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock$1.apply(BatchCommonSubGraphBasedOptimizer.scala:50) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:50) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:287) at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:120) at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:46) at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:630) at org.apache.flink.table.api.internal.StatementSetImpl.explain(StatementSetImpl.java:92) at com.jd.flink.sql.runtime.SQLExecHelper.execJob(SQLExecHelper.java:312)
{code}
 

 I think  RexNodeToExpressionConverter.visitLiteral  should dealt with INTERVAL 
{code:java}
   case INTERVAL_DAY_TIME  =>
        Duration.ofMillis(literal.getValue.asInstanceOf[Number].longValue())
 
      case  INTERVAL_YEAR_MONTH =>
        
{code}
 

 ",,hehuiyuan,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22021,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 04:31:07 UTC 2021,,,,,,,,,,"0|z0pxww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/21 04:07;hehuiyuan;Hi  [~lzljs3620320] , why not merge to 1.12 ?  

If this can be merged, I can do it.;;;","14/Apr/21 04:24;lzljs3620320;Yes, you can cherry-pick this to 1.12.;;;","14/Apr/21 04:25;lzljs3620320;You can re-open FLINK-22021 and create a PR for 1.12.;;;","14/Apr/21 04:31;hehuiyuan;[~lzljs3620320]  ok.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source schema in CREATE TABLE LIKE statements is not inferred correctly,FLINK-22260,13372027,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,airblader,airblader,airblader,13/Apr/21 12:02,28/May/21 09:07,13/Jul/23 08:07,15/Apr/21 08:17,,,,,,,,,1.13.0,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"When using a LIKE statement such as in the following (assume some_sink and some_source to be two tables with the same schema)
{code:java}
CREATE TEMPORARY TABLE b LIKE some_sink
INSERT INTO b SELECT * FROM some_source{code}
the source schema for the INSERT operation is not actually inferred correctly, causing the entire query to fail:
{quote}org.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default.default.b' do not match.
Cause: Different number of columns.

Query schema: [name: STRING, ts: TIMESTAMP(3) *ROWTIME*]
Sink schema:  []
{quote}",,airblader,dwysakowicz,jark,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 08:17:01 UTC 2021,,,,,,,,,,"0|z0pxm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 12:48;airblader;I will look into this right away.;;;","13/Apr/21 13:03;jark;{{LIKE}} will not derive computed columns by default, so the query schema doesn't match physical columns if source has computed columns. 
Could you check whether the name and ts columns are both computed column?;;;","13/Apr/21 13:05;airblader;[~jark] They are both physical columns.;;;","14/Apr/21 06:27;dwysakowicz;Is this really a blocker for the release? Is it a regression? If we don't have a fix in workings I'd be inclined to deprioritize it and potentially include in 1.13.1. WDYT?;;;","14/Apr/21 06:29;airblader;Yes, this is a regression, we have an internal test that passed and failed when we updates to release-1.13. I am currently investigating this and working to fix it.;;;","14/Apr/21 06:31;dwysakowicz;Ok, good to know. Let's keep it a blocker then. Could you move the ticket to ""IN PROGRESS""?;;;","14/Apr/21 06:38;airblader;Yes, sorry. I also found the root cause and will submit the PR shortly.;;;","14/Apr/21 06:41;dwysakowicz;Thank you!;;;","15/Apr/21 08:17;twalthr;Fixed in 1.13.0: 1e0da5e5756e4c39f260a1e18aad3460f7fc67d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterStopWithSavepointITCase failed due to status is FAILING,FLINK-22249,13371934,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,maguowei,maguowei,13/Apr/21 04:35,16/Apr/21 16:50,13/Jul/23 08:07,16/Apr/21 16:49,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16405&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4472


{code:java}
[ERROR] Failures: 
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheSuspend:133->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate:138->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR] Errors: 
[ERROR]   JobMasterStopWithSavepointITCase.suspendWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished:103->stopWithSavepointNormalExecutionHelper:113->setUpJobGraph:307 » IllegalState
[ERROR]   JobMasterStopWithSavepointITCase.testRestartCheckpointCoordinatorIfStopWithSavepointFails:237 » IllegalState
[INFO] 
[ERROR] Tests run: 1645, Failures: 2, Errors: 2, Skipped: 51

{code}
",,dwysakowicz,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22266,,,,FLINK-22248,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 16 16:49:12 UTC 2021,,,,,,,,,,"0|z0px1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 16:49;dwysakowicz;Hardened the tests via: 3ebf94051ea234b4bec41b36e802f6cbcf18521d...79eb5c79fd7f3b0b7518f70b7cb02c69e773aada;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse PrintUtils.MAX_COLUMN_WIDTH in CliChangelogResultView and CliTableResultView,FLINK-22245,13371920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,13/Apr/21 03:04,28/May/21 09:06,13/Jul/23 08:07,14/Apr/21 02:41,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"The max column width in CliChangelogResultView and CliTableResultView is 25.

In the picture, it's not enough. We should reuse the {{PrintUtils.MAX_COLUMN_WIDTH}}

 

 

 ",,fsk119,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/21 03:06;fsk119;sql-client.png;https://issues.apache.org/jira/secure/attachment/13023743/sql-client.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 02:41:21 UTC 2021,,,,,,,,,,"0|z0pwy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/21 02:41;jark;Fixed in master: a66487d4d2f77bb09b9961655cc5922c3e5c098c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve support for JdbcXaSinkFunction,FLINK-22239,13371589,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,ym,ym,12/Apr/21 11:28,08/Sep/21 09:58,13/Jul/23 08:07,20/Apr/21 20:17,,,,,,,,,1.13.0,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,,"JdbcXaSinkFunction uses Xa protocol/interface to implement exactly-once guarantee (within each subtask partition).

Xa is a protocol/interface designed for two-phase commit of distributed DBS (RMs).
XA guarantees that transactional updates are committed in all of the participating databases, or are fully rolled back out of all of the databases, reverting to the state prior to the start of the transaction.

Hence some of the dbs that support XA treats XA transaction as global trans, and some of them does not support multiple global trans (per connection) at a time, MYSQL for example (see FLINK-21743).

This ticket is a follow-up to address such limitations.",,libenchao,maver1ck,roman,txhsj,weihao,ym,,,,,,,,,,,,,,,,,,,,,FLINK-22311,,,,,,,,,,,,,,,,,,,,,FLINK-21743,,,,,,,,,,,,,,,,FLINK-22141,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 14 12:13:34 UTC 2021,,,,,,,,,,"0|z0puwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 06:04;ym;Possible solution:

Maintain a pool of connections to DB per sink task to simulate multiple xa transactions per sink task;;;","17/Apr/21 04:18;ym;I am reviewing the fix-PR,

With FLINK-22311 (a bug report from user using this feature connecting to Oracle), I suggest releasing the feature after manually testing the fix with 

https://github.com/apache/flink/pull/15627 (along with the failover part)

1. Postgres
2. MySQL
3. Oracle 

with proper documentation.
;;;","20/Apr/21 20:17;roman;Merged into 1.13 as 9e4e14adef2014f4c0b3dda1967f2811a5610723.
Merged into master as 3482390d7d9cca0f6e8c9d201730a9d02fe2ec8e.;;;","14/Jul/21 11:07;maver1ck;[~roman_khachatryan] 
 Changing defaults from this PR gives me following error on Oracle
{code:java}
09:35:18.980 [Source: Custom Source -> Sink: Unnamed (1/1)#0] WARN  org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Sink: Unnamed (1/1)#0 (0485d1334319956f744b35249e94ae7d) switched from INITIALIZING to FAILED with failure cause: org.apache.flink.util.FlinkRuntimeException: unable to recover, error -3: resource manager error has occurred. [null]
        at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.wrapException(XaFacadeImpl.java:356)
        at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.lambda$execute$12(XaFacadeImpl.java:267)
        at java.base/java.util.Optional.orElseThrow(Optional.java:408)
        at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.execute(XaFacadeImpl.java:267)
        at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.recover(XaFacadeImpl.java:227)
        at org.apache.flink.connector.jdbc.xa.XaFacadePoolingImpl.recover(XaFacadePoolingImpl.java:120)
        at org.apache.flink.connector.jdbc.xa.XaGroupOpsImpl.recoverAndRollback(XaGroupOpsImpl.java:109)
        at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.open(JdbcXaSinkFunction.java:261)
        at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
        at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
        at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:582)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:562)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:537)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
        at java.base/java.lang.Thread.run(Thread.java:829)
{code};;;","14/Jul/21 11:45;roman;[~maver1ck] could you clarify which defaults were changed?

Also the PR is merged, are you testing against master?;;;","14/Jul/21 11:53;maver1ck;I'm doing migration of my codebase to 1.13.1 and setting
{code:java}
private static final boolean DEFAULT_RECOVERED_AND_ROLLBACK = true;
{code}
makes tests for XA sink and Oracle failing.;;;","14/Jul/21 12:13;roman;Hmm..I just quickly searched for this error and found that Oracle [may require additional configuration|https://www.ibm.com/support/pages/receiving-xa-recover-operation-3-when-restarting-application-server-when-using-oracle-database] for XA_RECOVER.

Can it be something similar in your case?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Catalog retrieve Flink Properties error,FLINK-22207,13371187,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangzeyu,wangzeyu,wangzeyu,12/Apr/21 06:44,28/May/21 09:06,13/Jul/23 08:07,14/Apr/21 09:49,1.12.1,,,,,,,,1.13.0,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"If we use hive catalog and set flink propertie eg.""url"",and then when we add the other propertie like start with string ""flink."" like ""flink.url"" will show we the error ""java.lang.IllegalStateException: Duplicate key"" , and what's worse is because of this error we can`t drop or alter this tabel more.

I found in this method , ""org.apache.flink.table.catalog.hive.HiveCatalog.retrieveFlinkProperties"", replace all ""flink."" of propertie. So , the  propertie of HiveCatalog ""flink.url"" and ""flink.flink.url"" both result to ""url"". 

I think in the method ""org.apache.flink.table.catalog.hive.HiveCatalog.retrieveFlinkProperties"" we should use replaceFirst but not replace to handle propertie.",,libenchao,lirui,wangzeyu,xmarker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 09:49:23 UTC 2021,,,,,,,,,,"0|z0psfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/21 06:49;wangzeyu;So here's my solution , change replace to replaceFirst , and it is work in my project
{code:java}
private static Map<String, String> retrieveFlinkProperties(
        Map<String, String> hiveTableParams) {
    return hiveTableParams.entrySet().stream()
            .filter(
                    e ->
                            e.getKey().startsWith(FLINK_PROPERTY_PREFIX)
                                    || e.getKey().equals(CatalogConfig.IS_GENERIC))
            .collect(
                    Collectors.toMap(
                            e -> e.getKey().replaceFirst(FLINK_PROPERTY_PREFIX, """"),
                            e -> e.getValue()));
}
{code};;;","12/Apr/21 07:27;xmarker;As you said it should use replaceFirst to get flink properties,will you resolve this bug ? if you willn't,i can resolve it and add some unit test;;;","12/Apr/21 07:38;wangzeyu;i will resolve this bug;;;","14/Apr/21 09:49;ykt836;fixed: c29ee91abdb092f8dca5274a6628b71e88b57485;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaChangelogTableITCase.testKafkaCanalChangelogSource fail due to ConcurrentModificationException,FLINK-22203,13371172,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,maguowei,maguowei,12/Apr/21 03:45,28/Aug/21 12:11,13/Jul/23 08:07,21/May/21 04:28,1.13.0,,,,,,,,1.13.2,1.14.0,,,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16332&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7425

{code:java}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
	at java.util.HashMap$ValueIterator.next(HashMap.java:1474)
	at java.util.AbstractCollection.toArray(AbstractCollection.java:141)
	at java.util.ArrayList.addAll(ArrayList.java:583)
	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.lambda$getResults$0(TestValuesRuntimeFunctions.java:108)
	at java.util.HashMap$Values.forEach(HashMap.java:981)
	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.getResults(TestValuesRuntimeFunctions.java:108)
	at org.apache.flink.table.planner.factories.TestValuesTableFactory.getResults(TestValuesTableFactory.java:164)
	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.waitingExpectedResults(KafkaTableTestUtils.java:82)
	at org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.testKafkaCanalChangelogSource(KafkaChangelogTableITCase.java:348)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)

{code}
",,dwysakowicz,jark,JohnTeslaa,maguowei,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22779,,,FLINK-22720,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 04:28:03 UTC 2021,,,,,,,,,,"0|z0pscg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/21 06:25;dwysakowicz;cc [~jark] ;;;","27/Apr/21 23:30;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","19/May/21 10:52;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","20/May/21 13:23;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18178&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","21/May/21 04:28;jark;Fixed in master: 17d5643235d486be7f6a27fd5042311b6a95bcf7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread safety in ParquetColumnarRowInputFormat,FLINK-22202,13371165,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,12/Apr/21 02:11,07/Apr/22 06:32,13/Jul/23 08:07,13/Apr/21 07:39,,,,,,,,,1.13.0,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"In a {{VectorizedColumnBatch}}, the dictionary will be lazied deserialized. 

If there are multiple batches at the same time, there may be thread safety problems, because the deserialization of the dictionary depends on some internal structures.

We need set numBatchesToCirculate to 1 for ParquetColumnarRowInputFormat.",,hackergin,libenchao,lirui,lzljs3620320,yanchenyun,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20951,,,,,,,,,FLINK-21397,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 07:39:41 UTC 2021,,,,,,,,,,"0|z0psaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 07:39;lzljs3620320;master (1.13): 413ff6ae7e7a1bd6c5fe495a141f02d383165776;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkRelMdUniqueKeys#getTableUniqueKeys should filter out  the column indices which the does not exist in current row schema,FLINK-22196,13371095,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,neighborhood,neighborhood,neighborhood,11/Apr/21 06:13,13/Apr/21 09:00,13/Jul/23 08:07,13/Apr/21 09:00,1.12.2,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,,,,,,"Well since we have supported project push-down, some primary-key in original schema may not exist no longer more, so the non-existed column shall be filtered out.

 

Current code:
{code:java}
val columns = relOptTable.getRowType.getFieldNames
val columnIndices = schema.getPrimaryKey.get().getColumns map { c =>
  columns.indexOf(c)
}
{code}
May be supposed to correct to:
{code:java}
val columns = relOptTable.getRowType.getFieldNames
val columnIndices = schema
  .getPrimaryKey
  .get()
  .getColumns
  .map(columns.indexOf)
  .filterNot(_ == -1) // filter out the non-exist column
{code}
 ",,jark,libenchao,neighborhood,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 09:00:08 UTC 2021,,,,,,,,,,"0|z0prvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/21 06:20;neighborhood;cc [~jark]

cc [~godfreyhe]

 ;;;","11/Apr/21 07:49;jark;Looks like a bug. But if partial fields of primary key is projected, then the whole primary key (or unique key) is not exist anymore. We should return null instead of filtering out -1 indices. ;;;","13/Apr/21 09:00;neighborhood;This issue has been fixed by https://issues.apache.org/jira/browse/FLINK-22157;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNHighAvailabilityITCase.testClusterClientRetrieval because of TestTimedOutException,FLINK-22195,13371094,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,maguowei,maguowei,11/Apr/21 06:08,09/Jul/21 01:44,13/Jul/23 08:07,09/Jul/21 01:44,1.13.0,,,,,,,,1.13.2,1.14.0,,,,,,Deployment / YARN,Runtime / Coordination,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16308&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab&l=31562


{code:java}
org.junit.runners.model.TestTimedOutException: test timed out after 1800000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1223)
	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:593)
	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:418)
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.deploySessionCluster(YARNHighAvailabilityITCase.java:356)
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$testClusterClientRetrieval$2(YARNHighAvailabilityITCase.java:224)
	at org.apache.flink.yarn.YARNHighAvailabilityITCase$$Lambda$250/401276136.run(Unknown Source)
	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:287)
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.testClusterClientRetrieval(YARNHighAvailabilityITCase.java:219)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
",,maguowei,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22285,,,,,,FLINK-22662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 09 01:44:43 UTC 2021,,,,,,,,,,"0|z0prv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/21 04:00;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17181&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=29670;;;","14/May/21 04:30;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17959&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=29666;;;","04/Jun/21 02:59;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18653&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=29298;;;","06/Jul/21 03:03;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19944&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=29090;;;","09/Jul/21 01:44;xtsong;Fixed by FLINK-22662;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlinkStreamUserDefinedFunctionTests.test_udf_in_join_condition_2 fail due to NPE,FLINK-22191,13371090,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,maguowei,maguowei,11/Apr/21 05:19,28/May/21 09:03,13/Jul/23 08:07,12/Apr/21 06:28,1.13.0,,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16326&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=f5211ead-5e53-5af8-f827-4dbf08df26bb&l=21130

",,dian.fu,hxbks2ks,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22261,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 06:28:00 UTC 2021,,,,,,,,,,"0|z0pru8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/21 05:19;maguowei;CC [~dianfu];;;","11/Apr/21 05:25;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16322&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=20921

test_keyed_process_function_with_state fail also due to NPE
;;;","11/Apr/21 05:27;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16319&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=20921;;;","12/Apr/21 03:41;maguowei;test_timestamp_assigner_and_watermark_strategy failed also due to NPE
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16332&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=613f538c-bcef-59e6-f9cd-9714bec9fb97&l=22997;;;","12/Apr/21 06:28;hxbks2ks;Merged into master via 7debc0677aa1e3c0eaac8c21bdc5d045aa86401c
Merged into release-1.12 via e1d459a864d7f54841cf5b2d295e1970c9e26003;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove description of sync checkpoint mode of HashMapStateBackend,FLINK-22187,13370818,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,yunta,yunta,10/Apr/21 14:05,28/May/21 09:03,13/Jul/23 08:07,12/Apr/21 06:15,,,,,,,,,1.13.0,,,,,,,Documentation,,,,,0,pull-request-available,,,,,"Currently, there still existed [documentation|https://ci.apache.org/projects/flink/flink-docs-master/docs/ops/state/state_backends/#the-hashmapstatebackend] to tell users how to enable sync checkpoint mode of HashMapStateBackend. However, this constructor has been removed, and we should remove such documentation.",,dwysakowicz,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21935,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 06:15:12 UTC 2021,,,,,,,,,,"0|z0pqu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/21 06:15;dwysakowicz;Fixed in 2705d84ec8293f8bba0fe5a87fc82928f177cb5e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rest client shutdown on failure runs in netty thread,FLINK-22184,13370678,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Apr/21 14:06,28/May/21 09:04,13/Jul/23 08:07,12/Apr/21 08:43,1.10.0,,,,,,,,1.11.4,1.12.3,1.13.0,,,,,Client / Job Submission,,,,,0,pull-request-available,,,,,"Then using the CLI to run any command, if the request fails then the RestClient is shut down from the netty thread, which causes problems because when shutting down we try to shut down netty, but the netty thread is still busy shutting things down.",,klion26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 08:43:17 UTC 2021,,,,,,,,,,"0|z0ppyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/21 08:43;chesnay;master: f4b59f615438e76c2b42999fc0a8ebce6a543b07
1.12: 5b6e6234380f2a932137413adbbc1bd8d0a57ccc
1.11: 2766d309baefd9eed2f0ea0c7f7cbcb79af82b3e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RM reclaims slots while batch job is in progress,FLINK-22180,13370648,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,dwysakowicz,dwysakowicz,09/Apr/21 13:01,28/May/21 09:07,13/Jul/23 08:07,15/Apr/21 00:20,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16271&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4036

{code}
2021-04-09T09:51:37.9803869Z [INFO] Running org.apache.flink.test.iterative.ConnectedComponentsWithSolutionSetFirstITCase
2021-04-09T09:51:42.3003700Z Job execution failed.
2021-04-09T09:51:42.3125900Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-04-09T09:51:42.3128088Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-04-09T09:51:42.3129160Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-04-09T09:51:42.3130010Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-04-09T09:51:42.3130773Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-04-09T09:51:42.3131511Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-04-09T09:51:42.3137024Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-04-09T09:51:42.3138081Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
2021-04-09T09:51:42.3138945Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-04-09T09:51:42.3139739Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-04-09T09:51:42.3140492Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-04-09T09:51:42.3141212Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-04-09T09:51:42.3141999Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1066)
2021-04-09T09:51:42.3142892Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2021-04-09T09:51:42.3143460Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-04-09T09:51:42.3144072Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-04-09T09:51:42.3144857Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-04-09T09:51:42.3145476Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-04-09T09:51:42.3146230Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-04-09T09:51:42.3147022Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-04-09T09:51:42.3147857Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-04-09T09:51:42.3148552Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-04-09T09:51:42.3149277Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2021-04-09T09:51:42.3150172Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-04-09T09:51:42.3233576Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-04-09T09:51:42.3234583Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-04-09T09:51:42.3235304Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-04-09T09:51:42.3236015Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-04-09T09:51:42.3237181Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-04-09T09:51:42.3238165Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-04-09T09:51:42.3239041Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-04-09T09:51:42.3239798Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-04-09T09:51:42.3240522Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-04-09T09:51:42.3241216Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-04-09T09:51:42.3241986Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-04-09T09:51:42.3243232Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-04-09T09:51:42.3243930Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-04-09T09:51:42.3244748Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-04-09T09:51:42.3245429Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-04-09T09:51:42.3246212Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-04-09T09:51:42.3247118Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-04-09T09:51:42.3248306Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-04-09T09:51:42.3249304Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:206)
2021-04-09T09:51:42.3250180Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:196)
2021-04-09T09:51:42.3251112Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:187)
2021-04-09T09:51:42.3252068Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:680)
2021-04-09T09:51:42.3253223Z 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2021-04-09T09:51:42.3254507Z 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1462)
2021-04-09T09:51:42.3255444Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1112)
2021-04-09T09:51:42.3256235Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1052)
2021-04-09T09:51:42.3257025Z 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:883)
2021-04-09T09:51:42.3289873Z 	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$5(Execution.java:598)
2021-04-09T09:51:42.3290986Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-04-09T09:51:42.3291964Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-04-09T09:51:42.3293107Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2021-04-09T09:51:42.3294152Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-04-09T09:51:42.3295130Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-04-09T09:51:42.3295943Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-04-09T09:51:42.3296927Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-04-09T09:51:42.3297919Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-04-09T09:51:42.3298763Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-04-09T09:51:42.3299880Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-04-09T09:51:42.3300709Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-04-09T09:51:42.3301554Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-04-09T09:51:42.3302472Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-04-09T09:51:42.3303309Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-04-09T09:51:42.3304144Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-04-09T09:51:42.3304918Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-04-09T09:51:42.3305541Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-04-09T09:51:42.3306224Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-04-09T09:51:42.3306711Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-04-09T09:51:42.3307516Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-04-09T09:51:42.3308219Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-04-09T09:51:42.3308813Z 	... 4 more
2021-04-09T09:51:42.3309940Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: No task slot allocated for job ID eb55ab4ea4591c142078631839e1216f and allocation ID 802cf7ad875c5d2c3220b55a32b00401.
2021-04-09T09:51:42.3311414Z 	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
2021-04-09T09:51:42.3311964Z 	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
2021-04-09T09:51:42.3312509Z 	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)
2021-04-09T09:51:42.3313227Z 	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)
2021-04-09T09:51:42.3313894Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-04-09T09:51:42.3314805Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-04-09T09:51:42.3315888Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)
2021-04-09T09:51:42.3317047Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-04-09T09:51:42.3318060Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-04-09T09:51:42.3319103Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-04-09T09:51:42.3320125Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-04-09T09:51:42.3321082Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1064)
2021-04-09T09:51:42.3321711Z 	at akka.dispatch.OnComplete.internal(Future.scala:263)
2021-04-09T09:51:42.3322067Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-04-09T09:51:42.3322537Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-04-09T09:51:42.3322940Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-04-09T09:51:42.3323322Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-04-09T09:51:42.3323789Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-04-09T09:51:42.3324269Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-04-09T09:51:42.3324728Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-04-09T09:51:42.3325177Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-04-09T09:51:42.3325638Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2021-04-09T09:51:42.3326176Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-04-09T09:51:42.3326634Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-04-09T09:51:42.3327697Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-04-09T09:51:42.3328547Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-04-09T09:51:42.3329434Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-04-09T09:51:42.3330625Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-04-09T09:51:42.3331784Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-04-09T09:51:42.3333847Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-04-09T09:51:42.3334886Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-04-09T09:51:42.3335978Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-04-09T09:51:42.3336866Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-04-09T09:51:42.3337874Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-04-09T09:51:42.3338622Z 	... 4 more
2021-04-09T09:51:42.3339740Z Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: No task slot allocated for job ID eb55ab4ea4591c142078631839e1216f and allocation ID 802cf7ad875c5d2c3220b55a32b00401.
2021-04-09T09:51:42.3340997Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:593)
2021-04-09T09:51:42.3341839Z 	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
2021-04-09T09:51:42.3342854Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-04-09T09:51:42.3343581Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-04-09T09:51:42.3344553Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
2021-04-09T09:51:42.3374800Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
2021-04-09T09:51:42.3375800Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-04-09T09:51:42.3376525Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-04-09T09:51:42.3377185Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-04-09T09:51:42.3377950Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-04-09T09:51:42.3378595Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-04-09T09:51:42.3379277Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-04-09T09:51:42.3379924Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-04-09T09:51:42.3380572Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-04-09T09:51:42.3381245Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-04-09T09:51:42.3381877Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-04-09T09:51:42.3382769Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-04-09T09:51:42.3383336Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-04-09T09:51:42.3383880Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-04-09T09:51:42.3384422Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-04-09T09:51:42.3384921Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-04-09T09:51:42.3385319Z 	... 4 more
{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21859,,,FLINK-21751,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 00:20:07 UTC 2021,,,,,,,,,,"0|z0pps8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 12:04;chesnay;I've marked this as a blocker for the time being because I have a suspicion that FLINK-21751 introduced a regression where the RM might reclaim slots while they are being offered to (and subsequently acceptedby ) the JM.;;;","14/Apr/21 08:52;chesnay;Re-opening because it appears to be a separate issue from FLINK-21859,, that shows similar symptoms.;;;","15/Apr/21 00:20;chesnay;master: 9fd6ecf18ddb744a971f575ab11eb19b44383899;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of shared resource among Python Operators of the same slot is not released,FLINK-22172,13370562,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,09/Apr/21 06:31,28/May/21 09:02,13/Jul/23 08:07,10/Apr/21 03:01,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,"The problem was discussed here:
https://issues.apache.org/jira/browse/FLINK-20663?focusedCommentId=17317706&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17317706

In general, the python process should be terminated when the shared resource is released as many times as requested. However, this reference counting logic has been implemented twice. Consequently, the shared resource may be requested multiple times but only released once.",,dian.fu,hxbks2ks,xtsong,zhou_yb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 10 03:01:13 UTC 2021,,,,,,,,,,"0|z0pp94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/21 06:37;ykt836;what kind of resource, managed memory?;;;","09/Apr/21 06:42;hxbks2ks;[~ykt836] Yes.;;;","09/Apr/21 06:44;xtsong;[~ykt836], I've added the description, pointing to the original discussion in FLINK-20663.;;;","10/Apr/21 03:01;hxbks2ks;Merged into master 023568b83d971b439468d680bddc66d584c6f7c4
Merged into release-1.12 387eb5cee9e2a1a18588a23d828392dd63f76faa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beautify the CliTableauResultView when print,FLINK-22169,13370541,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,09/Apr/21 04:11,28/May/21 09:07,13/Jul/23 08:07,15/Apr/21 05:51,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"In batch mode, the print is not as same as before.

 !print.png! 

",,fsk119,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/21 04:13;fsk119;print.png;https://issues.apache.org/jira/secure/attachment/13023591/print.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 05:51:56 UTC 2021,,,,,,,,,,"0|z0pp4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/21 05:51;ykt836;fixed: 326a564ebc0e397654ccd4e5a83a77ef811c6a76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition insert with union all will fail,FLINK-22168,13370535,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Apr/21 03:52,28/May/21 11:07,13/Jul/23 08:07,22/Apr/21 10:03,,,,,,,,,1.13.0,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"INSERT INTO partitioned_sink (e,a,g,f,c,d) SELECT e,a,456,123,c,d FROM MyTable GROUP BY a,b,c,d,e UNION ALL SELECT e,a,789,456,c,d FROM MyTable GROUP BY a,b,c,d,e",,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 10:03:07 UTC 2021,,,,,,,,,,"0|z0pp34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 10:03;lzljs3620320;master (1.14): 04ce6008f29e93faf5de4bbb120597bf99552bbb

release-1.13: 72b3eb7b5b31a0bf241f3197cf86fd90ba723823;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partial insert not works when complex fields reorder,FLINK-22167,13370534,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Apr/21 03:49,28/May/21 09:06,13/Jul/23 08:07,14/Apr/21 08:37,,,,,,,,,1.13.0,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"{code:java}
util.tableEnv.executeSql(
  s""""""
     |create table partitioned_sink (
     |  `a` INT,
     |  `b` AS `a` + 1,
     |  `c` STRING,
     |  `d` STRING,
     |  `e` DOUBLE,
     |  `f` BIGINT,
     |  `g` INT
     |) PARTITIONED BY (`c`, `d`) with (
     |  'connector' = 'values',
     |  'sink-insert-only' = 'false'
     |)
     |"""""".stripMargin)

INSERT INTO sink (b,e,a,g,f,c,d) SELECT b,e,a,456,123,c,d FROM MyTable GROUP BY a,b,c,d,e
{code}
 ",,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 08:37:55 UTC 2021,,,,,,,,,,"0|z0pp2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/21 08:37;lzljs3620320;master (1.13): 73e777be1069058442e19e9c5fc6b8f53d6f714f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Empty values with sort willl fail,FLINK-22166,13370533,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Apr/21 03:49,28/May/21 09:04,13/Jul/23 08:07,13/Apr/21 05:52,,,,,,,,,1.13.0,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"SELECT * FROM (VALUES 1, 2, 3) AS T (a) WHERE a = 1 and a = 2 ORDER BY a",,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 05:52:21 UTC 2021,,,,,,,,,,"0|z0pp2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 05:52;lzljs3620320;master (1.13): 0e82a998afbb1d4be4fbf1a48c2a232ccc53fb9c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join & Select a part of composite primary key will cause ArrayIndexOutOfBoundsException,FLINK-22157,13370377,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,08/Apr/21 12:44,23/Sep/21 17:53,13/Jul/23 08:07,13/Apr/21 03:32,1.13.0,,,,,,,,1.12.5,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Add the following test case to {{org.apache.flink.table.planner.plan.stream.sql.join.JoinTest}} to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  util.tableEnv.executeSql(
    """"""
      |CREATE TABLE MyTable (
      |  pk1 INT,
      |  pk2 BIGINT,
      |  PRIMARY KEY (pk1, pk2) NOT ENFORCED
      |) WITH (
      |  'connector'='values'
      |)
      |"""""".stripMargin)
  util.verifyExecPlan(""SELECT A.a1 FROM A LEFT JOIN MyTable ON A.a1 = MyTable.pk1"")
}
{code}

The exception stack is
{code}
java.lang.RuntimeException: Error while applying rule StreamPhysicalJoinRule, args [rel#141:FlinkLogicalJoin.LOGICAL.any.None: 0.[NONE].[NONE](left=RelSubset#139,right=RelSubset#140,condition==($0, $1),joinType=left), rel#138:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#137,select=a1), rel#121:FlinkLogicalTableSourceScan.LOGICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, MyTable, project=[pk1]],fields=pk1)]

	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:281)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:889)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:583)
	at org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.myTest(JoinTest.scala:300)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.lang.RuntimeException: Error occurred while applying rule StreamPhysicalJoinRule
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:161)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalJoinRuleBase.onMatch(StreamPhysicalJoinRuleBase.scala:90)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
	... 49 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.calcite.util.ImmutableBitSet.of(ImmutableBitSet.java:113)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.getTableUniqueKeys(FlinkRelMdUniqueKeys.scala:76)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.getUniqueKeys(FlinkRelMdUniqueKeys.scala:59)
	at GeneratedMetadataHandler_UniqueKeys.getUniqueKeys_$(Unknown Source)
	at GeneratedMetadataHandler_UniqueKeys.getUniqueKeys(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getUniqueKeys(RelMetadataQuery.java:464)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.getUniqueKeys(FlinkRelMdUniqueKeys.scala:591)
	at GeneratedMetadataHandler_UniqueKeys.getUniqueKeys_$(Unknown Source)
	at GeneratedMetadataHandler_UniqueKeys.getUniqueKeys(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getUniqueKeys(RelMetadataQuery.java:464)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getUniqueKeys(RelMetadataQuery.java:445)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalJoin.getUniqueKeys(StreamPhysicalJoin.scala:111)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalJoin.explainTerms(StreamPhysicalJoin.scala:107)
	at org.apache.calcite.rel.AbstractRelNode.getDigestItems(AbstractRelNode.java:409)
	at org.apache.calcite.rel.AbstractRelNode.deepHashCode(AbstractRelNode.java:391)
	at org.apache.calcite.rel.AbstractRelNode$InnerRelDigest.hashCode(AbstractRelNode.java:443)
	at java.util.HashMap.hash(HashMap.java:339)
	at java.util.HashMap.get(HashMap.java:557)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1150)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148)
	... 53 more
{code}

This is because {{FlinkRelMdUniqueKeys#getTableUniqueKeys}} does not consider the case when projections are pushed down and cause only a part of the composite primary key to be selected.",,godfreyhe,jark,libenchao,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23135,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 03:32:17 UTC 2021,,,,,,,,,,"0|z0po40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 03:32;godfreyhe;Fixed in 1.13.0: 92fbe7f1fe5f0eade036b4184cdbab8f9b791647
Fixed in 1.12.5: d1b8c5fd54e1d104387ef6acb94b9a9698378ed3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXPLAIN statement should validate insert and query separately,FLINK-22155,13370370,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhaoWeiNan,fsk119,fsk119,08/Apr/21 12:28,13/Jun/22 02:23,13/Jul/23 08:07,20/May/21 14:55,1.13.0,,,,,,,,1.14.0,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"When explain insert statement, the validator validate the whole statement rather than validate the query. But when execute insert statement, the planner only validate the query part of the insert statement. It may brings the result of the explan is different from the actual plan. ",,airblader,fsk119,jark,libenchao,qingyue,twalthr,ZhaoWeiNan,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27965,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 14:55:41 UTC 2021,,,,,,,,,,"0|z0po2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 04:33;ZhaoWeiNan;please assgin to me , i will do it;;;","20/May/21 14:55;jark;Fixed in master: afe0fddfad0864218329d921215fb3850037d606;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushFilterIntoTableSourceScanRule fails to deal with IN expressions,FLINK-22154,13370364,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,08/Apr/21 12:06,28/May/21 09:07,13/Jul/23 08:07,15/Apr/21 11:34,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Add the following test case to {{PushFilterIntoLegacyTableSourceScanRuleTest}} to reproduce this bug. {{PushFilterIntoTableSourceScanRuleTest}} extends this class and will also be tested.

{code:scala}
@Test
def myTest(): Unit = {
  util.verifyRelPlan(""SELECT * FROM MyTable WHERE name IN ('Alice', 'Bob', 'Dave')"")
}
{code}

The exception stack is
{code}
java.lang.AssertionError: OR(OR(=($0, _UTF-16LE'Alice':VARCHAR(5) CHARACTER SET ""UTF-16LE""), =($0, _UTF-16LE'Bob':VARCHAR(5) CHARACTER SET ""UTF-16LE"")), =($0, _UTF-16LE'Dave':VARCHAR(5) CHARACTER SET ""UTF-16LE""))

	at org.apache.calcite.rel.core.Filter.<init>(Filter.java:76)
	at org.apache.calcite.rel.logical.LogicalFilter.<init>(LogicalFilter.java:68)
	at org.apache.calcite.rel.logical.LogicalFilter.copy(LogicalFilter.java:126)
	at org.apache.calcite.rel.logical.LogicalFilter.copy(LogicalFilter.java:45)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.pushFilterIntoScan(PushFilterIntoLegacyTableSourceScanRule.scala:130)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.onMatch(PushFilterIntoLegacyTableSourceScanRule.scala:77)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:281)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:889)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlan(TableTestBase.scala:400)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.myTest(PushFilterIntoLegacyTableSourceScanRuleTest.scala:76)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}

This is because {{Filter}} in calcite requires its {{condition}} to be ""flat"", so we should first simplify the conditions before constructing a filter.",,godfreyhe,jark,libenchao,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 11:34:49 UTC 2021,,,,,,,,,,"0|z0po14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/21 11:34;godfreyhe;Fixed in 1.13.0: 067f237d73635f98a454c7f23c1faf4d4c7add1a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Planner rules should use RexCall#equsls to check whether two rexCalls are equivalent,FLINK-22148,13370293,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,icshuo,icshuo,08/Apr/21 06:50,28/May/21 09:02,13/Jul/23 08:07,09/Apr/21 04:23,1.12.2,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Reproduce the bug by add the following test to `SemiAntiJoinTest`

 
{code:java}
// code placeholder
@Test
def testNotSimplifyJoinConditionWithSameDigest(): Unit = {
  val sqlQuery =
  """"""
    |SELECT a
    |FROM l
    |WHERE c NOT IN (
    |    SELECT f FROM r WHERE f = c)
    |"""""".stripMargin
  util.verifyRelPlan(sqlQuery)
}
{code}
 

CannotPlanException will be thrown, this is because Calcite planner will normalize a RexCall in the `equals` method (from 1.24), while in Flink planer rules, we still use toString to check two RexCalls are equivalent.",,godfreyhe,icshuo,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 04:23:48 UTC 2021,,,,,,,,,,"0|z0pnlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/21 06:52;icshuo;[~godfreyhe], I'll solve this problem.;;;","09/Apr/21 04:23;godfreyhe;Fixed in 1.13.0: b17e7b5d3504a4b70a9c9bcf50175a235e9186fe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink returns less rows than expected when using limit in SQL,FLINK-22143,13370085,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,iyupeng,iyupeng,iyupeng,07/Apr/21 12:15,28/May/21 09:01,13/Jul/23 08:07,09/Apr/21 03:45,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"Flink's blink runtime returns less rows than expected when querying Hive tables with limit.
{code:java}
// sql
select i_item_sk from tpcds_1g_snappy.item limit 5000;
{code}
 

Above query will return only *4998* lines in some cases.

 

This problem can be re-produced on below conditions:
 # A Hive table with parquet format.
 # Running SQL with limit using blink planner since Flink version 1.12.0
 # The input table is small. (With only 1 data file in which there is only 1 row group, e.g. 1 GB of TPCDS benchmark data)
 # The requested count of lines by `limit` is above the batch size (2048 by default)

 

After investigation, a bug is found lying in the *LimitableBulkFormat* class.

In this class, for each batch, *numRead* will be increased *1* more than actual count of rows returned by reader.readBatch().

The reason is that *numRead* get increased even when next() reaches then end of current batch.

If there is only 1 input split, no more lines will be merged into the final result. 

As a result, less lines will be returned by Flink.

 ",,iyupeng,jark,libenchao,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 03:45:19 UTC 2021,,,,,,,,,,"0|z0pmb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 12:35;jark;cc [~lzljs3620320];;;","09/Apr/21 03:45;ykt836;fixed: 3f4dd8229436ad2612df820f1bd83cc35e6325ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitEmumerator does not provide checkpoint id in snapshot,FLINK-22133,13369996,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,Brian Zhou,Brian Zhou,07/Apr/21 08:29,28/Aug/21 11:16,13/Jul/23 08:07,20/Apr/21 16:18,1.12.0,,,,,,,,1.13.0,,,,,,,Connectors / Common,,,,,0,pull-request-available,,,,,"In ExternallyInducedSource API, the checkpoint trigger exposes the checkpoint Id for the external client to identify the checkpoint. However, in the FLIP-27 source, the SplitEmumerator::snapshot() is a no-arg method. The connector cannot track the checkpoint ID from Flink",,becket_qin,Brian Zhou,dwysakowicz,sewen,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 18:20:03 UTC 2021,,,,,,,,,,"0|z0plrc:",9223372036854775807,The unified source API for connectors has a minor breaking change: The {{SplitEnumerator.snapshotState()}} method was adjusted to accept the Checkpoint ID of the checkpoint for which the snapshot is created.,,,,,,,,,,,,,,,,,,,"19/Apr/21 12:20;sewen;Originally, we were thinking to keep checkpoint IDs out of the {{SplitEnumerator}} completely, keeping it simple.

We somehow partially went away from this with adding the {{CheckpointListener}} interface, which gets checkpointIDs for completed checkpoints. In that, I guess it makes sense to add them also to the {{SplitEnumerator.snapshotState()}} method.

[~jqin] what do you think?

[~dwysakowicz] [~maguowei] This is a very simple change, but API breaking, so the sooner we do it the better. Do you think we should put this as a fix into the 1.13 release?

Alternatively, I can create a non-breaking version this way:
  - For 1.12.x and 1.13.x, we add a new default method {{snapshotState(long checkpointId)}} which calls the method {{snapshotState()}}. The default method is the one that the system calls. So everything keeps working as currently, but users can override the default method (although they still have to override the other method as well with an empty body, to make the code compile).

  - For 1.14, we remove the {{snapshotState()}} method and make the {{snapshotState(long checkpointId)}} a non-default method.



;;;","19/Apr/21 12:44;dwysakowicz;Given the limited scope of the change and that there is still not much adoption in external connectors (imo) I'd be fine with performing the final change in 1.13 already. 

I think the two options do not differ much in a relation to the feature freeze. They both include the same dangers that the checkpointId is not passed correctly. So the real question is just if we want to break the API in a single release or give users one extra release.  Again given the limited scope I'd do it already in 1.13.;;;","19/Apr/21 12:45;dwysakowicz;btw here is a link to the ML discussion thread: https://lists.apache.org/thread.html/r0743a2e27c5fd3654a769d4c2b77a0f68b2a5f1ef2f89b2504c088d8%40%3Cdev.flink.apache.org%3E;;;","20/Apr/21 16:16;sewen;Fixed in
  - 1.13.0 via fdcf730ce106cfe0050593a3ddc068a8b7222e94
  - 1.14.0 (master) via c4678d874b32cd996da49c39ea33b3420b77e17c;;;","20/Apr/21 16:16;sewen;Re-opening for release notes.;;;","03/May/21 16:58;thw;[~jqin] [~sewen] what do you think about back porting this change to 1.12? It would allow users/downstream projects to build on top of the FLIP-27 interfaces without facing breakage when going from 1.12 to 1.13.

The source API is experimental in 1.12 and we already made incompatible changes to it in 1.12.x releases?

 ;;;","07/May/21 00:54;becket_qin;[~thw] I think we only plan to backport some backwards incompatible changes to the Kafka source in 1.12.x, which is otherwise not usable. We did not introduce backwards incompatible changes to the general Source API in 1.12.x.

I am wondering if this patch is generally useful enough to justify the backport.  So far it seems the checkpoint ID is only used by Pravega at this point. I know some users have already developed a few sources on top of release 1.12. Backporting this change would break them immediately when they upgrade to 1.12.4. On the other hand, given that SplitEnumerator API is PublicEvolving, users would probably be OK with this minor API change when they upgrade from 1.12 to 1.13+.

What do you think?;;;","07/May/21 02:06;thw;It's a compromise. The assumption was that there isn't enough adoption to warrant the backward compatible option: https://issues.apache.org/jira/browse/FLINK-22133?focusedCommentId=17325008&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17325008

For those that are building on top of 1.12.x incurring the breakage with 1.12.4 is probably cheaper than when upgrading to 1.13. If, for example, the Iceberg project provides a source developed on top of Flink 1.12.x, then the same source won't work on 1.13.x.;;;","07/May/21 06:37;becket_qin;Thanks for the explanation, Thomas. So here are the pros and cons of backporting if I understand correctly:
 * Pros: those who develop sources on top of 1.12.4 would not experience API change.
 * Cons: We break the version contract. Some other users who have already implemented the source may get a surprise when upgrade to 1.12.4. (These users will still need to pick the new API if they go to 1.13+. It is just that such API changes in 1.13+ do not break our API contract with the users.)

May I first confirm if ""PublicEvolving"" APIs are allowed to change between minor versions? If it is allowed, then I think backporting totally makes sense. Otherwise, personally I feel keeping the contract seems more beneficial because we don't really know who is going to develop on top of 1.12.4 yet but there are users already implemented the Source in 1.12.3-. So breaking the contract potentially brings benefit to someone but will surely introduce surprise to the others.

If we do want to backport the feature to unblock some users who really need the checkpoint ID, would it be better to do what Stephan suggrested for 1.12?
{quote}... we add a new default method {{snapshotState(long checkpointId)}} which calls the method {{snapshotState()}}. The default method is the one that the system calls. So everything keeps working as currently, but users can override the default method (although they still have to override the other method as well with an empty body, to make the code compile).
{quote};;;","10/May/21 18:20;thw;Hi Becket, I think the suggestion from Stephan would be the best approach. It doesn't break existing users and at the same time allows to build customizations in a 1.13 compatible manner.

PublicEvolving should not be modified in binary incompatible way between patch releases and the japicmp check guards that. So if we were to make the same change as in 1.13, we would require an exclusion. But I think we better stick with the compatibility guarantee.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test unaligned checkpoints rescaling manually on a real cluster,FLINK-22132,13369994,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,pnowojski,pnowojski,07/Apr/21 08:27,22/Jun/21 13:55,13/Jul/23 08:07,27/Apr/21 13:49,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Checkpointing,,,,,0,release-testing,test-stability,,,,"To test unaligned checkpoints, we should use a few different applications that use different features.

The sinks should not be mocked but rather should be able to induce a fair amount of backpressure into the system. Quite possibly, it would be a good idea to have a way to add more backpressure to the sink by running the respective system on the cluster and be able to add/remove parallel instances.

The primary objective is to check if all data is recovered properly and if the semantics is correct (does state match input?). 

The secondary objective is to check if Flink UI shows the information correctly.

More details in the subtasks.",,AHeise,akalashnikov,knaufk,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 26 15:29:46 UTC 2021,,,,,,,,,,"0|z0plqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 09:23;arvid;Since the task is pretty huge. I'm splitting into subtasks for defining the application and executing the application.;;;","23/Apr/21 08:19;knaufk;As part of https://issues.apache.org/jira/browse/FLINK-22029 the ""Test"" Issue Type is removed. I migrated this one to a ""Bug"" and added the label ""test-stability"". If you think this should rather be an ""Improvement"", ""New Feature"" or ""Technical Debt"", feel free to change the issue type.;;;","26/Apr/21 15:29;akalashnikov;I checked the suggested scenarios. I didn't find any problem which would be specific for the unaligned checkpoint.

Settings:

*Cluster*: Amazon EMR (4 instances: 4 vCore, 16 GiB memory)
 *Cluster run*: ./bin/yarn-session.sh --detached
 *Job for testing*: DataStreamAllroundTestProgram and more simple TopSpeedWindowing.
 *Checkpoint*: unaligned
 *Job arguments*: \-\-environment.externalize_checkpoint true \-\-environment.parallelism 2 \-\-state_backend.checkpoint_directory s3://anton-flink-test/checkpoints  \-\-state_backend rocks
*Parallelism*: 1 - 9(Just in case, DataStreamAllroundTestProgram has 9 tasks so in max 9 * 9 = 81 subtasks)

 

A small notice from me. If hashmap use as a state backend, there are a lot of problems appear. For example, OOM or network issues(timeout) but it can be observed for both aligned and unaligned checkpoints. So again, I didn't find the specific unaligned checkpoint problems.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The job finished without any exception if error was thrown during state access,FLINK-22124,13369795,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,dian.fu,dian.fu,06/Apr/21 12:45,28/May/21 09:05,13/Jul/23 08:07,13/Apr/21 11:39,1.13.0,,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,"For the following job:

{code}
import logging

from pyflink.common import WatermarkStrategy, Row
from pyflink.common.serialization import Encoder
from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FileSink, OutputFileConfig, NumberSequenceSource
from pyflink.datastream.execution_mode import RuntimeExecutionMode
from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext
from pyflink.datastream.state import MapStateDescriptor


env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(2)
env.set_runtime_mode(RuntimeExecutionMode.BATCH)

seq_num_source = NumberSequenceSource(1, 1000)

file_sink = FileSink \
    .for_row_format('/Users/dianfu/code/src/apache/playgrounds/examples/output/data_stream_batch_state',
                    Encoder.simple_string_encoder()) \
    .with_output_file_config(OutputFileConfig.builder().with_part_prefix('pre').with_part_suffix('suf').build()) \
    .build()

ds = env.from_source(
    source=seq_num_source,
    watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),
    source_name='file_source',
    type_info=Types.LONG())


class MyKeyedProcessFunction(KeyedProcessFunction):

    def __init__(self):
        self.state = None

    def open(self, runtime_context: RuntimeContext):
        logging.info(""open"")
        state_desc = MapStateDescriptor('map', Types.LONG(), Types.LONG())
        self.state = runtime_context.get_map_state(state_desc)

    def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):
        existing = self.state.get(value[0])
        if existing is None:
            result = value[1]
            self.state.put(value[0], result)
        elif existing <= 10:
            result = value[1] + existing
            self.state.put(value[0], result)
        else:
            result = existing
        yield result


ds.map(lambda a: Row(a % 4, 1), output_type=Types.ROW([Types.LONG(), Types.LONG()])) \
    .key_by(lambda a: a[0]) \
    .process(MyKeyedProcessFunction(), Types.LONG()) \
    .sink_to(file_sink)

env.execute('data_stream_batch_state')
{code}

As it will encounter KeyError in `self.state.get(value[0])` if value[0] doesn't exist in the state, the job finished without any error message. This issue should be addressed. We should make sure the error message appears in the log file to help users to figure out what happens.",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 11:39:58 UTC 2021,,,,,,,,,,"0|z0pkio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 11:39;hxbks2ks;Merged into master via f82941852a4abda0cb1d60d54970031b750b2e4b
Merged into release-1.12 via feb1155c4e7458b6d5c3310cbf65c29a571b23e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkLogicalRankRuleBase should check if name of rankNumberType already exists in the input,FLINK-22121,13369783,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,06/Apr/21 12:08,28/May/21 09:04,13/Jul/23 08:07,13/Apr/21 03:54,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"Add the following test case to {{org.apache.flink.table.planner.plan.stream.sql.RankTest}} to reproduce this issue.

{code:scala}
@Test
def myTest(): Unit = {
  val sql =
    """"""
      |SELECT CAST(rna AS INT) AS rn1, CAST(rnb AS INT) AS rn2 FROM (
      |  SELECT *, row_number() over (partition by a order by b desc) AS rnb
      |  FROM (
      |    SELECT *, row_number() over (partition by a, c order by b desc) AS rna
      |    FROM MyTable
      |  )
      |  WHERE rna <= 100
      |)
      |WHERE rnb <= 100
      |"""""".stripMargin
  util.verifyExecPlan(sql)
}
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [w0$o0]

	at org.apache.flink.table.types.logical.RowType.validateFields(RowType.java:272)
	at org.apache.flink.table.types.logical.RowType.<init>(RowType.java:157)
	at org.apache.flink.table.types.logical.RowType.of(RowType.java:297)
	at org.apache.flink.table.types.logical.RowType.of(RowType.java:289)
	at org.apache.flink.table.planner.calcite.FlinkTypeFactory$.toLogicalRowType(FlinkTypeFactory.scala:632)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalRank.translateToExecNode(StreamPhysicalRank.scala:117)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.generate(ExecNodeGraphGenerator.java:74)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.generate(ExecNodeGraphGenerator.java:71)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.generate(ExecNodeGraphGenerator.java:54)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToExecNodeGraph(PlannerBase.scala:314)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:895)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:583)
{code}

This is because currently names of rank fields are all {{w0$o0}}, so if the input of a Rank is another Rank the exception will occur. To solve this, we should check if name of rank field has occurred in the input in {{FlinkLogicalRankRuleBase}}.",,godfreyhe,jark,libenchao,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 03:54:42 UTC 2021,,,,,,,,,,"0|z0pkg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 03:54;godfreyhe;Fixed in 1.13.0: a2c429d9a626d5f4ea47aa58a3e9f2da7ce1f47e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove duplicate code in generated code for map get,FLINK-22120,13369775,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,FrankZou,FrankZou,FrankZou,06/Apr/21 11:35,28/May/21 09:02,13/Jul/23 08:07,10/Apr/21 13:32,,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"There is a bug when we get values from a map with a complex expression key.

The reason is the `key.code` in ScalarOperatorGens#generateMapGet will be called twice.

 

This is a test case:
{code:java}
CREATE TABLE T (
     `name` VARCHAR,
     `map` MAP<VARCHAR, INT>
 ) WITH (
     ...
 );
SELECT `map`[coalesce(name, '')] FROM T;
{code}
An exception will be thrown while running this query:
{code:java}
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$21'
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67)
 at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
 at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:70)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:167)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:458)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:527)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:725)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:549)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)
 at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65)
 ... 8 more
 Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
 at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)
 ... 10 more
 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)
 at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
 ... 13 more
 Caused by: org.codehaus.commons.compiler.CompileException: Line 121, Column 27: Redefinition of local variable ""result$6"" 
 at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124)
 at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3662)
 at org.codehaus.janino.UnitCompiler.access$5800(UnitCompiler.java:215)
 at org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3543)
 at org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3511)
 at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3511)
 at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3510)
 at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3567)
 at org.codehaus.janino.UnitCompiler.access$4700(UnitCompiler.java:215)
 at org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3529)
 at org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3511)
 at org.codehaus.janino.Java$Block.accept(Java.java:2776)
{code}",,FrankZou,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 10 13:32:15 UTC 2021,,,,,,,,,,"0|z0pke8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/21 12:37;libenchao;[~FrankZou] thanks for reporting this bug, would you like to fix it?;;;","07/Apr/21 02:40;FrankZou;[~libenchao] Yes, I'd like to take it.;;;","10/Apr/21 13:32;jark;Fixed in master: 7581a62e07cd60941d94cf5c7cc6f4facf3510a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not print stack trace for checkpoint trigger failure if not all tasks are started.,FLINK-22117,13369738,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,06/Apr/21 09:25,28/May/21 09:09,13/Jul/23 08:07,17/Apr/21 08:59,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"Currently the stack trace is printed compared with the previous versions, but it might cover the actual exception that user want to locate. ",,gaoyunhaii,pnowojski,roman,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 19 07:17:13 UTC 2021,,,,,,,,,,"0|z0pk60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/21 09:38;wangyang0918;The exception is really annoying especially when not all the TaskManager register immediately.;;;","17/Apr/21 08:59;roman;Merged into master as 577113f0c339df844f2cc32b1d4a09d3da28085a;;;","19/Apr/21 07:06;pnowojski;[~gaoyunhaii] I would be afraid that {{INFO}} is still too high level. I would go to {{DEBUG}} or even remove it. Why do we need this to be logged? Previously nothing was logged in this scenario. Was lack of this message causing problems?;;;","19/Apr/21 07:17;roman;I see it was previously on INFO level here:

[https://github.com/apache/flink/blob/936489ddda86545392292ee8f82ba99b0bb1b204/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1952]

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UniqueKey constraint is lost with multiple sources join in SQL,FLINK-22113,13369712,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,matriv,okowr,okowr,06/Apr/21 07:36,15/Dec/21 01:44,13/Jul/23 08:07,08/Dec/21 02:46,1.13.0,,,,,,,,1.14.3,1.15.0,,,,,,Table SQL / Planner,,,,,2,auto-unassigned,pull-request-available,,,,"Hi team,
  
 We have a use case to join multiple data sources to generate a continuous updated view. We defined primary key constraint on all the input sources and all the keys are the subsets in the join condition. All joins are left join.
  
 In our case, the first two inputs can produce *JoinKeyContainsUniqueKey* input sepc, which is good and performant. While when it comes to the third input source, it's joined with the intermediate output table of the first two input tables, and the intermediate table does not carry key constraint information(although the thrid source input table does), so it results in a *NoUniqueKey* input sepc. Given NoUniqueKey inputs has dramatic performance implications per the[ Force Join Unique Key|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Force-Join-Unique-Key-td39521.html#a39651] email thread, we want to know if there is any mitigation solution for this.

 

Example:

Take the example from [https://github.com/ververica/flink-sql-cookbook/blob/master/joins/05/05_star_schema.md]
{code:java}
CREATE TEMPORARY TABLE passengers (
  passenger_key STRING,
  first_name STRING,
  last_name STRING,
  update_time TIMESTAMP(3),
  PRIMARY KEY (passenger_key) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'passengers',
  'properties.bootstrap.servers' = 'localhost:9092',
  'key.format' = 'raw',
  'value.format' = 'json'
);


CREATE TEMPORARY TABLE stations (
  station_key STRING,
  update_time TIMESTAMP(3),
  city STRING,
  PRIMARY KEY (station_key) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'stations',
  'properties.bootstrap.servers' = 'localhost:9092',
  'key.format' = 'raw',
  'value.format' = 'json'
);

CREATE TEMPORARY TABLE booking_channels (
  booking_channel_key STRING,
  update_time TIMESTAMP(3),
  channel STRING,
  PRIMARY KEY (booking_channel_key) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'booking_channels',
  'properties.bootstrap.servers' = 'localhost:9092',
  'key.format' = 'raw',
  'value.format' = 'json'
);

CREATE TEMPORARY TABLE train_activities (
  scheduled_departure_time TIMESTAMP(3),
  actual_departure_date TIMESTAMP(3),
  passenger_key STRING,
  origin_station_key STRING,
  destination_station_key STRING,
  booking_channel_key STRING,
  PRIMARY KEY (booking_channel_key, origin_station_key, destination_station_key) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'train_activities',
  'properties.bootstrap.servers' = 'localhost:9092',
  'key.format' = 'json',
  'value.format' = 'json'
);

SELECT 
  t.actual_departure_date, 
  p.first_name,
  p.last_name,
  b.channel, 
  os.city AS origin_station,
  ds.city AS destination_station
FROM train_activities_1 t
LEFT JOIN booking_channels b 
ON t.booking_channel_key = b.booking_channel_key
LEFT JOIN passengers p
ON t.passenger_key = p.passenger_key
LEFT JOIN stations os
ON t.origin_station_key = os.station_key
LEFT JOIN stations ds
ON t.destination_station_key = ds.station_key

{code}
 

 The query will generate exeuction plan of:

 
{code:java}
Flink SQL> explain
>  SELECT
>    t.actual_departure_date,
>    p.first_name,
>    p.last_name,
>    b.channel,
>    os.city AS origin_station,
>    ds.city AS destination_station
>  FROM train_activities_1 t
>  LEFT JOIN booking_channels b
>  ON t.booking_channel_key = b.booking_channel_key
>  LEFT JOIN passengers p
>  ON t.passenger_key = p.passenger_key
>  LEFT JOIN stations os
>  ON t.origin_station_key = os.station_key
>  LEFT JOIN stations ds
>  ON t.destination_station_key = ds.station_key;
== Abstract Syntax Tree ==
LogicalProject(actual_departure_date=[$1], first_name=[$10], last_name=[$11], channel=[$8], origin_station=[$15], destination_station=[$18])
+- LogicalJoin(condition=[=($4, $16)], joinType=[left])
   :- LogicalJoin(condition=[=($3, $13)], joinType=[left])
   :  :- LogicalJoin(condition=[=($2, $9)], joinType=[left])
   :  :  :- LogicalJoin(condition=[=($5, $6)], joinType=[left])
   :  :  :  :- LogicalTableScan(table=[[default_catalog, default_database, train_activities_1]])
   :  :  :  +- LogicalWatermarkAssigner(rowtime=[update_time], watermark=[-($1, 10000:INTERVAL SECOND)])
   :  :  :     +- LogicalTableScan(table=[[default_catalog, default_database, booking_channels]])
   :  :  +- LogicalTableScan(table=[[default_catalog, default_database, passengers]])
   :  +- LogicalTableScan(table=[[default_catalog, default_database, stations]])
   +- LogicalTableScan(table=[[default_catalog, default_database, stations]])



== Optimized Physical Plan ==
Calc(select=[actual_departure_date, first_name, last_name, channel, city AS origin_station, city0 AS destination_station])
+- Join(joinType=[LeftOuterJoin], where=[=(destination_station_key, station_key)], select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city, station_key, city0], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :- Exchange(distribution=[hash[destination_station_key]])
   :  +- Calc(select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city])
   :     +- Join(joinType=[LeftOuterJoin], where=[=(origin_station_key, station_key)], select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name, station_key, city], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :        :- Exchange(distribution=[hash[origin_station_key]])
   :        :  +- Calc(select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name])
   :        :     +- Join(joinType=[LeftOuterJoin], where=[=(passenger_key, passenger_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel, passenger_key0, first_name, last_name], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :        :        :- Exchange(distribution=[hash[passenger_key]])
   :        :        :  +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel])
   :        :        :     +- Join(joinType=[LeftOuterJoin], where=[=(booking_channel_key, booking_channel_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key, booking_channel_key0, channel], leftInputSpec=[HasUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :        :        :        :- Exchange(distribution=[hash[booking_channel_key]])
   :        :        :        :  +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key])
   :        :        :        :     +- ChangelogNormalize(key=[booking_channel_key, origin_station_key, destination_station_key])
   :        :        :        :        +- Exchange(distribution=[hash[booking_channel_key, origin_station_key, destination_station_key]])
   :        :        :        :           +- TableSourceScan(table=[[default_catalog, default_database, train_activities_1]], fields=[scheduled_departure_time, actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key])
   :        :        :        +- Exchange(distribution=[hash[booking_channel_key]])
   :        :        :           +- Calc(select=[booking_channel_key, channel])
   :        :        :              +- ChangelogNormalize(key=[booking_channel_key])
   :        :        :                 +- Exchange(distribution=[hash[booking_channel_key]])
   :        :        :                    +- TableSourceScan(table=[[default_catalog, default_database, booking_channels, watermark=[-($1, 10000:INTERVAL SECOND)]]], fields=[booking_channel_key, update_time, channel])
   :        :        +- Exchange(distribution=[hash[passenger_key]])
   :        :           +- Calc(select=[passenger_key, first_name, last_name])
   :        :              +- ChangelogNormalize(key=[passenger_key])
   :        :                 +- Exchange(distribution=[hash[passenger_key]])
   :        :                    +- TableSourceScan(table=[[default_catalog, default_database, passengers]], fields=[passenger_key, first_name, last_name, update_time])
   :        +- Exchange(distribution=[hash[station_key]])
   :           +- Calc(select=[station_key, city])
   :              +- ChangelogNormalize(key=[station_key])
   :                 +- Exchange(distribution=[hash[station_key]])
   :                    +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city])
   +- Exchange(distribution=[hash[station_key]])
      +- Calc(select=[station_key, city])
         +- ChangelogNormalize(key=[station_key])
            +- Exchange(distribution=[hash[station_key]])
               +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city])== Optimized Execution Plan ==
Calc(select=[actual_departure_date, first_name, last_name, channel, city AS origin_station, city0 AS destination_station])
+- Join(joinType=[LeftOuterJoin], where=[(destination_station_key = station_key)], select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city, station_key, city0], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :- Exchange(distribution=[hash[destination_station_key]])
   :  +- Calc(select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city])
   :     +- Join(joinType=[LeftOuterJoin], where=[(origin_station_key = station_key)], select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name, station_key, city], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :        :- Exchange(distribution=[hash[origin_station_key]])
   :        :  +- Calc(select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name])
   :        :     +- Join(joinType=[LeftOuterJoin], where=[(passenger_key = passenger_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel, passenger_key0, first_name, last_name], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :        :        :- Exchange(distribution=[hash[passenger_key]])
   :        :        :  +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel])
   :        :        :     +- Join(joinType=[LeftOuterJoin], where=[(booking_channel_key = booking_channel_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key, booking_channel_key0, channel], leftInputSpec=[HasUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :        :        :        :- Exchange(distribution=[hash[booking_channel_key]])
   :        :        :        :  +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key])
   :        :        :        :     +- ChangelogNormalize(key=[booking_channel_key, origin_station_key, destination_station_key])
   :        :        :        :        +- Exchange(distribution=[hash[booking_channel_key, origin_station_key, destination_station_key]])
   :        :        :        :           +- TableSourceScan(table=[[default_catalog, default_database, train_activities_1]], fields=[scheduled_departure_time, actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key])
   :        :        :        +- Exchange(distribution=[hash[booking_channel_key]])
   :        :        :           +- Calc(select=[booking_channel_key, channel])
   :        :        :              +- ChangelogNormalize(key=[booking_channel_key])
   :        :        :                 +- Exchange(distribution=[hash[booking_channel_key]])
   :        :        :                    +- TableSourceScan(table=[[default_catalog, default_database, booking_channels, watermark=[-($1, 10000:INTERVAL SECOND)]]], fields=[booking_channel_key, update_time, channel])
   :        :        +- Exchange(distribution=[hash[passenger_key]])
   :        :           +- Calc(select=[passenger_key, first_name, last_name])
   :        :              +- ChangelogNormalize(key=[passenger_key])
   :        :                 +- Exchange(distribution=[hash[passenger_key]])
   :        :                    +- TableSourceScan(table=[[default_catalog, default_database, passengers]], fields=[passenger_key, first_name, last_name, update_time])
   :        +- Exchange(distribution=[hash[station_key]])(reuse_id=[1])
   :           +- Calc(select=[station_key, city])
   :              +- ChangelogNormalize(key=[station_key])
   :                 +- Exchange(distribution=[hash[station_key]])
   :                    +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city])
   +- Reused(reference_id=[1])
{code}
 

 ",,aitozi,godfreyhe,hackergin,jark,jherico,lam167,libenchao,michael ran,neighborhood,nkruber,okowr,Paul Lin,qinjunjerry,twalthr,xuguangheng,zhihao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 08 02:46:24 UTC 2021,,,,,,,,,,"0|z0pk08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 03:30;zhihao;[~jark] Will it affect the correctness, or just performance? More specifically, in the internal state MapState<row, count> of the above join example, will the count always = 1, or will it increase gradually?
{quote}1) input doesn't have a unique key => MapState<row, count>, 
 where the map key is the input row and the map value is the number of equal rows.
{quote};;;","07/Apr/21 12:23;jark;[~zhihao], it only affects performance. The count will always 1, because when receiving updates from upstream, the updates will send in two records, -U and +U. So the count will be changed to 0 and then 1 after the update. 

Regarding the issue problem, I think the root cause may be there is some bug in {{FlinkRelMdUniqueKeys#getJoinUniqueKeys}} which results in we can't infer correct unique key for the join. 

I think there is no good workaround for now, but you may can try to add a Deduplication [1] partition by the unqiue key after the join. Note the Deduplication node has additional cost, so in general the performance may still not good. 

[1]: https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/#deduplication


 for this ;;;","07/Apr/21 12:49;okowr;Hi Jark, 

Thank you for the valuable information [~jark]. My follow up question is that, althougth the count is 1, it will still load the entire partition of the rocksdb and do seek operation to find the corresponding record for the join, and that's the reason why it affects the performance while not the correctness. Is it correct?;;;","08/Apr/21 06:32;jark;[~okowr], Yes. 

- {{JoinKeyContainsUniqueKey}} uses {{ValueState#value}} to get joined records which is a RocksDB get operation.
- {{HasUniqueKey}} uses {{MapState#get(uniqueKey)}} to get joined records which is also a RocksDB get operation.
- {{NoUniqueKey}} uses {{MapState#entries()}} to get joined records which is a RocksDB seek operation and is very heavy. ;;;","08/Apr/21 07:45;okowr;[~jark], thank you for the rely, it seems it's a sever performance bug for multiple join use cases. Do we have any plan for the fix?;;;","08/Apr/21 07:54;jark;[~okowr], I think we can fix this in the next version, but also welcome if anyone want to contribute a fix. ;;;","14/Apr/21 06:40;xuguangheng;Thanks [~jark] for the previous discussion.

As you mentioned in the discussion that the select statement may contribute to this issue, and we did some experiments as follow:
{code:sql}
CREATE TEMPORARY TABLE passengers (
  passenger_key STRING,
  PRIMARY KEY (passenger_key) NOT ENFORCED
) WITH (
  'connector' = 'values'
)

CREATE TEMPORARY TABLE booking_channels (
  booking_channel_key STRING,
  PRIMARY KEY (booking_channel_key) NOT ENFORCED
) WITH (
  'connector' = 'values'
)

CREATE TEMPORARY TABLE train_activities (
  passenger_key STRING,
  booking_channel_key STRING,
  PRIMARY KEY (booking_channel_key) NOT ENFORCED
) WITH (
  'connector' = 'values'
)

SELECT t.booking_channel_key as booking_channel_key, t.passenger_key as passenger_key, b.booking_channel_key as booking_channel_key_0
FROM train_activities t
LEFT JOIN booking_channels b
ON t.booking_channel_key = b.booking_channel_key
LEFT JOIN passengers p
on t.passenger_key = p.passenger_key

== Optimized Physical Plan ==
Calc(select=[booking_channel_key, passenger_key, booking_channel_key0 AS booking_channel_key_0])
+- Join(joinType=[LeftOuterJoin], where=[=(passenger_key, passenger_key0)], select=[passenger_key, booking_channel_key, booking_channel_key0, passenger_key0], leftInputSpec=[HasUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :- Exchange(distribution=[hash[passenger_key]])
   :  +- Join(joinType=[LeftOuterJoin], where=[=(booking_channel_key, booking_channel_key0)], select=[passenger_key, booking_channel_key, booking_channel_key0], leftInputSpec=[JoinKeyContainsUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :     :- Exchange(distribution=[hash[booking_channel_key]])
   :     :  +- TableSourceScan(table=[[default_catalog, default_database, train_activities]], fields=[passenger_key, booking_channel_key])
   :     +- Exchange(distribution=[hash[booking_channel_key]])
   :        +- TableSourceScan(table=[[default_catalog, default_database, booking_channels]], fields=[booking_channel_key])
   +- Exchange(distribution=[hash[passenger_key]])
      +- TableSourceScan(table=[[default_catalog, default_database, passengers]], fields=[passenger_key])

{code}
VS
{code:sql}
SELECT t.booking_channel_key as booking_channel_key, t.passenger_key as passenger_key
FROM train_activities t
LEFT JOIN booking_channels b
ON t.booking_channel_key = b.booking_channel_key
LEFT JOIN passengers p
on t.passenger_key = p.passenger_key


== Optimized Physical Plan ==
Calc(select=[booking_channel_key, passenger_key])
+- Join(joinType=[LeftOuterJoin], where=[=(passenger_key, passenger_key0)], select=[passenger_key, booking_channel_key, passenger_key0], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :- Exchange(distribution=[hash[passenger_key]])
   :  +- Calc(select=[passenger_key, booking_channel_key])
   :     +- Join(joinType=[LeftOuterJoin], where=[=(booking_channel_key, booking_channel_key0)], select=[passenger_key, booking_channel_key, booking_channel_key0], leftInputSpec=[JoinKeyContainsUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
   :        :- Exchange(distribution=[hash[booking_channel_key]])
   :        :  +- TableSourceScan(table=[[default_catalog, default_database, train_activities]], fields=[passenger_key, booking_channel_key])
   :        +- Exchange(distribution=[hash[booking_channel_key]])
   :           +- TableSourceScan(table=[[default_catalog, default_database, booking_channels]], fields=[booking_channel_key])
   +- Exchange(distribution=[hash[passenger_key]])
      +- TableSourceScan(table=[[default_catalog, default_database, passengers]], fields=[passenger_key])

{code}
 

*_The only difference is that we select booking_channel_key from both left and right table in the first case and only booking_channel_key from left table in the second case. The result is one has uniqueKey and the other doesn't._*

 
 I think *_t.booking_channel_key, t.passenger_key_* can be the unique key of the first join output, but it seems we lost this unique information. After diving deep into the join logic, I found there is something wrong while it was parsing the uniqueKey of the first join output.


 Specifically, in the FlinkRelMdUniqueKeys, there is a function
{code:java}
  private def getJoinUniqueKeys(
      joinInfo: JoinInfo,
      joinRelType: JoinRelType,
      leftType: RelDataType,
      leftUniqueKeys: JSet[ImmutableBitSet],
      rightUniqueKeys: JSet[ImmutableBitSet],
      isLeftUnique: JBoolean,
      isRightUnique: JBoolean,
      mq: RelMetadataQuery): JSet[ImmutableBitSet]
{code}
 

While it was getting the uniqueKey of the first join output, _*isLeftUnique*_ and _*isRightUnique*_ is null, which seems to be a bug. Besides I saw there was a *_TODO_* in FlinkRelMdColumnUniqueness while calling *_areTableColumnsUnique_* for a TableScan. This bug seems to be resolved by implementing the *_areTableColumnsUnique_* method for TableScan.

 

[~jark] Do you think it is a reasonable solution for this issue?

I can contribute my code changes if needed.

 ;;;","14/Apr/21 10:04;jark;From this use case, it seems this is a bug because the {{booking_channel_key}} column from right table should be able to represent unique key of left side. 

But I'm not sure where is the bug. Maybe need further investigation. {{FlinkRelMdColumnUniqueness}} has implemented {{areColumnsUnique}} for {{TableScan}} node. ;;;","15/Apr/21 03:00;xuguangheng;Hello [~jark],

Thanks for you reply. 

Yes, {{FlinkRelMdColumnUniqueness}} has implemented {{areColumnsUnique}} for TableScan node, but there is a {{TODO}} inside the implementation. It seems the implementation is not complete.

Can you assign the issue to me, I can try to fix it.;;;","10/May/21 06:22;xuguangheng;Hello [~jark],

The PR was submitted half month ago,  can you please help to review it?;;;","10/May/21 07:31;jark;cc [~godfreyhe];;;","24/May/21 22:50;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","01/Jun/21 10:49;xuguangheng;Hello [~godfreyhe] and [~jark],

This issue has been labeled as stale-assigned.
Can anyone help to review the PR:P?
;;;","15/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","23/Jun/21 22:38;flink-jira-bot;This issue was marked ""stale-assigned"" 7 days ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","24/Aug/21 08:19;zhihao;Hi [~fhueske@gmail.com]  [~rmetzger]

This RP is opened for 3 months and nobody reviews it. Please let us know what's missing and we can add on.

BTW, we also don't know what's the right channel to get PMC reviewers to review our PR, any suggestions?

Regards

Zhihao;;;","30/Aug/21 14:49;twalthr;[~zhihao] I can help reviewing it shortly. We are resolving a couple of primary key issues right now. Did you see FLINK-23915? Your example in the JIRA description uses TEMPORARY TABLE which could have caused issues as well. But this is an orthogonal issue, right?;;;","01/Sep/21 02:53;zhihao;[~twalthr] Thanks for your time to review it. Yes, it's a general issue unrelated to TEMPORARY keyword.;;;","08/Dec/21 02:46;godfreyhe;Fixed in
1.15.0: ac957ae07a49ab88ad46a14c6ea0d4891d5437d7
1.14.1: 0ec99243b8a5b0e41fa70d97c93476e338adc738;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading exception message if the number of arguments of a nested function is incorrect,FLINK-22109,13369688,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xmarker,TsReaper,TsReaper,06/Apr/21 05:17,28/Aug/21 11:17,13/Jul/23 08:07,29/Apr/21 16:36,1.13.0,,,,,,,,1.11.4,1.12.4,1.13.1,1.14.0,,,,Table SQL / API,,,,,0,pull-request-available,stale-assigned,,,,"Add the following test case to ScalarFunctionsTest to reproduce this issue.

{code:scala}
@Test
def myTest(): Unit = {
  testSqlApi(s""IFNULL(SUBSTR('abc'), 'def')"", ""abc"")
}
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: SQL validation failed. Unexpected error in type inference logic of function 'IFNULL'. This is a bug.

	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:155)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)
	at org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.org$apache$flink$table$planner$expressions$utils$ExpressionTestBase$$addSqlTestExpr(ExpressionTestBase.scala:282)
	at org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.testSqlApi(ExpressionTestBase.scala:188)
	at org.apache.flink.table.planner.expressions.ScalarFunctionsTest.myTest(ScalarFunctionsTest.scala:4067)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: org.apache.flink.table.api.TableException: Unexpected error in type inference logic of function 'IFNULL'. This is a bug.
	at org.apache.flink.table.types.inference.TypeInferenceUtil.createUnexpectedException(TypeInferenceUtil.java:203)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandInference.inferOperandTypes(TypeInferenceOperandInference.java:74)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1907)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:419)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4061)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3347)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:150)
	... 30 more
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 15 to line 1, column 27: Invalid number of arguments to function 'SUBSTR'. Was expecting 2 arguments
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4861)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1804)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:321)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:226)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5710)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5697)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1736)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1727)
	at org.apache.flink.table.planner.functions.inference.CallBindingCallContext$1.get(CallBindingCallContext.java:69)
	at org.apache.flink.table.planner.functions.inference.CallBindingCallContext$1.get(CallBindingCallContext.java:64)
	at java.util.AbstractList$Itr.next(AbstractList.java:358)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.flink.table.types.inference.strategies.CommonArgumentTypeStrategy.inferArgumentType(CommonArgumentTypeStrategy.java:58)
	at org.apache.flink.table.types.inference.strategies.SequenceInputTypeStrategy.inferInputTypes(SequenceInputTypeStrategy.java:76)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandInference.inferOperandTypesOrError(TypeInferenceOperandInference.java:90)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandInference.inferOperandTypes(TypeInferenceOperandInference.java:70)
	... 42 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Invalid number of arguments to function 'SUBSTR'. Was expecting 2 arguments
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
	... 67 more
{code}

The topmost exception message (This is a bug) is really misleading.",,godfreyhe,jark,libenchao,TsReaper,twalthr,xmarker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 16:36:38 UTC 2021,,,,,,,,,,"0|z0pjuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 07:15;xmarker;It seems TypeInferenceOperandInference.inferOperandTypes  thrown a CalciteContextException when call SqlValidatorImpl ,let me fix this bug do you mind ?;;;","08/Apr/21 06:35;jark;Hi [~twalthr], could you help to review the pull request? ;;;","08/Apr/21 07:47;twalthr;[~jark] yes I will;;;","21/Apr/21 22:43;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","29/Apr/21 12:37;jark;It seems the root cause already says ""Invalid number of arguments to function 'SUBSTR'. Was expecting 2 arguments"" which I think is clear enough. What do you think [~TsReaper]?;;;","29/Apr/21 16:36;twalthr;Fixed in 1.14.0: 115101c02a432ce6cfdbd2d0e231503f4a54d5e3
Fixed in 1.13.1: a6a4ea501bfb25667b67f3eacee47140766fbc4f
Fixed in 1.12.4: 563139b64420a987fcdcf766bb7504242b0781b7
Fixed in 1.11.4: c53a7153fae4d0605ce9bdaa9648801af92a9a41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResultType of GeneratedExpression in StringCallGen should be compatible with their definition in FlinkSqlOperatorTable,FLINK-22106,13369265,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,02/Apr/21 08:45,28/May/21 09:03,13/Jul/23 08:07,12/Apr/21 06:48,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"Add the following test case to {{TableEnvironmentITCase}} to reproduce this bug:

{code:scala}
@Test
def myTest(): Unit = {
  tEnv.executeSql(
    """"""
      |CREATE TABLE my_source (
      |  a VARCHAR(10)
      |) WITH (
      |  'connector'='values',
      |  'bounded'='true'
      |)
      |"""""".stripMargin)
  tEnv.explainSql(""SELECT ifnull(substring(a, 2, 5), 'null') FROM my_source"")
}
{code}

The exception stack is
{code}
org.apache.flink.table.planner.codegen.CodeGenException: Mismatch of function's argument data type 'VARCHAR(10)' and actual argument type 'STRING'.

	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$$anonfun$verifyArgumentTypes$1.apply(BridgingFunctionGenUtil.scala:323)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$$anonfun$verifyArgumentTypes$1.apply(BridgingFunctionGenUtil.scala:320)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.verifyArgumentTypes(BridgingFunctionGenUtil.scala:320)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCallWithDataType(BridgingFunctionGenUtil.scala:95)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCall(BridgingFunctionGenUtil.scala:65)
	at org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.generate(BridgingSqlFunctionCallGen.scala:73)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:832)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:529)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:56)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:155)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$2.apply(CalcCodeGenerator.scala:141)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$2.apply(CalcCodeGenerator.scala:141)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:141)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:167)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:50)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:127)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:71)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:70)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:70)
	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:109)
	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:47)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:691)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainSql(TableEnvironmentImpl.java:677)
{code}

This is because the return type inference of {{SUBSTR}} in {{FlinkSqlOperatorTable}} is {{ARG0_VARCHAR_FORCE_NULLABLE}}, but the result type of the GeneratedExpression is {{VARCHAR(INT_MAX)}}.",,libenchao,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 06:48:32 UTC 2021,,,,,,,,,,"0|z0ph8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/21 06:48;lzljs3620320;master (1.13): 73f66d91207b96958cdb596310781e56ffc0515e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SubtaskCheckpointCoordinatorTest.testForceAlignedCheckpointResultingInPriorityEvents unstable,FLINK-22105,13369255,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,rmetzger,rmetzger,02/Apr/21 07:43,06/Jul/21 20:46,13/Jul/23 08:07,06/Jul/21 20:46,1.12.4,1.13.0,,,,,,,1.12.5,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9021&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b

{code}
2021-04-01T19:29:55.2392858Z [ERROR] Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.921 s <<< FAILURE! - in org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest
2021-04-01T19:29:55.2396751Z [ERROR] testForceAlignedCheckpointResultingInPriorityEvents(org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest)  Time elapsed: 0.02 s  <<< ERROR!
2021-04-01T19:29:55.2397415Z java.lang.RuntimeException: unable to send request to worker
2021-04-01T19:29:55.2397956Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:228)
2021-04-01T19:29:55.2398603Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.finishOutput(ChannelStateWriterImpl.java:183)
2021-04-01T19:29:55.2399310Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:298)
2021-04-01T19:29:55.2400104Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.testForceAlignedCheckpointResultingInPriorityEvents(SubtaskCheckpointCoordinatorTest.java:215)
2021-04-01T19:29:55.2400746Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-04-01T19:29:55.2401202Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-04-01T19:29:55.2401746Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-04-01T19:29:55.2402237Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-04-01T19:29:55.2402722Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-04-01T19:29:55.2403270Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-04-01T19:29:55.2403818Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-04-01T19:29:55.2404354Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-04-01T19:29:55.2404854Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-04-01T19:29:55.2405359Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-04-01T19:29:55.2405896Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-04-01T19:29:55.2406393Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-04-01T19:29:55.2406855Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-04-01T19:29:55.2407331Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-04-01T19:29:55.2407806Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-04-01T19:29:55.2408279Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-04-01T19:29:55.2408907Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-04-01T19:29:55.2409403Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-04-01T19:29:55.2409954Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-04-01T19:29:55.2410524Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-04-01T19:29:55.2411318Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-04-01T19:29:55.2411880Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-04-01T19:29:55.2412467Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-04-01T19:29:55.2413006Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-04-01T19:29:55.2413519Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-04-01T19:29:55.2414082Z Caused by: java.lang.IllegalArgumentException: writer not found while processing request: writeOutput 0
2021-04-01T19:29:55.2414726Z 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.onWriterMissing(ChannelStateWriteRequest.java:223)
2021-04-01T19:29:55.2415463Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:80)
2021-04-01T19:29:55.2416331Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:59)
2021-04-01T19:29:55.2417066Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
2021-04-01T19:29:55.2417782Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
2021-04-01T19:29:55.2418329Z 	at java.lang.Thread.run(Thread.java:748)
2021-04-01T19:29:55.2418741Z 	Suppressed: java.lang.IllegalStateException: not running
2021-04-01T19:29:55.2419343Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.ensureRunning(ChannelStateWriteRequestExecutorImpl.java:152)
2021-04-01T19:29:55.2420249Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submitInternal(ChannelStateWriteRequestExecutorImpl.java:144)
2021-04-01T19:29:55.2421010Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submit(ChannelStateWriteRequestExecutorImpl.java:128)
2021-04-01T19:29:55.2421694Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:225)
2021-04-01T19:29:55.2422333Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.finishOutput(ChannelStateWriterImpl.java:183)
2021-04-01T19:29:55.2423029Z 		at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:298)
2021-04-01T19:29:55.2423820Z 		at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.testForceAlignedCheckpointResultingInPriorityEvents(SubtaskCheckpointCoordinatorTest.java:215)
2021-04-01T19:29:55.2425923Z 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-04-01T19:29:55.2426529Z 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-04-01T19:29:55.2427162Z 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-04-01T19:29:55.2427727Z 		at java.lang.reflect.Method.invoke(Method.java:498)
2021-04-01T19:29:55.2428283Z 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-04-01T19:29:55.2429008Z 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-04-01T19:29:55.2429611Z 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-04-01T19:29:55.2430357Z 		at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-04-01T19:29:55.2430921Z 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-04-01T19:29:55.2431809Z 		at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-04-01T19:29:55.2432459Z 		at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-04-01T19:29:55.2433013Z 		at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-04-01T19:29:55.2433513Z 		at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-04-01T19:29:55.2434043Z 		at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-04-01T19:29:55.2434568Z 		at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-04-01T19:29:55.2435093Z 		at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-04-01T19:29:55.2435609Z 		at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-04-01T19:29:55.2436161Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-04-01T19:29:55.2436772Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-04-01T19:29:55.2437399Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-04-01T19:29:55.2438127Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-04-01T19:29:55.2438881Z 		at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-04-01T19:29:55.2439509Z 		at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-04-01T19:29:55.2440088Z 		at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-04-01T19:29:55.2440642Z 		at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-04-01T19:29:55.2440899Z 
{code}",,rmetzger,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 06 20:46:11 UTC 2021,,,,,,,,,,"0|z0ph6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 09:22;roman;The immediate cause is that the test writes channel state but doesn't register a checkpoint with the writer.

The reason why it is unstable is that writer is async and it isn't closed in the test (which would fail).

 
 edit: production code path handles this case correctly (by calling channelStateWriter.start from SubtaskCheckpointCoordinatorImpl.initInputsCheckpoint)

-IIUC, this can also happen in production, for example given DAG:-
{code:java}
src ---pointwise--> op1 ---hash--> op2 ---> sink{code}
 # src will convert the barrier before sending to op1 to force aligned
 # op1 will NOT start writing channel state (because alignmentType != UNALIGNED)
 # op1 will convert the barrier back to unaligned before broadcasting it
 # op1 will call finishOutput as barrier is now UNALIGNED

-The fix is pretty simple though: in (4) check the alignment type of the original barrier (as it was received).-
 

cc: [~AHeise];;;","07/Apr/21 15:54;roman;Merged into master 3cf8597cf3bcccc847519fa12925a2b984e80304
 ;;;","05/Jul/21 02:46;xtsong;[~roman_khachatryan]

By any chance this issue also affects release-1.12?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19856&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=8723;;;","05/Jul/21 02:49;xtsong;I'm reopening the ticket and adding 1.12 as affected version for now. Feel free to change it back if I'm wrong.;;;","06/Jul/21 15:08;roman;[~xintongsong] 
 Yes, it's also present in 1.12 (I think it was ""backported"" by FLINK-22815 after this issue was closed).
 I'll open a backport PR.;;;","06/Jul/21 20:46;roman;Merged into 1.12 as ae24c979a2a5754de41dba9f0b336d14a181a03c..4ef1427b1f0b5882ade6f3fc776763ab6ade1674.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlClientTest.testExecuteSqlFile fail,FLINK-22104,13369246,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,maguowei,maguowei,02/Apr/21 06:22,28/May/21 09:10,13/Jul/23 08:07,19/Apr/21 03:06,1.13.0,,,,,,,,1.13.0,1.14.0,,,,,,Table SQL / Client,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15996&view=logs&j=0e05564a-179a-5e65-3527-d468802ba62b&t=0f3e9f39-333c-5f88-6bce-3254b2cb21e7&l=8654

{code:java}

java.lang.AssertionError: 

Expected: a string containing ""LOAD MODULE\t\tLoad a module. Syntax: 'LOAD MODULE <name> [WITH ('<key1>' = '<value1>' [, '<key2>' = '<value2>', ...])];'""
     but: was ""No default environment specified.
Searching for '/tmp/junit2344703144390059316/conf/sql-client-defaults.yaml'...not found.
{code}

",,fsk119,jark,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22080,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 19 03:06:58 UTC 2021,,,,,,,,,,"0|z0ph4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/21 03:51;fsk119;The actual line is
{code:java}
LOAD MODULE   Load a module. Syntax: 'LOAD MODULE <name> [WITH ('<key1>' =Q '<value1>' [, '<key2>' = '<value2>', ...])];'
{code}
But the expected line is
{code:java}
LOAD MODULE   Load a module. Syntax: 'LOAD MODULE <name> [WITH ('<key1>' = '<value1>' [, '<key2>' = '<value2>', ...])];'
{code}
The main problem is the we hijack the {{System.in}} and the client echo the input to the output. 

However, we don't need {{QUIT}} in this case. ;;;","19/Apr/21 03:06;jark;Fixed in 
 - master: 6dd276e0dc1e5ac008d9ad818331143b86601d79
 - release-1.13: a329d81c837eef8c7af69adec0031dfb90639482;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testNumberOfBuiltinFunctions fail,FLINK-22103,13369245,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,maguowei,maguowei,02/Apr/21 06:00,28/May/21 08:58,13/Jul/23 08:07,06/Apr/21 05:50,1.13.0,,,,,,,,1.13.0,,,,,,,Connectors / Hive,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15996&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab&l=24281

{code:java}
[ERROR] Tests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.867 s <<< FAILURE! - in org.apache.flink.table.module.hive.HiveModuleTest
[ERROR] testNumberOfBuiltinFunctions(org.apache.flink.table.module.hive.HiveModuleTest)  Time elapsed: 0.006 s  <<< FAILURE!
java.lang.AssertionError: expected:<222> but was:<220>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.table.module.hive.HiveModuleTest.verifyNumBuiltInFunctions(HiveModuleTest.java:89)
	at org.apache.flink.table.module.hive.HiveModuleTest.testNumberOfBuiltinFunctions(HiveModuleTest.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)

{code}
",,jark,lirui,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 06 05:50:23 UTC 2021,,,,,,,,,,"0|z0ph4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 06:01;maguowei;cc [~lirui];;;","02/Apr/21 06:14;lirui;Sorry to forget to update the test for hive-1.2.1. I'll submit a PR to fix this shortly.;;;","06/Apr/21 05:50;jark;Fixed in master: 7a4b67e65545507d6cc2531965617a5a05694138;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlinkBatchUserDefinedTableFunctionTests fail,FLINK-22101,13369242,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,maguowei,maguowei,02/Apr/21 05:41,02/Apr/21 10:56,13/Jul/23 08:07,02/Apr/21 10:56,1.13.0,,,,,,,,,,,,,,,API / Python,,,,,0,test-stability,,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15996&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22771,,dian.fu,hxbks2ks,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22076,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 02 10:56:53 UTC 2021,,,,,,,,,,"0|z0ph3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 05:42;maguowei;cc [~dianfu];;;","02/Apr/21 05:58;dian.fu;[~maguowei] Thanks for reporting this issue.

It may be caused by the similar issue with FLINK-22076 although the exception stack is not exactly the same. Will try to address it ASAP.;;;","02/Apr/21 10:56;hxbks2ks;This issue should have been addressed in [FLINK-22076|https://issues.apache.org/jira/browse/FLINK-22076]. Feel free to reopen it if it still happens.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testChangelogResultViewClearEmptyResult fail.,FLINK-22097,13369231,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,maguowei,maguowei,02/Apr/21 03:05,28/May/21 09:01,13/Jul/23 08:07,08/Apr/21 14:42,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15968&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=8539


{code:java}
Exception in thread ""Thread-9"" java.lang.NullPointerException
	at org.apache.flink.table.client.cli.CliClient.isPlainTerminal(CliClient.java:181)
	at org.apache.flink.table.client.cli.CliClient.clearTerminal(CliClient.java:169)
	at org.apache.flink.table.client.cli.CliView.display(CliView.java:191)
	at org.apache.flink.table.client.cli.CliChangelogResultView.display(CliChangelogResultView.java:101)
	at org.apache.flink.table.client.cli.CliResultView$RefreshThread.run(CliResultView.java:267)

{code}

{code:java}
java.lang.AssertionError: Invalid number of cancellations.
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.flink.table.client.cli.CliResultViewTest.testResultViewClearResult(CliResultViewTest.java:117)
	at org.apache.flink.table.client.cli.CliResultViewTest.testChangelogResultViewClearEmptyResult(CliResultViewTest.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}
",,fsk119,jark,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 14:42:44 UTC 2021,,,,,,,,,,"0|z0ph1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 04:03;ykt836;cc [~fsk119];;;","08/Apr/21 02:45;fsk119;The failed test is mainly to test the view thread has the ability to exit gracefully when users(main thread) interrupt the view thread.

We have 3 threads in the test: main thread, view thread and refresh thread.
 - The main thread has the resource {{Terminal}} and uses the the resource to create the view thread. When the view thread starts, it set the interrupt flag on the view thread and *close* the {{Terminal}} until view thread exits
 - When view thread start, it start the refresh thread and monitors the user input. In the test, it only monitors interrput signal. When get the signal, it mark the flag {{isRunning}} of the refresh thread {{false}} and notify the refresh thread.
 - The refresh thread is used to fetch the data from the remote periodically and display the results on {{Terminal}}. When exits, the refresh thread will invoke the \{{Executor.cancelQuery}} and count down the {{cancellation}}.

The reason why we can NPE is because when view thread exits, it only notifies the refresh thread instead of waiting for the refresh thread exits. It's possible
 # the view thread notifies the refresh thread
 # the refresh thread wakes up and prepares to display the result on the terminal
 # the view thread exits
 # the main thread finds the view thread exit and closes the resource
 # the refresh thread gets NPE
 # the main thread find the {{cancellation}} is not as same as the expected

Therefore, we should request the view thread wait for the refresh thread exit.

To reproduce the bug, we can request the refresh thread sleep 1 seconds before display. 

 ;;;","08/Apr/21 14:42;jark;Fixed in master: f542453759e4ddc85e23bc117c4d915edc8c169f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ServerTransportErrorHandlingTest.testRemoteClose fail ,FLINK-22096,13369219,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,maguowei,maguowei,02/Apr/21 00:38,15/Dec/21 01:44,13/Jul/23 08:07,14/Dec/21 06:56,1.13.0,1.14.0,1.15.0,,,,,,1.13.6,1.14.3,1.15.0,,,,,Runtime / Network,,,,,0,pull-request-available,stale-assigned,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15966&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=6580

{code:java}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.415 s <<< FAILURE! - in org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest
[ERROR] testRemoteClose(org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest)  Time elapsed: 1.338 s  <<< ERROR!
org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use

{code}
",,kevin.cyj,kezhuw,maguowei,pnowojski,tanyuxin,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 14 06:56:36 UTC 2021,,,,,,,,,,"0|z0pgyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/21 22:55;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","27/May/21 23:04;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Oct/21 06:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24829&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7464;;;","09/Oct/21 02:36;tanyuxin;Hi, I want to take a look at this test issue. Could someone help assign it to me? :D;;;","09/Oct/21 02:55;maguowei;Thank [~tanyuxin] for your attention. I assign this jira to you, please feel free to contact me if you have any questions;;;","21/Oct/21 05:32;tanyuxin;In the test case, a `{{BindException`}} may be thrown when initializing Netty server. To solve this problem, a retry is added when calling `{{initServerAndClient`}} method. And I submitted a PR on my point of view. 
What do you think about the case? [~maguowei] [~kevin.cyj]. Please correct me at any time if I missed anything, thanks.;;;","05/Dec/21 10:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","13/Dec/21 09:24;tanyuxin;Added 2 PRs for the branches of release-1.13 and release-1.14.;;;","14/Dec/21 06:56;kevin.cyj;Fixed via:

481f30539c79e212f8974a07ff261c389bfbdbe2 on master

256153e0f6407e9d2414fc171fbf3d97a5818995 on 1.14

f394abddc9f4cff45c6669ef940760f7e583b257 on 1.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating python release binaries does not work,FLINK-22095,13369196,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,dwysakowicz,dwysakowicz,01/Apr/21 19:40,28/Aug/21 11:14,13/Jul/23 08:07,02/Apr/21 08:04,1.13.0,,,,,,,,1.13.0,,,,,,,Release System,,,,,0,pull-request-available,,,,,"The {{create_binary_release.sh}} fails for creating python binary release:

{code}
[7/7] Cythonizing pyflink/fn_execution/table/window_aggregate_fast.pyx
/home/dwysakowicz/projects/flink/flink-python/dev/.conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/dwysakowicz/projects/flink/flink-python/pyflink/fn_execution/table/window_aggregate_fast.pxd
  tree = Parsing.p_module(s, pxd, full_module_name)
find: 'deps/tmp': No such file or directory
Traceback (most recent call last):
  File ""setup.py"", line 264, in <module>
    subprocess.check_output([collect_licenses_file_sh, TEMP_PATH, TEMP_PATH])
  File ""/home/dwysakowicz/projects/flink/flink-python/dev/.conda/lib/python3.7/subprocess.py"", line 395, in check_output
    **kwargs).stdout
  File ""/home/dwysakowicz/projects/flink/flink-python/dev/.conda/lib/python3.7/subprocess.py"", line 487, in run
    output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '['/home/dwysakowicz/projects/flink/tools/releasing/collect_license_files.sh', 'deps', 'deps']' returned non-zero exit status 1.
{code}

One problem that I was able to fix myself is a wrong path in {{apache-flink-libraries/setup.py}}:

{code}
            collect_licenses_file_sh = os.path.abspath(os.path.join(
                this_directory, "".."", ""tools"", ""releasing"", ""collect_license_files.sh""))
{code}

should be:

{code}
            collect_licenses_file_sh = os.path.abspath(os.path.join(
                this_directory, "".."", "".."", ""tools"", ""releasing"", ""collect_license_files.sh""))
{code}",,dian.fu,dwysakowicz,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 08:30:46 UTC 2021,,,,,,,,,,"0|z0pgtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 19:41;dwysakowicz;cc [~dianfu];;;","02/Apr/21 01:36;dian.fu;cc [~hxbks2ks], it seems relates to the recent PyFlink package split. Could you help to take a look?;;;","02/Apr/21 08:04;hxbks2ks;Merged into master via 69d9bce0526b82a1bd8890d10802a4c25b6ef2fa;;;","07/Apr/21 08:30;dwysakowicz;Hey [~hxbks2ks] just letting you know that I created the 1.13 branch accidentally when creating the rc0. It is not a proper 1.13 branch yet. There is no need to merge it there yet. We will announce the actual cut off in the ML. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update release scripts after documentation switch to Hugo,FLINK-22094,13369186,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,01/Apr/21 18:20,28/May/21 08:59,13/Jul/23 08:07,07/Apr/21 11:02,1.13.0,,,,,,,,1.13.0,,,,,,,Release System,,,,,0,pull-request-available,,,,,The release scripts has not been upgraded after migration to Hugo.,,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 11:02:22 UTC 2021,,,,,,,,,,"0|z0pgrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 11:02;dwysakowicz;Fixed in a43df52f8367c47de2f68f43dcbdd5e8699f47b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unstable test ""ThreadInfoSampleServiceTest.testShouldThrowExceptionIfTaskIsNotRunningBeforeSampling""",FLINK-22093,13369136,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,sewen,sewen,01/Apr/21 14:48,01/Apr/21 15:09,13/Jul/23 08:07,01/Apr/21 14:59,,,,,,,,,1.13.0,,,,,,,Tests,,,,,0,test-stability,,,,,"The test {{ThreadInfoSampleServiceTest.testShouldThrowExceptionIfTaskIsNotRunningBeforeSampling()}} failed in all profiles in my latest CI build (even though it passes locally).

  - https://dev.azure.com/sewen0794/Flink/_build/results?buildId=250&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b&l=8102

  - https://dev.azure.com/sewen0794/Flink/_build/results?buildId=250&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a&l=6698

  - https://dev.azure.com/sewen0794/Flink/_build/results?buildId=250&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 14:59:06 UTC 2021,,,,,,,,,,"0|z0pgg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 14:59;chesnay;master: 0a7255262184bc4ebc8f1cfa7d6ebda6a7d83260;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveConf can cache hiveSiteURL from classpath and cause FileNotFoundException,FLINK-22092,13369120,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,01/Apr/21 13:37,28/Aug/21 11:17,13/Jul/23 08:07,27/Apr/21 05:28,,,,,,,,,1.13.0,,,,,,,Connectors / Hive,,,,,0,pull-request-available,stale-assigned,,,,"It turns out that FLINK-19702 is incomplete and {{HiveConf}} may still automatically load hive-site from classpath and set {{hiveSiteURL}} variable. This can cause problems, e.g. create a HiveCatalog reading hive-site from classpath, drop this catalog and also remove the hive-site file, create another HiveCatalog with hive-conf-dir pointing to another location, the 2nd HiveCatalog cannot be created because {{HiveConf}} has remembered the hive-site location from the previous one and complains the file can no longer be found.",,hackergin,libenchao,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 16 10:42:44 UTC 2021,,,,,,,,,,"0|z0pgco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 12:36;lirui;Put more thoughts into this and I guess it's not a good idea to stop reading hive-site from classpath because that may break existing use cases. Instead, we can just prevent HiveConf from reading hive-site from the {{hiveSiteURL}} variable.;;;","16/Apr/21 10:42;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceLegacyITCase hangs/fails on azure,FLINK-22085,13369044,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,dwysakowicz,dwysakowicz,01/Apr/21 07:44,23/Sep/21 17:27,13/Jul/23 08:07,27/Aug/21 03:02,1.13.0,1.14.0,,,,,,,1.14.0,,,,,,,Connectors / Kafka,,,,,0,pull-request-available,stale-assigned,test-stability,,,"1) Observations

a) The Azure pipeline would occasionally hang without printing any test error information.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15939&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=8219]

b) By running the test KafkaSourceLegacyITCase::testBrokerFailure() with INFO level logging, the the test would hang with the following error message printed repeatedly:
{code:java}
20451 [New I/O boss #50] ERROR org.apache.flink.networking.NetworkFailureHandler [] - Closing communication channel because of an exception
java.net.ConnectException: Connection refused: localhost/127.0.0.1:50073
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_151]
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_151]
        at org.apache.flink.shaded.testutils.org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152) ~[flink-test-utils_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.shaded.testutils.org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105) [flink-test-utils_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.shaded.testutils.org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79) [flink-test-utils_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.shaded.testutils.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) [flink-test-utils_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.shaded.testutils.org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42) [flink-test-utils_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.shaded.testutils.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [flink-test-utils_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.shaded.testutils.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [flink-test-utils_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_151]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_151]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]
{code}
*2) Root cause explanations*

The test would hang because it enters the following loop:
 - closeOnFlush() is called for a given channel
 - closeOnFlush() calls channel.write(..)
 - channel.write() triggers the exceptionCaught(...) callback
 - closeOnFlush() is called for the same channel again.

*3) Solution*

Update closeOnFlush() so that, if a channel is being closed by this method, then closeOnFlush() would not try to write to this channel if it is called on this channel again.",,dwysakowicz,gaoyunhaii,lindong,maguowei,mapohl,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23223,,,,,,,,,,,,,,,,,,,,,FLINK-22520,FLINK-22416,FLINK-22457,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 27 03:02:28 UTC 2021,,,,,,,,,,"0|z0pfvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 09:49;dwysakowicz;Also a failure: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15944&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=9071;;;","01/Apr/21 09:53;dwysakowicz;[~jqin] Could you take a look?;;;","02/Apr/21 05:50;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15996&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=10356

A failure because of creating topic timeout.;;;","02/Apr/21 06:41;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16005&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7928;;;","09/Apr/21 06:38;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16232&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7410;;;","09/Apr/21 06:39;dwysakowicz;[~lindong] Do you mind taking a look?;;;","09/Apr/21 09:14;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16254&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6919

{code}
[ERROR] testAutoOffsetRetrievalAndCommitToKafka(org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase)  Time elapsed: 30.446 s  <<< FAILURE!
java.lang.AssertionError: expected:<50> but was:<null>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runAutoOffsetRetrievalAndCommitToKafka(KafkaConsumerTestBase.java:386)
	at org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase.testAutoOffsetRetrievalAndCommitToKafka(KafkaSourceLegacyITCase.java:155)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{code};;;","09/Apr/21 15:15;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16281&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6539;;;","11/Apr/21 05:48;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16311&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6862;;;","12/Apr/21 01:31;lindong;[~dwysakowicz] Sure, I will take a look.;;;","12/Apr/21 06:00;dwysakowicz;Thx [~lindong]  I will assign this to you.;;;","14/Apr/21 06:37;dwysakowicz;Any progress [~lindong] ?;;;","14/Apr/21 07:15;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16470&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","14/Apr/21 14:18;lindong;[~dwysakowicz] Sorry, I was busy with other work and didn't realize this is a blocker for 1.13 release until today.

This looks like a flaky test which didn't have any useful log information in the Azure pipeline log. I was not able to reproduce this on my Macbook yet.

Since this is a blocker for 1.13 release and we want to address this asap, I opened [https://github.com/apache/flink/pull/15617] to change the test log level to INFO. Could you help approve this PR?

In the mean time, I will take a deeper look in the test source code and run more tests locally to hopefully reproduce it with INFO level logging.;;;","14/Apr/21 14:38;dwysakowicz;I am afraid we should not increase the log level. Could you verify that in your personal azure account? https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository

I marked it as a blocker because I've seen quite regularly. My intention is to mainly verify if the problem is in the production code or is a flakiness of the test.;;;","14/Apr/21 15:37;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16524&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6862;;;","14/Apr/21 15:51;lindong;Sounds good. I will use my personal azure account.

I am able to reproduce it on my Macbook with INFO level logging. The following stacktrace is repeated printed in the log. I am looking into this.

{code:java}
348201 [Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Failed to trigger checkpoint for job a9516740f56272d7a3730023fec0bead.)
org.apache.flink.runtime.checkpoint.CheckpointException: some tasks of job a9516740f56272d7a3730023fec0bead has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
        at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:101) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) ~[?:1.8.0_151]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
{code}

 
 ;;;","14/Apr/21 20:31;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16544&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6539;;;","15/Apr/21 07:13;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16574&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6862;;;","15/Apr/21 14:35;lindong;[~dwysakowicz] I have updated this JIRA with the root cause explanation. Could you help review https://github.com/apache/flink/pull/15633?;;;","15/Apr/21 19:33;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16590&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6539;;;","16/Apr/21 10:42;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16590&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6539;;;","19/Apr/21 09:02;dwysakowicz;Fixed in:
* master
** 98424e6383bcce107844cbeecc2e9df4ffb4272a
* 1.13
** 766f7175d62f5ef572027331e3fceb31b1b4cc4b;;;","20/Apr/21 08:49;mapohl;[~lindong] {{KafkaSourceLegacyITCase}} timed out in [that build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16789&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6593]. The build itself included the fix [98424e6|https://github.com/apache/flink/commit/98424e6383bcce107844cbeecc2e9df4ffb4272a] provided by this issue. May you have another look at it to double-check whether it's related or a completely different issue/;;;","20/Apr/21 09:29;lindong;Thanks for the information [~mapohl]. Previously I have verified that I can successfully run KafkaSourceLegacyITCase::testBrokerFailure 100 times without issue.

Let me try to reproduce the issue by running KafkaSourceLegacyITCase.;;;","20/Apr/21 15:09;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16858&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","21/Apr/21 01:59;guoyangze;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16849&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6916;;;","21/Apr/21 07:46;dwysakowicz;Any luck reproducing the issue [~lindong]?;;;","21/Apr/21 15:56;lindong;[~dwysakowicz] Yeah I am able to reproduce the test hang once. I found the reason why the test hangs (after a test failure) and uploaded [https://github.com/apache/flink/pull/15713] to fix this this.

I am still trying to debug the test failure.;;;","22/Apr/21 04:26;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16996&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6883;;;","22/Apr/21 09:46;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16996&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=41635;;;","22/Apr/21 09:49;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16996&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b;;;","22/Apr/21 09:52;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17001&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=42003;;;","22/Apr/21 09:53;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17000&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=41341;;;","22/Apr/21 16:47;dwysakowicz;Merge another fix in:
* 1.14 (master)
** 58a7c80fa35424608ad44d1d6691d1407be0092a
* 1.13
** 28eea07cea69f1d7bf0d2062e65e08f7671b8ebc;;;","23/Apr/21 14:29;lindong;[~dwysakowicz] FYI, after applying https://github.com/apache/flink/pull/15713, I am able to run KafkaSourceLegacyITCase 100 times without any failure. 

The bug fixed by https://github.com/apache/flink/pull/15713 would cause test hang only if there is a Flink job that keeps running. After reading through related code, I still could not find a full explanation of why there is such a Flink job in the first place.

I will stop investigating this bug for now. Let's see if there is still test failure and if we can find more useful information from the Azure pipeline log.


;;;","23/Apr/21 14:31;dwysakowicz;Thanks a lot [~lindong]. Really good job on stabilising the test! Your plan sounds good to me to wait and see if it still occurs.;;;","26/Apr/21 14:26;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17206&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6612;;;","26/Apr/21 14:27;dwysakowicz;I will remove timeouts on master to possibly get thread dumps.;;;","26/Apr/21 14:31;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17212&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7062;;;","27/Apr/21 05:52;lindong;Thank you [~dwysakowicz] for the information.

For the first test failure [1], it could be because the Azure pipeline is very slow and the it takes more than 60 seconds (due to long GC) to complete that test. Maybe we can see if increasing the timeout to 120 seconds could reduce the failure rate of this test.

For the second test failure [2], it appears that the test failed due to ""OperatorEvent from an OperatorCoordinator to a task was lost"". This is relate to https://github.com/apache/flink/pull/15605 which was committed recently. 

Given that KafkaSourceLegacyITCase no longer hangs and the comment history in this JIRA is already very long, I opened https://issues.apache.org/jira/browse/FLINK-22488 to track the issue of ""OperatorEvent from an OperatorCoordinator to a task was lost"". Maybe we can close this JIRA and continue the discussion in FLINK-22488.

[1] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17206&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6612
[2] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17212&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7062;;;","27/Apr/21 07:38;dwysakowicz;One observation:
* some of the sub-tasks finish right away after starting, which makes it impossible to take any checkpoints/savepoints
{code}
[Source: KafkaSource -> Map -> Map (3/8)#1] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
22:02:57,135 [Source: KafkaSource -> Map -> Map (5/8)#1] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Reader received NoMoreSplits event.
22:02:57,136 [Source: KafkaSource -> Map -> Map (4/8)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource -> Map -> Map (4/8)#1 (334496a6fd2c371dd842e7ecb61811d9) switched from RUNNING to FINISHED.


22:05:27,161 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  6: count=1000, totalCount=1000
22:05:27,162 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  1: count=1000, totalCount=1000
22:05:27,163 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  7: count=1000, totalCount=1000
22:05:27,163 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  0: count=1000, totalCount=1000
22:05:27,163 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  5: count=1000, totalCount=1000
22:05:27,628 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:28,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:28,628 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:29,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:29,627 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:30,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:30,628 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:31,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:31,628 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:32,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
{code};;;","27/Apr/21 09:21;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17243&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=41700;;;","27/Apr/21 09:55;lindong;[~dwysakowicz] I see. Previously the test would be fail with org.junit.runners.model.TestTimedOutException if it does not end within 60 seconds. I suppose the test is hanging now because we have removed timeouts.

Now that we have verifies that the test would indeed hang if we do not timeout tests, I think we can add back the timeout now to avoid test hang.

Regarding why that test could not end by itself: I have seen and investigated this before. I didn't find the root cause as explained in the earlier comment [1]. Take testMultipleSourcesOnePartition as example, this test might not end by itself if the ValidatingExactlyOnceSink in the Pipeline could not receive the expected number of messages, which means that some message is lost in the pipeline.

Previously I have checked the FlinkKafkaConsumer code but could not find any hypothesis of why message could be lost without triggering an exception in this class. But FLINK-21996 gives me new hint as it suggests that messages could be lost in the pipeline when any AddSplitEvent is lost. I currently don't have enough knowledge to prove or disprove this hypothesis. Will need more time to read the code. 

It will be great if other Flink developers could help provide more ideas or debug how a message could be lost in a Flink job.

[1] https://issues.apache.org/jira/browse/FLINK-22085?focusedCommentId=17330815&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17330815




;;;","27/Apr/21 10:25;dwysakowicz;{quote}
Now that we have verifies that the test would indeed hang if we do not timeout tests, I think we can add back the timeout now to avoid test hang.
{quote}

It's better not to add the timeouts because now we have thread dumps and we know the test actually hangs busy looping. With a timeout it could've hanged in a random spot without an informative traces. Therefore we should not add timeouts.;;;","27/Apr/21 12:04;lindong;[~dwysakowicz] Hmm... why would the test hang in a random spot if we add timeouts? Regarding those bugs which we are currently investigating, if we add back the timeout, then the test will just fail fast with TestTimedOutException. Not sure if I missed something here.

Note that if we don't add back the timeout, then those bugs will waste Azure pipeline resource. And it also makes developer wait longer for the test result. I personally believe new thread dumps will give us the same information as the thread dump we already get. Just my 2 cents.;;;","27/Apr/21 12:07;dwysakowicz;If you have concerns regarding timeouts, please join the discussion at: https://lists.apache.org/thread.html/r0710f90f30e230d3f11b459a6edc5afd5b2e990ab9533b1442adfaf5%40%3Cdev.flink.apache.org%3E;;;","17/May/21 06:44;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17993&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6634;;;","18/May/21 07:45;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18055&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","18/May/21 07:47;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18052&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","04/Jun/21 01:49;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18651&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6378;;;","10/Jun/21 02:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18851&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6366;;;","10/Jun/21 02:53;xtsong;[~lindong], any updates on this one?;;;","15/Jun/21 00:35;lindong;[~xintongsong] No... after spending 2+ hours over the weekend to look into this, I don't think I will be able to find the answer to this bug any time soon.

Previously I spent considerable (from 4/11 to 4/26) to investigate this bug and found 2+ issues. I couldn't not find any more Kafka-related bugs by reading the code and the thread dump. It is not clear from the thread dump where the deadlock comes from.  In order for me to further figure this out, I would need to have deeper understanding of Flink runtime works by reading the source Flink code, which I have not had time to do since I joined the current team.

If anyone could help read the threadump from the log and give some idea regarding why the test deadlocks, I would be happy to continue the investigation.

;;;","15/Jun/21 02:29;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18957&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6632;;;","15/Jun/21 02:32;xtsong;[~lindong], thanks for the updates. I'll try to find another pair of eyes to take a look.

Do you mind if I unassign you for the moment? That helps remind us on the kanban board that this ticket is not being taken care of.;;;","15/Jun/21 02:46;lindong;[~xintongsong] Sure, please feel free to un-assign me. Thanks!;;;","15/Jun/21 06:47;xtsong;[~gaoyunhaii], could you help take a look at the stacks from the runtime angles?;;;","15/Jun/21 06:52;gaoyunhaii;[~xintongsong] OK, I'll have a look~;;;","16/Jun/21 02:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19003&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6601;;;","16/Jun/21 02:15;xtsong;Thanks a log, [~gaoyunhaii]. I've assigned you to the ticket.;;;","17/Jun/21 13:14;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19048&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","18/Jun/21 01:34;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19069&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6632;;;","21/Jun/21 02:07;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19124&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6632;;;","21/Jun/21 02:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19152&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6601;;;","21/Jun/21 02:48;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19175&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6687;;;","21/Jun/21 03:06;gaoyunhaii;I checked the last several cases, it seems they are all hang at testMultipleSourcesOnePartition(org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase) with the last log shows
{code:java}
01:34:47,793 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job d9229593ea3e8c25751e0b146ba62ee1 since some tasks of job d9229593ea3e8c25751e0b146ba62ee1 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
{code}
I'll first focus on this issue.;;;","21/Jun/21 03:32;lindong;[~gaoyunhaii] Thanks for the investigation. Previously I have looked into this particular INFO message. From what I know, it appears to be expected by this test.

What happens is that some subtask (e.g. 7 out of 8 subtasks) have finished correctly when the checkpoint is triggered. And Flink would not take the checkpoint if any subtask has finished.

The issue here is that a subtask never finishes. According to the logic of this particular test, this could be because the sink of this subtask never received the expected number of messages, which I could not find any explanation so far.
;;;","21/Jun/21 12:17;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19202&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","22/Jun/21 06:11;gaoyunhaii;Hi [~lindong] thanks for the explanation and it indeed helps a lot. 

First sorry that I still have not found the root cause, I further checked the case, and the rough process is
  
1. A generate job starts up and generates 5 partitions, each with 1000 records.
2. A consumer job starts up, it has two tasks, the first is kafka source -> failing identity map -> validate map with parallelism 8, and the second  task is the validating exactly-once sink, with parallelism 1. 
3. Since there are only 5 partitions, 3 of the source tasks would finished immediately after startup.
4. One of the failing identity map would trigger failover in the middle. 
5. After restarts, the job would re-run. Note that no checkpoint taken in the first run due to finished tasks, the job would run from scratch after failover. And similar to the first run 3 of the 8 source tasks would finish immediately.
6. The original design is that the sink task would check if 5000 records are received, if so, it would throw a SuccessException() to terminate the job.

The main problem here is that the debug thread prints that each of the 5 alive failing identity map have processed 1000 records, but the sink still do not throw the exception. I could not see other possible execution path for now. To make it clear how many records the sink has received, I first open [a PR to also add debug thread to print the sink status | https://github.com/apache/flink/pull/16233]. Could some one have a look at this PR first? Very thanks~;;;","23/Jun/21 06:54;xtsong;Logs added in db73b33538ab8b2d9edc91e6fe9795b48e3a2e4d;;;","29/Jun/21 02:11;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19643&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","29/Jun/21 02:13;xtsong;[~gaoyunhaii], 
New instance found. Please take another look.;;;","30/Jun/21 05:59;gaoyunhaii;Thanks [~xintongsong] for the review and monitoring the tests~ Afterward I tried to repeat this test on Azure: [https://github.com/apache/flink/pull/16317], and it seems that the issue could reproduce in Azure for some probability. With all these instances I found that:
 # The failing identity map indeed process required (namely 1000) records after failover.
 # The sink might lost all the records from some map tasks, but the number of maps get lost is random. And I have not met with the case that a part of records from a single task get lost.

As a whole, I think it seems the case is not related to Kafka, but might have some relationship with local network stack. One possible related point is that the test has set buffer timeout to 0. I'll try to further analyze the causes. ;;;","01/Jul/21 06:26;gaoyunhaii;Hi all, I should have found the reason: for flush timeout = 0, the flush happens only after the records emitted and there is no standalone flush thread, then after failover:
 # The (source -> map -> map) task (parallel = 8) started, it first try to restore the partition state, after this it broadcast EndOfChannelStateEvent, this would block the channel. 
 # Then for the following records emitted by the (source -> map -> map) tasks, it would not notifyDataAvailable since isBlocked = true.
 # After the (sink) task (parallel = 1) received all the EndOfChannelStateEvent, it would resume all the subpartitions. After this it would check if the subpartition is available, if so, it would queue the corresponding local input channel
 # However, if before 3, the (source -> map -> map) task has emitted all the 1000 records, then these record would not be notified during resuming since the subpartition has isBlock = false, but when it check the availability of the subpartition, it would return isAvailable() = false since flush requested = false. Then the data won't be notified in the future

The bug could be reproduced locally by add sleep in the UpstreamRecoveryTrackImpl#handleEndOfRecovery to delay the step 3. ;;;","02/Jul/21 01:21;lindong;Thanks you [~gaoyunhaii] for the explanation!

Do you plan to submit a PR to fix this? I will help take a deeper look into this explanation and the PR.;;;","02/Jul/21 11:34;gaoyunhaii;No problem, I'll create a new issue and link it to this issue. I'll also think a bit on the possible fix options~;;;","02/Jul/21 15:42;arvid;I think having `notifyDataAvailable` at the end of recovery certainly makes sense. Extra `notifyDataAvailable` are rather cheap when not on the hot-path.;;;","04/Jul/21 07:16;gaoyunhaii;Very thanks [~arvid], I also agree with we could have one more notifyDataAvailable at the end of recovery and other resume cases. I'll have a look at the details~;;;","05/Jul/21 03:53;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19865&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19;;;","05/Jul/21 07:48;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19884&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=41430;;;","09/Jul/21 03:29;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6529;;;","08/Aug/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","27/Aug/21 03:02;gaoyunhaii;Since we have fix this case via https://issues.apache.org/jira/browse/FLINK-23223 and it has not pop up for a long time, I'll close the issue for now.;;;"
RescalingITCase fails with adaptive scheduler,FLINK-22084,13369040,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,austince,dwysakowicz,dwysakowicz,01/Apr/21 07:26,22/Jun/21 13:55,13/Jul/23 08:07,08/Apr/21 22:18,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15934&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4472

{code}
2021-03-31T22:16:07.8416407Z [ERROR] testSavepointRescalingOutKeyedStateDerivedMaxParallelism[backend = rocksdb](org.apache.flink.test.checkpointing.RescalingITCase)  Time elapsed: 9.945 s  <<< ERROR!
2021-03-31T22:16:07.8417534Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-03-31T22:16:07.8418516Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-03-31T22:16:07.8419281Z 	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:63)
2021-03-31T22:16:07.8420142Z 	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingKeyedState(RescalingITCase.java:251)
2021-03-31T22:16:07.8421173Z 	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingOutKeyedStateDerivedMaxParallelism(RescalingITCase.java:168)
2021-03-31T22:16:07.8421985Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-31T22:16:07.8422651Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-31T22:16:07.8423649Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-31T22:16:07.8424231Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-31T22:16:07.8424657Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-03-31T22:16:07.8425147Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-03-31T22:16:07.8425609Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-03-31T22:16:07.8426183Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-03-31T22:16:07.8569060Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-03-31T22:16:07.8569781Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-03-31T22:16:07.8570451Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-03-31T22:16:07.8571040Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-31T22:16:07.8571604Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-03-31T22:16:07.8572303Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-03-31T22:16:07.8573259Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-03-31T22:16:07.8573975Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-31T22:16:07.8574660Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-31T22:16:07.8575359Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-31T22:16:07.8576037Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-31T22:16:07.8576728Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-31T22:16:07.8577588Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-31T22:16:07.8578181Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-03-31T22:16:07.8578771Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-03-31T22:16:07.8579402Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-31T22:16:07.8580061Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-31T22:16:07.8580774Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-31T22:16:07.8581480Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-31T22:16:07.8582148Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-31T22:16:07.8582896Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-03-31T22:16:07.8583762Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-31T22:16:07.8584427Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-31T22:16:07.8585069Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-31T22:16:07.8585671Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-03-31T22:16:07.8586254Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-03-31T22:16:07.8586875Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-31T22:16:07.8587643Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-31T22:16:07.8779731Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-31T22:16:07.8780398Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-31T22:16:07.8781024Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-31T22:16:07.8781702Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-31T22:16:07.8782346Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-03-31T22:16:07.8783166Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-03-31T22:16:07.8784006Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-03-31T22:16:07.8784796Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-03-31T22:16:07.8785556Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-03-31T22:16:07.8786346Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-03-31T22:16:07.8787299Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-03-31T22:16:07.8788104Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-03-31T22:16:07.8815851Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-03-31T22:16:07.8816576Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-03-31T22:16:07.8819737Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint Checkpoint Metadata. Max parallelism mismatch between checkpoint/savepoint state and new program. Cannot map operator 20ba6b65f97481d5570070de90e4e791 with max parallelism 13 to new program with max parallelism 128. This indicates that the program has been changed in a non-compatible way after the checkpoint/savepoint.
2021-03-31T22:16:07.8821554Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$switchExecutor$23(FutureUtils.java:1362)
2021-03-31T22:16:07.8822349Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2021-03-31T22:16:07.8823178Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2021-03-31T22:16:07.8823948Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2021-03-31T22:16:07.8824698Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-03-31T22:16:07.8825485Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-03-31T22:16:07.8826318Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-03-31T22:16:07.8827203Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-03-31T22:16:07.8827925Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-03-31T22:16:07.8828561Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-03-31T22:16:07.8829192Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-03-31T22:16:07.8829860Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-03-31T22:16:07.8830536Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-03-31T22:16:07.8831187Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-03-31T22:16:07.8831853Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-03-31T22:16:07.8832477Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-03-31T22:16:07.8833155Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-03-31T22:16:07.8833784Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-03-31T22:16:07.8834351Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-03-31T22:16:07.8835134Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-03-31T22:16:07.8835693Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-03-31T22:16:07.8836208Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-03-31T22:16:07.8836805Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-03-31T22:16:07.8837571Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-03-31T22:16:07.8838263Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-03-31T22:16:07.8838962Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-03-31T22:16:07.8841344Z Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint Checkpoint Metadata. Max parallelism mismatch between checkpoint/savepoint state and new program. Cannot map operator 20ba6b65f97481d5570070de90e4e791 with max parallelism 13 to new program with max parallelism 128. This indicates that the program has been changed in a non-compatible way after the checkpoint/savepoint.
2021-03-31T22:16:07.8843188Z 	at org.apache.flink.runtime.scheduler.adaptive.BackgroundTask.lambda$new$0(BackgroundTask.java:59)
2021-03-31T22:16:07.8843956Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-03-31T22:16:07.8844829Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-03-31T22:16:07.8845596Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2021-03-31T22:16:07.8846306Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-03-31T22:16:07.8846964Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-03-31T22:16:07.8847833Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2021-03-31T22:16:07.8848782Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2021-03-31T22:16:07.8849613Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-03-31T22:16:07.8850360Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-03-31T22:16:07.8850983Z 	at java.lang.Thread.run(Thread.java:748)
2021-03-31T22:16:07.8852976Z Caused by: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint Checkpoint Metadata. Max parallelism mismatch between checkpoint/savepoint state and new program. Cannot map operator 20ba6b65f97481d5570070de90e4e791 with max parallelism 13 to new program with max parallelism 128. This indicates that the program has been changed in a non-compatible way after the checkpoint/savepoint.
2021-03-31T22:16:07.8854549Z 	at org.apache.flink.runtime.checkpoint.Checkpoints.loadAndValidateCheckpoint(Checkpoints.java:181)
2021-03-31T22:16:07.8855424Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1630)
2021-03-31T22:16:07.8856440Z 	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163)
2021-03-31T22:16:07.8862100Z 	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138)
2021-03-31T22:16:07.8863316Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphAndRestoreState(AdaptiveScheduler.java:971)
2021-03-31T22:16:07.8864391Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$createExecutionGraphAndRestoreStateAsync$24(AdaptiveScheduler.java:961)
2021-03-31T22:16:07.8865366Z 	at org.apache.flink.runtime.scheduler.adaptive.BackgroundTask.lambda$new$0(BackgroundTask.java:57)
2021-03-31T22:16:07.8865913Z 	... 10 more
{code}",,AHeise,austince,dwysakowicz,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 06:42:54 UTC 2021,,,,,,,,,,"0|z0pfuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/21 06:39;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16056&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4468;;;","06/Apr/21 06:56;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16044&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4805;;;","06/Apr/21 07:10;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16036&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4748;;;","06/Apr/21 07:41;trohrmann;[~austince], [~chesnay] could this be caused by FLINK-21844?;;;","06/Apr/21 09:30;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16030&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4748;;;","06/Apr/21 14:22;austince;[~trohrmann] very likely – I'll take this.;;;","06/Apr/21 14:36;chesnay;The test seems to use different maxParallelism values for subsequent job submissions; the first one uses 13, while in one of the test cases it later uses -1 to let Flink decide, and we come up with 128.

I would think that this use-case shouldn't even be supported because it technically modifies the maxParallelism, but apparently it was supported so far.;;;","06/Apr/21 16:51;austince;That is unfortunate, and yes I agree that it seems like it should not be supported. I guess this was supported before by allowing autoconfigured max parallelism to be overridden by the state restore.

I believe this is broken because we first compute the ""initialParallelismStore"", which defaults/ records if it has been autoconfigured, and then use that to create an ""adjusted"" parallelism store when creating the execution graph *without transferring autoconfiguration details*, thus hiding if a vertex's max parallelism can be rescaled by the state.;;;","07/Apr/21 06:29;arvid;We shouldn't support changes in maxParallelism. But in this case, we are telling Flink in the second run to use a value by itself and that should be set to 13.

I'm mostly wondering why it was working before and doesn't work anymore? Are we replacing -1 with 128 at the correct place?

Btw f it's now too complicated to support, I'd also be fine with dropping it (since it's super rare), but I still think that this is a valid use.;;;","07/Apr/21 06:50;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16093&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4808;;;","07/Apr/21 08:51;trohrmann;I think this is one of these cases where we try to be super smart instead of requiring explicit configuration and failing if the job is not properly configured.

Unfortunately, it is now supported by Flink and ideally we don't break this behaviour. If we decouple the state restore from the {{ExecutionGraph}} creation (or at least move it out of the EG and do it before we create the EG), then it should be easy to fix. Now we probably need a workaround until this is possible.;;;","08/Apr/21 09:52;trohrmann;[~austince] can you move this ticket to in progress?;;;","08/Apr/21 22:18;chesnay;master: 4a6518b3f6301589e6b5869bb295dd146092fc7e;;;","09/Apr/21 06:42;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16232&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4751

The run did not include the fix commit yet. Therefore I am not reopening the issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python tests fail on azure,FLINK-22083,13369037,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dwysakowicz,dwysakowicz,01/Apr/21 07:19,02/Apr/21 10:56,13/Jul/23 08:07,02/Apr/21 10:56,1.13.0,,,,,,,,1.13.0,,,,,,,API / Python,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15934&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=23503

{code}
E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
E                   	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
E                   	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
E                   	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
E                   	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
E                   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
E                   	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
E                   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
E                   	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
E                   	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
E                   	... 4 more
E                   Caused by: java.lang.Exception: The user defined 'open(Configuration)' method in class org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap caused an exception: Failed to create stage bundle factory! INFO:root:Initializing python harness: /__w/1/s/flink-python/pyflink/fn_execution/beam/beam_boot.py --id=313-1 --provision_endpoint=localhost:42447
E                   
E                   	at org.apache.flink.runtime.operators.BatchTask.openUserCode(BatchTask.java:1499)
E                   	at org.apache.flink.runtime.operators.chaining.ChainedFlatMapDriver.openTask(ChainedFlatMapDriver.java:47)
E                   	at org.apache.flink.runtime.operators.BatchTask.openChainedTasks(BatchTask.java:1541)
E                   	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:164)
E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:776)
E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
E                   	at java.lang.Thread.run(Thread.java:748)
E                   Caused by: java.lang.RuntimeException: Failed to create stage bundle factory! INFO:root:Initializing python harness: /__w/1/s/flink-python/pyflink/fn_execution/beam/beam_boot.py --id=313-1 --provision_endpoint=localhost:42447
E                   
E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.createStageBundleFactory(BeamPythonFunctionRunner.java:429)
E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:279)
E                   	at org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.open(AbstractPythonStatelessFunctionFlatMap.java:188)
E                   	at org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.open(PythonTableFunctionFlatMap.java:84)
E                   	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
E                   	at org.apache.flink.runtime.operators.BatchTask.openUserCode(BatchTask.java:1493)
E                   	... 6 more
E                   Caused by: org.apache.beam.vendor.guava.v26_0_jre.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Process died with exit code 0
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2050)
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.get(LocalCache.java:3952)
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)
E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:451)
E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:436)
E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.forStage(DefaultJobBundleFactory.java:303)
E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.createStageBundleFactory(BeamPythonFunctionRunner.java:427)
E                   	... 11 more
E                   Caused by: java.lang.IllegalStateException: Process died with exit code 0
E                   	at org.apache.beam.runners.fnexecution.environment.ProcessManager$RunningProcess.isAliveOrThrow(ProcessManager.java:75)
E                   	at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:112)
E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:252)
E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:231)
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
E                   	... 19 more
{code}",,dian.fu,dwysakowicz,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22076,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 02 10:55:57 UTC 2021,,,,,,,,,,"0|z0pfu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 06:02;dian.fu;It may be caused by the similar issue with FLINK-22076 although the exception stack is not exactly the same. Will try to address it ASAP.;;;","02/Apr/21 10:55;hxbks2ks;This issue should have been addressed in [FLINK-22076|https://issues.apache.org/jira/browse/FLINK-22076]. Feel free to reopen it if it still happens.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nested projection push down doesn't work for data such as row(array(row)),FLINK-22082,13369019,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,dian.fu,dian.fu,01/Apr/21 05:30,28/May/21 09:04,13/Jul/23 08:07,13/Apr/21 08:03,1.12.0,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"For the following job:

{code}
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import TableConfig, StreamTableEnvironment

config = TableConfig()
env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env, config)

source_ddl = """"""
    CREATE TABLE InTable (
        `ID` STRING,
        `Timestamp` TIMESTAMP(3),
        `Result` ROW(
            `data` ROW(`value` BIGINT) ARRAY),
        WATERMARK FOR `Timestamp` AS `Timestamp`
    ) WITH (
        'connector' = 'filesystem',
        'format' = 'json',
        'path' = '/tmp/1.txt'
    )
""""""

sink_ddl = """"""
    CREATE TABLE OutTable (
        `ID` STRING,
        `value` BIGINT
    ) WITH (
        'connector' = 'print'
    )
""""""

t_env.execute_sql(source_ddl)
t_env.execute_sql(sink_ddl)

table = t_env.from_path('InTable')
table \
    .select(
        table.ID,
        table.Result.data.at(1).value) \
    .execute_insert('OutTable') \
    .wait()
{code}

It will thrown the following exception:
{code}
: scala.MatchError: ITEM($2.data, 1) (of class org.apache.calcite.rex.RexCall)
	at org.apache.flink.table.planner.plan.utils.NestedSchemaExtractor.internalVisit$1(NestedProjectionUtil.scala:273)
	at org.apache.flink.table.planner.plan.utils.NestedSchemaExtractor.visitFieldAccess(NestedProjectionUtil.scala:283)
	at org.apache.flink.table.planner.plan.utils.NestedSchemaExtractor.visitFieldAccess(NestedProjectionUtil.scala:269)
	at org.apache.calcite.rex.RexFieldAccess.accept(RexFieldAccess.java:92)
	at org.apache.flink.table.planner.plan.utils.NestedProjectionUtil$$anonfun$build$1.apply(NestedProjectionUtil.scala:112)
	at org.apache.flink.table.planner.plan.utils.NestedProjectionUtil$$anonfun$build$1.apply(NestedProjectionUtil.scala:111)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.flink.table.planner.plan.utils.NestedProjectionUtil$.build(NestedProjectionUtil.scala:111)
	at org.apache.flink.table.planner.plan.utils.NestedProjectionUtil.build(NestedProjectionUtil.scala)
	at org.apache.flink.table.planner.plan.rules.logical.ProjectWatermarkAssignerTransposeRule.getUsedFieldsInTopLevelProjectAndWatermarkAssigner(ProjectWatermarkAssignerTransposeRule.java:155)
	at org.apache.flink.table.planner.plan.rules.logical.ProjectWatermarkAssignerTransposeRule.matches(ProjectWatermarkAssignerTransposeRule.java:65)
{code}

See https://stackoverflow.com/questions/66888486/pyflink-extract-nested-fields-from-json-array for more details",,dian.fu,fsk119,godfreyhe,jark,leonard,libenchao,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21746,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 03:12:28 UTC 2021,,,,,,,,,,"0|z0pfq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 05:31;dian.fu;cc [~fsk119];;;","01/Apr/21 05:39;jark;{{FileSystemTableSource}} doesn't support nested projection pushdown yet. ;;;","01/Apr/21 06:02;dian.fu;Does kafka source support nested projection pushdown? Actually it uses Kafka source in the original job: https://stackoverflow.com/questions/66888486/pyflink-extract-nested-fields-from-json-array

I just changed it to filesystem for ease of validation(as FileSystemTableSource is bundled in the blink planner). For the exception itself, it seems that it has nothing to do with what the source is. ;;;","01/Apr/21 14:57;jark;[~dian.fu] kafka source doesn't support even the basic projection pushdown. ;;;","01/Apr/21 14:57;jark;But I think we shouldn't throw the exception, the job should be able to run. So this is a bug. ;;;","02/Apr/21 01:25;dian.fu;[~jark] Thanks a lot for the confirmation. Agree with you. NestedProjectionUtil should handle properly for this case and make sure the job still able to run.;;;","13/Apr/21 03:12;godfreyhe;Fixed in 1.13.0: 2bbb1ba4072b9f03f9d3f9a17e004b5fe1eb4aa9
Fixed in 1.12.3: 9ad783e4ce2c397fe5a85fd6b45b5bd1a0758c50;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Entropy key not resolved if flink-s3-fs-hadoop is added as a plugin,FLINK-22081,13369013,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,foxss,foxss,foxss,01/Apr/21 04:48,22/Jun/21 13:55,13/Jul/23 08:07,08/Apr/21 07:33,1.10.3,1.11.3,1.12.2,1.13.0,,,,,1.11.4,1.12.3,1.13.0,,,,,FileSystems,,,,,0,pull-request-available,,,,,"Using flink 1.11.2

I added the flink-s3-fs-hadoop jar in plugins dir but I am seeing the checkpoints paths like {{s3://my_app/__ENTROPY__/app_name-staging/flink/checkpoints/e10f47968ae74934bd833108d2272419/chk-3071}} which means the entropy injection key is not being resolved. After some debugging I found that in the [EntropyInjector|https://github.com/apache/flink/blob/release-1.10.0/flink-core/src/main/java/org/apache/flink/core/fs/EntropyInjector.java#L97] we check if the given fileSystem is of type {{ClassLoaderFixingFileSystem}} and if so we check if the filesysystem is of type {{SafetyNetWrapperFileSystem as well as it's delegate }}but don't check for {{[ClassLoaderFixingFileSystem|https://github.com/apache/flink/blob/release-1.10.0/flink-core/src/main/java/org/apache/flink/core/fs/PluginFileSystemFactory.java#L65]}} directly in getEntorpyFs method which would be the type if S3 file system dependencies are added as a plugin.

 

Repro steps: 

Flink 1.11.2 with flink-s3-fs-hadoop as plugin and turn on entropy injection key _entropy_
observe checkpoint dir with entropy marker not removed.
s3a://xxx/dev/checkpoints/_entropy_/xenon/event-stream-splitter/jobid/chk-5/  
compare to removed when running Flink 1.9.1
s3a://xxx/dev/checkpoints/xenon/event-stream-splitter/jobid/chk-5/  

Add some logging to getEntropyFs, observe it return null because passed in parameter is not {{SafetyNetWrapperFileSystem}} but {{ClassLoaderFixingFileSystem}}

Apply patch, build release and run same job, resolved issue as attachment shows

 

 ",,AHeise,foxss,shazeline,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17359,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/21 05:20;foxss;image (13).png;https://issues.apache.org/jira/secure/attachment/13023282/image+%2813%29.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 07:33:05 UTC 2021,,,,,,,,,,"0|z0pfow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 05:25;foxss;[~AHeise] could you assign this Jira to me and help review pr?;;;","08/Apr/21 07:29;arvid;Merged into master as 2d3559e66db, into 1.12 as a9b34a3db23, and into 1.11 as 3bd44e083c8.;;;","08/Apr/21 07:33;arvid;Merging into 1.10 is quite an effort and the version is officially not maintained anymore. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong way to calculate cross-region ConsumedPartitionGroups in PipelinedRegionSchedulingStrategy,FLINK-22077,13369004,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Thesharing,Thesharing,Thesharing,01/Apr/21 03:44,28/May/21 09:01,13/Jul/23 08:07,08/Apr/21 09:50,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"h3. Introduction

We implement a wrong way to calculate cross-region ConsumedPartitionGroups in {{PipelinedRegionSchedulingStrategy}} in FLINK-21330, it slows down the procedure of {{onExecutionStateChange}}, makes the complexity increase from O(N) to O(N^2). Also the semantic of cross-region is totally wrong.
h3. Details

In {{PipelinedRegionSchedulingStrategy#startScheduling}}, as expected, we need to schedule all region with no external blocking edges, i.e., source regions. To decrease the complexity, we choose to schedule all the regions that has no external BLOCKING ConsumedPartitionGroups.

However, for the case illustrated in FLINK-22017, the region 2 has a ConsumedPartitionGroup, which has both internal and external blocking IntermediateResultPartitions. If we choose one to represent the entire ConsumedPartitionGroup, it may choose the internal one, and make the entire group internal. Region 2 will be scheduled.

As Region 1 is not finished, Region 2 cannot transition to running. A deadlock may happen if resource is not enough for both two regions.

To make it right, we introduced cross-region ConsumedPartitionGroups in FLINK-21330. The ConsumedPartitionGroups with both internal and external blocking IntermediateResultPartitions will be recorded. When we call {{startScheduling}}, these ConsumedPartitionGroups will be treated as external ones, and region 2 will not be scheduled.

But we have to admit that the implementation of cross-region is wrong. The ConsumedPartitionGroups that has multiple producer regions will be treated as cross-region groups. It is not the same logic as we mentioned above. The semantic is totally wrong. Also all the ALL-TO-ALL BLOCKING ConsumedPartitionGroups will be treated as cross-region groups, since their producers are in different regions. (Each producer has its own region.) This slows down the complexity from O(N) to O(N^2) for ALL-TO-ALL BLOCKING edges.
h3. Solution

To correctly calculate the cross-region ConsumedPartitionGroups, we can just calculate the producer regions for all ConsumedPartitionGroups, and then iterate all the regions and its ConsumedPartitionGroups. If the ConsumedPartitionGroup has two or more producer regions, and the regions contains the current region, it is a cross-region ConsumedPartitionGroup. This meets the correct semantics, and makes sure ALL-TO-ALL BLOCKING ConsumedPartitionGroups will not be treated as cross-region ones. This fix will also decrease the complexity from O(N^2) to O(N).",,guoyangze,maguowei,RocMarshal,Thesharing,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 09:50:36 UTC 2021,,,,,,,,,,"0|z0pfmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 08:32;Thesharing;cc [~trohrmann]  I personally think it's necessary to add this bug-fix to release 1.13.
 ;;;","06/Apr/21 10:11;trohrmann;Thanks for analysing the problem [~Thesharing]. I think we should indeed fix this problem for the {{1.13.0}} release. Can you work on it? I will help with the review.;;;","07/Apr/21 07:31;Thesharing;Thank you, [~trohrmann]! I've came up with a pull request, would you mind reviewing it when you got free time?;;;","08/Apr/21 09:50;trohrmann;Fixed via 9390483bbfd6c54057ffb8ea53165e9d666bb530;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python Test failed with ""OSError: [Errno 12] Cannot allocate memory""",FLINK-22076,13368986,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,01/Apr/21 02:14,28/Aug/21 11:14,13/Jul/23 08:07,02/Apr/21 10:53,1.13.0,,,,,,,,1.13.0,,,,,,,API / Python,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/sewen0794/Flink/_build/results?buildId=249&view=logs&j=fba17979-6d2e-591d-72f1-97cf42797c11&t=443dc6bf-b240-56df-6acf-c882d4b238da&l=21533

Python Test failed with ""OSError: [Errno 12] Cannot allocate memory"" in Azure Pipeline. I am not sure if it is caused by insufficient machine memory on Azure.
",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22101,FLINK-22083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 02 10:53:46 UTC 2021,,,,,,,,,,"0|z0pfiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 10:53;hxbks2ks;Merged into master via 000c69dcfb9632c152a45a7971bdef7e3ef0556e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testRequirementCheckOnlyTriggeredOnce(org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest) failed,FLINK-22074,13368911,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,leonard,leonard,31/Mar/21 16:10,28/May/21 11:04,13/Jul/23 08:07,27/Apr/21 02:09,1.13.0,,,,,,,,1.13.0,,,,,,,Tests,,,,,0,pull-request-available,test-stability,,,,"[ERROR] Tests run: 14, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.654 s <<< FAILURE! - in org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest 
[ERROR] testRequirementCheckOnlyTriggeredOnce(org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest) Time elapsed: 0.059 s <<< FAILURE! 
java.lang.AssertionError: Expected to fail with a timeout. 
 at org.junit.Assert.fail(Assert.java:88) 
 at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.assertFutureNotComplete(FineGrainedSlotManagerTestBase.java:126) 
 at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest$10.lambda$new$3(FineGrainedSlotManagerTest.java:605) 
 at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase$Context.runTest(FineGrainedSlotManagerTestBase.java:197) 
 at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest$10.<init>(FineGrainedSlotManagerTest.java:581) 
 at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testRequirementCheckOnlyTriggeredOnce(FineGrainedSlotManagerTest.java:565) 
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
 at java.lang.reflect.Method.invoke(Method.java:498) 
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 
 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) 
 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) 
 at org.junit.rules.RunRules.evaluate(RunRules.java:20) 
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 
 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) 
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) 
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) 
 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) 
 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) 
 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) 
 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) 
 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)",,guoyangze,leonard,maguowei,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 27 02:09:52 UTC 2021,,,,,,,,,,"0|z0pf2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 16:11;leonard;failed link https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15895&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6000;;;","25/Apr/21 09:48;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17143&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=5959
;;;","27/Apr/21 02:09;xtsong;Fixed via
* master (1.14): 6236473f8fb90cbde9673d1fa0c51659bc9e0c8c
* release-1.13: ac90590061bdb21bac56247d998aeda1ba6c5e2c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check Log Pollution for 1.13 release,FLINK-22069,13368850,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,sewen,sewen,31/Mar/21 11:02,06/Aug/21 11:51,13/Jul/23 08:07,06/Aug/21 11:51,,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,auto-deprioritized-critical,auto-deprioritized-major,pull-request-available,,,"We should check for log pollution and confusing log lines before the release.



",,knaufk,libenchao,maguowei,RocMarshal,sewen,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 05 22:37:42 UTC 2021,,,,,,,,,,"0|z0pep4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/21 12:35;chesnay;h3. JM

Can't really fix it, but this will probably raise some eyebrows:
{code}
2021-04-09 13:11:09,002 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - DataSink (collect()) (1/1) (c8368e6fff97b7439058fdbdc6a9fd3d) switched from DEPLOYING to RECOVERING.
{code}

;;;","09/Apr/21 15:44;sewen;When checkpoints are being skipped because not all sources are ready, we log a big stack trace.
This can happen multiple times during job startup (while TMs are starting, pulling artifacts, tasks are being scheduled), and it creates the impression that something is wrong, when this is really a totally normal and expected situation.

We can probably reduce this to an INFO level statement, because this isn't something bad or unexpected, and the stack trace doesn't add any information (there are really only RPC worker methods in the trace).

{code}
7862 [Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Failed to trigger checkpoint for job b7476b12c6b7da7b72fa4dfb4fc0741a.)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Source: numbers -> Map -> Sink: Data stream collect sink (1/1) of job b7476b12c6b7da7b72fa4dfb4fc0741a has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:152) ~[classes/:?]
	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:114) ~[classes/:?]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[classes/:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
{code};;;","09/Apr/21 15:59;sewen;Random log exception when tasks finish after a checkpoint was started.

{code:java}
9407 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1617983491976 for job d8225dac771bf607cf8dd869964c6265.
9501 [Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0 (3f61526eef13676a7d96010799c04e1c) switched from RUNNING to FINISHED.
9501 [Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0 (3f61526eef13676a7d96010799c04e1c).
9501 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0 3f61526eef13676a7d96010799c04e1c.
9501 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Source: numbers -> Map -> Sink: Data stream collect sink (1/1)#0 - asynchronous part of checkpoint 1 could not be completed.
java.util.concurrent.CancellationException: null
	at java.util.concurrent.FutureTask.report(FutureTask.java:121) ~[?:?]
	at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:?]
	at org.apache.flink.runtime.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:636) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:60) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:128) [classes/:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:834) [?:?]

{code};;;","12/Apr/21 07:35;chesnay;I've opened a PR addressing some of the found issues:
- explicitly configure rest.address in the flink-dist config
- restructure ExternalResourceUtils to log ""Enabled external resources:"" less often
- changed the ""Converting recovered input channels"" message to debug
- changed the ""Registering task at network"" message to debug
- moved the logging of resource requirements entirely to the slot manager
- added a special case to the resource requirements logging if they is empty
- added a startup logging message to the RM, and changed the declarative slot manager startup message to debug
- do not log ""send next input split"" if the split is null
- deduplicated/synced the logging logic in the CheckpointStorageLoader, and added quotes
- CollectResultFetcher now checks the exception, and logs on debug if the job has not been initialized yet
- Do not print filesystem-factory hashcodes when they are initialied;;;","12/Apr/21 08:40;trohrmann;Thanks for collecting all the logging problems [~chesnay] and [~sewen]. For easier tracking what is addressed and what not I would suggest to create for every log problem a subtask. I know that this is a bit of overhead but I have a really hard time figuring out what is fixed and what not. Also splitting the logging issues will make discussing them a lot easier.

Maybe we can start with the items which won't be fixed by the PR [~chesnay].;;;","12/Apr/21 09:14;chesnay;I've moved everything into subtasks. The RECOVERING issue is covered by FLINK-22215, and was left out as a subtask.;;;","15/Apr/21 08:03;trohrmann;I think we have solved some of the issues here. Shall we decrease the priority of this ticket to critical so that it no longer blocks the release? We should try to address the other issues as we go along.;;;","15/Apr/21 13:22;sewen;There is one that doesn't yet seem to be fixed that I found really bad during debugging: logging the full exception stack trace when a checkpoint cannot be triggered due to the fact that some tasks are still starting up: FLINK-22229

It happens during almost every startup, is often unavoidable, and is a completely fine and expected situation. The stack trace makes it look scary. I would say it is not a blocker, but we should try hard to get that fix in.;;;","15/Apr/21 13:57;trohrmann;I think the checkpointing folks are aware of it (FLINK-22229) and try to do it as soon as possible.;;;","05/May/21 13:59;trohrmann;[~chesnay] do you intend to continue working on this issue?;;;","05/May/21 14:26;chesnay;No, I've unassigned myself. We could convert the remaining tickets into independent tickets, or create a new 1.14 log pollution ticket. I see this ticket was renamed and the fixVersion updated to 1.14, but I'm not sure how well this approach will scale in the long-term (are we gonna keep expanding this ticket for 1.15, ... ?); a separate ticket might be better.;;;","06/May/21 12:48;trohrmann;I'd be fine with creating a new ticket and moving the unresolved tickets
over to it. I agree that we should not rename tickets.

On Wed, May 5, 2021 at 4:27 PM Chesnay Schepler (Jira) <jira@apache.org>

;;;","19/May/21 10:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/May/21 23:02;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","27/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","05/Jul/21 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointWindowReaderITCase.testApplyEvictorWindowStateReader,FLINK-22067,13368839,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,trohrmann,trohrmann,31/Mar/21 10:13,28/Aug/21 12:09,13/Jul/23 08:07,08/Jun/21 19:51,1.14.0,,,,,,,,1.14.0,,,,,,,API / State Processor,,,,,0,auto-deprioritized-critical,pull-request-available,test-stability,,,"The test case {{SavepointWindowReaderITCase.testApplyEvictorWindowStateReader}} failed on AZP with:

{code}

	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:69)
	... 33 more
Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
	at com.sun.proxy.$Proxy32.triggerSavepoint(Unknown Source)
	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:716)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:751)
	at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:714)
	at org.apache.flink.client.program.MiniClusterClient.triggerSavepoint(MiniClusterClient.java:101)
	at org.apache.flink.state.api.utils.SavepointTestBase.triggerSavepoint(SavepointTestBase.java:93)
	at org.apache.flink.state.api.utils.SavepointTestBase.lambda$takeSavepoint$0(SavepointTestBase.java:68)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1646)
	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#-390276455]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15809&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=9197",,dwysakowicz,maguowei,rmetzger,roman,sjwiesman,trohrmann,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22481,FLINK-22595,,,,,,,,,,FLINK-22932,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/21 10:34;rmetzger;isolated_logs_builD_9072.log;https://issues.apache.org/jira/secure/attachment/13025153/isolated_logs_builD_9072.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 19:51:38 UTC 2021,,,,,,,,,,"0|z0pemo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 06:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15996&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=e5682198-9e22-5770-69f6-7551182edea8&l=8854


{code:java}

Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022)
	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:69)
	... 43 more
Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
	at com.sun.proxy.$Proxy31.triggerSavepoint(Unknown Source)
	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:716)
	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680)
	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
	at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094)
	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:751)
	at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:714)
	at org.apache.flink.client.program.MiniClusterClient.triggerSavepoint(MiniClusterClient.java:101)
	at org.apache.flink.state.api.utils.SavepointTestBase.triggerSavepoint(SavepointTestBase.java:93)
	at org.apache.flink.state.api.utils.SavepointTestBase.lambda$takeSavepoint$0(SavepointTestBase.java:68)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1742)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1728)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#2045669472]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)

{code}
;;;","22/Apr/21 12:23;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:48;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","07/May/21 10:27;rmetzger;Fails on my personal CI (this is just master) https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9072&view=logs&j=d0dc8a09-802e-543a-1851-c31096d61b33&t=5952d6c2-ad1d-5ad4-5bfc-48bb7f31ebd9

The stack trace is a bit different this time:

{code}
2021-05-07T10:23:58.3619103Z May 07 10:23:58 [ERROR] Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.603 s <<< FAILURE! - in org.apache.flink.state.api.RocksDBStateBackendWindowITCase
2021-05-07T10:23:58.3638516Z May 07 10:23:58 [ERROR] testApplyEvictorWindowStateReader(org.apache.flink.state.api.RocksDBStateBackendWindowITCase)  Time elapsed: 2.288 s  <<< ERROR!
2021-05-07T10:23:58.3639625Z May 07 10:23:58 java.lang.RuntimeException: Failed to take savepoint
2021-05-07T10:23:58.3640269Z May 07 10:23:58 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:63)
2021-05-07T10:23:58.3641069Z May 07 10:23:58 	at org.apache.flink.state.api.SavepointWindowReaderITCase.testApplyEvictorWindowStateReader(SavepointWindowReaderITCase.java:361)
2021-05-07T10:23:58.3641776Z May 07 10:23:58 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-07T10:23:58.3642383Z May 07 10:23:58 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-07T10:23:58.3643071Z May 07 10:23:58 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-07T10:23:58.3643696Z May 07 10:23:58 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-07T10:23:58.3644458Z May 07 10:23:58 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-07T10:23:58.3645155Z May 07 10:23:58 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-07T10:23:58.3646757Z May 07 10:23:58 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-07T10:23:58.3663386Z May 07 10:23:58 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-07T10:23:58.3664098Z May 07 10:23:58 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-07T10:23:58.3664765Z May 07 10:23:58 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-07T10:23:58.3665593Z May 07 10:23:58 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-07T10:23:58.3666179Z May 07 10:23:58 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-07T10:23:58.3666762Z May 07 10:23:58 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-07T10:23:58.3667430Z May 07 10:23:58 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-07T10:23:58.3668113Z May 07 10:23:58 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-07T10:23:58.3670532Z May 07 10:23:58 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-07T10:23:58.3671229Z May 07 10:23:58 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-07T10:23:58.3671842Z May 07 10:23:58 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-07T10:23:58.3672455Z May 07 10:23:58 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-07T10:23:58.3673062Z May 07 10:23:58 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-07T10:23:58.3673669Z May 07 10:23:58 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-07T10:23:58.3708725Z May 07 10:23:58 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-07T10:23:58.3709393Z May 07 10:23:58 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-07T10:23:58.3709977Z May 07 10:23:58 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-07T10:23:58.3710616Z May 07 10:23:58 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-05-07T10:23:58.3711330Z May 07 10:23:58 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-05-07T10:23:58.3712051Z May 07 10:23:58 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-05-07T10:23:58.3712746Z May 07 10:23:58 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-05-07T10:23:58.3713469Z May 07 10:23:58 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-07T10:23:58.3714414Z May 07 10:23:58 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-07T10:23:58.3715098Z May 07 10:23:58 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-07T10:23:58.3715752Z May 07 10:23:58 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-05-07T10:23:58.3767009Z May 07 10:23:58 Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Window(org.apache.flink.state.api.utils.WaitingWindowAssigner@716e431d, EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (1/4) of job 7febc061a35ab7a41806df6aaa10f60b has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2021-05-07T10:23:58.3768604Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-05-07T10:23:58.3769467Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-05-07T10:23:58.3770158Z May 07 10:23:58 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:61)
2021-05-07T10:23:58.3770709Z May 07 10:23:58 	... 33 more
2021-05-07T10:23:58.3772641Z May 07 10:23:58 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Window(org.apache.flink.state.api.utils.WaitingWindowAssigner@716e431d, EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (1/4) of job 7febc061a35ab7a41806df6aaa10f60b has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2021-05-07T10:23:58.3774104Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2021-05-07T10:23:58.3774796Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2021-05-07T10:23:58.3775493Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2021-05-07T10:23:58.3776167Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-05-07T10:23:58.3776845Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-07T10:23:58.3777530Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-05-07T10:23:58.3778274Z May 07 10:23:58 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$null$0(CheckpointCoordinator.java:482)
2021-05-07T10:23:58.3778999Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-05-07T10:23:58.3779695Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-05-07T10:23:58.3780391Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-05-07T10:23:58.3781091Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-05-07T10:23:58.3781905Z May 07 10:23:58 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointTriggerRequest.completeExceptionally(CheckpointCoordinator.java:2047)
2021-05-07T10:23:58.3782762Z May 07 10:23:58 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:853)
2021-05-07T10:23:58.3783585Z May 07 10:23:58 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$7(CheckpointCoordinator.java:608)
2021-05-07T10:23:58.3784349Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2021-05-07T10:23:58.3785023Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2021-05-07T10:23:58.3786194Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2021-05-07T10:23:58.3786854Z May 07 10:23:58 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-05-07T10:23:58.3787447Z May 07 10:23:58 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-05-07T10:23:58.3788156Z May 07 10:23:58 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2021-05-07T10:23:58.3788973Z May 07 10:23:58 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2021-05-07T10:23:58.3789753Z May 07 10:23:58 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-05-07T10:23:58.3806658Z May 07 10:23:58 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-05-07T10:23:58.3807290Z May 07 10:23:58 	at java.lang.Thread.run(Thread.java:748)
2021-05-07T10:23:58.3810695Z May 07 10:23:58 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Window(org.apache.flink.state.api.utils.WaitingWindowAssigner@716e431d, EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (1/4) of job 7febc061a35ab7a41806df6aaa10f60b has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2021-05-07T10:23:58.3823381Z May 07 10:23:58 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:152)
2021-05-07T10:23:58.3824333Z May 07 10:23:58 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:114)
2021-05-07T10:23:58.3865445Z May 07 10:23:58 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2021-05-07T10:23:58.3866280Z May 07 10:23:58 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-05-07T10:23:58.3867545Z May 07 10:23:58 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-05-07T10:23:58.3868716Z May 07 10:23:58 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-05-07T10:23:58.3869590Z May 07 10:23:58 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-05-07T10:23:58.3870219Z May 07 10:23:58 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-05-07T10:23:58.3876399Z May 07 10:23:58 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-05-07T10:23:58.3877044Z May 07 10:23:58 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-05-07T10:23:58.3877656Z May 07 10:23:58 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-05-07T10:23:58.3878283Z May 07 10:23:58 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-05-07T10:23:58.3879916Z May 07 10:23:58 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-07T10:23:58.3880581Z May 07 10:23:58 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-05-07T10:23:58.3881176Z May 07 10:23:58 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-05-07T10:23:58.3881769Z May 07 10:23:58 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-05-07T10:23:58.3882372Z May 07 10:23:58 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-05-07T10:23:58.3882936Z May 07 10:23:58 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-05-07T10:23:58.3883498Z May 07 10:23:58 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-05-07T10:23:58.3884049Z May 07 10:23:58 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-05-07T10:23:58.3884578Z May 07 10:23:58 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-05-07T10:23:58.3885151Z May 07 10:23:58 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-05-07T10:23:58.3886011Z May 07 10:23:58 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-05-07T10:23:58.3886660Z May 07 10:23:58 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-05-07T10:23:58.3887311Z May 07 10:23:58 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-05-07T10:23:58.3887802Z May 07 10:23:58 
2021-05-07T10:23:59.6186290Z May 07 10:23:59 [INFO] Running org.apache.flink.state.api.MemoryStateBackendReaderKeyedStateITCase
{code};;;","07/May/21 10:35;rmetzger;I attached ""isolated_logs_builD_9072.log"" showing the output of the failing run from the link I posted;;;","07/May/21 10:45;rmetzger;[~akalashnikov] I saw that you've recently touched the test that is failing: https://github.com/apache/flink/pull/15728/files#diff-c025a391bd4f0558df226dd65f097d090cc1b45ce2eb6c34a862e9c3b619fa93R359-R385
Could you take a look at the failure I reported? Maybe the test failure I've reported is actually a different issue than [~trohrmann]'s initial report (he saw a RPC timeout, in my case the test seems to trigger a savepoint before all tasks are running);;;","07/May/21 10:58;rmetzger;Same issue in {{SavepointWindowReaderITCase.testWindowTriggerStateReader}} ?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17666&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c;;;","07/May/21 11:01;rmetzger;testReduceWindowStateReader https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17676&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c;;;","07/May/21 21:21;roman;I think the issue is caused by [ae402b|https://github.com/apache/flink/commit/ae402bc9134201892dac59e0421e5b3834430656] which fails check/savepoints if not all tasks are running (previously we checked only sources).

The change uses OperatorLatch to wait for non-souce vertices to run. However it doesn't take DoP into account. Previously, test waitied only for sources, which aren't parallel in these tests.

The same issue is with all SavepointTestBase descendants, so FLINK-22481 is likely a duplicate.

To fix this, I propose to use API to wait for vertices using Flink API. Draft PR: 
 [https://github.com/apache/flink/pull/15858]

`WaitingFunction` and it's descendants then could be deleted.

WDYT?

cc: [~akalashnikov], [~sjwiesman]
  ;;;","10/May/21 13:11;rmetzger;Thanks for looking into the failure. I temporarily disabled all related tests https://github.com/apache/flink/commit/464e0d0997762e69be606f8afd7c796d97d24072;;;","10/May/21 15:58;sjwiesman;[~roman_khachatryan] I agree FLINK-22481 is likely a duplicate. I've added a few notes to the draft PR but the approach makes sense to me and I'm happy to drop WaitingFunction / simplify the test infrastructure here.;;;","11/May/21 06:49;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17817&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=9355;;;","18/May/21 07:24;roman;I've removed 1.13* from affected versions since the change causing the issue is only present in 1.14 (master).;;;","18/May/21 07:25;roman;Merged as 0528f8479ba80b41a0b1169a97a246585d26e29f into master.
 ;;;","07/Jun/21 02:30;xtsong;There's an instance with the exactly same error stack on the 1.13 branch.
Reopening the ticket and adding back 1.13 as affected version.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18709&view=logs&j=a8bc9173-2af6-5ba8-775c-12063b4f1d54&t=46a16c18-c679-5905-432b-9be5d8e27bc6&l=10183;;;","08/Jun/21 06:59;roman;I think this is another issue, as savepoint was actually triggered but did not complete in time:
{code}
22:27:18,896 [                main] INFO  org.apache.flink.state.api.RocksDBStateBackendWindowITCase   [] -
Test testApplyEvictorWindowStateReader(org.apache.flink.state.api.RocksDBStateBackendWindowITCase) is running.
22:27:19,673 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
22:27:20,213 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=SAVEPOINT) @ 1623018440163 for job 4425257f98cff335afad592f4d445058.
22:27:30,203 [                main] ERROR org.apache.flink.state.api.RocksDBStateBackendWindowITCase   [] -
Test testApplyEvictorWindowStateReader(org.apache.flink.state.api.RocksDBStateBackendWindowITCase) failed with:
java.lang.RuntimeException: Failed to take savepoint
{code};;;","08/Jun/21 19:51;roman;I've created FLINK-22932 to track the issue (and reverting the recent changes to this ticket).
Please feel free to undo if this is indeed the same problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't submit statement set when no insert  is added in the sql client,FLINK-22064,13368827,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,fsk119,fsk119,31/Mar/21 09:28,28/Aug/21 12:15,13/Jul/23 08:07,02/Apr/21 08:45,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,!https://static.dingtalk.com/media/lALPD4Bhs9KtV3jM4s0F2A_1496_226.png_720x720g.jpg?renderWidth=1496&renderHeight=226&renderOrientation=1&isLocal=0&bizType=im!,,fsk119,hackergin,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 02 08:45:32 UTC 2021,,,,,,,,,,"0|z0pek0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 11:34;hackergin;[~jark]  I can help to fix this. ;;;","31/Mar/21 11:36;jark;Assigned to you [~hackergin];;;","02/Apr/21 08:45;jark;Fixed in master: 15914848373c8579bb9bf881d873e6a6edd809c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lookup Join outputs wrong results for some scenario,FLINK-22063,13368802,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,icshuo,icshuo,icshuo,31/Mar/21 07:30,28/May/21 09:06,13/Jul/23 08:07,14/Apr/21 10:36,1.12.2,,,,,,,,1.13.0,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"Reproduce the bug as following:

In LookupJoinITCase, given the sql 

{code:sql}
SELECT 
	T.id, T.len, D.id, T.content, D.name 
FROM src AS T JOIN user_table for system_time as of T.proctime AS D 
ON T.id = D.id and cast(T.len as bigint) = D.id
{code}

the following execution plan is generated:

{code:java}
LegacySink(name=[DataStreamTableSink], fields=[id, len, id0, content, name])
+- Calc(select=[id, len, id0, content, name])
   +- LookupJoin(table=[**], joinType=[InnerJoin], async=[false], lookup=[id=len0], select=[id, len, content, len0, id, name])
      +- Calc(select=[id, len, content, CAST(len) AS len0])
         +- TableSourceScan(table=[[**]], fields=[id, len, content])
{code}

As we can see, the condition `T.id = D.id` is lost, so a wrong result may be produced.",,hackergin,icshuo,jark,leonard,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 10:36:43 UTC 2021,,,,,,,,,,"0|z0peeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/21 01:42;leonard;Hi, [~icshuo] Do you have any progress?;;;","08/Apr/21 06:55;icshuo;[~Leonard Xu] Not start yet, but will soon..;;;","14/Apr/21 10:36;lzljs3620320;master (1.13): da22f75c1df752b70047033b62204fff2b309bb0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""Data Sources"" (FLIP-27 sources overview page) page from 1.12.x to master",FLINK-22062,13368796,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sjwiesman,rmetzger,rmetzger,31/Mar/21 06:52,28/May/21 09:01,13/Jul/23 08:07,08/Apr/21 16:23,1.13.0,,,,,,,,1.13.0,,,,,,,API / DataStream,Documentation,,,,0,pull-request-available,,,,,This page https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html is not available in the latest docs on master.,,dwysakowicz,rmetzger,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 16:23:34 UTC 2021,,,,,,,,,,"0|z0pedc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 06:54;rmetzger;[~sjwiesman] do you still have your magic migration tool around to do this? ;;;","31/Mar/21 12:29;sjwiesman;Yes, I will take this.;;;","07/Apr/21 12:41;dwysakowicz;Hey, [~sjwiesman] any progress on this issue?;;;","08/Apr/21 16:23;sjwiesman;fixed in master: 021c8d87a6fa415dd8b1cf2e2ae8139a4ba55a0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The DEFAULT_NON_SPLITTABLE_FILE_ENUMERATOR defined in FileSource should points to NonSplittingRecursiveEnumerator,FLINK-22061,13368792,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,31/Mar/21 06:28,28/Aug/21 11:11,13/Jul/23 08:07,31/Mar/21 11:58,1.12.0,,,,,,,,1.12.3,1.13.0,,,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,,,,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 31 11:58:50 UTC 2021,,,,,,,,,,"0|z0pecg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 11:58;dian.fu;Fixed in
- master via 5d7031726ae47ecc57e2e369f010dba47896d9aa
- release-1.12 via 619fedb31ed33e3501be115fc618aa41f5dab8bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPC main thread executor may schedule commands with wrong time unit of delay,FLINK-22055,13368763,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Amon,xtsong,xtsong,31/Mar/21 03:24,28/Aug/21 11:13,13/Jul/23 08:07,02/Apr/21 02:07,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"There's a typo in \{{RpcEndpoint#MainThreadExecutor#schedule(callable, delay, unit)}}, that may cause the command being scheduled with wrong time unit.

 

The issue is Major priority because currently it's not causing any actual damage.",,Amon,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 02 02:07:41 UTC 2021,,,,,,,,,,"0|z0pe60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 08:49;Amon;[~xintongsong]
I am est08zw, assign the ticket to me, thanks!;;;","31/Mar/21 11:15;xtsong;[~Amon], you are assigned.;;;","02/Apr/21 02:07;xtsong;Fixed via
 * master (1.13): 5c43291494bb57c0b4cf5668227fe2cadc0bb2a4
 * release-1.12: 9ff3b0ee294222007c6a6af57541c48854f6645a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NumberSequenceSource causes fatal exception when less splits than parallelism.,FLINK-22053,13368719,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,AHeise,30/Mar/21 20:34,28/Aug/21 11:12,13/Jul/23 08:07,01/Apr/21 06:17,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,API / Core,,,,,0,pull-request-available,,,,,"If more splits than 

{noformat}
Caused by: java.lang.IllegalArgumentException: 'from' must be <= 'to'
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138) ~[flink-core-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.api.connector.source.lib.NumberSequenceSource$NumberSequenceSplit.<init>(NumberSequenceSource.java:148) ~[flink-core-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.api.connector.source.lib.NumberSequenceSource.createEnumerator(NumberSequenceSource.java:111) ~[flink-core-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:126) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.applyCall(RecreateOnResetOperatorCoordinator.java:296) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.start(RecreateOnResetOperatorCoordinator.java:71) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:182) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:85) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:501) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:955) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:873) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:383) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:605) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:180) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?]
	... 12 more
{noformat}

To reproduce


{noformat}
    @Test
    public void testLessSplitsThanParallelism() throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(12);
        env.fromSequence(0, 10);
        env.execute();
    }
{noformat}

",,AHeise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 06:16:47 UTC 2021,,,,,,,,,,"0|z0pdw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 06:16;arvid;Merged into master as a93251b4599e7c7d77ec6f0796825a93224eb010, merged into 1.12 as 945684114092f590e6cb90b78ce7fe4ccc7ada6c.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to release input gate (IllegalReferenceCountException),FLINK-22050,13368664,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,30/Mar/21 14:16,28/Aug/21 11:14,13/Jul/23 08:07,06/Apr/21 09:17,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Network,,,,,0,pull-request-available,,,,,"When running UnalignedCheckpointITCase continuously, this error is periodically reported (though tests pass):
{code}
[INFO] Running org.apache.flink.test.checkpointing.UnalignedCheckpointITCase
34948 [Canceler for keyed (19/20)#1 (532afce89e3c90301e37b2f5e9a0bded).] ERROR org.apache.flink.runtime.taskmanager.Task [] - Failed to release input gate for task keyed (19/20)#1.
org.apache.flink.shaded.netty4.io.netty.util.IllegalReferenceCountException: refCnt: 0, decrement: 1
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.toLiveRealRefCnt(ReferenceCountUpdater.java:74) ~[flink-shaded-netty-4.1.49.Final-12.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.release(ReferenceCountUpdater.java:138) ~[flink-shaded-netty-4.1.49.Final-12.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) ~[flink-shaded-netty-4.1.49.Final-12.0.jar:?]
        at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:156) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.releaseAllBuffers(BufferManager.java:234) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.releaseAllResources(RemoteInputChannel.java:267) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:555) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:119) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.closeNetworkResources(Task.java:978) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1559) [classes/:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
38445 [Canceler for keyed (19/20)#2 (f94480c534db164b2cdf7f44682593d1).] ERROR org.apache.flink.runtime.taskmanager.Task [] - Failed to release input gate for task keyed (19/20)#2.
org.apache.flink.shaded.netty4.io.netty.util.IllegalReferenceCountException: refCnt: 0
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.ensureAccessible(AbstractByteBuf.java:1489) ~[flink-shaded-netty-4.1.49.Final-12.0.jar:?]
        at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.getMemorySegment(NetworkBuffer.java:139) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.releaseAllBuffers(BufferManager.java:232) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.releaseAllResources(RemoteInputChannel.java:267) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:555) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:119) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.closeNetworkResources(Task.java:978) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1559) [classes/:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{code}",,kevin.cyj,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 06 09:17:47 UTC 2021,,,,,,,,,,"0|z0pdk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/21 09:17;roman;Merged into master as 4ffcafd7599169e41d4b48cbfac07d5e7338bdf0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set log level for shaded zookeeper logger,FLINK-22045,13368626,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,mapohl,mapohl,30/Mar/21 11:17,28/Aug/21 12:19,13/Jul/23 08:07,21/Jun/21 17:04,1.13.0,,,,,,,,1.13.2,1.14.0,,,,,,Runtime / Coordination,,,,,0,auto-deprioritized-major,pull-request-available,,,,"FLINK-16163 introduced shaded dependencies for zookeeper. We missed adapting the logs accordingly which resulted in DEBUG log messages printed for zookeeper when using the shaded zookeeper dependency.

We might want to add a logger for {{org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.}} to all {{log4j.properties}} files.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16163,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21329,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 21 17:04:28 UTC 2021,,,,,,,,,,"0|z0pdbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/21 23:47;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/May/21 10:51;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Jun/21 17:04;chesnay;master: 93afd3eeadd25dcd448bdc1941f19e69f4fa7ebd
1.13: cb5eef6d0aac8c9c51fc41c9fd3d5a4b14eb0fd2 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the redundant blocking queue from DeployingTasksBenchmarkBase,FLINK-22037,13368560,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Thesharing,Thesharing,Thesharing,30/Mar/21 07:51,28/Aug/21 11:09,13/Jul/23 08:07,31/Mar/21 01:57,1.13.0,,,,,,,,1.13.0,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,,"We find that the {{BlockingQueue}} that used to preserve {{TaskDeploymentDescriptors}} which are never used later. Since a {{TaskDeploymentDescriptor}} would cost massive heap memory, it may introduce unnecessary garbage collection and make the result of benchmarks related to deployment unstable. Therefore, we think it's better to remove the redundant blocking queue in {{DeployingTasksBenchmarkBase}}.
 ",,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 31 01:57:57 UTC 2021,,,,,,,,,,"0|z0pcww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 01:57;zhuzh;Fixed via b89736b1dd6f350d16529def539f1a9ebac909f1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushFilterIntoLegacyTableSourceScanRule fails to deal with INTERVAL types,FLINK-22021,13368505,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,30/Mar/21 02:46,28/Aug/21 11:15,13/Jul/23 08:07,31/Mar/21 02:16,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Add the following test case to {{PushFilterIntoLegacyTableSourceScanRuleTest}} to reproduce this bug:

{code:scala}
@Test
  def testWithInterval(): Unit = {
    val schema = TableSchema
      .builder()
      .field(""a"", DataTypes.STRING)
      .field(""b"", DataTypes.STRING)
      .build()

    val data = List(Row.of(""2021-03-30 10:00:00"", ""2021-03-30 11:00:00""))
    TestLegacyFilterableTableSource.createTemporaryTable(
      util.tableEnv,
      schema,
      ""MTable"",
      isBounded = true,
      data,
      List(""a"", ""b""))

    util.verifyRelPlan(
      """"""
        |SELECT * FROM MTable
        |WHERE
        |  TIMESTAMPADD(HOUR, 1, TO_TIMESTAMP(a)) >= TO_TIMESTAMP(b)
        |  OR
        |  TIMESTAMPADD(YEAR, 1, TO_TIMESTAMP(b)) >= TO_TIMESTAMP(a)
        |"""""".stripMargin)
  }
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: Data type 'INTERVAL SECOND(3) NOT NULL' with conversion class 'java.time.Duration' does not support a value literal of class 'java.math.BigDecimal'.

	at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:286)
	at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:79)
	at org.apache.flink.table.expressions.ApiExpressionUtils.valueLiteral(ApiExpressionUtils.java:251)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:451)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexLiteral.accept(RexLiteral.java:1173)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:458)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:458)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:458)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:458)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:136)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:135)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:135)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:101)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.pushFilterIntoScan(PushFilterIntoLegacyTableSourceScanRule.scala:90)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.onMatch(PushFilterIntoLegacyTableSourceScanRule.scala:77)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:81)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:304)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:927)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:797)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlan(TableTestBase.scala:402)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.testWithInterval(PushFilterIntoLegacyTableSourceScanRuleTest.scala:188)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}

This is because {{RexNodeExtractor#visitLiteral}} does not deal with interval types.",,jark,libenchao,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 03:05:24 UTC 2021,,,,,,,,,,"0|z0pcko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 02:16;lzljs3620320;master: f22efd321614b783d1e5604bf59b0a19946d9ccc;;;","20/Apr/21 03:05;lzljs3620320;release-1.12: 8c146a5610a6438326789cf57390154e8f749329;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regions may never be scheduled when there are cross-region blocking edges,FLINK-22017,13368374,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Thesharing,Thesharing,Thesharing,29/Mar/21 13:10,14/Dec/21 09:05,13/Jul/23 08:07,15/Jul/21 16:02,1.11.3,1.12.2,1.13.0,,,,,,1.14.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"For the topology with cross-region blocking edges, there are regions that may never be scheduled. The case is illustrated in the figure below.

!Illustration.jpg!

Let's denote the vertices with layer_number. It's clear that the edge connects v2_2 and v3_2 crosses region 1 and region 2. Since region 1 has no blocking edges connected to other regions, it will be scheduled first. When vertex2_2 is finished, PipelinedRegionSchedulingStrategy will trigger {{onExecutionStateChange}} for it.

As expected, region 2 will be scheduled since all its consumed partitions are consumable. But in fact region 2 won't be scheduled, because the result partition of vertex2_2 is not tagged as consumable. Whether it is consumable or not is determined by its IntermediateDataSet.

However, an IntermediateDataSet is consumable if and only if all the producers of its IntermediateResultPartitions are finished. This IntermediateDataSet will never be consumable since vertex2_3 is not scheduled. All in all, this forms a deadlock that a region will never be scheduled because it's not scheduled.

As a solution we should let BLOCKING result partitions be consumable individually. Note that this will result in the scheduling to become execution-vertex-wise instead of stage-wise, with a nice side effect towards better resource utilization. The PipelinedRegionSchedulingStrategy can be simplified along with change to get rid of the correlatedResultPartitions.",,guoyangze,kezhuw,klion26,maguowei,RocMarshal,Thesharing,trohrmann,wanglijie,wangm92,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21734,,,,,,,,,,,FLINK-23833,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/21 12:35;Thesharing;Illustration.jpg;https://issues.apache.org/jira/secure/attachment/13023120/Illustration.jpg",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 15 16:02:36 UTC 2021,,,,,,,,,,"0|z0pbrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/21 10:40;trohrmann;Thanks for reporting this issue [~Thesharing]. I think this is the same issue as FLINK-21734. I will close the other issue because of the very nice illustration here. We should definitely fix this issue.;;;","30/Mar/21 10:42;trohrmann;[~Thesharing], [~zhuzh] do you think that we can fix this issue for the next release?;;;","15/Jul/21 16:02;zhuzh;Fixed via 
d2005268b1eeb0fe928b69c5e56ca54862fbf508
eb8100f7afe1cd2b6fceb55b174de097db752fc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushFilterIntoTableSourceScanRule fails to deal with NULLs,FLINK-22016,13368368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,29/Mar/21 12:29,28/May/21 09:09,13/Jul/23 08:07,17/Apr/21 01:36,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Add the following test case to {{PushFilterIntoTableSourceScanRuleTest}} to reproduce this bug:

{code:java}
@Test
public void myTest() {
    String ddl =
            ""CREATE TABLE MTable (""
                    + ""  a STRING,""
                    + ""  b STRING""
                    + "") WITH (""
                    + ""  'connector' = 'values',""
                    + ""  'bounded' = 'true'""
                    + "")"";
    util().tableEnv().executeSql(ddl);
    util().verifyRelPlan(""WITH MView AS (SELECT CASE\n""
            + ""  WHEN a = b THEN a\n""
            + ""  ELSE CAST(NULL AS STRING)\n""
            + ""  END AS a\n""
            + ""  FROM MTable)\n""
            + ""SELECT a FROM MView WHERE a IS NOT NULL"");
}
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: Data type 'STRING NOT NULL' does not support null values.

	at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:272)
	at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:79)
	at org.apache.flink.table.expressions.ApiExpressionUtils.valueLiteral(ApiExpressionUtils.java:251)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:463)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexLiteral.accept(RexLiteral.java:1173)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:470)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:470)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:138)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:137)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:137)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor.extractConjunctiveConditions(RexNodeExtractor.scala)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.pushFilterIntoScan(PushFilterIntoTableSourceScanRule.java:121)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.onMatch(PushFilterIntoTableSourceScanRule.java:100)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:276)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:889)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlan(TableTestBase.scala:400)
{code}

It seems that this bug is related to commit 957c49d56c80416ae712ae79cdd2784bb2387c80 by [~dwysakowicz]. This commit is a hotfix related to no issue. It adds a {{notNull}} to the return value of RexNodeExtractor#visitLiteral.",,jark,libenchao,TsReaper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 17 01:36:30 UTC 2021,,,,,,,,,,"0|z0pbq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/21 10:03;twalthr;[~TsReaper] The legacy table sources are reaching end-of-life. I'm not sure if we should further investigate time in fixing this issue unless there is a good reason.;;;","31/Mar/21 02:28;TsReaper;[~twalthr] new table sources also have this issue. I've updated the title and the description.;;;","17/Apr/21 01:36;ykt836;fixed: 1562ed02796798ad450b3d4a778974ec086e0419;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL filter containing OR and IS NULL will produce an incorrect result.,FLINK-22015,13368352,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,TsReaper,TsReaper,TsReaper,29/Mar/21 11:12,28/Aug/21 12:16,13/Jul/23 08:07,12/Apr/21 07:51,1.13.0,,,,,,,,1.12.5,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Add the following test case to {{CalcITCase}} to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  checkResult(
    """"""
      |WITH myView AS (SELECT a, CASE
      |  WHEN a = 1 THEN '1'
      |  ELSE CAST(NULL AS STRING)
      |  END AS s
      |FROM SmallTable3)
      |SELECT a FROM myView WHERE s = '2' OR s IS NULL
      |"""""".stripMargin,
    Seq(row(2), row(3)))
}
{code}

However if we remove the {{s = '2'}} the result will be correct.",,jark,jingzhang,leonard,libenchao,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21162,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21162,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 11 10:20:33 UTC 2021,,,,,,,,,,"0|z0pbmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/21 11:47;jark;Maybe the same reason with FLINK-21162.;;;","29/Mar/21 11:58;TsReaper;Indeed. Thanks for pointing this out [~jark].;;;","12/Apr/21 07:51;ykt836;fixed: a3be1cc5ed55d8d93a56fa22d3f3421ae6b65722;;;","11/Jun/21 10:20;ykt836;also fixed in 1.12.5: faf7cc43beebce3fee528ec5637e9387b95bec99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Could not run more than 20 jobs in a native K8s session when K8s HA enabled,FLINK-22006,13368291,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,wangyang0918,wangyang0918,29/Mar/21 06:28,28/May/21 09:00,13/Jul/23 08:07,07/Apr/21 16:45,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Coordination,,,,,0,k8s-ha,pull-request-available,,,,"Currently, if we start a native K8s session cluster when K8s HA enabled, we could not run more than 20 streaming jobs. 

 

The latest job is always initializing, and the previous one is created and waiting to be assigned. It seems that some internal resources have been exhausted, e.g. okhttp thread pool , tcp connections or something else.

!image-2021-03-24-18-08-42-116.png!",,kezhuw,qinjunjerry,trohrmann,wangm92,wangyang0918,xtsong,yittg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22047,,,FLINK-22054,,,,,,,,,,,,,,,,FLINK-21942,,,,,,,,,,"29/Mar/21 06:28;wangyang0918;image-2021-03-24-18-08-42-116.png;https://issues.apache.org/jira/secure/attachment/13023095/image-2021-03-24-18-08-42-116.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 16:45:12 UTC 2021,,,,,,,,,,"0|z0pb94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/21 10:09;yittg;[~fly_in_gis] Did you reproduce the issue?  I think it's easy to reproduce.

And i believe the problem is about the okhttp connectionPool. Fabric8 kubernetes client will create a separate webSocket connection for each config map watching. So after it exhausting all connections in the connection pool, new webSocket request can not be established. Then the new Job can not be submitted successfully.

So my advice is creating a single config map watching and dispatching events for different usages if possible and if we still using the fabric8 kubernetes client.

But before this, i would give more effort to provide more detail to prove it.;;;","29/Mar/21 12:02;wangyang0918;Yes, I could reproduce this issue. And I believe the root cause is fabric8 Kubernetes client has configured the {{MaxRequests}} of {{OkHttpClient#Dispatcher}}[1] to 64[2], which means we could not create more than 64 watchers in the JobManager pod.

Normally, it could be configured in Flink via {{-Denv.java.opts=""-Dkubernetes.max.concurrent.requests=1000""}}. Unfortunately, the fabric8 Kubernetes client version 4.9.2 has a bug[3], which causes the max concurrent requests could not be set via system properties.

 

The fabric8 Kubernetes client 4.10 has introduced too many changes and I do not suggest to bump the dependency in Flink now. Instead, I think we could try to set the max concurrent requests in {{DefaultKubeClientFactory#fromConfiguration}} when building the Kubernetes client.

 

Your suggestion about using a single ConfigMap watch is somewhat reasonable. However, in current HA design, {{DefaultLeaderRetrievalService}} is not aware of all the running jobs. So it is not very easy to dispatch different watcher events to corresponding listeners. It is a big change and we need more discussion.

 
 # [https://github.com/fabric8io/kubernetes-client/blob/master/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/utils/HttpClientUtils.java#L166]
 # [https://github.com/fabric8io/kubernetes-client/blob/master/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/Config.java#L135]
 # [https://github.com/fabric8io/kubernetes-client/issues/2531];;;","29/Mar/21 15:08;yittg;Yeah, there are the two configurations is relative, maxRequests and maxRequestsPerHost. For webSocket, maxRequestsPerHost is ignored[1]. So the actual effective configuration is maxRequests with default value 64.

And here are two small PoCs running with the corresponding dependency versions, and you can only get at most 64 ADDED events or 64 open logs.
{code:java}
package org.example;

import io.fabric8.kubernetes.api.model.ConfigMap;
import io.fabric8.kubernetes.client.*;

public class K {
  public static void main(String[] args) {
      Config config = Config.autoConfigure(null);
//      config.setMaxConcurrentRequests(1000);
//      config.setMaxConcurrentRequestsPerHost(1000);
      final NamespacedKubernetesClient client = new DefaultKubernetesClient(config);

      for (int i = 0; i < 100; i++) {
          client.configMaps().inNamespace(""default"").withName(""abc-"" + i).watch(new Watcher<ConfigMap>() {
              @Override
              public void eventReceived(Action action, ConfigMap configMap) {
                  System.out.println(action + configMap.getMetadata().getName());
              }

              @Override
              public void onClose(KubernetesClientException e) {}
          });
      }
  }
}

{code}
{code:java}
package org.example;

import okhttp3.*;

public class T {
  public static void main(String[] args) {
    OkHttpClient client = new OkHttpClient();
    //    client.dispatcher().setMaxRequests(1000);
    //    client.dispatcher().setMaxRequestsPerHost(1000);
    for (int i = 0; i < 100; i++) {
      String tag = ""open"" + i;
      client.newWebSocket(
          new Request.Builder().url(""http://127.0.0.1:9999"").build(),
          new WebSocketListener() {
            @Override
            public void onOpen(WebSocket webSocket, Response response) {
              super.onOpen(webSocket, response);
              System.out.println(tag);
            }
          });
    }
  }
}
{code}
And i think it's ok to walk around this issue by setting a larger value for maxRequests temporarily.

1. [https://github.com/square/okhttp/blob/parent-3.12.1/okhttp/src/main/java/okhttp3/Dispatcher.java#L196];;;","30/Mar/21 03:14;wangyang0918;Thanks for the confirmation. I will start to work on this by supporting to set max concurrent requests via java opts {{kubernetes.max.concurrent.requests}} in {{DefaultKubeClientFactory}}. After then users could configure this value bigger via {{-Denv.java.opts=""-Dkubernetes.max.concurrent.requests=1000""}} in Flink. 

 

In the future when we bump the fabric8 Kubernetes client to new version(greater than 4.13.0), the parsing java opts logics in {{DefaultKubeClientFactory}} could be removed. But it is also harmless to keep them.;;;","30/Mar/21 10:28;trohrmann;+1 for the quick fix and for fixing this problem properly by using a shared ConfigMap watcher in the next release.

Does the Fabric8 client supports to watch multiple ConfigMaps using a single connection? Hopefully, I would assume that the change from Flink's perspective is not too large. The {{DefaultLeaderRetrievalService}} share the same {{FlinkKubeClient}} which could run the single ConfigMap watcher if we can add new ConfigMaps to watch lazily.;;;","30/Mar/21 11:00;yittg;[~trohrmann] Roughly speaking, yes. I think we can just replace the implementation of watchConfigMap in Fabric8FlinkKubeClient.But there are still some details to consider.
I'm trying to figure out a RIGHT implementation demo. If it's ok, I will create an improvement ticket for it. Then we can make it happen, :);;;","30/Mar/21 11:46;wangyang0918;Yes, Fabric8 Kubernetes client supports to watch multiple ConfigMaps using a single connection. Actually, we are watching all the TaskManager pods in a same way.

For the implementation, wrapping {{Fabric8FlinkKubeClient#watchConfigMaps}} with a dispatcher and single connection is a good solution. Then the caller will not care about the implementation details and we do not need any changes to the {{DefaultLeaderRetrievalService}} and {{KubernetesLeaderRetrievalDriver}}.;;;","31/Mar/21 03:03;yittg;The watching for configMap is a little different from that for pods. The watchings for configMap will start and go after the watching starts. I think we should do something to keep the same semantic with watching separately.

Here [FLINK-22054] i create another issue to track the sharded ConfigMap watcher proposal.;;;","01/Apr/21 16:30;trohrmann;[~fly_in_gis] do you think that you have time for fixing this issue for the 1.13.0 release?;;;","02/Apr/21 03:47;wangyang0918;[~trohrmann] I will attach a PR for this ticket today.;;;","07/Apr/21 16:45;trohrmann;Fixed via

master:
ec677a188e9239ca80a723206dbbd807789731e7
0eed15de872768cd2ec4a7a3384e49f7bbaeb483

1.12.3.:
aef2392a591edc63b2c1734044cc1a43628a43b1
4ea99332e1997eebca1f3f0a9d9229b8265fe32c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1) ,FLINK-22005,13368273,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,maguowei,maguowei,29/Mar/21 03:40,12/Apr/21 10:28,13/Jul/23 08:07,29/Mar/21 04:24,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,test-stability,,,,,"The test fail because of Waiting for Elasticsearch records indefinitely.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15583&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=19826
",,dwysakowicz,leonard,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 10:28:57 UTC 2021,,,,,,,,,,"0|z0pb54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/21 03:44;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15583&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=19760;;;","29/Mar/21 03:47;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15589&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=58087;;;","29/Mar/21 03:49;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15580&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=48499;;;","29/Mar/21 04:13;leonard;[~maguowei] Could you rebase to latest master, this issue has  been fixed in [https://github.com/apache/flink/pull/15394#event-4516849115];;;","29/Mar/21 04:22;maguowei;thanks [~Leonard Xu]. I find that this does not appear in the following test(28/29). So I close this tickets.;;;","12/Apr/21 10:28;dwysakowicz;fix in the master c375f4cfd394c10b54110eac446873055b716b89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase fail,FLINK-22003,13368267,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,maguowei,maguowei,29/Mar/21 03:10,22/Jun/21 13:55,13/Jul/23 08:07,06/Apr/21 16:00,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15601&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc&l=4142


{code:java}
[ERROR] execute[parallel pipeline with remote channels, p = 5](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 60.018 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 60000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1859)
	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:69)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1839)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1822)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:138)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:184)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
",,AHeise,gaoyunhaii,guoyangze,maguowei,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22087,,,,,,,,,,,FLINK-22087,FLINK-22088,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 06 16:00:08 UTC 2021,,,,,,,,,,"0|z0pb3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 12:03;arvid;This one is really strange:

Usually, when we trigger a checkpoint, we see some actions on the source
{noformat}
22:24:51,290 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1616883891289 for job bb1c23aad807944d0a54775098106574.
22:24:51,290 [SourceCoordinator-Source: source] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase [] - snapshotState EnumeratorState{unassignedSplits=[], numRestarts=0, numCompletedCheckpoints=0}
22:24:51,291 [Flink Netty Server (0) Thread 0] TRACE org.apache.flink.runtime.io.network.logger.NetworkActionsLogger [] - [Source: source (2/5)#0 (8f1ca6eb04b6e2341c658cc0b1ac7c6c)] PipelinedSubpartition#pollBuffer Buffer{size=38, hash=924008396} @ ResultSubpartitionInfo{partitionIdx=0, subPartitionIdx=0}
{noformat}

In this case, after checkpoint 11 is triggered nothing happens. 

{noformat}
22:24:54,694 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 11 (type=CHECKPOINT) @ 1616883893885 for job bb1c23aad807944d0a54775098106574.
22:24:54,694 [ failing-map (5/5)#4] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - failing-map (5/5)#4 (7c0e288b2cd57831596d58e8ce31e435) switched from CREATED to DEPLOYING.
{noformat}

Actually, it should have been canceled, as obviously not all tasks are running similar to

{noformat}
22:24:51,044 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job bb1c23aad807944d0a54775098106574.)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Source: source (1/5) of job bb1c23aad807944d0a54775098106574 has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
{noformat}

I'm currently assuming that there is a race condition in the code of FLINK-21067.;;;","31/Mar/21 16:09;roman;I think that checkpoint 11 should not be aborted. A checkpoint (triggering) is aborted if: 
 * any of the tasks are not even CREATED (executionAttemp == null on JM) - not the case here
 * any of triggering tasks (sources) hasn't been started - but failing-map is not the source

But probably the issue is in the network/task - if non-source task in CREATED/DEPLOYING state doesn't reject the checkpoint.

 

After increasing test timeout from 1 to 5 minutes I don't see this issue on AWS (haven't parsed all the logs yet).

 

And probably we should decrease checkpoint timeout in UC IT case settings? IIUC, the default is currently used, which is 10 minutes.

WDYT?;;;","31/Mar/21 17:12;arvid;In general, I have seen long recovery times in these tests from time to time, so it's quite possible that it's indeed just a test-issue. I have wrongly assumed that all tasks need to be running for the checkpoints to be triggered.

I already removed the timeout - it's rather arbitrary and we should never add it to master. I'm also positive that FLINK-17012 would alleviate a bit of the pain if we abort all checkpoints if the source is still in recovery.;;;","01/Apr/21 05:51;gaoyunhaii;Hi, I think [~roman_khachatryan] is exactly right for the conditions of successfully triggering, but I read the log and found there might be still some issues since the source task 3/5, 4/5 and 5/5 are transited to RUNNING in JM side after the checkpoint 11 is triggered. 

After analyzed the log, I think it is very likely due to that when computing the tasks to trigger, the failover is still not done yet, thus some sources might acquired and checked against their prior execution. And it seems FLINK-21067 indeed has problem in that it ignores the tasks in CANCELING/CANCELED state, which is different from the prior logic. If this case happens, the prior execution would fail triggering and cause the checkpoint would have to wait for timeout to be canceled. Very sorry for introducing the bug and I'll open a PR for it today to change the condition to only allows for RUNNING status (Although we would also trigger checkpoint after tasks finished, but the finished tasks should not be in the list of tasks to trigger). 

But it also comes to me that there _might be_ another case: when we compute tasks to trigger and checking status, the failover might not happen yet, then all the tasks are running and then the checkpoint would be triggered. The triggering would also fail due to the prior execution is gone and cause the checkpoint to wait for timeout. This behavior should already exists before  FLINK-21067 if the case did exist. I'll have a double confirmation about this case.;;;","01/Apr/21 10:21;gaoyunhaii;I checked the case of tasks are all running when computes tasks to trigger, but tasks are failover during actually trigger, the current implementation has considered the case, but there might be some corner case that the checkpoint could still get actually triggered: [FLINK-22088|https://issues.apache.org/jira/browse/FLINK-22088].

However, even if we fixed the above two issues, there might still be cases that if failover happens during checkpoint triggering, the trigger still get success but the execution get gone before trigger success. For example, the failover might happen right before the checkpoint coordinator going to trigger the source tasks, and when the trigger happen the tasks get gone. In this case, the checkpoint would still have to wait for timeout, which might cause the test fail.  Therefore, I think we may still need to set timeout explicitly for this test. ;;;","01/Apr/21 13:41;roman;Thanks for your analysis [~gaoyunhaii], I completely agree with it.

Would you like to provide a fix? (I can also do that).;;;","01/Apr/21 14:38;gaoyunhaii;Yes, no problem, I'll open the PR soon today.;;;","01/Apr/21 14:47;roman;[~gaoyunhaii], I went ahead and opened a [PR|https://github.com/apache/flink/pull/15466] too :) Please feel free to open yours and I'll close mine.;;;","01/Apr/21 15:03;gaoyunhaii;Hi [~roman_khachatryan] Very thanks for fixing the issue! I think we do not need to repeat the work as long as we have already have one fix, and I could also help with the review~ :D;;;","06/Apr/21 16:00;roman;Merged into master as a40dfe0438afe04d1ac68c30bb8d238be45232cf..9cb161005861365b9dadfd016fcc321475c3d890.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions from JobMaster initialization are not forwarded to the user,FLINK-22001,13368227,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,rmetzger,rmetzger,28/Mar/21 18:33,22/Jun/21 13:55,13/Jul/23 08:07,22/Apr/21 06:57,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Steps to reproduce:
Set up a streaming job with an invalid parallelism configuration, for example:
{code}
.setParallelism(15).setMaxParallelism(1);
{code}

This should report the following exception to the user:
{code}
Caused by: org.apache.flink.runtime.JobException: Vertex Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction)'s parallelism (15) is higher than the max parallelism (1). Please lower the parallelism or increase the max parallelism.
		at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:160)
		at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:781)
		at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:193)
		at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:106)
		at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:252)
		at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:185)
		at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:119)
		at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132)
		at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110)
		at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340)
		at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317)
		at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:94)
		at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:39)
		at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.startJobMasterServiceSafely(JobManagerRunnerImpl.java:363)
		... 13 more
{code}

However, what the user sees is 
{code}
2021-03-28 20:32:33,935 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 419f60eac551619fc1081c670ced3649 reached globally terminal state FAILED.

...

2021-03-28 20:32:33,974 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
2021-03-28 20:32:33,977 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
Exception in thread ""main"" org.apache.flink.util.FlinkException: Failed to execute job 'CarTopSpeedWindowingExample'.
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1975)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1853)
	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:69)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1839)
	at org.apache.flink.streaming.examples.windowing.TopSpeedWindowing.main(TopSpeedWindowing.java:101)
Caused by: java.lang.RuntimeException: Error while waiting for job to be initialized
	at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:160)
	at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$submitJob$2(PerJobMiniClusterFactory.java:83)
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: JobMaster has been shut down.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$null$0(PerJobMiniClusterFactory.java:89)
	at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:144)
	... 9 more
Caused by: org.apache.flink.util.FlinkException: JobMaster has been shut down.
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.closeAsync(JobManagerRunnerImpl.java:197)
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995)
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137)
	at org.apache.flink.runtime.dispatcher.DispatcherJob.lambda$closeAsync$8(DispatcherJob.java:273)
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
	at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:848)
	at java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2168)
	at org.apache.flink.runtime.dispatcher.DispatcherJob.closeAsync(DispatcherJob.java:268)
	at org.apache.flink.runtime.dispatcher.Dispatcher.removeJob(Dispatcher.java:754)
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$runJob$4(Dispatcher.java:432)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:814)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-03-28 20:32:34,011 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
{code}

",,AHeise,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 06:57:10 UTC 2021,,,,,,,,,,"0|z0pauw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/21 12:17;arvid;Another case here https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15427&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=9343;;;","22/Apr/21 06:57;trohrmann;Fixed via

master: 75d27d183a0f489e7e7cf6144cda4966ceb46ce1
1.13.0: ca9595317ba984120da00bc284c6109d0b4655d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient RPC failure without TaskManager failure can lead to split assignment loss,FLINK-21996,13367996,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,26/Mar/21 20:06,08/Jun/22 02:06,13/Jul/23 08:07,21/Apr/21 14:36,1.12.2,,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"NOTE: This bug has not been actually observed. It is based on reviews of the current implementation.
I would expect it to be a pretty rare case, bu at scale, even the rare cases happen often enough.

h2. Problem

Intermediate RPC messages from JM to TM can get dropped, even when the TM is not marked as failed.
That can happen when the connection can be recovered before the heartbeat times out.

So RPCs generally retry, or handle failures: For example Deploy-Task-RPC retries, Trigger-Checkpoint RPC aborts the checkpoint on failure and triggers a new checkpoint.

The ""Send OperatorEvent"" RPC call (from Coordinator to Operator) gives you a Future with the acknowledgement. But if that one fails, we are in the situation where we do not know whether the event sending was successful or not (only the ack failed).

This is especially tricky for split assignments and checkpoints. Consider this sequence of actions:
  1. Coordinator assigns a split. Ack not yet received.
  2. Coordinator takes a checkpoint. Split was sent before the checkpoint, so is not included on the Coordinator.
  3. Split assignment RPC response is ""failed"".
  4. Checkpoint completes.

Now we don't know whether the split was in the checkpoint on the Operator (TaskManager) or not, and with that we don't know whether we should add it back to the coordinator. We need to do something to make sure the split is now either on the coordinator or on the Operator. Currently, the split is implicitly assumed to be on the Operator; if it isn't, then that split is lost.

Not, it is worth pointing out that this is a pretty rare situation, because it means that the RPC with the split assignment fails and the one for the checkpoint succeeds, even though they are in close proximity. The way the Akka-based RPC transport works (with retries, etc.), this can happen, but isn't very likely. That why we haven't so far seen this bug in practice or haven't gotten a report for it, yet.


h2. Proposed solution

The solution has two components:

  1. Fallback to consistent point: If the system doesn't know whether two parts are still consistent with each other (here coordinator and Operator), fall back to a consistent point. Here that is the case when the Ack-Future for the ""Send Operator Event"" RPC fails or times out. Then we call the scheduler to trigger a failover of the target operator to latest checkpoint and signaling the coordinator the same. That restores consistency. We can later optimize this (see below).

  2. We cannot trigger checkpoints while we are ""in limbo"" concerning our knowledge about splits. Concretely that means that the Coordinator can only acknowledge the checkpoint once the Acks for pending Operator Event RPCs (Assign-Splits) have arrived. The checkpoint future is conditional on all pending RPC futures. If the RPC futures fail (or time out) then the checkpoint cannot complete (and the target operator will anyways go through a failover). In the common case, RPC round trip time is milliseconds, which would be added to the checkpoint latency if the checkpoint happends to overlap with a split assignment (most won't).


h2. Possible Future Improvements

Step (1) above can be optimized by going with retries first and sequence numbers to deduplicate the calls. That can help reduce the number of cases were a failover is needed. However, the number of situations where the RPC would need a retry and has a chance of succeeding (the TM is not down) should be very few to begin with, so whether this optimization is worth it remains to be seen.",,dwysakowicz,JohnTeslaa,kevin.cyj,kezhuw,leonard,maguowei,mapohl,mason6345,pnowojski,renqs,sewen,trohrmann,ym,,,,,,,,,,,,,,FLINK-18071,,,,,,,,,,,,,FLINK-22129,FLINK-20254,FLINK-22100,,,FLINK-23233,FLINK-22417,FLINK-22420,FLINK-22129,,,,,,,,,,,,,,,,FLINK-22129,FLINK-20254,FLINK-22397,FLINK-22415,FLINK-22345,FLINK-22420,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 14:35:57 UTC 2021,,,,,,,,,,"0|z0p9g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 20:07;sewen;[~trohrmann] What do you think of the proposed solution? 
I have played around with the code a bit, the implementation is pretty simple. The harder part is setting up tests for this.;;;","29/Mar/21 15:01;trohrmann;I think your solution proposal for the problem looks good to me. Also to exclude the optimization aspect for the time being since we don't know how much it would actually bring in the end.;;;","10/Apr/21 19:55;kezhuw;Is there any design guarantee for no source operator coordinator in checkpointing [~sewen] ? I did not see one from docs, nor exclusion either.

For no source operator coordinator, it is possible  that events sending before checkpoint arrives after checkpoint in case of all good. A relative long latency between sending and arriving could make checkpoint from upstream arrives earlier than events. I constructed a [test case|https://github.com/kezhuw/flink/commit/b3aeb30298ace48c8ea11be488281d6eba8930b1#diff-7007c5266035493493e3a1476d98442bf50a7f023b801b48c6b6dc15436c9499R657] for demonstration.;;;","12/Apr/21 10:34;sewen;[~kezhuw] I think the guarantees that we need on the Task and TaskManager are that all events are delivered into the Mailbox in order. So the OperatorEvent (assignSplits) and the Checkpoint Trigger Event may not overtake each other.

The TM receives them through the same RPC endpoint (TaskExecutor) so they are in order. 
Let's double check that this order isn't messed up in the Task and StreamTask. ;;;","12/Apr/21 10:57;kezhuw;[~sewen] Taking no source tasks into account, these guarantees are not enough. There is no checkpoint trigger for no source tasks. So, basically, there is no happens-before between checkpoint and operator event in no source tasks.;;;","12/Apr/21 11:12;kezhuw;bq. We cannot trigger checkpoints while we are ""in limbo"" concerning our knowledge about splits. Concretely that means that the Coordinator can only acknowledge the checkpoint once the Acks for pending Operator Event RPCs (Assign-Splits) have arrived.

Extending this to no source tasks and all events should solve all cases.;;;","12/Apr/21 12:17;sewen;[~kezhuw] Yes, for operators that are not sources, there are no guarantees about the order of sent events and checkpoints. I think for now it is okay to leave it at that, because the only case where we need this for now are sources. Let's think about extending this in the future when we see cases for this.;;;","16/Apr/21 22:45;sewen;Fixed in 1.13.0 via
  - de509fc2439c2cacb02c9caa40168b3765e84ceb
  - 904536271081f393a8c29341831275080b04255a;;;","20/Apr/21 12:11;sewen;Fixed in 1.12.3 via
  - da48ac8b0f2b01b6048993cd2d1f8d267fffc0ae
  - 19b5a1b234aad91e9747c85e1cabcffc1de0817e;;;","21/Apr/21 10:23;mapohl;[~sewen]: we have a [build failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16923&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=12543] in a build that includes the fixes of this issue. Shall we reopen this issue? The changes introduced in the corresponding branch aren't related as it contains a basic fix casting int to long affecting only the network memory metrics (FLINK-22385).;;;","21/Apr/21 11:43;sewen;I see. The issue here is that an ask timeout on the split assignment now leads to a failover, and some test setups do not allow any failovers.

We could adjust those tests to allow for failovers, unless something in the tests forbids this.;;;","21/Apr/21 13:51;mapohl;ok, I'm gonna re-open the issue to cover the test failure...;;;","21/Apr/21 14:12;trohrmann;cc [~dwysakowicz] fyi;;;","21/Apr/21 14:35;dwysakowicz;Sorry, I should've posted that I am aware of the issue. I actually synced offline and created FLINK-22397 to track the test failure. Let's keep this issue closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix availability notification in UnionInputGate,FLINK-21992,13367824,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,arvid,AHeise,26/Mar/21 10:24,22/Jun/21 14:07,13/Jul/23 08:07,14/Apr/21 19:28,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"A user on mailing list reported that his job gets stuck with unaligned checkpoint enabled.
http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Source-Operators-Stuck-in-the-requestBufferBuilderBlocking-tt42530.html

We received two similar reports in the past, but the users didn't follow up, so it was not as easy to diagnose as this time where the initial report already contains many relevant data points. 

Beside a buffer leak, there could also be an issue with priority notification.",,AHeise,bupt_ljy,dwysakowicz,gary.wu,guoyangze,kevin.cyj,maguowei,nkruber,trohrmann,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 19:28:40 UTC 2021,,,,,,,,,,"0|z0p8eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/21 07:28;arvid;It turns out that there is an issue with notification. We managed to reliable reproduce it with:
* Unaligned checkpoints with
* Unions going into
* Two input tasks.

The root cause is a bug in {{UnionInputGate}} introduced in FLINK-19026. The available notification of {{UnionInputGate}} is simply reset too early, leading to stuck tasks.

The bug can probably also be triggered with single input tasks but there are certain factors that rectify the bug: If you drain a union gate entirely without looking at availability after the first buffer, the bug would not be visible. Since we hot-loop at plenty of places until running out of data, it might be that just the combination of the three things actually makes it visible.;;;","13/Apr/21 07:48;dwysakowicz;Do we target that for 1.13?;;;","14/Apr/21 02:33;maguowei;We also see the same stack in our internal tests without unaligned checkpoint enabled.

{code:java}
[stat_date, cate_id, user_id, biz, MIN(visit_time_int) FILTER $f5 AS min$0, MIN(visit_time_int) FILTER $f6 AS min$1, MIN(visit_time_int) FILTER $f7 AS min$2, MIN(visit_time_int) FILTER $f8 AS min$3, MIN(visit_time_int) FILTER $f9 AS min$4]))) (68/256)#0"" Id=101 WAITING on java.util.concurrent.CompletableFuture$Signaller@2e0b208c
at sun.misc.Unsafe.park(Native Method)
-  waiting on java.util.concurrent.CompletableFuture$Signaller@2e0b208c
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:189)
at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1693)
at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)
at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:319)
at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:291)
...
{code}
;;;","14/Apr/21 15:35;arvid;Merged into master as 7c3abe11a28d54a585985ef908f36d8cf5857e14.;;;","14/Apr/21 19:28;arvid;Merged into 1.12 as e2cbfad6a3cdafe3d568bb43a1d048f9533b29ec.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceStreamTask will always hang if the CheckpointedFunction#snapshotState throws an exception.,FLINK-21990,13367813,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,Ming Li,Ming Li,26/Mar/21 09:26,28/Aug/21 11:14,13/Jul/23 08:07,16/Apr/21 06:22,1.11.0,1.12.0,,,,,,,1.13.0,,,,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,,"If the source in {{SourceStreamTask}} implements {{CheckpointedFunction}} and an exception is thrown in the snapshotState method, then the {{SourceStreamTask}} will always hang.

The main reason is that the checkpoint is executed in the mailbox. When the {{CheckpointedFunction#snapshotState}}  of the source throws an exception, the StreamTask#cleanUpInvoke will be called, where it will wait for the end of the {{LegacySourceFunctionThread}} of the source. However, the source thread does not end by itself (this requires the user to control it), the {{Task}} will hang at this time, and the JobMaster has no perception of this behavior.
{code:java}
protected void cleanUpInvoke() throws Exception {
    getCompletionFuture().exceptionally(unused -> null).join(); //wait for the end of the source
    // clean up everything we initialized
    isRunning = false;
    ...
}{code}
I think we should call the cancel method of the source first, and then wait for the end.

The following is my test code, the test branch is Flink's master branch.
{code:java}
@Test
public void testSourceFailure() throws Exception {
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.enableCheckpointing(2000L);
    env.setRestartStrategy(RestartStrategies.noRestart());
    env.addSource(new FailedSource()).addSink(new DiscardingSink<>());
    JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph());
    try {
        // assert that the job only execute checkpoint once and only failed once.
        TestUtils.submitJobAndWaitForResult(
                cluster.getClusterClient(), jobGraph, getClass().getClassLoader());
    } catch (JobExecutionException jobException) {
        Optional<FlinkRuntimeException> throwable =
                ExceptionUtils.findThrowable(jobException, FlinkRuntimeException.class);
        Assert.assertTrue(throwable.isPresent());
        Assert.assertEquals(
                CheckpointFailureManager.EXCEEDED_CHECKPOINT_TOLERABLE_FAILURE_MESSAGE,
                throwable.get().getMessage());
    }
    // assert that the job only failed once.
    Assert.assertEquals(1, StringGeneratingSourceFunction.INITIALIZE_TIMES.get());
}

private static class FailedSource extends RichParallelSourceFunction<String>
        implements CheckpointedFunction {

    private transient boolean running;

    @Override
    public void open(Configuration parameters) throws Exception {
        running = true;
    }

    @Override
    public void run(SourceContext<String> ctx) throws Exception {
        while (running) {
            ctx.collect(""test"");
        }
    }

    @Override
    public void cancel() {
        running = false;
    }

    @Override
    public void snapshotState(FunctionSnapshotContext context) throws Exception {
        throw new RuntimeException(""source failed"");
    }

    @Override
    public void initializeState(FunctionInitializationContext context) throws Exception {}
}
{code}",,bupt_ljy,Feifan Wang,kezhuw,Ming Li,pnowojski,roman,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 16 06:22:00 UTC 2021,,,,,,,,,,"0|z0p8c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/21 02:57;wind_ljy;[~pnowojski] We've encountered the problem when upgrading Flink version from 1.9 to 1.11. This seems to be a very serious problem and can be reproduced with the test case above. Could you spare some time and take a look? ;;;","29/Mar/21 03:52;kezhuw;Ideally, this should be fixed in [1.12.2|https://github.com/apache/flink/blob/release-1.12.2/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java#L175] and [master|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java#L181]. But I do run to hang in test case(modified version). Either that run does not touch blocking operations or interruption state was eaten up somewhere(FLINK-21186 ?) I think that fix worth a test to gain confidence on this.

cc [~AHeise];;;","29/Mar/21 04:06;kezhuw;A {{disableChaining}} in between failed source and downstream sink passes the test case. But still hang for a while before run into blocking operations. Seems that simple {{Thread.interrupt}} is not that enough, a cooperative {{SourceFunction.cancel}} should help.;;;","16/Apr/21 06:22;roman;Merged into master 345bf34b858d3c9014559dfab5a4e0dea5b0832d.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
taskmanager native memory not release timely after restart,FLINK-21986,13367764,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,26/Mar/21 03:59,10/Oct/21 19:10,13/Jul/23 08:07,20/Apr/21 08:33,1.11.3,1.12.1,1.13.0,,,,,,1.11.4,1.12.3,1.13.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,,"I run a regular join job with flink_1.12.1 , and find taskmanager native memory not release timely after restart cause by exceeded checkpoint tolerable failure threshold.

*problem job information：*
 # job first restart cause by exceeded checkpoint tolerable failure threshold.
 # then taskmanager be killed by yarn many times
 # in this case，tm heap is set to 7.68G，bug all tm heap size is under 4.2G
 !image-2021-03-25-15-53-44-214.png|width=496,height=103!
 # nonheap size increase after restart，but still under 160M.
 !https://km.sankuai.com/api/file/cdn/706284607/716474606?contentType=1&isNewContent=false&isNewContent=false|width=493,height=102!
 # taskmanager process memory increase 3-4G after restart（this figure show one of taskmanager）
 !image-2021-03-25-16-07-29-083.png|width=493,height=107!

 

*my guess：*

[RocksDB wiki|https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#memory-management] mentioned ：Many of the Java Objects used in the RocksJava API will be backed by C++ objects for which the Java Objects have ownership. As C++ has no notion of automatic garbage collection for its heap in the way that Java does, we must explicitly free the memory used by the C++ objects when we are finished with them.

So, is it possible that RocksDBStateBackend not call AbstractNativeReference#close() to release memory use by RocksDB C++ Object ?

*I make a change:*

        Actively call System.gc() and System.runFinalization() every minute.

 *And run this test again:*
 # taskmanager process memory no obvious increase
 !image-2021-03-26-11-46-06-828.png|width=495,height=93!
 # job run for several days，and restart many times，but no taskmanager killed by yarn like before

 

*Summary：*
 # first，there is some native memory can not release timely after restart in this situation
 # I guess it maybe RocksDB C++ object，but I hive not check it from source code of RocksDBStateBackend

 ","flink version：1.12.1
run ：yarn session
job type：mock source -> regular join
 
checkpoint interval: 3m
Taskmanager memory : 16G
 ",clouding,Feifan Wang,hackergin,jackwangcs,klion26,libenchao,liyu,sap1ens,trohrmann,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/21 08:45;Feifan Wang;82544.svg;https://issues.apache.org/jira/secure/attachment/13023223/82544.svg","25/Mar/21 07:53;Feifan Wang;image-2021-03-25-15-53-44-214.png;https://issues.apache.org/jira/secure/attachment/13022982/image-2021-03-25-15-53-44-214.png","25/Mar/21 08:07;Feifan Wang;image-2021-03-25-16-07-29-083.png;https://issues.apache.org/jira/secure/attachment/13022981/image-2021-03-25-16-07-29-083.png","26/Mar/21 03:46;Feifan Wang;image-2021-03-26-11-46-06-828.png;https://issues.apache.org/jira/secure/attachment/13022980/image-2021-03-26-11-46-06-828.png","26/Mar/21 03:47;Feifan Wang;image-2021-03-26-11-47-21-388.png;https://issues.apache.org/jira/secure/attachment/13022979/image-2021-03-26-11-47-21-388.png",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 10 19:10:42 UTC 2021,,,,,,,,,,"0|z0p81c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 06:58;yunta;Hi [~Feifan Wang], Flink community has ever come across similar problem in FLINK-18712 and solved by FLINK-19125 for released docker image, could you try to follow such steps to see whether still problem existed?;;;","26/Mar/21 07:26;Feifan Wang;Hi [~yunta], I hive read -FLINK-18712- and FLINK-19125  , and run two test job to verify that problem before run the test that actively call System.gc() and System.runFinalization().
 # first one, set MALLOC_ARENA_MAX=1
 In view of configure containerized.taskmanager.env.MALLOC_ARENA_MAX=1 in flink-conf.yaml not worker in yarn (maybe yarn's configure override it), I set this env variable in yarn.container-start-command-template , and confirmed  it work ( by check /proc/${tm_pid}/environ).
 # second one, use jemalloc instead of glibc to run taskmanager

But, this two test job still has this problem. After that , I run the test actively call System.gc() and System.runFinalization().;;;","26/Mar/21 11:29;yunta;[~Feifan Wang], have you confirmed that the second job using jemalloc allocator, you could use {{pmap}} to see whether jemalloc {{.so}} file is loaded in your process.

BTW, since all rocks objects have been disposed when rocksDB keyed state backend is disposed, I doubted whether System#gc() and System#runFinalization() could help it. Could you make your case as a simple example so that it could be reproduced just like what FLINK-18712 did.;;;","26/Mar/21 12:19;Feifan Wang;[~yunta], I confirmed second test job use jemalloc allocator by {color:#0747a6}_lsof -p ${taskmanager_pid} | grep 'jemalloc'_ {color:#172b4d}. And pmap show difference memony usage between problem job and those test jobs.{color}{color}
 * in problem job, there are many 64M memory blocks
 * in first test job with MALLOC_ARENA_MAX=1, no such many 64M blocks, but a huge memory block amost 12G
 * in second test job which use jemalloc,  no such many 64M blocks too, and some 1G-2G blocks

All above there jobs has a huge memory block with approximate jvm heap size (7.68G).;;;","26/Mar/21 12:41;yunta;[~Feifan Wang], have you ever used [jemalloc|https://github.com/jemalloc/jemalloc/wiki/Use-Case%3A-Heap-Profiling] or [tcmalloc|https://gperftools.github.io/gperftools/heapprofile.html] to help memory profiling? I think they're the final two methods to see what caused the memory leak. You might need to rebuild jemalloc if truing on {{--enable-prof}}.;;;","26/Mar/21 12:45;Feifan Wang;[~yunta], this problem seems like some garbage java object maintain native memory until be cleaned up by garbage collector. But heap memory usage is always below 60% in those job, so no full gc occured. Third test job avoid memory usage exceed by call System#gc() and System#runFinalization() , it proves some native memory can be release along with java object cleaned up. It very like situation described in [RocksJava memory-management|https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#memory-management].

 

My test job is a little complex, I will simplify it later.
 ;;;","26/Mar/21 12:49;Feifan Wang;[~yunta], I haven't tried jemalloc or tcmalloc memory profiling, this sounds a good idea. I will learn how to use it and try out.
 ;;;","31/Mar/21 08:51;Feifan Wang;[~yunta], I have re run second test job which use jemalloc and truing on --enable-prof, and jeprof shows most memory is used by rocksdb::UncompressBlockContentsForCompressionType.
{code:java}
Total: 12594.6 MB
 10725.4  85.2%  85.2%  10725.4  85.2% rocksdb::UncompressBlockContentsForCompressionType
  1226.3   9.7%  94.9%   1226.3   9.7% je_prof_backtrace
   220.0   1.7%  96.6%    249.4   2.0% rocksdb::LRUCacheShard::Insert
   184.0   1.5%  98.1%    184.0   1.5% rocksdb::Arena::AllocateNewBlock
   149.5   1.2%  99.3%  10874.9  86.3% rocksdb::BlockBasedTable::PartitionedIndexIteratorState::NewSecondaryIterator
    48.0   0.4%  99.7%     48.0   0.4% init
    29.4   0.2%  99.9%     29.4   0.2% rocksdb::LRUHandleTable::Resize
     3.6   0.0%  99.9%      4.1   0.0% readCEN
     3.1   0.0% 100.0%      3.1   0.0% std::string::_Rep::_S_create
     1.6   0.0% 100.0%      1.6   0.0% rocksdb::WritableFileWriter::WritableFileWriter
{code}
Here is svg file produce by jeprof [^82544.svg] .

It's not as expected. Managed Memory is set to 5.9G, but rocksdb used 10.5G.;;;","01/Apr/21 06:58;yunta;[~Feifan Wang] I think this is really weird after looking at your prof svg. Could you make your example as public so that I could run the job by myselfy.

If it's not easy to extract the logic, could you please provided the RocksDB LOG (need to set DEBUG level, and you could set it in Flink-1.13 https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/config/#state-backend-rocksdb-log-level ).;;;","02/Apr/21 03:33;Feifan Wang;Hi [~yunta], I simplify test code and put it to [flink-21986-regular-join-test-case|https://github.com/zoltar9264/flink-21986-regular-join-test-case] , you can run the test job from it. Notice, some configuration in top.zoltar.flink.issue.config.ConfigConstant, you can modify it.;;;","14/Apr/21 18:44;Feifan Wang;Hi [~yunta], I confirm that I found the cause of this problem :

        org.apache.flink.contrib.streaming.state.RocksDBOperationUtils#addColumnFamilyOptionsToCloseLater() {color:#FF0000}called ColumnFamilyHandle#getDescriptor() twice by mistake{color}。

 
{code:java}
public static void addColumnFamilyOptionsToCloseLater(
        List<ColumnFamilyOptions> columnFamilyOptions, ColumnFamilyHandle columnFamilyHandle) {
    try {
        if (columnFamilyHandle != null && columnFamilyHandle.getDescriptor() != null) {
            columnFamilyOptions.add(columnFamilyHandle.getDescriptor().getOptions());
        }
    } catch (RocksDBException e) {
        // ignore
    }
}
{code}
As every time call ColumnFamilyHandle#getDescriptor() create a new ColumnFamilyDescriptor which hold a new ColumnFamilyOptions instance, RocksDBOperationUtils#addColumnFamilyOptionsToCloseLater create two ColumnFamilyOptions instances actually，but only second be add to ""closeLater list"". As a result, some ColumnFamilyOptions instances has not been closed.

I modified this method that only call ColumnFamilyHandle#getDescriptor() once and run the test job again which result shows the modification works.

 

Please assign this issue to me, I will open a pull request to fix this problem, thanks.

 ;;;","15/Apr/21 07:47;Feifan Wang;Hi [~yunta],  I opened a pull request to fix this problem, but flinkbot prompted ""{color:#de350b}This pull request references an unassigned Jira ticket{color}"".

So please assign this issue to me, thanks. ;;;","18/Apr/21 09:22;Feifan Wang;Hi [~dwysakowicz], sorry for disturbance, can you help me review this problem, and assign this issue to me ？;;;","20/Apr/21 07:54;trohrmann;Thanks for reporting this issue and analyzing it [~Feifan Wang]. From what you describe I believe that we should fix this problem for the upcoming release. cc [~dwysakowicz].

[~yunta] could you help reviewing this change? If not then we should find somebody else taking a look at the PR.;;;","20/Apr/21 07:59;trohrmann;The change is so simple that I can review and merge it. I will take care of it.;;;","20/Apr/21 08:07;yunta;Sorry for late reply and glad to see you finally figured out the root cause [~Feifan Wang], great work! ;;;","20/Apr/21 08:32;yunta;Unlike previous RocksDB memory related problem, the created {{ColumnFamilyDescriptor}} is just a java object instead of RocksObject which holds no native reference, that might explain why force GC could help avoid such behavior. Current PR could avoid one time to create {{ColumnFamilyDescriptor}}, however, I wonder why another more time to create could cause memory leak. Maybe we need some other efforts to dig into this problem.;;;","20/Apr/21 08:33;trohrmann;Fixed via

master: 57ba2ff64add32df2598e1e35ee19a76eff8194c
1.13.0: 0e1468a4aadf68019f034eaad8bbf50a8ecf9589
1.12.3: 2cd2fecc121266109f9635a24ca956022b8ab283
1.11.4: 4dfef1b2e079a0f0e07aec994f721adf27a47b9a;;;","20/Apr/21 08:34;trohrmann;I've merged the PR because it is definitely an improvement (in the worst case it avoids duplicate Java object creation). If the problem should still exist, then please re-open this issue.;;;","18/May/21 03:30;yunta;[~Feifan Wang], could you tell me what the docker image, the parallelism, the related memory configuration and what operations you would take when you run flink-21986-regular-join-test-case? It seems I did not reproduce the problem of memory continue growing up after failover restore when running flink-21986-regular-join-test-case.
;;;","19/May/21 03:01;Feifan Wang;Hi [~yunta], thank you for your continued attention to this issue.

You can try reproduce this problem use flink-21986-regular-join-test-case with 25 taskmanager and 8 slots per taskmanager, and taskmanager process memory set to 16G. By the way, I test it use yarn session instead of docker. And flink version is 1.12.1. Other settings can keep default , such as mock source parallelism、join parallelism, this part configuration is in src/main/java/top/zoltar/flink/issue/config/ConfigConstant.java .

After start the job，you hive no need to create some failover manually , in my test, it will restart in about one hour.;;;","10/Oct/21 19:10;sap1ens;We had a very similar problem in 1.13. Found this issue [https://github.com/facebook/rocksdb/issues/4112] and decided to try disabling RocksDB block cache:

 
{quote}state.backend.rocksdb.options-factory: xxx.NoBlockCacheRocksDbOptionsFactory
state.backend.rocksdb.memory.managed: false{quote}
 
Where NoBlockCacheRocksDbOptionsFactory essentially does:
 
{quote}val blockBasedTableConfig = new BlockBasedTableConfig()
blockBasedTableConfig.setNoBlockCache(true)
// Needed in order to disable block cache
blockBasedTableConfig.setCacheIndexAndFilterBlocks(false)
blockBasedTableConfig.setCacheIndexAndFilterBlocksWithHighPriority(false)
blockBasedTableConfig.setPinL0FilterAndIndexBlocksInCache(false){quote}
 
We did NOT see big performance degradation when running on SSDs. 
 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperRunningJobsRegistry creates an empty znode,FLINK-21980,13367714,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rburnett,rburnett,rburnett,25/Mar/21 21:28,28/May/21 09:00,13/Jul/23 08:07,07/Apr/21 17:00,1.10.3,1.11.3,1.12.2,1.9.3,,,,,1.11.4,1.12.3,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"ZooKeeperRunningJobsRegistry#writeEnumToZooKeeper calls
{code:java}
this.client.newNamespaceAwareEnsurePath(zkPath).ensure(client.getZookeeperClient());{code}
This creates an empty znode in zookeeper.  If the job manager is interrupted at this point the job manager cannot recover.  When trying to restore jobs on a restarted job manager, ZooKeeperRunningJobsRegistry#getJobSchedulingStatus will throw an exception due to the empty znode. 

Behavior was verified in a test environment where the job manager was interrupted at that point in execution leaving ZK in the following state:
{code:java}
zk: localhost:2181(CONNECTED) 2] ls /flink/default
[checkpoint-counter, checkpoints, jobgraphs, leader, leaderlatch, running_job_registry]
[zk: localhost:2181(CONNECTED) 3] ls /flink/default/running_job_registry 
[c982053dd0b9100967e6a9d89202f2a5]
[zk: localhost:2181(CONNECTED) 4] get /flink/default/running_job_registry/c982053dd0b9100967e6a9d89202f2a5 

[zk: localhost:2181(CONNECTED) 5] 
{code}",,kezhuw,klion26,mxm,rburnett,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21928,FLINK-11813,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 17:00:58 UTC 2021,,,,,,,,,,"0|z0p7q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 09:16;mxm;Thanks for filing this [~rburnett]! If raised the priority to Critical because this looks like a pretty serious bug in the HA code.;;;","26/Mar/21 15:48;rburnett;I think the best solution would be to check if the node exists before setting.  If it doesn't then -only ensure that runningJobPath exists and create the node with data.- create with data using creatingParentContainersIfNeeded.

Given the lifecycle of the node, it alternatively seems reasonable to treat an empty node as PENDING instead of an exception.;;;","26/Mar/21 17:42;trohrmann;Sounds like a good idea [~rburnett]. We could also use {{client.createContainers}}. Do you wanna take a stab at it?;;;","26/Mar/21 18:03;rburnett;Sure, I'll put together a PR.;;;","07/Apr/21 17:00;trohrmann;Fixed via

master: c580c9b335c16003dd1ab39f7d334c2cc5b7edcd
1.12.3: a68101fa0d306bbc6a4d4887f3379ae27633e142
1.11.4: 2a89f8de2af25ff375a5b1e63a11f446af653d96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job can be restarted from the beginning after it reached a terminal state,FLINK-21979,13367670,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,trohrmann,trohrmann,25/Mar/21 17:34,02/Sep/21 12:57,13/Jul/23 08:07,09/Aug/21 14:40,1.11.3,1.12.2,1.13.0,,,,,,1.14.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Currently, the {{JobMaster}} removes all checkpoints after a job reaches a globally terminal state. Then it notifies the {{Dispatcher}} about the termination of the job. The {{Dispatcher}} then removes the job from the {{SubmittedJobGraphStore}}. If the {{Dispatcher}} process fails before doing that it might get restarted. In this case, the {{Dispatcher}} would still find the job in the {{SubmittedJobGraphStore}} and recover it. Since the {{CompletedCheckpointStore}} is empty, it would start executing this job from the beginning.

I think we must not remove job state before the job has not been marked as done or made inaccessible for any restarted processes. Concretely, we should first remove the job from the {{SubmittedJobGraphStore}} and only then delete the checkpoints. Ideally all the job related cleanup operation happens atomically.",,david.artiga,jingzhang,klion26,Paul Lin,trohrmann,wangm92,xiangcaohello,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21928,FLINK-10333,FLINK-11813,,,,,,,,,,,,,,FLINK-19816,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 09 14:40:45 UTC 2021,,,,,,,,,,"0|z0p7gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 12:23;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:48;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/May/21 22:56;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/May/21 08:01;trohrmann;We need to fix this problem in {{1.14.0}}.;;;","27/May/21 23:05;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","07/Jun/21 22:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","15/Jun/21 22:41;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","23/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Jul/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Aug/21 14:40;trohrmann;Fixed via 1027acd61f8e2c067b20d287dcca3eb5557b9319;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove hamcrest dependency from SchedulerBenchmarkBase,FLINK-21975,13367563,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Thesharing,Thesharing,Thesharing,25/Mar/21 12:27,26/Mar/21 03:27,13/Jul/23 08:07,26/Mar/21 03:12,1.13.0,,,,,,,,1.13.0,,,,,,,Benchmarks,Runtime / Coordination,,,,0,pull-request-available,,,,,"When we are trying to add BenchmarkExecutor for benchmarks introduced in FLINK 21731, we find that the dependency of hamcrest is introduced into the benchmark by mistake. Since there's no hamcrest dependency in flink-benchmark, this will break the execution of scheduler benchmarks. Therefore, we need to remove this dependency from {{SchedulerBenchmarkBase}}.",,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21731,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 03:12:13 UTC 2021,,,,,,,,,,"0|z0p6so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 03:12;zhuzh;Fixed via 94ce6f9f638f7e346344e6e078ecbdd8933b44d6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to match quoted values for 'SET key=val' statement,FLINK-21974,13367552,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,25/Mar/21 11:51,26/Mar/21 04:06,13/Jul/23 08:07,26/Mar/21 04:06,,,,,,,,,1.13.0,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"When execute sql,
{code:java}
SET key_placeholder='test name ';
{code}
the value of the {{key_placeholder}} is {{test_name}} rather than {{'test name'}}",,fsk119,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 04:06:42 UTC 2021,,,,,,,,,,"0|z0p6q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 04:06;jark;Fixed in master: 17a4cba319addb24496d8f27339d49c1901a8a4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonTimestampsAndWatermarksOperator emitted the Long.MAX_VALUE watermark before emitting all the data,FLINK-21969,13367440,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,zhongwei,zhongwei,25/Mar/21 06:17,28/Aug/21 11:08,13/Jul/23 08:07,10/Apr/21 12:41,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,"Currently the PythonTimestampsAndWatermarksOperator emitted the Long.MAX_VALUE watermark before emitting all the data, which makes some registered timer can not be triggered on bounded stream, we need to fix this.

!image-2021-03-25-14-17-06-873.png|width=493,height=284!",,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21559,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/21 06:17;zhongwei;image-2021-03-25-14-17-06-873.png;https://issues.apache.org/jira/secure/attachment/13022926/image-2021-03-25-14-17-06-873.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 10 12:40:42 UTC 2021,,,,,,,,,,"0|z0p61c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/21 12:40;zhongwei;Merged into master via 5b3a8f3b25dcac71375d1a81fa63d935c8bf3eab

Merged into release-1.12 via 70d514360ce34dc47e4e95311baf1eb816380c98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReactiveModelITCase.testScaleDownOnTaskManagerLoss failed / hangs,FLINK-21963,13367302,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,mapohl,mapohl,24/Mar/21 16:59,28/Aug/21 11:14,13/Jul/23 08:07,02/Apr/21 09:20,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=360&view=logs&j=e0582806-6d85-5dc5-7eb4-4289d3d0de6b&t=9fea6cf4-6ce3-5c26-d059-69f4d4cec7d1&l=4442] failed (not exclusively) due to {{ReactiveModelITCase.testScaleDownOnTaskManagerLoss}}.

I was able to reproduce it locally having the {{DefaultScheduler}} enabled. The test seems to get into an infinite loop:

{code}
[...]
76125 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Unnamed (4/4)#8738 92b920a905c55fc85a76c79b3acef161.
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{0896a914cffb9d6631dc061ff4f485b4})
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot externally (SlotRequestId{0896a914cffb9d6631dc061ff4f485b4})
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Free reserved slot aec00279d7404b26a104ee906695d27a.
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot (SlotRequestId{0896a914cffb9d6631dc061ff4f485b4})
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state Executing to Restarting.
76125 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a) switched from state RUNNING to CANCELLING.
76125 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (2/4) (843d0c154f55a15a9bb1e705ae282032) switched from RUNNING to CANCELING.
76125 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (3/4) (ba0dd94db26abc376ee73522410b8094) switched from RUNNING to CANCELING.
76125 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (4/4) (92b920a905c55fc85a76c79b3acef161) switched from RUNNING to CANCELING.
76126 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 843d0c154f55a15a9bb1e705ae282032.
76126 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution ba0dd94db26abc376ee73522410b8094.
76126 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 92b920a905c55fc85a76c79b3acef161.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (3/4) (ba0dd94db26abc376ee73522410b8094) switched from CANCELING to CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (3/4) - execution #8738 to FAILED while being CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{786f89cafa4833afb26d0eb5da265a38})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot externally (SlotRequestId{786f89cafa4833afb26d0eb5da265a38})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Free reserved slot dde6780a1f8df3d0b1b1b454e28f8566.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot (SlotRequestId{786f89cafa4833afb26d0eb5da265a38})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Cannot run 'newResourcesAvailable' because the actual state is Restarting and not ResourceConsumer.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Discarding the results produced by task execution ba0dd94db26abc376ee73522410b8094.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (2/4) (843d0c154f55a15a9bb1e705ae282032) switched from CANCELING to CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (2/4) - execution #8739 to FAILED while being CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{86ab3cc76a17ec876b12bbaa6efcfa8c})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot externally (SlotRequestId{86ab3cc76a17ec876b12bbaa6efcfa8c})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Free reserved slot 25dd5bd3007772fe2cc69568cad2d882.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot (SlotRequestId{86ab3cc76a17ec876b12bbaa6efcfa8c})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Cannot run 'newResourcesAvailable' because the actual state is Restarting and not ResourceConsumer.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Discarding the results produced by task execution 843d0c154f55a15a9bb1e705ae282032.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (4/4) (92b920a905c55fc85a76c79b3acef161) switched from CANCELING to CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (4/4) - execution #8738 to FAILED while being CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{1fde7e6e5c69ce0ac831b5bc7de6a90d})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot externally (SlotRequestId{1fde7e6e5c69ce0ac831b5bc7de6a90d})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Free reserved slot 3315697ecf20a1249d7dad268892bcc9.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot (SlotRequestId{1fde7e6e5c69ce0ac831b5bc7de6a90d})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Cannot run 'newResourcesAvailable' because the actual state is Restarting and not ResourceConsumer.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a) switched from state CANCELLING to CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - ExecutionGraph 4b5f437c7c47c8be9f8d8bf08e78910a reached terminal state CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Stopping checkpoint coordinator for job 4b5f437c7c47c8be9f8d8bf08e78910a.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Discarding the results produced by task execution 92b920a905c55fc85a76c79b3acef161.
76126 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution ba0dd94db26abc376ee73522410b8094.
76126 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 92b920a905c55fc85a76c79b3acef161.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Ignoring scheduled action because expected state org.apache.flink.runtime.scheduler.adaptive.Executing@480dd446 is not the actual state org.apache.flink.runtime.scheduler.adaptive.Restarting@64a59f58.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Declare new resource requirements for job 4b5f437c7c47c8be9f8d8bf08e78910a.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=32768}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=4}}
76126 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 843d0c154f55a15a9bb1e705ae282032.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state Restarting to WaitingForResources.
76127 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution ba0dd94db26abc376ee73522410b8094.
76127 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 92b920a905c55fc85a76c79b3acef161.
76127 [SIGINT handler] WARN  org.apache.flink.util.TestSignalHandler [] - RECEIVED SIGNAL 2: SIGINT. Shutting down as requested.
76127 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state WaitingForResources to CreatingExecutionGraph.
76127 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 843d0c154f55a15a9bb1e705ae282032.
76127 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Running initialization on master for job Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a).
76127 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution ba0dd94db26abc376ee73522410b8094.
76127 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Successfully ran initialization on master in 0 ms.
76127 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Adding 1 vertices from job graph Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a).
76127 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
76127 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Unnamed) to 0 predecessors.
76127 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 92b920a905c55fc85a76c79b3acef161.
76127 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 843d0c154f55a15a9bb1e705ae282032.
76127 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 4 pipelined regions in 0 ms
76127 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Successfully created execution graph from job graph Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a).
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'totalNumberOfCheckpoints'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (3/4) - execution #8738 to FAILED while being CANCELED.
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numberOfInProgressCheckpoints'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numberOfCompletedCheckpoints'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numberOfFailedCheckpoints'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointRestoreTimestamp'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointSize'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointDuration'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointProcessedData'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (4/4) - execution #8738 to FAILED while being CANCELED.
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointPersistedData'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointExternalPath'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@90aee29
76128 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76128 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Checkpoint storage is set to JobManager
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (2/4) - execution #8739 to FAILED while being CANCELED.
76128 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'restartingTime'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'downtime'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'uptime'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Status of the shared state registry of job 4b5f437c7c47c8be9f8d8bf08e78910a after restore: SharedStateRegistry{registeredStates={}}.
76128 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - No checkpoint found during restore.
76128 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Resetting the master hooks.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a) switched from state CREATED to RUNNING.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool [] - Reserve free slot with allocation id aec00279d7404b26a104ee906695d27a.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Allocating logical slot from shared slot (SlotRequestId{88b040e446ee408f792334a2ec437a42})
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool [] - Reserve free slot with allocation id 25dd5bd3007772fe2cc69568cad2d882.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Allocating logical slot from shared slot (SlotRequestId{96939e4bb685186000b4001d96082081})
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool [] - Reserve free slot with allocation id dde6780a1f8df3d0b1b1b454e28f8566.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Allocating logical slot from shared slot (SlotRequestId{c879c9aa7a7cb6dfbc12502ce7a8ed12})
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool [] - Reserve free slot with allocation id 3315697ecf20a1249d7dad268892bcc9.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Allocating logical slot from shared slot (SlotRequestId{53a25924bcd9fe2db23c22e0bf17effe})
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Successfully reserved and assigned the required slots for the ExecutionGraph.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state CreatingExecutionGraph to Executing.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (1/4) (9132ba5f6b087654fb351138ce74e710) switched from CREATED to DEPLOYING.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> Sink: Unnamed (1/4) (attempt #8740) with attempt id 9132ba5f6b087654fb351138ce74e710 to 3ebdf185-dcde-4ad2-b567-2f14c1b86fd1 @ localhost (dataPort=-1) with allocation id aec00279d7404b26a104ee906695d27a
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (2/4) (7384c32213a4e9cd3aa6ee5875b1e532) switched from CREATED to DEPLOYING.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> Sink: Unnamed (2/4) (attempt #8740) with attempt id 7384c32213a4e9cd3aa6ee5875b1e532 to 3ebdf185-dcde-4ad2-b567-2f14c1b86fd1 @ localhost (dataPort=-1) with allocation id 25dd5bd3007772fe2cc69568cad2d882
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (3/4) (497dd733deeb255e05bae82f0e41527d) switched from CREATED to DEPLOYING.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> Sink: Unnamed (3/4) (attempt #8739) with attempt id 497dd733deeb255e05bae82f0e41527d to 6cf04c09-5378-4bf3-aedd-e9a52076ec99 @ localhost (dataPort=-1) with allocation id dde6780a1f8df3d0b1b1b454e28f8566
76128 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot aec00279d7404b26a104ee906695d27a.
76128 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot dde6780a1f8df3d0b1b1b454e28f8566.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (4/4) (639cf6da5847c7f4250839aeb2552df9) switched from CREATED to DEPLOYING.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> Sink: Unnamed (4/4) (attempt #8739) with attempt id 639cf6da5847c7f4250839aeb2552df9 to 6cf04c09-5378-4bf3-aedd-e9a52076ec99 @ localhost (dataPort=-1) with allocation id 3315697ecf20a1249d7dad268892bcc9
76128 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Found existing local state store for 4b5f437c7c47c8be9f8d8bf08e78910a - cbc357ccb763df2852fee8c4fc7d55f2 - 2 under allocation id dde6780a1f8df3d0b1b1b454e28f8566: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@6d90093b
76128 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Found existing local state store for 4b5f437c7c47c8be9f8d8bf08e78910a - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id aec00279d7404b26a104ee906695d27a: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@64e96e04
76128 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d), deploy into slot with allocation id dde6780a1f8df3d0b1b1b454e28f8566.
76129 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3315697ecf20a1249d7dad268892bcc9.
76129 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710), deploy into slot with allocation id aec00279d7404b26a104ee906695d27a.
76129 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) switched from CREATED to DEPLOYING.
76129 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) [DEPLOYING]
76129 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Found existing local state store for 4b5f437c7c47c8be9f8d8bf08e78910a - cbc357ccb763df2852fee8c4fc7d55f2 - 3 under allocation id 3315697ecf20a1249d7dad268892bcc9: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@23a8b928
76130 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) switched from CREATED to DEPLOYING.
76130 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) [DEPLOYING]
76130 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 25dd5bd3007772fe2cc69568cad2d882.
76130 [TransientBlobCache shutdown hook] INFO  org.apache.flink.runtime.blob.TransientBlobCache [] - Shutting down BLOB cache
76130 [TaskExecutorLocalStateStoresManager shutdown hook] INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
76130 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Found existing local state store for 4b5f437c7c47c8be9f8d8bf08e78910a - cbc357ccb763df2852fee8c4fc7d55f2 - 1 under allocation id 25dd5bd3007772fe2cc69568cad2d882: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@389f2d5c
76130 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532), deploy into slot with allocation id 25dd5bd3007772fe2cc69568cad2d882.
76130 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9), deploy into slot with allocation id 3315697ecf20a1249d7dad268892bcc9.
76131 [PermanentBlobCache shutdown hook] INFO  org.apache.flink.runtime.blob.PermanentBlobCache [] - Shutting down BLOB cache
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) switched from CREATED to DEPLOYING.
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) [DEPLOYING]
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Getting user code class loader for task 497dd733deeb255e05bae82f0e41527d at library cache manager took 0 milliseconds
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Getting user code class loader for task 9132ba5f6b087654fb351138ce74e710 at library cache manager took 0 milliseconds
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Getting user code class loader for task 639cf6da5847c7f4250839aeb2552df9 at library cache manager took 0 milliseconds
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7b20c610
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@252878a9
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@30eb707b
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Checkpoint storage is set to JobManager
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Checkpoint storage is set to JobManager
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76131 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) switched from CREATED to DEPLOYING.
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Checkpoint storage is set to JobManager
76131 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) [DEPLOYING]
76131 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Getting user code class loader for task 7384c32213a4e9cd3aa6ee5875b1e532 at library cache manager took 0 milliseconds
76132 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) [DEPLOYING].
76132 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4c1406b8
76132 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76132 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Checkpoint storage is set to JobManager
76134 [FileChannelManagerImpl-io shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl [] - FileChannelManager removed spill file directory /var/folders/bd/6xl5m4z90j9438dv5bxg2n180000gn/T/junit2858931828265752695/junit7422691420779266555/flink-io-12fb17cf-712c-45f8-ab94-32fc7f5b5571
76136 [TaskExecutorLocalStateStoresManager shutdown hook] INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
76136 [IOManagerAsync shutdown hook] DEBUG org.apache.flink.runtime.io.disk.iomanager.IOManager [] - Shutting down I/O manager.
76136 [IOManagerAsync shutdown hook] DEBUG org.apache.flink.runtime.io.disk.iomanager.IOManager [] - Shutting down I/O manager.
76136 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) switched from DEPLOYING to RUNNING.
76136 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) switched from DEPLOYING to RUNNING.
76136 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Initializing Source: Custom Source -> Sink: Unnamed (4/4)#8739.
76136 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Initializing Source: Custom Source -> Sink: Unnamed (2/4)#8740.
76136 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) switched from DEPLOYING to RUNNING.
76136 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) switched from DEPLOYING to RUNNING.
76136 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Initializing Source: Custom Source -> Sink: Unnamed (1/4)#8740.
76136 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Initializing Source: Custom Source -> Sink: Unnamed (3/4)#8739.
76136 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (2/4) (7384c32213a4e9cd3aa6ee5875b1e532) switched from DEPLOYING to RUNNING.
76136 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (4/4) (639cf6da5847c7f4250839aeb2552df9) switched from DEPLOYING to RUNNING.
76136 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (1/4) (9132ba5f6b087654fb351138ce74e710) switched from DEPLOYING to RUNNING.
76136 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (3/4) (497dd733deeb255e05bae82f0e41527d) switched from DEPLOYING to RUNNING.
76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Invoking Source: Custom Source -> Sink: Unnamed (3/4)#8739
76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Invoking Source: Custom Source -> Sink: Unnamed (4/4)#8739
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Invoking Source: Custom Source -> Sink: Unnamed (1/4)#8740
76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(3/4) with empty state.
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Invoking Source: Custom Source -> Sink: Unnamed (2/4)#8740
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/4) with empty state.
76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(4/4) with empty state.
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(2/4) with empty state.
76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] WARN  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228)
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] WARN  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228)
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] WARN  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228)
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9).
76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Release task Source: Custom Source -> Sink: Unnamed (4/4)#8739 network resources (state: FAILED).
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710).
76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d).
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Release task Source: Custom Source -> Sink: Unnamed (1/4)#8740 network resources (state: FAILED).
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] WARN  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228)
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Release task Source: Custom Source -> Sink: Unnamed (3/4)#8739 network resources (state: FAILED).
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532).
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Release task Source: Custom Source -> Sink: Unnamed (2/4)#8740 network resources (state: FAILED).
76137 [FileCache shutdown hook] INFO  org.apache.flink.runtime.filecache.FileCache [] - removed file cache directory /var/folders/bd/6xl5m4z90j9438dv5bxg2n180000gn/T/junit2858931828265752695/junit7422691420779266555/flink-dist-cache-13391a9e-181b-4d3c-b373-0ac0203f301e
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) [FAILED]
76138 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) [FAILED]
76138 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Unnamed (1/4)#8740 9132ba5f6b087654fb351138ce74e710.
76138 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Unnamed (2/4)#8740 7384c32213a4e9cd3aa6ee5875b1e532.
76138 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (1/4) (9132ba5f6b087654fb351138ce74e710) switched from RUNNING to FAILED on 3ebdf185-dcde-4ad2-b567-2f14c1b86fd1 @ localhost (dataPort=-1).
java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228) ~[test-classes/:?]
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215) ~[test-classes/:?]
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[classes/:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
[...]
{code}",,dwysakowicz,guoyangze,maguowei,mapohl,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22079,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 12 07:16:58 UTC 2021,,,,,,,,,,"0|z0p56o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 04:01;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15486&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9953;;;","26/Mar/21 10:34;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=364&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4437;;;","26/Mar/21 14:16;trohrmann;[~rmetzger] can you take care of it?;;;","26/Mar/21 14:26;rmetzger;Yes, I'll take a look!;;;","29/Mar/21 13:52;rmetzger;The test failure is difficult to reproduce locally (stopped after 229 runs).

In my opinion, the test instability comes from the following problem: The job is scaling up initially, despite being mainly a test for scale down. Since the resource stabilization timeout is 0, we are deploying on the first available slot.
In case of the test failure, the tasks get deployed on one TM, then cancelled for scale up and deployed again on two TMs. 
The counting for the number of running instances is then wrong in the CountDownLatch

On a side note:
It seems that the Execution Graph gets out of sync, but eventually reaches consistency again. This is showing all log statements involving ca080a6391c7887c02a00ee3203b901a from this failure: https://dev.azure.com/mapohl/flink/_build/results?buildId=360&view=logs&j=e0582806-6d85-5dc5-7eb4-4289d3d0de6b&t=9fea6cf4-6ce3-5c26-d059-69f4d4cec7d1&l=4442
ca080a6391c7887c02a00ee3203b901a is in state CANCELLED on the JobManager, when the TaskManager receives it.
My assumption is/was that we should only mark something as ""CANCELLED"" when the cancellation has succeeded. 

{code}
14:52:16,090 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/2) (ca080a6391c7887c02a00ee3203b901a) switched from CREATED to DEPLOYING.
14:52:16,090 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source -> Sink: Unnamed (1/2) (attempt #0) with attempt id ca080a6391c7887c02a00ee3203b901a to 5420c4ef-c20e-49fa-9dd9-5df302da28c2 @ localhost (dataPort=-1) with allocation id ed7c4f301d07c27b59aa38d37c9f6057
14:52:16,092 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/2) (ca080a6391c7887c02a00ee3203b901a) switched from DEPLOYING to CANCELING.
14:52:16,094 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/2) (ca080a6391c7887c02a00ee3203b901a) switched from CANCELING to CANCELED.
14:52:16,094 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution ca080a6391c7887c02a00ee3203b901a.
14:52:16,120 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a), deploy into slot with allocation id ed7c4f301d07c27b59aa38d37c9f6057.
14:52:16,130 [Source: Custom Source -> Sink: Unnamed (1/2)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a) switched from CREATED to DEPLOYING.
14:52:16,168 [Source: Custom Source -> Sink: Unnamed (1/2)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a) [DEPLOYING].
14:52:16,182 [Source: Custom Source -> Sink: Unnamed (1/2)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a) [DEPLOYING].
14:52:16,182 [Source: Custom Source -> Sink: Unnamed (1/2)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a) switched from DEPLOYING to RUNNING.
14:52:16,183 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to fail task externally Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a).
14:52:16,184 [flink-akka.actor.default-dispatcher-2] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a) switched from RUNNING to FAILED with failure cause: org.apache.flink.runtime.jobmaster.ExecutionGraphException: The execution attempt ca080a6391c7887c02a00ee3203b901a was not found.
14:52:16,185 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a).
14:52:16,241 [Source: Custom Source -> Sink: Unnamed (1/2)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (1/2)#0 (ca080a6391c7887c02a00ee3203b901a).
14:52:16,242 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Unnamed (1/2)#0 ca080a6391c7887c02a00ee3203b901a.
14:52:16,265 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Cannot find task to fail for execution ca080a6391c7887c02a00ee3203b901a with exception:
org.apache.flink.runtime.jobmaster.ExecutionGraphException: The execution attempt ca080a6391c7887c02a00ee3203b901a was not found.
{code}

I assume this is the expected behavior. If not, we should file a separate ticket. I don't think this caused the test instability. 
;;;","29/Mar/21 18:54;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=367&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4440;;;","31/Mar/21 13:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15865&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=3931;;;","01/Apr/21 07:20;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15934&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4440;;;","02/Apr/21 02:49;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15961&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=3917;;;","02/Apr/21 09:20;rmetzger;Resolved in https://github.com/apache/flink/commit/ac6317fd5bcaf90a88dee74e0171a55933395907;;;","11/May/21 15:58;mapohl;[~rmetzger] {{testScaleDownOnTaskManagerLoss}} failed again: https://dev.azure.com/mapohl/flink/_build/results?buildId=422&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4961

Shall we reopen the issue or create a new one? ;;;","11/May/21 16:05;trohrmann;Did the built include the fix for FLINK-22406 [~mapohl]?;;;","12/May/21 07:02;mapohl;Sorry, I should have been more specific on that one: Yes, [ac6317f|https://github.com/apache/flink/commit/ac6317fd5bcaf90a88dee74e0171a55933395907] was include in the build.;;;","12/May/21 07:08;chesnay;[~mapohl] I'm reasonably sure that it is not included because the stacktrace show usages of the ParallelismTrackingSource, which was removed in FLINK-22406. The build you linked is from before you rebased your branch.;;;","12/May/21 07:16;mapohl;Oh, you're right. I misread [~trohrmann]'s question and only checked the fix from this Jira issue (FLINK-21963) but ignored that [~trohrmann] asked for FLINK-22406. You're observation is right. [271ac1b|https://github.com/apache/flink/commit/271ac1b8bac100b613c313e11773109db15015e5] was not included in the build. Sorry for the confusion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test failures occur due to the test not waiting for the ExecutionGraph to be created in Adaptive Scheduler,FLINK-21954,13367233,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,mapohl,mapohl,24/Mar/21 13:07,28/May/21 09:00,13/Jul/23 08:07,08/Apr/21 08:27,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"Various tests are failing due the test not waiting for the ExecutionGraph to be created:
* [JobMasterTest.testRestoringFromSavepoint|https://dev.azure.com/mapohl/flink/_build/results?buildId=356&view=logs&j=243b38e1-22e7-598a-c8ae-385dce2c28b5&t=fea482b6-4f61-51f4-2584-f73df532b395&l=8266]
* {{JobMasterTest.testRequestNextInputSplitWithGlobalFailover}}
* {{JobMasterQueryableStateTest.testRequestKvStateOfWrongJob}}
* {{JobMasterQueryableStateTest.testRequestKvStateWithIrrelevantRegistration}}
* {{JobMasterQueryableStateTest.testDuplicatedKvStateRegistrationsFailTask}}
* {{JobMasterQueryableStateTest.testRegisterKvState}}

We might have to double-check whether other tests are affected as well.",,dwysakowicz,maguowei,mapohl,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21988,FLINK-21987,,,,,,,FLINK-21075,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 08:27:50 UTC 2021,,,,,,,,,,"0|z0p4rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 04:15;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15486&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=7037

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15486&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=7175

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15486&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8453;;;","29/Mar/21 02:48;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15632&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8502;;;","29/Mar/21 03:11;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15601&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8552;;;","29/Mar/21 03:58;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15573&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8502;;;","30/Mar/21 07:32;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15718&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8410;;;","30/Mar/21 07:33;dwysakowicz;What is the plan for this issue?;;;","30/Mar/21 07:50;trohrmann;The plan is to fix it asap. I'll try to look into it today.;;;","30/Mar/21 07:54;dwysakowicz;Thank you [~trohrmann] ;);;;","31/Mar/21 03:51;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15823&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8566;;;","01/Apr/21 07:21;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15934&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8580;;;","02/Apr/21 06:13;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15996&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8397;;;","06/Apr/21 06:36;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16056&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8576

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16044&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8576

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16036&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8397

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16030&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8397;;;","07/Apr/21 06:49;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16093&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8407;;;","08/Apr/21 08:27;trohrmann;Fixed via

a26a5f0bfe5436288bdd633a0773b76d8647c5b7
ce4beb6370ed3d4aeac3bbdcf798c5ce11a4d8ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make all the ""Connection reset by peer"" exception wrapped as RemoteTransportException",FLINK-21952,13367227,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,24/Mar/21 12:41,23/Sep/21 17:26,13/Jul/23 08:07,21/Jun/21 07:36,1.12.4,1.13.1,1.14.0,,,,,,1.12.5,1.13.2,1.14.0,,,,,Runtime / Network,,,,,1,pull-request-available,,,,,"In CreditBasedPartitionRequestClientHandler#exceptionCaught, the IOException or the exception with exact message ""Connection reset by peer"" are marked as RemoteTransportException. 

However, with the current Netty implementation, sometimes it might throw 
{code:java}
org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
{code}
in some case. It would be also wrapped as LocalTransportException, which might cause some confusion. ",,gaoyunhaii,nobleyd,pnowojski,Thesharing,wangm92,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 21 07:36:40 UTC 2021,,,,,,,,,,"0|z0p4q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/21 23:28;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","03/May/21 05:08;pnowojski;How would you propose [~gaoyunhaii] to solve this?;;;","06/May/21 09:10;gaoyunhaii;Hi [~pnowojski] sorry I also do not come up with one solution that does not rely on the Netty implementation. The method come to me is to add another condition specially: 
{code:java}
if ((cause instanceof IOException
                && cause.getMessage().equals(""Connection reset by peer""))
        || (cause instanceof Errors.NativeIoException
                && cause.getMessage().contains(""Connection reset by peer""))) {
   ...
}{code}
I checked other possible netty native errors, in addition to ""connection reset by peer"", the ones related to network also includes ""connection refused"" and ""network is unreachable"", but they should only happen during connection establishment and should not need to consider here.

 

 ;;;","17/May/21 07:26;pnowojski;What about just using 
{code:java}
if (cause.getMessage().contains(""Connection reset by peer"")) {
   ...
}
{code}
? Do you think it would cause some problems?;;;","20/May/21 04:17;gaoyunhaii;Hi [~pnowojski] Very sorry for missing the notification and response later.  I think it works since the exception might be caught here are all thrown by the downstream task Netty stack, and their kinds are limited, thus directly distinguish them via keywords should be enough.;;;","20/May/21 08:45;gaoyunhaii;I'll open a small PR for this issue (If there are no other problem).;;;","20/May/21 10:30;pnowojski;No problem at all :);;;","28/May/21 11:25;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","28/May/21 11:32;gaoyunhaii;I'll fix this issue as soon as possible.;;;","11/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","21/Jun/21 07:36;pnowojski;merged commit 241c7a4 into apache:master
merged to release-1.13 as 78327def574
merged to release-1.12 as 710b5f69771
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FlinkRelMdUtil.numDistinctVals produces exceptional Double.NaN result  when domainSize is in range(0,1)",FLINK-21946,13367184,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,24/Mar/21 08:45,26/Mar/21 02:04,13/Jul/23 08:07,26/Mar/21 02:04,1.11.3,1.12.2,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"CALCITE-4351 introduced a new 'more accurate' formula for estimating the number of distinct values, FLINK-19780 fixed one bad case when number of rows are large, but still there's exception path when domainSize is in range(0,1), the following code in calcite‘s  RelMdUtil.numDistinctVals will produce Double.NaN result:
{code}double expo = numSel * Math.log(1.0 - 1.0 / dSize);{code}

Before CALCITE-4351 was fixed, we should continue the fixing in flink.",,godfreyhe,jark,libenchao,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-4351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 02:04:06 UTC 2021,,,,,,,,,,"0|z0p4gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/21 09:30;lincoln.86xy;add pr link: https://github.com/apache/flink/pull/15357;;;","26/Mar/21 02:04;godfreyhe;Fixed in 1.13.0: 754fd6b73c3d7271b6c1fcc3416b2c3996073e43;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractArrowPythonAggregateFunctionOperator.dispose should consider whether arrowSerializer is null,FLINK-21944,13367174,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,24/Mar/21 07:56,24/Mar/21 12:56,13/Jul/23 08:07,24/Mar/21 12:56,,,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,,,,,,"{code}
java.lang.RuntimeException: Failed to create stage bundle factory!java.lang.RuntimeException: Failed to create stage bundle factory! at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.createStageBundleFactory(BeamPythonFunctionRunner.java:415) ~[?:?] at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:259) ~[?:?] at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:113) ~[?:?] at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:116) ~[?:?] at org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.open(AbstractArrowPythonAggregateFunctionOperator.java:90) ~[?:?] at org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.open(StreamArrowPythonGroupWindowAggregateFunctionOperator.java:180) ~[?:?] at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:426) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:535) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:525) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:565) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_281] Suppressed: java.lang.NullPointerException at org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.dispose(AbstractArrowPythonAggregateFunctionOperator.java:105) ~[?:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:783) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:762) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:681) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) ~[flink-dist_2.12-1.12.1.jar:1.12.1] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_281]
{code}",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 12:56:45 UTC 2021,,,,,,,,,,"0|z0p4e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/21 12:56;dian.fu;Fixed in
- master via 0e04b2e736d959471270ee77266d0b638189e828
- release-1.12 via 719bf5ea469d55cf162b238105b767774d01edce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesLeaderRetrievalDriver not closed after terminated which lead to connection leak,FLINK-21942,13367137,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,yittg,yittg,24/Mar/21 03:57,28/Aug/21 11:13,13/Jul/23 08:07,01/Apr/21 17:25,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Coordination,,,,,0,k8s-ha,pull-request-available,,,,"Looks like KubernetesLeaderRetrievalDriver is not closed even if the KubernetesLeaderElectionDriver is closed and job reach globally terminated.
This will lead to many configmap watching be still active with connections to K8s.

When the connections exceeds max concurrent requests, those new configmap watching can not be started. Finally leads to all new jobs submitted timeout.

[~fly_in_gis] [~trohrmann] This may be related to FLINK-20695, could you confirm this issue?
But when many jobs are running in same session cluster, the config map watching is required to be active. Maybe we should merge all config maps watching?",,kezhuw,trohrmann,wangm92,wangyang0918,yittg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22054,FLINK-22006,,,,,,,,,,,,,,,FLINK-20695,,,,,,,,,,"24/Mar/21 10:08;yittg;image-2021-03-24-18-08-30-196.png;https://issues.apache.org/jira/secure/attachment/13022893/image-2021-03-24-18-08-30-196.png","24/Mar/21 10:08;yittg;image-2021-03-24-18-08-42-116.png;https://issues.apache.org/jira/secure/attachment/13022894/image-2021-03-24-18-08-42-116.png","25/Mar/21 02:27;yittg;jstack.l;https://issues.apache.org/jira/secure/attachment/13022919/jstack.l",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 17:25:59 UTC 2021,,,,,,,,,,"0|z0p460:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/21 07:48;wangyang0918;I do not think we have Kubernetes watch leak currently. When a Flink job reached to terminal state(e.g. canceled), {{KubernetesLeaderRetrievalDriver}} could be stopped. And I have verified in the application mode and session mode. When I cancel the job, I could find the following logs in the JobManager log.
{code:java}
2021-03-24 06:49:11,958 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Stopping DefaultLeaderRetrievalService.2021-03-24 06:49:11,958 INFO  org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver [] - Stopping KubernetesLeaderRetrievalDriver{configMapName='standalone-k8s-ha-session-resourcemanager-leader'}.{code}
Could you please double check for that?

 

Currently, in Kubernetes HA service, each job will have the dedicated leader election and retrieval service. It is clearer and also aligned with ZooKeeper HA service. AFAIK, I do not think this will take a burden to the APIServer, which could be started with multiple replicas. Have you found that the APIServer become the bottleneck because of Flink watch?;;;","24/Mar/21 08:07;yittg;Did you find any log like 
{code}
Stopping KubernetesLeaderRetrievalDriver{configMapName='xxxx-{jobid}-jobmanager-leader'}.
{code}
excepts for those timeout jobs.

Furthermore, if many jobs are running (exceeds the max concurrent requests 64 which looks like can not be configured), the API server is OK. The problem is that the new watch for new job(election and retrieve) can not be established.

I'll try to provide a detailed steps to reproduce it.
;;;","24/Mar/21 10:14;yittg;kubernetes session cluster with options:
{code}
cluster.io-pool.size=16
kubernetes.jobmanager.cpu=2
{code}
h2. the simplest scenario

run batch job one by one, e.g. examples/batch/WordCount.jar

!image-2021-03-24-18-08-30-196.png|width=990,height=629!

In this scenario, the latest job is created and waiting to be assigned, the previous one failed after waiting 5m. The FINISHED jobs only occupy one config map watch, so almost 60 jobs can be run successfully.


h2. another scenario:

since resource limit set like following:
{code}
kubernetes.taskmanager.cpu=2
taskmanager.numberOfTaskSlots: 10
{code}

run streaming job, examples/streaming/StateMachineExample.jar --error-rate 0.5 --sleep 100

!image-2021-03-24-18-08-42-116.png|width=990,height=629!

In this scenario, the latest job is always initializing, and the previous one is created and waiting to be assigned. The RUNNING jobs occupy three config map watch, so almost 20 jobs can be running successfully.;;;","24/Mar/21 11:59;wangyang0918;Could you please increase {{cluster.io-pool.size}} bigger or share the jstack when the job submission is blocking?;;;","25/Mar/21 02:33;yittg;[~fly_in_gis] Sure, I've uploaded a ( [^jstack.l] ) corresponding the first scenario. 
You can focus on those OkHttp And OkHttp Websocket threads first.
I don't think add more cluster.io-pool.size will help, so it's still 16 as shown in stack.;;;","25/Mar/21 07:45;wangyang0918;[~yittg] Thanks for providing the information.

After more investigation, I have to admit that we have the leader ConfigMap watch leak currently. When the job reaches to terminal state, the jobmanager leader retrieval service in ResourceManager is not stopped correctly. We start the ""job leader id monitoring"" in {{ResourceManager#registerJobManager}}, but we do not stop it when we {{disconnectJobManager}}. cc [~trohrmann]

 

For the second problem(could not start more than 60 batch jobs or 20 streaming jobs in a session), I am trying to reproduce it.;;;","25/Mar/21 08:35;trohrmann;I think the closing of the {{LeaderRetrievalService}} happens when {{ResourceManager.removeJob}} is called. This happens currently only after a job has no leader for at least 5 minutes (configurable via {{resourcemanager.job.timeout}}). With FLINK-21751 we added the state of the job when {{disconnectJobManager}} is called. We could use this to call {{removeJob}} when the job status is globally terminal. This could solve the problem.;;;","25/Mar/21 09:09;yittg;[~trohrmann] Absolutely yes, this could solve the leak of globally terminated jobs.

In addition, I think we need more discussion about the watching mechanism in K8s HA.;;;","29/Mar/21 06:21;wangyang0918;I will attach a PR for this issue. It will solve the Kubernetes ConfigMap watch leak, as well as the ZooKeeper connection.

Actually, we have the same issue for ZooKeeper HA service for long time.

 

Unfortunately, I still have not enough time to find out the root cause why we only could run about 20 jobs in a Kubernetes session cluster with HA enabled. Since it is a separate issue, I will create another ticket.;;;","01/Apr/21 17:25;trohrmann;Fixed via

1.13.0:
8aa510b705bdcfe5b8ff69bc0e294a56b437f53e
6b40ff1f384c5a2253c8393c3612d3384ae6bfc5
2eb5d1ce886824fb9eb61847ab56ffba4223a2bf

1.12.3:
3409e7f7e52d1dcb70ce238177bcd837f9bb15d3
8c475b3f0e40be34325a7b37a5b4dbbca738b55d
c25dc3f83e07adf4f0788d09201b03bfc8e92801;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testSavepointRescalingOutPartitionedOperatorStateList fail,FLINK-21941,13367136,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,maguowei,maguowei,24/Mar/21 03:57,28/May/21 09:08,13/Jul/23 08:07,15/Apr/21 13:06,1.12.2,,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15277&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4117

{code:java}
[ERROR] testSavepointRescalingOutPartitionedOperatorStateList[backend = filesystem](org.apache.flink.test.checkpointing.RescalingITCase)  Time elapsed: 1.756 s  <<< ERROR!
java.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (8c806e07748f451ef6927883a33d3b5e)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingPartitionedOperatorState(RescalingITCase.java:529)
	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingOutPartitionedOperatorStateList(RescalingITCase.java:476)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

{code}
",,dwysakowicz,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 13:06:02 UTC 2021,,,,,,,,,,"0|z0p45s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/21 09:13;maguowei;testSavepointRescalingInBroadcastOperatorState[backend = rocksdb](org.apache.flink.test.checkpointing.RescalingITCase) fail.


{code:java}
[ERROR] Tests run: 24, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 128.256 s <<< FAILURE! - in org.apache.flink.test.checkpointing.RescalingITCase
[ERROR] testSavepointRescalingInBroadcastOperatorState[backend = rocksdb](org.apache.flink.test.checkpointing.RescalingITCase)  Time elapsed: 1.827 s  <<< ERROR!
java.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (3176d802f1088f374108726e2a75c7be)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingPartitionedOperatorState(RescalingITCase.java:529)
	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingInBroadcastOperatorState(RescalingITCase.java:458)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)

{code}
;;;","15/Apr/21 13:06;dwysakowicz;Fixed in:
* master
** 28deff965d63313095114118367075d08d8c075d 87efae4d3180a52e16240a0b4bbb197f85acd22c
* 1.12.3
** 42c768bb4ae64b7973188343e3bffe5618878c3c 6f3436eeed4f40f03f2f9d88fe2922baa9ee7d8a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[kinesis][efo] EFO consumer treats interrupts as retryable exceptions,FLINK-21933,13367031,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,23/Mar/21 15:20,25/Mar/21 13:53,13/Jul/23 08:07,25/Mar/21 13:53,1.12.0,1.12.1,1.12.2,,,,,,1.12.3,1.13.0,,,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,,"*Background*
When an Flink job is cancelled or failed, the source threads get interrupted. The EFO consumer is treating this exception as retryable and will retry until MAX_RETRIES is exceeded.

*Scope*
Update error handling to treat Interrupted exceptions as non-recoverable. The source should gracefully shutdown.",,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-23 15:20:15.0,,,,,,,,,,"0|z0p3ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-statebackend-rocksdb crashes with Error occurred in starting fork,FLINK-21929,13366970,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,rmetzger,rmetzger,23/Mar/21 12:32,30/Mar/21 09:10,13/Jul/23 08:07,26/Mar/21 09:40,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9001&view=results

{code}
2021-03-23T09:11:12.1861967Z [INFO] BUILD FAILURE
2021-03-23T09:11:12.1863007Z [INFO] ------------------------------------------------------------------------
2021-03-23T09:11:12.1863492Z [INFO] Total time: 42:35 min
2021-03-23T09:11:12.1864171Z [INFO] Finished at: 2021-03-23T09:11:12+00:00
2021-03-23T09:11:12.8003245Z [INFO] Final Memory: 137M/806M
2021-03-23T09:11:12.8006310Z [INFO] ------------------------------------------------------------------------
2021-03-23T09:11:12.8082409Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-statebackend-rocksdb_2.11: There are test failures.
2021-03-23T09:11:12.8086652Z [ERROR] 
2021-03-23T09:11:12.8092462Z [ERROR] Please refer to /__w/1/s/flink-state-backends/flink-statebackend-rocksdb/target/surefire-reports for the individual test results.
2021-03-23T09:11:12.8096948Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2021-03-23T09:11:12.8101388Z [ERROR] ExecutionException Error occurred in starting fork, check output in log
2021-03-23T09:11:12.8105868Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log
2021-03-23T09:11:12.8110518Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2021-03-23T09:11:12.8115518Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:382)
2021-03-23T09:11:12.8120811Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
2021-03-23T09:11:12.8126356Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2021-03-23T09:11:12.8127129Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2021-03-23T09:11:12.8131291Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2021-03-23T09:11:12.8132369Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2021-03-23T09:11:12.8133397Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2021-03-23T09:11:12.8134116Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2021-03-23T09:11:12.8134793Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2021-03-23T09:11:12.8135621Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2021-03-23T09:11:12.8136323Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2021-03-23T09:11:12.8141570Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2021-03-23T09:11:12.8142374Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2021-03-23T09:11:12.8145665Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2021-03-23T09:11:12.8146407Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2021-03-23T09:11:12.8148835Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2021-03-23T09:11:12.8151299Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2021-03-23T09:11:12.8152244Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2021-03-23T09:11:12.8152806Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2021-03-23T09:11:12.8155818Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-23T09:11:12.8159757Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-23T09:11:12.8177288Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-23T09:11:12.8178021Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-23T09:11:12.8179802Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2021-03-23T09:11:12.8183929Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2021-03-23T09:11:12.8187563Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2021-03-23T09:11:12.8192413Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2021-03-23T09:11:12.8196538Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log
2021-03-23T09:11:12.8201660Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:622)
2021-03-23T09:11:12.8203999Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2021-03-23T09:11:12.8204879Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:371)
2021-03-23T09:11:12.8205665Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:347)
2021-03-23T09:11:12.8206513Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-03-23T09:11:12.8207169Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-03-23T09:11:12.8209376Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-03-23T09:11:12.8209955Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2021-03-23T09:11:12.8211107Z [ERROR] -> [Help 1]
2021-03-23T09:11:12.8211559Z [ERROR] 
2021-03-23T09:11:12.8309016Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2021-03-23T09:11:12.8310211Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2021-03-23T09:11:12.8311401Z [ERROR] 
2021-03-23T09:11:12.8311976Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2021-03-23T09:11:12.8312635Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
2021-03-23T09:11:12.8313300Z [ERROR] 
2021-03-23T09:11:12.8314248Z [ERROR] After correcting the problems, you can resume the build with the command
2021-03-23T09:11:12.8315462Z [ERROR]   mvn <goals> -rf :flink-statebackend-rocksdb_2.11
2021-03-23T09:11:12.8687280Z Process exited with EXIT CODE: 1.
2021-03-23T09:11:12.8687830Z Trying to KILL watchdog (359).
2021-03-23T09:11:12.8696302Z /__w/1/s/tools/ci/watchdog.sh: line 100:   359 Terminated              watchdog
2021-03-23T09:11:16.4147696Z Searching for .dump, .dumpstream and related files in '/__w/1/s'
2021-03-23T09:11:21.6569652Z Moving '/__w/1/s/flink-runtime/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun2.dump' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:21.6645588Z Moving '/__w/1/s/flink-runtime/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun1.dump' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:21.6683479Z Moving '/__w/1/s/flink-runtime/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:21.6726888Z Moving '/__w/1/s/flink-core/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:21.6772031Z Moving '/__w/1/s/flink-metrics/flink-metrics-core/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:21.6807831Z Moving '/__w/1/s/flink-java/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:21.6847619Z Moving '/__w/1/s/flink-runtime-web/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:21.6882147Z Moving '/__w/1/s/flink-state-backends/flink-statebackend-rocksdb/core.18126' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1251457Z Moving '/__w/1/s/flink-state-backends/flink-statebackend-rocksdb/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1287286Z Moving '/__w/1/s/flink-test-utils-parent/flink-test-utils/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1322850Z Moving '/__w/1/s/flink-clients/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1360757Z Moving '/__w/1/s/flink-streaming-java/java_pid5009.hprof' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1421701Z Moving '/__w/1/s/flink-streaming-java/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun2.dump' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1460325Z Moving '/__w/1/s/flink-streaming-java/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun1.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1492941Z Moving '/__w/1/s/flink-streaming-java/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun1.dump' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1523043Z Moving '/__w/1/s/flink-streaming-java/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1554119Z Moving '/__w/1/s/flink-optimizer/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1589618Z Moving '/__w/1/s/flink-scala/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')
2021-03-23T09:11:22.1638143Z Compressing debug files
{code}

I'm not sure if this includes one or two issues.

One issue seems to be in the flink-streaming-java tests:
{code}
# Created at 2021-03-23T09:03:49.666
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'java.lang.OutOfMemoryError: Java heap space'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'java.lang.OutOfMemoryError: Java heap space'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)


# Created at 2021-03-23T09:03:49.667
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'Dumping heap to java_pid5009.hprof ...'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'Dumping heap to java_pid5009.hprof ...'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)


# Created at 2021-03-23T09:03:49.693
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'Heap dump file created [2834354 bytes in 0.027 secs]'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'Heap dump file created [2834354 bytes in 0.027 secs]'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)
{code}

There's a coredump included:

 !image-2021-03-23-13-18-41-836.png! 
Looks like the testInitialSizeCompoutation test is causing the / a failure.

The second issue is (which is reported as a test failure): 
{code}
# Created at 2021-03-23T09:09:39.376
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError

# Created at 2021-03-23T09:09:39.535
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError

# Created at 2021-03-23T09:11:10.037
pure virtual method called

# Created at 2021-03-23T09:11:10.037
terminate called without an active exception

# Created at 2021-03-23T09:11:12.170
Aborted (core dumped)
{code}

From the coredump, I see the following:
{code}
(gdb) where
#0  0x00007f9343508438 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007f934350a03a in __GI_abort () at abort.c:89
#2  0x00007f9341b8e84d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007f9341b8c6b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007f9341b8c701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007f9341b8d23f in __cxa_pure_virtual () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007f92ed39c0d5 in rocksdb::DBImpl::CloseHelper() () from /tmp/librocksdbjni8334465068904677424.so
#7  0x00007f92ed3a717b in rocksdb::DBImpl::~DBImpl() () from /tmp/librocksdbjni8334465068904677424.so
#8  0x00007f92ed3a7451 in rocksdb::DBImpl::~DBImpl() () from /tmp/librocksdbjni8334465068904677424.so
#9  0x00007f932d801aa8 in ?? ()
#10 0x0000000085677fe8 in ?? ()
#11 0x00007f931c8da910 in ?? ()
#12 0x00007f931c8da968 in ?? ()
#13 0x00007f932d007ffd in ?? ()
#14 0x0000000000000000 in ?? ()
{code}


",,dwysakowicz,kezhuw,pnowojski,rmetzger,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21965,FLINK-22025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/21 12:18;rmetzger;image-2021-03-23-13-18-41-836.png;https://issues.apache.org/jira/secure/attachment/13022817/image-2021-03-23-13-18-41-836.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 10:30:10 UTC 2021,,,,,,,,,,"0|z0p34w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 13:15;trohrmann;Another instance: https://dev.azure.com/tillrohrmann/flink/_build/results?buildId=534&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=8737;;;","23/Mar/21 18:22;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9006&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b;;;","23/Mar/21 18:22;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9006&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85;;;","24/Mar/21 17:24;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15377&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=8743
;;;","24/Mar/21 17:26;dwysakowicz;Is there a chance you could help us debug the issue [~yunta] ?;;;","24/Mar/21 17:32;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15352&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=21970;;;","26/Mar/21 02:57;yunta;[~dwysakowicz] Sorry for late reply as I am still suffering for apache Jira not sending me any notifications. 

I think this blocker is caused by [pull/15287|https://github.com/apache/flink/pull/15287] in which new test {{StateBackendTestBase#testKeyGroupedInternalPriorityQueue}} did not close and dispose the [keyed state backend|https://github.com/apache/flink/blob/30e0e1e0726763a9bec21bbb9cca5e92e31b792d/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateBackendTestBase.java#L283] finally. (cc [~pnowojski] )

BTW, I think  [pull/15287|https://github.com/apache/flink/pull/15287] might not have the correct Jira ticket id.;;;","26/Mar/21 07:59;dwysakowicz;Thank you so much for your help [~yunta]! Hopefully you saved us quite some time ;);;;","26/Mar/21 09:40;yunta;Merged in master with:

d70f9ff111c55c063a1d27c8cb97ae633e00e4c8 and 97f257e46824772728a1a595edb57e5633903a55;;;","26/Mar/21 10:30;pnowojski;Thanks for fixing this issue [~yunta] and sorry for causing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DuplicateJobSubmissionException after JobManager failover,FLINK-21928,13366926,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,uce,uce,23/Mar/21 08:37,28/Feb/22 12:48,13/Jul/23 08:07,15/Jul/21 09:28,1.10.3,1.11.3,1.12.2,1.13.0,,,,,1.14.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,stale-critical,,,,"Consider the following scenario:
 * Environment: StandaloneApplicationClusterEntryPoint using a fixed job ID, high availability enabled
 * Flink job reaches a globally terminal state

 * Flink job is marked as finished in the high-availability service's RunningJobsRegistry

 * The JobManager fails over

On recovery, the [Dispatcher throws DuplicateJobSubmissionException, because the job is marked as done in the RunningJobsRegistry|https://github.com/apache/flink/blob/release-1.12.2/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L332-L340].

When this happens, users cannot get out of the situation without manually redeploying the JobManager process and changing the job ID^1^.

The desired semantics are that we don't want to re-execute a job that has reached a globally terminal state. In this particular case, we know that the job has already reached such a state (as it has been marked in the registry). Therefore, we could handle this case by executing the regular termination sequence instead of throwing a DuplicateJobSubmission.

---

^1^ With ZooKeeper HA, the respective node is not ephemeral. In Kubernetes HA, there is no  notion of ephemeral data that is tied to a session in the first place afaik.

 ","StandaloneApplicationClusterEntryPoint using a fixed job ID, High Availability enabled",rmetzger,trohrmann,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26333,,,,,,,,,,,FLINK-11813,,,,,,,,,,,,,,,,FLINK-21979,FLINK-21980,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 15 09:24:49 UTC 2021,,,,,,,,,,"0|z0p2v4:",9223372036854775807,"The fix for this problem only works if the ApplicationMode is used with a single job submission and if the user code does not access the `JobExecutionResult`. If any of these conditions is violated, then Flink cannot guarantee that the whole Flink application is executed.

Additionally, it is still required that the user cleans up the corresponding HA entries for the running jobs registry because these entries won't be reliably cleaned up when encountering the situation described by FLINK-21928.",,,,,,,,,,,,,,,,,,,"25/Mar/21 14:57;trohrmann;What would be the desired return value for the client as the response to the submit call? If we say that we have the job already executed then we might not have the final result of the job which we can tell the client?

In general, I think the {{RunningJobsRegistry}} does not properly work. The underlying problem is that the registry needs to contain information which can outlive the lifetime of a {{Dispatcher}} in order to work. See FLINK-11813 for more details. 

One idea to fix it could be that the registry is owned by the deployer of a Flink cluster and, hence, the cluster no longer clears the entries of the registry. The user of the Flink cluster is then responsible for cleaning up the registry after the cluster has been shut down. Moreover, we could say that this feature needs to be explicitly activated and is actually only useful for guarding against standby {{JobManagers}} which should not execute the job after it has been completed by another {{JobManager}}.

As a kind of a quick fix, we could think about simply removing the registry since it is broken anyway.;;;","22/Apr/21 12:23;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:49;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/May/21 22:56;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","27/May/21 23:05;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","07/Jun/21 22:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","15/Jun/21 22:41;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","23/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Jul/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","15/Jul/21 09:24;trohrmann;Fixed via

f483008db867364adc66acbf0d207ca3a50e75e5
172b9e322151c079cb1c31323ec6697da48e67de
4a78097d0385749daceafd8326930c8cc5f26f1a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExpressionReducer compile fail when running long term,FLINK-21927,13366916,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xmarker,yimeisun,yimeisun,23/Mar/21 08:16,28/Aug/21 11:13,13/Jul/23 08:07,02/Apr/21 01:01,1.12.2,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Our online service use flink table api as SQL parse and optimize component.   When service  run several months we found a lot of exceptions and most of SQLs run fail.  The exception is below: 
    org.codehaus.commons.compiler.CompileException: Line 2, Column 39: '{' expected instead of '-'

ExpressionReducer use function newName() in CodeGenUtils.scala to generate class name, and use a increasing AtomicInteger variable as class name suffix. When run several months, the AtomicInteger is overflow, and the generated class name is invalid like ""ExpressionReducer$-2147483648"".

If we change the field nameCounter's type to AtomicLong in CodeGenUtils.scala, I think it would work well.
",,jark,libenchao,twalthr,xmarker,yimeisun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 02 01:01:49 UTC 2021,,,,,,,,,,"0|z0p2sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 09:26;xmarker;This issue seems easy, let me resolve it?  :D;;;","23/Mar/21 10:57;twalthr;[~xmarker] thanks for working on this. I will assign this issue to you.;;;","02/Apr/21 01:01;ykt836;fixed: 56f275c989933bfdb06c9a85117b14a53464c1f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SplitAggregateRule will be abnormal, when the sum/count and avg in SQL at the same time",FLINK-21923,13366897,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,23/Mar/21 06:51,28/May/21 09:13,13/Jul/23 08:07,26/Apr/21 02:46,1.10.0,,,,,,,,1.13.1,1.14.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"SplitAggregateRule optimizes one-layer aggregation to two-layer aggregation to improve computing performance under data skew.
In the partial phase, avg will be translated into count and sum. If count already exists in the original SQL at this time, the engine will remove the duplicate count, and then add Project to calculate and restore the optimized count result value.
{code:java}
    relBuilder.aggregate(
      relBuilder.groupKey(fullGroupSet, ImmutableList.of[ImmutableBitSet](fullGroupSet)),
      newPartialAggCalls)
    relBuilder.peek().asInstanceOf[FlinkLogicalAggregate]
      .setPartialFinalType(PartialFinalType.PARTIAL)
{code}
so `relBuilder.peek()` will return `FlinkLogicalCalc` not `FlinkLogicalAggregate`,
then will throw exception like 
{code:java}
java.lang.ClassCastException: org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalCalc cannot be cast to org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalAggregate

	at org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.onMatch(SplitAggregateRule.scala:286)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:284)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:889)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:283)
	at org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.testAggWithFilterClause2(SplitAggregateRuleTest.scala:205)
{code}
We can reproduce stably and pass the test cases in `SplitAggregateRuleTest`

{code:java}
  @Test
  def testAggFilterClauseBothWithAvgAndCount(): Unit = {
    util.tableEnv.getConfig.getConfiguration.setBoolean(
      OptimizerConfigOptions.TABLE_OPTIMIZER_DISTINCT_AGG_SPLIT_ENABLED, true)
    val sqlQuery =
      s""""""
         |SELECT
         |  a,
         |  COUNT(DISTINCT b) FILTER (WHERE NOT b = 2),
         |  SUM(b) FILTER (WHERE NOT b = 5),
         |  COUNT(b),
         |  AVG(b),
         |  SUM(b)
         |FROM MyTable
         |GROUP BY a
         |"""""".stripMargin
    util.verifyRelPlan(sqlQuery)
  }
{code}

",,godfreyhe,jark,jingzhang,libenchao,tartarus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 26 02:46:00 UTC 2021,,,,,,,,,,"0|z0p2oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 06:52;tartarus;[~jark] Please assign it to me, thanks;;;","06/Apr/21 08:13;tartarus;`LogicalTableAggregate` does not have a `PartialFinalType` member variable, so TableAggregate cannot support Partial-final either.
Then `SplitAggregateRule` can skip TableAggregate, and then use `FlinkLogicalAggregate.create()` to avoid ClassCastException.
[~jark] what do you think of this?;;;","16/Apr/21 10:43;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","26/Apr/21 02:46;godfreyhe;Fixed in 1.14.0: b2e65a41914766ab4b1f3495f7196611561fea4c
Fixed in 1.13.0: a3363b91b144edfbae5ab114984ded622d3f8fbc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The method partition_by in Over doesn't work for expression dsl,FLINK-21922,13366893,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,23/Mar/21 06:28,23/Mar/21 09:33,13/Jul/23 08:07,23/Mar/21 09:33,1.12.0,,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,"For the following example:
{code}

t = t_env.from_elements([(1, 1, ""Hello"")], ['a', 'b', 'c'])

result = t.over_window(
 Over.partition_by(t.c)
 .order_by(""a"")
 .preceding(expr.row_interval(2))
 .following(expr.CURRENT_ROW)
 .alias(""w""))
{code}

It will throw the following exception:

{code}
org.apache.flink.api.python.shaded.py4j.Py4JException: Method partitionBy([class org.apache.flink.table.api.ApiExpression]) does not existorg.apache.flink.api.python.shaded.py4j.Py4JException: Method partitionBy([class org.apache.flink.table.api.ApiExpression]) does not exist at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318) at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339) at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:276) at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79) at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748)
{code}",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 09:33:01 UTC 2021,,,,,,,,,,"0|z0p2ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 09:33;dian.fu;Fixed in
- master via 1892a24d4096d531847cca5267243ce588f90f6e
- release-1.12 via 40d4d0444fccadfa50d8ba466a113d2728fa9f4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add back missing  kafka.md zh docs,FLINK-21917,13366859,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yangsanity,leonard,leonard,23/Mar/21 01:43,27/Mar/21 03:56,13/Jul/23 08:07,27/Mar/21 03:56,1.13.0,,,,,,,,1.13.0,,,,,,,Documentation,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"We lost some Chinese doc pages when migrate  doc website from Jekyll to Hugo in ([https://github.com/apache/flink/pull/14903]),  such as `jdbc zh`, `kafka zh`.

we should add them back.

 ",,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 27 03:55:51 UTC 2021,,,,,,,,,,"0|z0p2g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/21 03:55;jark;Fixed in master: 5b33c79b5dfa1260fa85affdc7d938a01da97f82;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong Checks whether the service has been started.,FLINK-21910,13366681,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pengkangjing,pengkangjing,pengkangjing,22/Mar/21 08:46,23/Mar/21 13:53,13/Jul/23 08:07,23/Mar/21 13:53,1.11.3,1.12.2,1.13.0,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"As the pic below.   jobLeaderIdActions shouldn't be null  when JobLeaderIdService is started, so this check is incorrect.

 

!image-2021-03-22-16-52-44-183.png!",,pengkangjing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/21 08:52;pengkangjing;image-2021-03-22-16-52-44-183.png;https://issues.apache.org/jira/secure/attachment/13022757/image-2021-03-22-16-52-44-183.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 13:53:36 UTC 2021,,,,,,,,,,"0|z0p1co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 13:53;trohrmann;Fixed via 91c15110d6c116b8921057c9bcc46e4ac474ef30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parseJmJvmArgsAndExportLogs: command not found warning when starting StateFun,FLINK-21904,13366634,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tzulitai,tzulitai,tzulitai,22/Mar/21 03:49,23/Mar/21 09:44,13/Jul/23 08:07,23/Mar/21 09:44,statefun-2.2.2,,,,,,,,statefun-3.0.0,,,,,,,Stateful Functions,,,,,0,pull-request-available,,,,,"I'm seeing this warning in E2E logs:
{code}
11:37:12,572 ERROR org.apache.flink.statefun.e2e.remote.ExactlyOnceWithRemoteFnE2E  - /opt/flink/bin/standalone-job.sh: line 43: parseJmJvmArgsAndExportLogs: command not found
{code}

This was caused by FLINK-19662, which renamed {{parseJmJvmArgsAndExportLogs}} to {{parseJmArgsAndExportLogs}}.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 09:44:19 UTC 2021,,,,,,,,,,"0|z0p128:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 09:44;tzulitai;flink-statefun/master: 7005f6a75e0a24445dd9b59fda6162cce8018b48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupAggregateJsonPlanTest.testDistinctAggCalls fail,FLINK-21896,13366614,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,maguowei,maguowei,22/Mar/21 02:19,14/Jan/22 09:03,13/Jul/23 08:07,24/Mar/21 13:14,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6364


{code:java}

	at org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.testDistinctAggCalls(GroupAggregateJsonPlanTest.java:148)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)

{code}
",,dwysakowicz,godfreyhe,jark,maguowei,soumyajitsahu,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 14 09:03:10 UTC 2022,,,,,,,,,,"0|z0p0xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/21 03:21;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15115&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6791;;;","22/Mar/21 06:14;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15137&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6791;;;","23/Mar/21 04:09;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15215&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6543;;;","23/Mar/21 07:09;dwysakowicz;Hey [~jark] [~godfreyhe] could you take a look?;;;","23/Mar/21 07:33;godfreyhe;Thanks for reporting this, I will take this issue. ;;;","23/Mar/21 07:49;dwysakowicz;Thank you [~godfreyhe]!;;;","23/Mar/21 14:35;godfreyhe;The reason is the serialization results for an object without {{serialVersionUID}} are different between jdk8 and jdk11. The following code can reproduce the difference:

{code:java}
      StructuredType structuredType =
                StructuredType.newBuilder(MapView.class)
                        .attributes(
                                Collections.singletonList(
                                        new StructuredType.StructuredAttribute(
                                                ""map"",
                                                new MapType(new IntType(), new BigIntType(false)))))
                        .build();
        System.out.println(EncodingUtils.encodeObjectToString(structuredType));
{code}

the result of jdk8 is
{code:java}
rO0ABXNyADNvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnR5cGVzLmxvZ2ljYWwuU3RydWN0dXJlZFR5cGUt-WvI46_69wIABVoADmlzSW5zdGFudGlhYmxlTAAKYXR0cmlidXRlc3QAEExqYXZhL3V0aWwvTGlzdDtMAAtjb21
......

{code}

the result of jdk11 is
{code:java}
rO0ABXNyADNvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnR5cGVzLmxvZ2ljYWwuU3RydWN0dXJlZFR5cGWW-4qkzgeWwgIABVoADmlzSW5zdGFudGlhYmxlTAAKYXR0cmlidXRlc3QAEExqYXZhL3V0aWwvTGlzdDtMAAtjb21......
{code}

Because StructuredType has no serialVersionUID.

Why there are only two cases will occur such errors? because the two cases have distinct aggregate with local aggregate, which will produce RawType (RawType will encode the inner serializer object (which contains StructuredType) into string as the value of {{getSerializerString}}, see RawType#getSerializerString), the full structure of RawType in failed cases is:

{code:java}
StructuredType structuredType =
                StructuredType.newBuilder(MapView.class)
                        .attributes(
                                Collections.singletonList(
                                        new StructuredType.StructuredAttribute(
                                                ""map"",
                                                new MapType(new IntType(), new BigIntType(false)))))
                        .build();

 new RawType<>(
                        MapView.class,
                        ExternalSerializer.of(
                                new FieldsDataType(
                                        structuredType,
                                        Collections.singletonList(
                                                new KeyValueDataType(
                                                        new MapType(
                                                                new IntType(),
                                                                new BigIntType(false)),
                                                        new AtomicDataType(new IntType()),
                                                        new AtomicDataType(
                                                                new BigIntType(false)))))))
{code}

The solution is we add serialVersionUID for StructuredType.
And I suggest we should add serialVersionUID for all logical types.




;;;","24/Mar/21 04:03;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15316&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6795;;;","24/Mar/21 13:14;godfreyhe;Fixed in 1.13.0: d3d320be60b07fb4be1b133a5fae507183bd694f;;;","13/Jan/22 21:04;soumyajitsahu;[~godfreyhe] [~twalthr] This would break upgrade from Flink 12 to 13, right?
If serialization was done using Flink 12, and then app was redeployed using Flink 13, then the deserialization throws this error.

To mitigate, we had to delete our app's state and start the flink app afresh.


java.io.InvalidClassException: org.apache.flink.table.types.logical.LogicalType; local class incompatible: stream classdesc serialVersionUID = -7381419642101800907, local class serialVersionUID = 1
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2003)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1850)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2003)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1850)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2160)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:615)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:593);;;","14/Jan/22 09:03;twalthr;[~soumyajitsahu] We currently don't support upgrades for Table API & SQL:
https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/table/concepts/overview/#stateful-upgrades-and-evolution

But we are actively working on it. See:
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncrementalAggregateJsonPlanTest.testIncrementalAggregate fail,FLINK-21895,13366613,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,maguowei,maguowei,22/Mar/21 02:16,24/Mar/21 13:14,13/Jul/23 08:07,24/Mar/21 13:14,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6303


{code:java}
org.junit.ComparisonFailure: expected:<...C5TdHJ1Y3R1cmVkVHlwZ[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')""}]},""description"":""LocalGroupAggregate(groupBy=[a, $f2], partialFinalType=[PARTIAL], select=[a, $f2, COUNT(distinct$0 c) AS count$0, DISTINCT(c) AS distinct$0])""},{""class"":""org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecExchange"",""id"": 0,""inputProperties"":[{""requiredDistribution"":{""type"":""HASH"",""keys"":[0,1]},""damBehavior"":""PIPELINED"",""priority"":0}],""outputType"":{""type"":""ROW"",""nullable"":true,""fields"":[{""a"":""BIGINT""},{""$f2"":""INT""},{""count$0"":""BIGINT""},{""distinct$0"":""RAW('org.apache.flink.table.api.dataview.MapView', 'AFZvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkV4dGVybmFsU2VyaWFsaXplciRFeHRlcm5hbFNlcmlhbGl6ZXJTbmFwc2hvdAAAAAMADecEAAAAAaztAAVzcgArb3JnLmFwYWNoZS5mbGluay50YWJsZS50eXBlcy5GaWVsZHNEYXRhVHlwZfSwrBytgZ9fAgABTAAOZmllbGREYXRhVHlwZXN0ABBMamF2YS91dGlsL0xpc3Q7eHIAJW9yZy5hcGFjaGUuZmxpbmsudGFibGUudHlwZXMuRGF0YVR5cGV5y2rIj5/EeAIAAkwAD2NvbnZlcnNpb25DbGFzc3QAEUxqYXZhL2xhbmcvQ2xhc3M7TAALbG9naWNhbFR5cGV0ADJMb3JnL2FwYWNoZS9mbGluay90YWJsZS90eXBlcy9sb2dpY2FsL0xvZ2ljYWxUeXBlO3hwdnIAK29yZy5hcGFjaGUuZmxpbmsudGFibGUuYXBpLmRhdGF2aWV3Lk1hcFZpZXcAAAAAAAAAAAAAAHhwc3IAM29yZy5hcGFjaGUuZmxpbmsudGFibGUudHlwZXMubG9naWNhbC5TdHJ1Y3R1cmVkVHlwZS35a8jjr/r3]AgAFWgAOaXNJbnN0YW50...> but was:<...C5TdHJ1Y3R1cmVkVHlwZ[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')""}]},""description"":""LocalGroupAggregate(groupBy=[a, $f2], partialFinalType=[PARTIAL], select=[a, $f2, COUNT(distinct$0 c) AS count$0, DISTINCT(c) AS distinct$0])""},{""class"":""org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecExchange"",""id"": 0,""inputProperties"":[{""requiredDistribution"":{""type"":""HASH"",""keys"":[0,1]},""damBehavior"":""PIPELINED"",""priority"":0}],""outputType"":{""type"":""ROW"",""nullable"":true,""fields"":[{""a"":""BIGINT""},{""$f2"":""INT""},{""count$0"":""BIGINT""},{""distinct$0"":""RAW('org.apache.flink.table.api.dataview.MapView', 'AFZvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkV4dGVybmFsU2VyaWFsaXplciRFeHRlcm5hbFNlcmlhbGl6ZXJTbmFwc2hvdAAAAAMADecEAAAAAaztAAVzcgArb3JnLmFwYWNoZS5mbGluay50YWJsZS50eXBlcy5GaWVsZHNEYXRhVHlwZfSwrBytgZ9fAgABTAAOZmllbGREYXRhVHlwZXN0ABBMamF2YS91dGlsL0xpc3Q7eHIAJW9yZy5hcGFjaGUuZmxpbmsudGFibGUudHlwZXMuRGF0YVR5cGV5y2rIj5/EeAIAAkwAD2NvbnZlcnNpb25DbGFzc3QAEUxqYXZhL2xhbmcvQ2xhc3M7TAALbG9naWNhbFR5cGV0ADJMb3JnL2FwYWNoZS9mbGluay90YWJsZS90eXBlcy9sb2dpY2FsL0xvZ2ljYWxUeXBlO3hwdnIAK29yZy5hcGFjaGUuZmxpbmsudGFibGUuYXBpLmRhdGF2aWV3Lk1hcFZpZXcAAAAAAAAAAAAAAHhwc3IAM29yZy5hcGFjaGUuZmxpbmsudGFibGUudHlwZXMubG9naWNhbC5TdHJ1Y3R1cmVkVHlwZZb7iqTOB5bC]AgAFWgAOaXNJbnN0YW50...>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyJsonPlan(TableTestBase.scala:750)
	at org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.testIncrementalAggregate(IncrementalAggregateJsonPlanTest.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)

{code}
",,dwysakowicz,godfreyhe,jark,maguowei,Terry1897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 13:14:50 UTC 2021,,,,,,,,,,"0|z0p0xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/21 03:21;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15115&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6791;;;","22/Mar/21 06:14;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15137&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6791;;;","23/Mar/21 04:09;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15215&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6543;;;","23/Mar/21 07:09;dwysakowicz;Hey [~jark] [~godfreyhe] could you take a look?;;;","23/Mar/21 14:35;godfreyhe;see https://issues.apache.org/jira/browse/FLINK-21896;;;","24/Mar/21 04:04;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15316&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6795;;;","24/Mar/21 13:14;godfreyhe;Fixed in 1.13.0: d3d320be60b07fb4be1b133a5fae507183bd694f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A ValidationException will be thrown out if partition key of Rank is an expression result of input Node.,FLINK-21893,13366571,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jingzhang,jingzhang,21/Mar/21 13:03,21/Mar/21 13:14,13/Jul/23 08:07,21/Mar/21 13:13,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,,"A ValidationException will be thrown out if partition key of Rank is an expression result of input Node. e.g If run the following sql, A validationException will be thrown out.

 
{code:java}
//代码占位符
@Test
def test(): Unit = {
  val data = List(
    (2001L, 2L),
    (2002L, 3L)
  )

  val ds = failingDataSource(data).toTable(tEnv, 'video_id, 'cnt, 'proctime.proctime)
  tEnv.registerTable(""T"", ds)

  val sql =
    """"""
      |SELECT
      |  video_id,
      |  cnt,
      |  rownum_2
      |FROM
      |(
      |  SELECT
      |    video_id,
      |    cnt,
      |    ROW_NUMBER() OVER (
      |      ORDER BY cnt DESC
      |    ) AS rownum_2
      |    FROM
      |    (
      |      SELECT
      |        video_id,
      |        cnt,
      |        ROW_NUMBER() OVER (
      |          PARTITION BY bucket_id
      |          ORDER BY cnt DESC
      |        ) AS rownum_1
      |        FROM
      |        (
      |           SELECT
      |             video_id,
      |             cnt,
      |             MOD(video_id, 64) as bucket_id
      |             FROM T
      |         )
      |    )
      |    WHERE rownum_1 <= 1000
      |)
      |WHERE rownum_2 <= 1000
      |"""""".stripMargin

  val sink = new TestingRetractSink
  tEnv.sqlQuery(sql).toRetractStream[Row].addSink(sink).setParallelism(1)
  env.execute()
}
{code}
Exception detail
{code:java}
//代码占位符
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [$2]org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [$2]
 at org.apache.flink.table.types.logical.RowType.validateFields(RowType.java:277) at org.apache.flink.table.types.logical.RowType.<init>(RowType.java:158) at org.apache.flink.table.types.logical.RowType.<init>(RowType.java:162) at org.apache.flink.table.types.logical.RowType.of(RowType.java:294) at org.apache.flink.table.planner.calcite.FlinkTypeFactory$.toLogicalRowType(FlinkTypeFactory.scala:503) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.translateToPlanInternal(StreamExecRank.scala:212) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.translateToPlanInternal(StreamExecRank.scala:53) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.translateToPlan(StreamExecRank.scala:53) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.translateToPlanInternal(StreamExecExchange.scala:84) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.translateToPlanInternal(StreamExecExchange.scala:44) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) {code}
 

When apply `FlinkLogicalRankRuleBase`  on `FlinkLogicalCalc`-`FlinkLogicalOverAggregate`,  a `FlinkLogicalRank` with following rowType will be get. Then a `StreamExecRank` with same rowType would be generated 

!image-2021-03-21-21-07-43-230.png!",,jark,jingzhang,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/21 13:07;jingzhang;image-2021-03-21-21-07-43-230.png;https://issues.apache.org/jira/secure/attachment/13022732/image-2021-03-21-21-07-43-230.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 21 13:14:19 UTC 2021,,,,,,,,,,"0|z0p0o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/21 13:11;jark;Would you like to fix this [~qingru zhang]?;;;","21/Mar/21 13:14;jingzhang;[~jark] sorry, the bug exists in 1.10 version, but not exists in master branch. It seems the bug is already fix. I've close the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The .staging_xxx directory isn't deleted after writing data to hive table in batch mode,FLINK-21891,13366563,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tiny,tiny,tiny,21/Mar/21 09:49,28/May/21 09:03,13/Jul/23 08:07,12/Apr/21 08:19,1.12.0,,,,,,,,1.13.0,,,,,,,Connectors / Hive,Table SQL / Runtime,,,,0,pull-request-available,,,,,"In flink 1.12.0, use Blink Planner to read data from Hbase and write the results to Hive via Flink SQL.

The .staging_xxx files on HDFS:
 /user/hive/warehouse/user.db/t_hive_user_group_result/.staging_1616074732697
 /user/hive/warehouse/user.db/t_hive_user_group_result/.staging_1616120408195
 /user/hive/warehouse/user.db/t_hive_user_group_result/.staging_1616121007337
 /user/hive/warehouse/user.db/t_hive_user_group_result/.staging_1616121607484
 /user/hive/warehouse/user.db/t_hive_user_group_result/.staging_1616206808142

I found the following code in `org.apache.flink.table.filesystem.FileSystemOutputFormat` caused the problem:
{code:java}
import java.io.File;

@Override
public void finalizeGlobal(int parallelism) {
    try {        
        FileSystemCommitter committer = new FileSystemCommitter(  
          fsFactory, msFactory, overwrite, tmpPath, partitionColumns.length);           
        committer.commitUpToCheckpoint(CHECKPOINT_ID);    
    } catch (Exception e) {        
        throw new TableException(""Exception in finalizeGlobal"", e);    
    } finally {        
        new File(tmpPath.getPath()).delete();   // the error code    
    }
}

{code}
The code in finally code block `new File(..)` can't convert `tmpPath` to HDFS file instance, I think the following code is more correct and works for me:
{code:java}
fsFactory.create(tmpPath.toUri()).delete(tmpPath, true);

{code}
A similar code has appeared in the class of PartitionTempFileManager.
  ",,jark,libenchao,lirui,tiny,,,,,,,,,,,,,,,,,,,,,,FLINK-20230,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20230,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 08:19:24 UTC 2021,,,,,,,,,,"0|z0p0mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 02:26;jark;cc [~lirui];;;","08/Apr/21 12:21;lirui;Hi [~tiny], thanks for reporting the issue and the analysis. I think the proposed fix is correct. I'll open a PR for it, unless you would like to work on it.;;;","08/Apr/21 14:01;tiny;Hi [~lirui], thank you, I'd like to try to fix the issue.;;;","08/Apr/21 14:46;jark;I assigned this issue to you [~tiny].;;;","10/Apr/21 06:47;tiny;[~jark], [~lirui] I have opened a pr, can u help to review it, thanks.;;;","12/Apr/21 08:19;tiny;solved by https://issues.apache.org/jira/browse/FLINK-20230;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ActiveResourceManagerTest.testWorkerRegistrationTimeoutNotCountingAllocationTime fails on AZP,FLINK-21879,13366371,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,fpaul,fpaul,19/Mar/21 14:29,28/Aug/21 11:14,13/Jul/23 08:07,16/Apr/21 08:38,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15047&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=6760
{code:java}
[ERROR] testWorkerRegistrationTimeoutNotCountingAllocationTime(org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest)  Time elapsed: 0.388 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: an instance of org.apache.flink.runtime.registration.RegistrationResponse$Success
     but: <Rejected TaskExecutor registration at the ResourceManger because: The ResourceManager does not recognize this TaskExecutor.> is a org.apache.flink.runtime.taskexecutor.TaskExecutorRegistrationRejection
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest$13.lambda$new$2(ActiveResourceManagerTest.java:789)
	at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest$Context.runTest(ActiveResourceManagerTest.java:857)
	at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest$13.<init>(ActiveResourceManagerTest.java:770)
	at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.testWorkerRegistrationTimeoutNotCountingAllocationTime(ActiveResourceManagerTest.java:753)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,dwysakowicz,fpaul,kevin.cyj,maguowei,roman,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 16 08:38:33 UTC 2021,,,,,,,,,,"0|z0ozfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/21 15:00;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15036&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=6752;;;","21/Mar/21 19:59;roman;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15060&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2];;;","22/Mar/21 05:12;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15137&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=8252;;;","22/Mar/21 17:19;trohrmann;I think the problem could be caused by completing the {{requestResourceFuture}} not in the main thread as required by {{ResourceManagerDriver#requestResource}}.;;;","23/Mar/21 13:26;trohrmann;Fixed via d80fa036efbd8de03d04535d636355be2c357c81;;;","30/Mar/21 05:21;kevin.cyj;The test seems still fails: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15744&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a];;;","30/Mar/21 07:24;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15710&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=6772;;;","30/Mar/21 14:20;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15774&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=6687;;;","31/Mar/21 20:19;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15915&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","01/Apr/21 20:01;trohrmann;The problem seems to be that AZP is not very fast. Test relies on the fact that a `TaskExecutor` registers within 50ms. However, in the logs on can see that there are gaps of 250ms which causes the test to fail.;;;","06/Apr/21 10:20;trohrmann;Further hardened via

master: 97f3330021a768cf796241618cc34652b024ffcd;;;","07/Apr/21 08:32;dwysakowicz;Hey [~trohrmann] just letting you know that I created the 1.13 branch accidentally when creating the rc0. It is not a proper 1.13 branch yet. There is no need to merge it there yet. We will announce the actual cut off in the ML. ;;;","13/Apr/21 05:54;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16357&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=5875

;;;","16/Apr/21 08:38;trohrmann;Hopefully finally fixed via ebfdf2a5ba744c6fca2dc793878ffecc6be34c15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GSRKinesisPubsubClient is not compiling in IntelliJ,FLINK-21875,13366344,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dannycranmer,pnowojski,pnowojski,19/Mar/21 12:40,04/Apr/21 18:14,13/Jul/23 08:07,19/Mar/21 21:34,1.13.0,,,,,,,,1.13.0,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"When trying to compile Flink from IntelliJ:
{noformat}
(...)/flink/flink-end-to-end-tests/flink-glue-schema-registry-test/src/main/java/org/apache/flink/glue/schema/registry/test/GSRKinesisPubsubClient.java:21:76
java: package org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.model does not exist
(...)/flink/flink-end-to-end-tests/flink-glue-schema-registry-test/src/main/java/org/apache/flink/glue/schema/registry/test/GSRKinesisPubsubClient.java:22:76
java: package org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.model does not exist
{noformat}
IntelliJ is not applying shading, so code shouldn't be referring to shaded classes.

CC [~mohitpaliwal] ",,dannycranmer,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19667,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 04 18:14:23 UTC 2021,,,,,,,,,,"0|z0oz9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/21 12:43;rmetzger;[~danny.cranmer] / [~dannycranmer] Can you take a look?;;;","19/Mar/21 13:01;dannycranmer;Yes I will take a look now;;;","19/Mar/21 13:54;dannycranmer;I will merge once the e2e tests pass:
- https://github.com/apache/flink/pull/15290
;;;","04/Apr/21 18:14;rmetzger;Thanks a lot for resolving this so quickly!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for out-dated slot allocation confirmations is insufficient,FLINK-21874,13366330,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,19/Mar/21 11:50,23/Mar/21 07:44,13/Jul/23 08:07,23/Mar/21 07:44,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The {{DeclarativeSlotManager}} accounts for the possibility of slot allocation confirmations coming in after the slot has already been freed fully allocated (e.g., through a SlotReport) by checking whether there is still a pending request for that slot.
However, this only works properly if the slot was allocated.

If the slot was freed in the mean time then it could've already been re-assigned to another job, which result in an IllegalStateException once the confirmation arrives.
It can also happen that the slot is reassigned to the same job, which can result in a premature configuratiom of the allocation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 07:44:45 UTC 2021,,,,,,,,,,"0|z0oz6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 07:44;chesnay;master: 63be0cd6d6434b054dde9a1779299e97d0b0545c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatedSourceRescaleITCase.testUpscaling fails on AZP,FLINK-21873,13366310,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,trohrmann,trohrmann,19/Mar/21 10:39,22/Jun/21 14:04,13/Jul/23 08:07,07/Apr/21 21:16,1.13.0,,,,,,,,1.13.0,,,,,,,Connectors / Common,,,,,0,pull-request-available,test-stability,,,,"The test {{CoordinatedSourceRescaleITCase.testUpscaling}} fails on AZP with

{code}

	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
Caused by: java.lang.Exception: successfully restored checkpoint
	at org.apache.flink.connector.base.source.reader.CoordinatedSourceRescaleITCase$FailingMapFunction.map(CoordinatedSourceRescaleITCase.java:139)
	at org.apache.flink.connector.base.source.reader.CoordinatedSourceRescaleITCase$FailingMapFunction.map(CoordinatedSourceRescaleITCase.java:126)
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:161)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101)
	at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.pollNext(IteratorSourceReader.java:95)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:275)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:408)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:190)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:624)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:588)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14997&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22049

cc [~AHeise]",,AHeise,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 21:16:17 UTC 2021,,,,,,,,,,"0|z0oz28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 09:54;arvid;The exception actually showed that everything works as expected. However, the message is currently checked by identity, while apparently, we just received an equal message (I feel that this is connected to the inlining of Java's string pool). So the proper fix is to just check for equality.

While doing so, I have also switched to use Flink's internal ExceptionUtils and made it more lenient in this way.;;;","07/Apr/21 21:16;arvid;Merged into master as 9953206599910983425dceea7a48164370fa605b and 1.12 as 69cd36d9aa6e4aeb2ad827020d125712307ab585.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outdated slot offer responses may release slot prematurely,FLINK-21859,13366059,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,kevin.cyj,kevin.cyj,18/Mar/21 11:25,28/May/21 09:07,13/Jul/23 08:07,14/Apr/21 21:54,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Here is the error stack:
{code:java}
2021-03-18 19:05:31org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy   
   at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:130 undefined)   
   at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:81 undefined)   
   at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221 undefined)   
   at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212 undefined)   
   at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203 undefined)   
   at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:701 undefined)   
   at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51 undefined)   
   at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1449 undefined)   
   at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1105 undefined)   
   at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1045 undefined)   
   at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:754 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:195 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:182 undefined)   
   at org.apache.flink.runtime.scheduler.SharedSlot.lambda$release$4(SharedSlot.java:271 undefined)   
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656 undefined)   
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:669 undefined)   
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:1997 undefined)   
   at org.apache.flink.runtime.scheduler.SharedSlot.release(SharedSlot.java:271 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:152 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releasePayload(DefaultDeclarativeSlotPool.java:385 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.lambda$releaseSlot$1(DefaultDeclarativeSlotPool.java:376 undefined)   
   at java.util.Optional.ifPresent(Optional.java:159 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlot(DefaultDeclarativeSlotPool.java:374 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.failAllocation(DeclarativeSlotPoolService.java:198 undefined)   
   at org.apache.flink.runtime.jobmaster.JobMaster.internalFailAllocation(JobMaster.java:650 undefined)   
   at org.apache.flink.runtime.jobmaster.JobMaster.failSlot(JobMaster.java:636 undefined)   
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62 undefined)   
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43 undefined)   
   at java.lang.reflect.Method.invoke(Method.java:498 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:301 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212 undefined)   
   at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158 undefined)   
   at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26 undefined)   
   at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21 undefined)   
   at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123 undefined)   
   at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21 undefined)   
   at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170 undefined)   
   at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171 undefined)   
   at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171 undefined)   
   at akka.actor.Actor$class.aroundReceive(Actor.scala:517 undefined)   
   at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225 undefined)   
   at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592 undefined)   
   at akka.actor.ActorCell.invoke(ActorCell.scala:561 undefined)   
   at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258 undefined)   
   at akka.dispatch.Mailbox.run(Mailbox.scala:225 undefined)   
   at akka.dispatch.Mailbox.exec(Mailbox.scala:235 undefined)   
   at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260 undefined)   
   at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339 undefined)   
   at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979 undefined)   
   at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107 undefined)
Caused by: org.apache.flink.util.FlinkException: Could not mark slot 61a637e3977c58a0e6b73533c419297d active.   
   at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$handleAcceptedSlotOffers$18(TaskExecutor.java:1469 undefined)   
   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760 undefined)   
   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736 undefined)   
   at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208 undefined)    ... 19 more
{code}",,kevin.cyj,lincoln.86xy,maguowei,tanyuxin,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22180,,,,,,FLINK-21751,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/21 13:09;kevin.cyj;jm.log.zip;https://issues.apache.org/jira/secure/attachment/13023767/jm.log.zip","31/Mar/21 07:42;kevin.cyj;jm.log.zip;https://issues.apache.org/jira/secure/attachment/13023219/jm.log.zip","31/Mar/21 07:58;kevin.cyj;tm.log.zip;https://issues.apache.org/jira/secure/attachment/13023220/tm.log.zip","13/Apr/21 13:09;kevin.cyj;tm1.log.zip;https://issues.apache.org/jira/secure/attachment/13023768/tm1.log.zip","13/Apr/21 13:09;kevin.cyj;tm2.log.zip;https://issues.apache.org/jira/secure/attachment/13023769/tm2.log.zip",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 21:54:01 UTC 2021,,,,,,,,,,"0|z0oxig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/21 09:35;trohrmann;Thanks for reporting this issue [~kevin.cyj]. Do you have the logs available of the run where the problem occurred?;;;","30/Mar/21 09:39;trohrmann;The problem can occur if the {{JobMaster}} does not activate the slots fast enough or if the slots have been freed in the meantime. The {{JobMaster}} needs to accept the slots within {{taskmanager.slot.timeout}} which defaults to {{10 s}}.;;;","30/Mar/21 11:14;kevin.cyj;[~trohrmann] I did not keep the log, I encountered the issue when running tpcds test on a cluster for several times. The problem is not hard to reproduce, I will share the log when the issue reproduces. (These days, I am running tpcds test for sort-merge shuffle.);;;","31/Mar/21 07:43;kevin.cyj;[~trohrmann] I have uploaded the log, hope it can help.;;;","31/Mar/21 07:59;kevin.cyj;tm log updated, the previous one is not right.;;;","09/Apr/21 16:52;trohrmann;From the logs it looks as if the {{TaskManager}} offers the slots to the leading {{JobManager}}. Some of the slots are rejected and thereby immediately released on the {{TaskManager}}. However, on the {{JobManager}} side this slot is then still used which is weird. This looks like a bug in Flink.

[~kevin.cyj] can you reproduce the problem? If yes, could you maybe run it with DEBUG logs enabled? This should tell us more about what's going wrong.;;;","09/Apr/21 17:15;trohrmann;What I cannot really explain is why we immediately after offering the slots free 2 of the 3 offered slots:

{code}
2021-03-31 15:13:15,852 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@k28a13215.eu95sqa.tbsite.net:56822/user/rpc/jobmanager_4 for job 3c97aeb7d225871ce2a453788c686486.
2021-03-31 15:13:15,852 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 3c97aeb7d225871ce2a453788c686486.
2021-03-31 15:13:15,853 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 3c97aeb7d225871ce2a453788c686486.
2021-03-31 15:13:15,907 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request a758efc7798f2424d9b96c1d77cc2c82 for job 3c97aeb7d225871ce2a453788c686486 from resource manager with leader id 00000000000000000000000000000000.
2021-03-31 15:13:15,908 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for a758efc7798f2424d9b96c1d77cc2c82.
2021-03-31 15:13:15,908 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 3c97aeb7d225871ce2a453788c686486.
2021-03-31 15:13:15,923 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:4, state:ALLOCATED, resource profile: ResourceProfile{cpuCores=0.1000000000000000, taskHeapMemory=1.183gb (1270035251 bytes), taskOffHeapMemory=0 bytes, managedMemory=1.172gb (1258291200 bytes), networkMemory=409.600mb (429496729 bytes)}, allocationId: bee6ef66e651bd6a29cafd61cb74dd10, jobId: 3c97aeb7d225871ce2a453788c686486).
2021-03-31 15:13:15,923 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:9, state:ALLOCATED, resource profile: ResourceProfile{cpuCores=0.1000000000000000, taskHeapMemory=1.183gb (1270035251 bytes), taskOffHeapMemory=0 bytes, managedMemory=1.172gb (1258291200 bytes), networkMemory=409.600mb (429496729 bytes)}, allocationId: 9b0e7ead445183e9d7f0536872da8602, jobId: 3c97aeb7d225871ce2a453788c686486).
{code};;;","12/Apr/21 06:59;kevin.cyj;OK, I will try to reproduce the issue with DEBUG logs enabled.;;;","13/Apr/21 13:12;kevin.cyj;[~trohrmann] I have updated the log with DEBUG enabled.;;;","13/Apr/21 13:22;chesnay;Could be the same issue as FLINK-22180.;;;","13/Apr/21 15:21;trohrmann;Thanks for posting the new logs [~kevin.cyj]. From what I can tell, it looks as if the {{JobMaster}} rejects the offered slot:

{code}
2021-04-13 19:38:01,455 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 6bea78f0eeb44c254e8de2ca115206b2 for job e0c187ce5b5c6e251b33eb800ee0614f from resource manager with leader id 00000000000000000000000000000000.
2021-04-13 19:38:01,455 DEBUG org.apache.flink.runtime.memory.MemoryManager                [] - Initialized MemoryManager with total memory size 1258291200 and page size 32768.
2021-04-13 19:38:01,455 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 6bea78f0eeb44c254e8de2ca115206b2.
2021-04-13 19:38:01,455 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job e0c187ce5b5c6e251b33eb800ee0614f.
2021-04-13 19:38:01,542 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request f991e63eea4150a74e83eb515984a89c for job e0c187ce5b5c6e251b33eb800ee0614f from resource manager with leader id 00000000000000000000000000000000.
2021-04-13 19:38:01,542 DEBUG org.apache.flink.runtime.memory.MemoryManager                [] - Initialized MemoryManager with total memory size 1258291200 and page size 32768.
2021-04-13 19:38:01,542 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for f991e63eea4150a74e83eb515984a89c.
2021-04-13 19:38:01,542 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job e0c187ce5b5c6e251b33eb800ee0614f.
2021-04-13 19:38:01,579 DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Free slot with allocation id 6bea78f0eeb44c254e8de2ca115206b2 because: The slot was rejected by the JobManager.
2021-04-13 19:38:01,579 DEBUG org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:4, state:ALLOCATED, resource profile: ResourceProfile{cpuCores=0.1000000000000000, taskHeapMemory=1.133gb (1216348160 bytes), taskOffHeapMemory=0 bytes, managedMemory=1.172gb (1258291200 bytes), networkMemory=409.600mb (429496729 bytes)}, allocationId: 6bea78f0eeb44c254e8de2ca115206b2, jobId: e0c187ce5b5c6e251b33eb800ee0614f).
java.lang.Exception: The slot was rejected by the JobManager.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$handleAcceptedSlotOffers$18(TaskExecutor.java:1564) ~[flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) ~[?:1.8.0_102]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-vvr-5.0-SNAPSHOT.jar:1.14-vvr-5.0-SNAPSHOT]
{code}

The problem is that the JobManager log does not contain the log statements for this time (19:38:01). It only starts at 19:38:03 and, thus, it is not very clear what is happening on the {{JobMaster}} side. But it looks as if the {{JobMaster}} rejects the slot and still uses it. This sounds like a bug to me.

Not sure whether it is possible to also share the JobManager logs from 19:38:00 - 19:38:03 [~kevin.cyj]. This would be super helpful.;;;","13/Apr/21 15:40;trohrmann;I think I know what the problem is: The problem is that we offer slot {{6bea78f0eeb44c254e8de2ca115206b2}} twice. The first time we offer the slot it gets rejected. However before we process the answer we offer this slot a second time. If now the second slot offering is accepted, then we accept a slot which has already been freed.

What this means is that we must not free/release slots which are still offered. One way to solve this problem is that we only allow a single slot offering for a job to take place. Alternatively, we must filter out responses which are outdated because a new slot offering has already been sent.;;;","14/Apr/21 21:54;chesnay;master: c9fd6955630d320fce42cc36abe98a5eb65fd1b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflow for large parallelism jobs when processing EndOfChannelStateEvent,FLINK-21857,13366027,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,kevin.cyj,kevin.cyj,18/Mar/21 08:53,24/Mar/21 17:41,13/Jul/23 08:07,24/Mar/21 17:41,,,,,,,,,1.13.0,,,,,,,Runtime / Network,,,,,0,pull-request-available,,,,,"CheckpointedInputGate#handleEvent calls pollNext recursively when processing EndOfChannelStateEvent, for large parallelism job of large amount of input channels, the stack can become really deep thus causing StackOverflow. The following is the stack:
{code:java}
12:11
java.lang.StackOverflowError
  at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:650)
  at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:625)
  at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:611)
  at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:109)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:149)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:202)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:158)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:202)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:158)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:202)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:158)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:202)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:158)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:202)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:158)
  at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:202){code}",,bupt_ljy,dwysakowicz,kevin.cyj,maguowei,roman,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 17:41:29 UTC 2021,,,,,,,,,,"0|z0oxbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/21 09:06;kevin.cyj;cc @[~pnowojski] [~AHeise] [~roman_khachatryan];;;","18/Mar/21 10:00;roman;Thanks for reporting [~kevin.cyj]. 
What is the DoP?;;;","18/Mar/21 11:35;kevin.cyj;[~roman_khachatryan] 8000 * 8000 can reproduce the stack overflow error consistently. ;;;","22/Mar/21 11:06;dwysakowicz;I'll look into it.;;;","24/Mar/21 17:41;dwysakowicz;Fixed in 0a122f3fd684541a8ac0887bfa7c49d7bdd24b17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of using Python UDF from sub-query as input param of Python UDTF,FLINK-21856,13366020,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,hxbks2ks,hxbks2ks,18/Mar/21 08:18,18/Aug/21 08:02,13/Jul/23 08:07,18/Aug/21 08:02,1.11.3,1.12.2,1.13.0,,,,,,,,,,,,,API / Python,,,,,0,auto-deprioritized-major,,,,,"This example comes from the user. splitStr is a Python UDTF. train_and_predict is a Python UDF.
{code:python}
t_env.sql_query(""""""
 select A.hotime ,
         A.before_ta ,
         A.before_rssi ,
         A.after_ta ,
         A.after_rssil ,
         A.nb_tath ,
         A.nb_rssith ,
         nbr_rssi ,
         nbr_ta
 from (SELECT
     hotime ,
     before_ta ,
     before_rssi ,
     after_ta ,
     after_rssil ,
     nb_tath ,
     nb_rssith ,
     train_and_predict(nb_tath, nb_rssith) predict
 FROM
     source) as  A,LATERAL TABLE(splitStr(predict)) as T(nbr_rssi, nbr_ta)
 """""")

{code}

The root cause is that `train_and_predict` is a RexCorrelVariable which we don't have relevant logic to deal with.

 A workaround is to use the Table API.

",,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 22:56:00 UTC 2021,,,,,,,,,,"0|z0ox9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 10:47;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 22:56;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NOt able to read config in flink Migration ,FLINK-21845,13365794,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,yogeshk,yogeshk,17/Mar/21 10:38,26/Apr/21 19:08,13/Jul/23 08:07,26/Apr/21 19:08,1.9.0,,,,,,,,,,,,,,,,,,,,0,,,,,,"I am facing a issue to migrate the application from samza to flink , not able to read config file.

at the class time all the config ara loading but when the Apache dofn call then config are coming null.

please suggest to me is there anything flink-config.YAML need to add some configuration .

 

 ",,mapohl,yogeshk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Mon Apr 26 19:08:50 UTC 2021,,,,,,,,,,"0|z0ovvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/21 13:45;mapohl;Hi [~yogeshk], thanks for reaching out to the Flink community. It sounds like you need some assistance in migrating from Samza to Flink. I'd like to encourage you to move your question to [Flink's user mailing list|https://flink.apache.org/community.html#mailing-lists]. The user mailing list is the more appropriate channel to ask for issues that are not necessarily Flink bugs. It's also more likely for you to get help faster.

Additionally, I want to point out that for solving your problem it would help for you to provide more details on the actual issue. Things that might help are used commands, logs and/or stacktraces that were collected.;;;","17/Mar/21 14:13;yogeshk;[~mapohl] Thanks for the quick reply , 
I am not able to join user mailing list , but i have sent an email there .... 


here ...
 
As per Matthias suggestion , I would like to connect user flink channel .
 
I need urgent help as i am having some issue while migrating application from samza to flink.
I am loading properties config when SPP Pipeline run at first time only , that time its loading properly and using all over the dofn .
 
But then when I consume some message than config is coming null .. 

1. When i start with Direct Runner locally pipeline working fine.
 
QUESTIONS HERE :
 
1. is that flink loading static config at a class time , is there any any configuration needed for that .
2. FACING ISSUE  WHILE CONFIG NOT LOADING PROPERLY .
 
I am not sure i am able to explain here properly .
 
If i need some help i will available to taking call anytime ... today ... 
Please let me know i need some help .
 
 
Thanks,
Yogesh K ;;;","17/Mar/21 14:14;yogeshk;Want to reaopen . 
please any one  in team can you help me .;;;","17/Mar/21 14:16;yogeshk;<[user@flink.apache.org|mailto:user@flink.apache.org]> email id is not correct.;;;","17/Mar/21 14:19;yogeshk;let me know if i need to join zoom call , I will share zoom link as well ,

 ;;;","22/Mar/21 10:59;mapohl;You can send an email to {{user@flink.apache.org}} to ask for help. That should work without problems. What is the problem you're facing with the user mailing list that prevents you from posting your issue there?

Additionally, some questions for you to answer:
* How do you deploy Flink?
* What's the commands you execute to start your job?
* Can you provide logs (located in the Flink's {{./log}} folder)? That might help to see what's going on.;;;","31/Mar/21 09:39;mapohl;[~yogeshk] did you manage to solve your problem?;;;","31/Mar/21 17:45;yogeshk;Hi Matthias,

Ya , I tried but still I am facing a same issue .


;;;","01/Apr/21 09:04;mapohl;Not sure whether I missed it but have you sent a proper description of your problem to the mailing list? You can just subscribe to {{user@flink.apache.org}} as described in the docs or just send an email describing your problem to {{user@flink.apache.org}}. Consider the items I listed above as a guideline for what you might want to share about your issue. Feel free to add anything else that might be helpful.;;;","26/Apr/21 18:36;mapohl;[~yogeshk] have you had the change to work on the issue?;;;","26/Apr/21 19:08;yogeshk;resolve the issue. [~mapohl];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TemporalRowTimeJoinOperator.java will lead to the state expansion by short-life-cycle & huge RowData, although config idle.state.retention.time",FLINK-21833,13365685,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zicat,zicat,zicat,17/Mar/21 03:07,19/Jan/22 02:55,13/Jul/23 08:07,08/Apr/21 14:48,1.12.2,,,,,,,,1.12.3,1.13.0,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"In our company cases, short-life-cycle & huge RowData will use this operator to join each other. Every key called session_id will be expired after 2min.

With idle.state.retention.time configuration, after 2min if not used, the leftState and rightState will be cleaned up by the operator but nextLeftIndex & registeredTimer state data will be stored forever.

After running a day(about 20 million session_id in our company cases), the checkpoint operator will cause the job crash.

I have found the bug, and fixed it.

!image-2021-03-17-11-06-21-768.png!",,jark,leonard,libenchao,zicat,,,,,,,,,,,,,,,,,,,,,,FLINK-25695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/21 03:06;zicat;image-2021-03-17-11-06-21-768.png;https://issues.apache.org/jira/secure/attachment/13022434/image-2021-03-17-11-06-21-768.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 14:48:33 UTC 2021,,,,,,,,,,"0|z0ov7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/21 03:17;jark;cc [~Leonard Xu]
;;;","17/Mar/21 03:27;leonard;[~zicat] Thanks for the report, I'll help review this PR;;;","18/Mar/21 03:41;zicat;Please take a look at this. We depend on this in our production environment. Thank you very much. [~Leonard Xu];;;","08/Apr/21 14:48;jark;Fixed in 
- master: 6ac9b46413b890e5992c8f046834bc2e75da3b39
- release-1.12: 688378e61b8d6e8d3b18adfd34ba26888d70da56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Kafka Source might break subtask and split assignment upon rescale,FLINK-21817,13365454,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,kezhuw,kezhuw,16/Mar/21 09:16,28/Aug/21 11:13,13/Jul/23 08:07,08/Apr/21 22:46,1.12.2,,,,,,,,1.12.3,1.13.0,,,,,,Connectors / Common,,,,,0,pull-request-available,,,,,"On restoring, splits are add back directly to {{SourceReader}} in {{SourceOperator}}. In no rescaled restoring, bindings between splits and subtasks are preserved due to no repartition in {{RoundRobinOperatorStateRepartitioner}}. But in rescaled restoring, these operator states will be redistributed cross existing subtasks. This might break possible assignment from {{SourceEnumerator}}.

Given {{KafkaSource}} as an example, the partition to subtask assignment is decided by {{KafkaSourceEnumerator.getSplitOwner}}. The mappings will break after rescaling.

I pushed [a test case|https://github.com/kezhuw/flink/commit/9dc13cd9d7355d468a6ac8aae1b14b3a267581b6#diff-ad6e86c3757199ac3247687a71f9c34ee67b9ac743ae88a9f660950f27bec6eeR238] using {{KafkaSource}} for evaluation.

I think it requires api addition to solve in generic and configurable way.

Is it a valid issue ? I am not that sure.

cc  [~jqin] [~sewen]",,becket_qin,dwysakowicz,guoyangze,kezhuw,renqs,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 22:41:05 UTC 2021,,,,,,,,,,"0|z0ots8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/21 07:54;renqs;Thanks for the ticket [~kezhuw]! I took a look of new source's code, and this bug does exist when the job rescales. 

Some possible solutions: 

1. SourceOperator reports its restored splits back to {{SourceCoordinator}} at registration, and {{SourceCoordinator}} updates its {{SplitsAssignmentTracker}} in the context. 

2. SourceOperator reports its restored splits back to {{SourceCoordinator}} at registration, and if these splits don't match with the {{SplitsAssignmentTracker}}, {{SourceCoordinator}} will revoke them and re-assign correct splits to the reader. 

3. Instead of using {{RoundRobinOperatorStateRepartitioner}}, we can design a new partitioner, which will distribute splits according to the assignment tracked by {{SourceCoordinator}}. 

Personally I prefer the second one, because as the leader of {{SourceReader}}, {{SourceCoordinator}} holds the ground truth. For the first option, round-robin style partitioning might break the decision made by {{SourceEnumerator}}. Also I think the second one is easier to implement compared to the third solution.  

In order to make the second solution work, we also needs a split revocation mechanism to revoke mis-assigned splits after rescaling. ;;;","22/Mar/21 21:44;sewen;I agree that this is a bug in the Kafka Enumerator.

I think we need to make sure users understand that the Enumerator should not in any case make any assumption about what is going on at the readers after a failure or restore. That is an anti-pattern in my experience, doing bookkeeping twice and trying to keep it in sync. It will very often break at some points.

A similar bug was FLINK-21452, and the solution was to drop the bookkeeping on the enumerator side, and rely on reader registrations to learn about the current state.

Following a similar argument, I would suggest to go with variant (1) here.
;;;","22/Mar/21 22:41;sewen;For clarification, I think that the FLIP-27 sources as a whole are not affected by this.
There is the {{SplitAssignmentTracker}} which is mainly necessary to return splits on task failures. After a checkpoint completed (or after recovery with a completed checkpoint), the subtask specific information should no longer be used.

So my take is that this is purely an issue of the new Kafka Source, that's why I updated the title.;;;","23/Mar/21 00:28;becket_qin;I think solution 1 and 2 [~renqs] mentioned do not conflict with each other. They are more like two steps. In another word, it is the enumerator's call whether it wants to adjust the splits assignment after the readers reported the assignment upon reader registration. Revoking a split assignment seems a useful primitive for other cases as well, e.g. workload balance.

So to fix the inconsistent state, we only need to do (1).;;;","23/Mar/21 06:39;kezhuw;I guess we four are probably talking about two inconsistencies:
 # Inconsistency between {{SplitAssignmentTracker}} and operator instances.
 # Inconsistency between split enumerator and operator instances about possible contract that split assignment enforced by concrete split enumerators. For {{KafkaSource}}, the contract is {{KafkaSourceEnumerator.getSplitOwner}}.

I did not aware of #1, but there are related anyway.

IIUC, {{SplitAssignmentTracker}} is responsible for region recovering from last successful checkpoint. If this is the case, I think it makes no sense to snapshot and restore it.

For #2, I am not sure whether it is a valid feature request. I assume yes for now. I think {{void addReader(int subtaskId, List<SplitT> splits)}} will help {{SourceOperator}} to piggyback restored splits on open(eg. no {{SourceReader.addSplits}} on operator startup). To make it configurable, a boolean flag in {{Source}} could guide {{SourceOperator}} how to do on restoring. Comparing to legacy source, this will not only make {{SplitEnumerator}} a split assigner but *the* assigner.
{quote}I think we need to make sure users understand that the Enumerator should not in any case make any assumption about what is going on at the readers after a failure or restore. That is an anti-pattern in my experience, doing bookkeeping twice and trying to keep it in sync. It will very often break at some points.
{quote}
I agree this. {{KafkaSourceEnumState.currentAssignment}} is misleading also and will inconsistent with operator instances after rescaling.

For solutions offered by  [~renqs], I guess we all opt out solution#3. For solution#1, I did not see values to snapshot and restore {{SplitAssignmentTracker}}. For solution#2, how to revoke in case of inconsistency#2 is valid ?;;;","23/Mar/21 09:09;renqs;Thanks for the inspiring discussion!

About {{SplitAssignmentTracker}}, I agree with [~sewen] and [~kezhuw]. I think it’s not necessary to keep it in the state of {{SourceCoordinator}} since it will finally lose track of the per-subtask assignment after a global failover.

About split revoking, I’m +1 with [~becket_qin]. Whether or not to use it for correcting split assignment, this is still useful under scenarios like load balancing, and provides more flexibility for assignment decision made by split enumerator.

According to opinions above, at least we have these TODOs on the current FLIP-27 source no matter which solution we finally choose:
 1. A mechanism for readers reporting current assignment back to coordinator. This can be added to {{ReaderRegistrationEvent}}
 2. An interface for passing these assignments to SplitEnumerator, like {{addReader(int subtaskId, List<SplitT> splits)}} mentioned by [~kezhuw]. Since {{SplitEnumerator}} is a public interface, this change requires a FLIP.;;;","23/Mar/21 11:33;sewen;Thanks for this discussion and the analysis of the problems.

I would say that to fix this problem, let's not offer a general solution on the basic source enumerator (the {{SourceCoordinator}}) interface to track which readers have which split. I would suggest that if we want to track this in the Kafka Source, then let's implement it only in the Kafka Source with a custom event that is sent as a first thing after registration.

But we may even skip this for now, and not track splits in the Kafka Source for this release. There is no functionality now that needs this tracking, and it is a source of errors, so I would suggest to add the split tracking later when we actually need it and have better test coverage with a common test suite that includes tests that change the parallelism. Then we are confident to catch issues better.


So my suggested actions would be:

(1) Remove the SplitAssignmentTracker from the snapshots, because it isn't needed and might actually cause problems.
(2) Remove the split-to-reader tracking in the Kafka Source.

==> These two are 1.13 blockers

(3) Ensure our source test suite is in place
(4) Add split-to-reader-tracking to Kafka Source when we need this functionality
(5) Move this logic to the common source interface once it is stable in the Kafka Source


As a side note, I think split rebalancing isn't something we should start doing now, because there is no way to guarantee order of events in a partition when rebalancing: The events read by the newly assigned reader can overtake events from the old reader. We need something like epochs to establish clear before/after relationships for the in-flight events, which we implicitly get through checkpoints, that's why rescaling is safe.

The discussion about how to preserve order semantics per key with in-flight data is something we did for unaligned checkpoints. It is really complex to come up with a simple but effective semantical model, and unaligned checkpoints currently work only on {{keyBy()}} and {{rebalance()}} and {{random()}} data exchanges (not {{forward()}} or {{rescale()}}) because only for those three cases is it possible to provide meaningful semantics with the current parallel streams model that Flink has.
Just quickly doing something with source split rebalancing now will open up all sorts of problems and bugs if we don't answer the question about order guarantees before. So let's do one thing at a time. First get the basics of the new sources running very well. Then approach the next set of issues.;;;","23/Mar/21 13:50;kezhuw;+1 for gradual procedure.
{quote} But we may even skip this for now, and not track splits in the Kafka Source for this release. There is no functionality now that needs this tracking
{quote}
I actually could not give it a name for configuring, though old Kafka consumer do respect a same assignment.

For (2), did you means {{KafkaSourceEnumerator.readerIdToSplitAssignments}} or {{KafkaSourceEnumState.currentAssignment}} [~sewen] ? This tracking currently does not hurt correctness, but still worth to drop. It also demands special care as it is stored in state.

Hmm, I guess (1) also does not hurt correctness for now. Both (1), (2) are just unnecessary atm :(. Anyway, worth to fix.

[~renqs] I guess with help of default method and {{SplitEnumerator.addSplitsBack}}(fallback to this if default is not overrode), it could be a no breaking change.;;;","26/Mar/21 15:10;dwysakowicz;Sorry, I've not checked the discussion. Just wanted to check if someone is working on a fix for it as it is marked as a blocker for the release. Could we assign the ticket to that person if there is one?;;;","26/Mar/21 15:16;renqs;I can work on those two blockers mentioned by [~sewen] above. [~dwysakowicz] Could you please assign this ticket to me? Thanks~  ;;;","26/Mar/21 15:27;dwysakowicz;Sure, [~renqs] Thank you!;;;","01/Apr/21 17:48;sewen;Fixed in 1.13.0 via 
  - 65b43c45c69c20a51f2926d29be46c779fe65af
  - 2be15338c00e75361566fe3010091ea61f0af645;;;","07/Apr/21 12:40;dwysakowicz;Double checking... Is the task still open, because it needs to backported to 1.12? Or is there another reason?;;;","08/Apr/21 15:03;sewen;Exactly, it is because of the 1.12 backport.
Let me try to merge this to the 1.12 branch today, and if it doesn't finish, I'll close this and open a separate issue for the backport.;;;","08/Apr/21 22:41;sewen;Fixed in 1.12 via
  - 035932e20e853b3dbc80a2a4826248ae2854711e
  - 59f54d7725c3b277327bf317d99e9dcb17194437;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native K8s does not respect TM memory setups decided by RM,FLINK-21793,13365174,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,15/Mar/21 08:34,16/Mar/21 09:12,13/Jul/23 08:07,16/Mar/21 09:12,1.13.0,,,,,,,,1.13.0,,,,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"Currently, {{kubernetes-taskmanager.sh}} decides JVM options (Xmx, Xms, etc.) from a cluster-wised configuration overwritten by TM-specific dynamic properties. The problem is that, if the cluster-wised configuration contains some unwanted options, there's no way to remove/unset them via dynamic properties, which leads to inconsistent JVM options.

One of the key improvement of FLIP-49 was to make all memory calculations happen in one place, which is the RM side for active RM modes. According to this principle, I'd suggest to generate the memory related JVM options on RM side, and passing them to TM through ENVs.

This only affects fine-grained resource management.",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 09:12:46 UTC 2021,,,,,,,,,,"0|z0os20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/21 08:35;xtsong;cc [~fly_in_gis];;;","16/Mar/21 09:12;xtsong;Fixed via
* master (1.13): 4f9ec4762c3e7f8fcde47adda15a2fe1e3a2ca78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testResourceCanBeAllocatedForDifferentJobWithDeclarationBeforeSlotFree fail because of timeout,FLINK-21779,13365053,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,maguowei,maguowei,14/Mar/21 12:23,31/May/21 08:05,13/Jul/23 08:07,31/May/21 08:04,1.13.0,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,auto-deprioritized-critical,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14567&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a&l=6685


{code:java}

	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.assertFutureCompleteAndReturn(FineGrainedSlotManagerTestBase.java:120)
	at org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase$4.lambda$new$4(AbstractFineGrainedSlotManagerITCase.java:374)
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase$Context.runTest(FineGrainedSlotManagerTestBase.java:197)
	at org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase$4.<init>(AbstractFineGrainedSlotManagerITCase.java:310)
	at org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase.testResourceCanBeAllocatedForDifferentJobAfterFree(AbstractFineGrainedSlotManagerITCase.java:308)
	at org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase.testResourceCanBeAllocatedForDifferentJobWithDeclarationBeforeSlotFree(AbstractFineGrainedSlotManagerITCase.java:262)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


{code}
",,guoyangze,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21587,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 31 08:05:15 UTC 2021,,,,,,,,,,"0|z0orb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 11:21;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15280&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=6943;;;","22/Apr/21 12:24;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","19/May/21 10:52;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","31/May/21 08:05;guoyangze;Close as we could not reproduce it after FLINK-21587. Please reopen it if it occurs again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception message does not point to the right path.,FLINK-21754,13364929,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,Amon,Amon,13/Mar/21 08:24,28/Aug/21 11:14,13/Jul/23 08:07,06/Apr/21 07:54,1.12.2,,,,,,,,1.13.0,,,,,,,API / Core,,,,,0,pull-request-available,,,,,"In org.apache.flink.configuration.GlobalConfiguration,

between line 126~133, we are checking whether conf/flink-conf.yaml file exists, not the conf directory.

so in line 131, we should use  yamlConfigFile.getAbsolutePath() instead of  confDirFile.getAbsolutePath().",,Amon,dwysakowicz,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 08:31:34 UTC 2021,,,,,,,,,,"0|z0oqjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/21 15:11;RocMarshal;[~trohrmann@apache.org] Could you assign this ticket to  me ? Thank you.;;;","06/Apr/21 07:54;trohrmann;Fixed via

master: 986fe1f7ec53491e716edc9e5a9f98e2c3b64c1f;;;","07/Apr/21 08:31;dwysakowicz;Hey [~trohrmann] just letting you know that I created the 1.13 branch accidentally when creating the rc0. It is not a proper 1.13 branch yet. There is no need to merge it there yet. We will announce the actual cut off in the ML. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cycle references between memory manager and gc cleaner action,FLINK-21753,13364925,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kezhuw,kezhuw,kezhuw,13/Mar/21 07:39,16/Mar/21 10:08,13/Jul/23 08:07,16/Mar/21 10:08,1.10.3,1.11.3,1.12.2,,,,,,1.11.4,1.12.3,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,,"{{MemoryManager.allocatePages}} uses {{this::releasePage}} as cleanup action in {{MemorySegmentFactory.allocateOffHeapUnsafeMemory}}. The cleanup function is used as gc cleaner action there. This creates a cycle referencing between memory manager and gc cleaner *if allocated memory segment is not {{MemoryManager.release}} in time.* Symptoms should be different based on versions:
 * Before 1.12.2: memory will not be reclaimed until gc after {{MemoryManager.release}}
 * * 1.12.2: memory will not be reclaimed until {{MemorySegment.free}} or gc after {{MemoryManager.release}}

I quotes javadoc from jdk {{java.lang.ref.Cleaner}} here for references:

{quote}The cleaning action is invoked only after the associated object becomes phantom reachable, so it is important that the object implementing the cleaning action does not hold references to the object. In this example, a static class encapsulates the cleaning state and action. An ""inner"" class, anonymous or not, must not be used because it implicitly contains a reference to the outer instance, preventing it from becoming phantom reachable. The choice of a new cleaner or sharing an existing cleaner is determined by the use case.
{quote}
See also FLINK-13985 FLINK-21419.

I pushed [test case|https://github.com/kezhuw/flink/commit/9a5d71d3a6be50611cf2f5c65c39f51353167306] in my repository after FLINK-21419 (which merged in 1.12.2 but not before) for evaluation.

cc [~azagrebin] [~xintongsong] [~trohrmann] [~ykt836] [~nicholasjiang] ",,guoyangze,kezhuw,nicholasjiang,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 10:08:48 UTC 2021,,,,,,,,,,"0|z0oqio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/21 12:19;trohrmann;I think currently the idea is to no longer rely on GC for cleaning up memory. Instead we eagerly release the memory when {{MemorySegment.free}} is being called.;;;","13/Mar/21 12:45;kezhuw;It is a good direction, master does not have this since FLINK-21419.;;;","15/Mar/21 03:39;xtsong;Thanks [~kezhuw].

While the master branch does not have the problem, I think this makes a good fix for the previous versions.

Do you want to open a PR against the 1.11/1.12 branches? For 1.10, I don't think there's likely another bugfix release.;;;","15/Mar/21 05:34;kezhuw;HI [~xintongsong], I am interesting in fix for 1.12/1.11 branches. I have following options:
 # Static class in `MemoryManager` to wrap cleanup action.
 # Or `UnsafeMemoryBudget.releaseMemoryLater` to return a runnable.

If it was master branch, I would suggest forbid anonymous/inner class cleanup. But just for maintenance versions, I think we could relax this.

What do you think ? Which approach do you prefer ?;;;","15/Mar/21 05:50;xtsong;I think returning a runnable from {{UnsafeMemoryBudget}} should be good enough. We might look for a better name for {{releaseMemoryLater}} though.

I'm assigning you to the ticket.;;;","15/Mar/21 07:13;nicholasjiang;[~kezhuw], will you copy the `UnsafeMemoryBudget.releaseMemoryLater` to 1.11/1.12 branches for fixing the problem above mentioned?;;;","15/Mar/21 08:50;kezhuw;[~nicholasjiang] Only that two branches need this, not master.;;;","16/Mar/21 10:08;xtsong;Fixed via:
* release-1.11: a0eeac3bdb7f83f9b73cdcdd8b507f6d0069115c
* release-1.12: 86a185665e2451b37335326535795576429f8cde;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException on restore in PojoSerializer,FLINK-21752,13364698,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,roman,roman,12/Mar/21 17:19,25/Mar/22 13:23,13/Jul/23 08:07,04/Feb/22 12:03,1.13.5,1.14.3,1.15.0,,,,,,1.13.7,1.14.4,1.15.0,,,,,API / Type Serialization System,,,,,4,pull-request-available,,,,,"As originally reported in [thread|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Schema-Evolution-Cannot-restore-from-savepoint-after-deleting-field-from-POJO-td42162.html], after removing a field from a class restore from savepoint fails with the following exception:

{code}
2021-03-10T20:51:30.406Z INFO  org.apache.flink.runtime.taskmanager.Task:960 … (6/8) (d630d5ff0d7ae4fbc428b151abebab52) switched from RUNNING to FAILED. java.lang.Exception: Exception while creating StreamOperatorStateContext.
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:195)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:253)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:901)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:415)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedCoProcessOperator_c535ac415eeb524d67c88f4a481077d2_(6/8) from any of the 1 provided restore options.
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:307)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:135)
        ... 6 common frames omitted
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)
        at org.apache.flink.runtime.state.memory.MemoryStateBackend.createKeyedStateBackend(MemoryStateBackend.java:347)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:291)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
        ... 8 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.<init>(PojoSerializer.java:123)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:186)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:56)
        at org.apache.flink.api.common.typeutils.CompositeSerializer$PrecomputedParameters.precompute(CompositeSerializer.java:228)
        at org.apache.flink.api.common.typeutils.CompositeSerializer.<init>(CompositeSerializer.java:51)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializer.<init>(TtlStateFactory.java:250)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:359)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:330)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546)
        at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
        at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:505)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.snapshotsToRestoreSerializers(NestedSerializersSnapshotDelegate.java:225)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.getRestoredNestedSerializers(NestedSerializersSnapshotDelegate.java:83)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at org.apache.flink.runtime.state.StateSerializerProvider.previousSchemaSerializer(StateSerializerProvider.java:189)
        at org.apache.flink.runtime.state.StateSerializerProvider.currentSchemaSerializer(StateSerializerProvider.java:164)
        at org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.getStateSerializer(RegisteredKeyValueStateBackendMetaInfo.java:136)
        at org.apache.flink.runtime.state.heap.StateTable.getStateSerializer(StateTable.java:315)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:54)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:36)
        at org.apache.flink.runtime.state.heap.StateTable.<init>(StateTable.java:98)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.<init>(CopyOnWriteStateTable.java:49)
        at org.apache.flink.runtime.state.heap.AsyncSnapshotStrategySynchronicityBehavior.newStateTable(AsyncSnapshotStrategySynchronicityBehavior.java:41)
        at org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.newStateTable(HeapSnapshotStrategy.java:243)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.createOrCheckStateForMetaInfo(HeapRestoreOperation.java:185)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:152)
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)
        ... 12 common frames omitted
{code}
",,alexeyt820,asardaes,azocankara,bdine,dwysakowicz,fpaul,gaoyunhaii,kezhuw,leonard,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 25 13:13:41 UTC 2022,,,,,,,,,,"0|z0op48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 10:48;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","22/Apr/21 13:43;asardaes;Sigh, I don't think I can alter labels, but I do find this issue rather critical.;;;","29/Apr/21 22:56;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","27/Oct/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","05/Nov/21 10:39;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","02/Feb/22 19:31;asardaes;Original thread in the new archive: https://lists.apache.org/thread/05wg9ny9d5r0wox8y8kbc9cqy5j86z25;;;","03/Feb/22 08:41;bdine;This is for sure pretty critical,
The ""workaround"" is to not remove the field but I am not sure if this is acceptable

I could work on that, but someone need to point out to me where to start, 

Do I work on the PojoSerializer, to make this case not throwing an exception ?

Or do I try to find the root cause, namely why the field serializer of the deleted field is still present ? 

 ;;;","03/Feb/22 09:26;fpaul;[~bdine] thanks for your interest. First I'd like to clarify which Flink versions are affected by this since 1.9.3 is officially not supported anymore. Is this the only version where you see this problem?;;;","03/Feb/22 09:31;bdine;[~fpaul] Reproduced in Flink 1.13.5 & 1.14.3

The job was originally developed in 1.11.x and has gone through several Flink version upgrade towards 1.14, 

I find this bug pretty weird, as I do not have it with all of deleted field,

 ;;;","03/Feb/22 09:49;fpaul;Thanks for the fast reply. The community is currently a bit busy due to the upcoming feature freeze for 1.15 at the end of the next. I had a quick look and was wondering because it seems that there exists a test to verify the deletion behavior [1].

 

If you have time you can start to have a look.

 

[1] https://github.com/apache/flink/blob/49537320e00bfd221bcdd3ab5d84ae9a8d00d7e7/flink-core/src/test/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializerSnapshotTest.java#L140 ;;;","03/Feb/22 10:17;bdine;I might be wrong, 
But in the test we have fields = new Field[] \{null, NAME_FIELD.field, null} & fieldSerializer = new TypeSerializer[] \{ IntSerializer.INSTANCE, StringSerializer.INSTANCE, DoubleSerializer.INSTANCE }

But when I follow the NPE stack trace, 
Using PojoSerializer.duplicate() with those null fields & this fieldSerializer -> new PojoSerializer(this.clazz, duplicateFieldSerializers, this.fields, this.executionConfig);
In the called constructor we have :

 
{code:java}
this.numFields = fieldSerializers.length;
[..]
for (int i = 0; i < numFields; i++) {
    this.fields[i].setAccessible(true);
}
{code}
 

So this exception make sense here : numFields will be equal to length of fieldSerializer = 3 and we will have this.fields[0] & this.fields[2] = null -> throwing this NPE
Maybe I am missing something, I am not really familiar with state serializer workflow

 ;;;","03/Feb/22 11:27;dwysakowicz;cc [~trohrmann] [~gaoyunhaii] Do you think it is a blocker for 1.15? I think it is quite a serious issue in our serialization stack. It is quite easily reproducible if we e.g. change {{TypeSerializerUpgradeTestBase#assertSerializerIsValid}}:
{code}
    private static <T> void assertSerializerIsValid(
            TypeSerializer<T> serializer, DataInputView dataInput, Matcher<T> testDataMatcher)
            throws Exception {

        DataInputView serializedData =
                readAndThenWriteData(dataInput, serializer, serializer, testDataMatcher);
        TypeSerializerSnapshot<T> snapshot = writeAndThenReadSerializerSnapshot(serializer);
        TypeSerializer<T> restoreSerializer = snapshot.restoreSerializer()*.duplicate()*;
        readAndThenWriteData(serializedData, restoreSerializer, restoreSerializer, testDataMatcher);
    }
{code};;;","03/Feb/22 11:30;fpaul;[~dwysakowicz] I tend to agree it should be a blocker.;;;","03/Feb/22 13:02;trohrmann;Yes, I agree. This is quite bad given that we provide [schema evolution for POJO types|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/#supported-data-types-for-schema-evolution]. Let's fix it in all currently supported releases.;;;","04/Feb/22 12:03;dwysakowicz;Fixed in:
* master
** cde85c5a9798d3600e911959cec6f78743dc15b0..860f0a292857ed6ee25dc2fd407ebf45f2c4cda5
* 1.14.4
** 150f768573cb526a2c67123da61f64b199d0ea7a..b9f7a4f02e969cf98d7b3acc549b27adad234079
* 1.13.7
** 3d2329e9744eb89e31669f2096c4bce7dde7898a..695372cd0a5bece795d1248f32e46ec9748a40e8;;;","04/Feb/22 14:54;bdine;Thanks for quick fix guys;;;","04/Feb/22 14:56;dwysakowicz;I am sorry to admit it's not been that quick ;) It took almost a year :(;;;","04/Feb/22 15:02;bdine;This was pretty quick according to my first comment yesterday when I found the bug :) but yea maybe [~asardaes] has waited a little more :D;;;","25/Mar/22 13:13;bdine;Hi [~dwysakowicz] 

After upgrading to 1.14.4, we are facing another issue when restoring the state with the deleted field : 

NPE has been corrected, but we have another exception : 

Caused by: java.lang.IllegalStateException: field serializer snapshots should be present. 

when restoring the fields serializer of the POJO

Full stack :

 
java.lang.Exception: Exception while creating StreamOperatorStateContext.
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_d9f814bb951ceee0ef1734a68b047efa_(1/1) from any of the 1 provided restore options.
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)
    ... 11 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:394)
    at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:462)
    at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:93)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
    ... 13 more
Caused by: java.lang.IllegalStateException: field serializer snapshots should be present.
    at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
    at org.apache.flink.api.java.typeutils.runtime.PojoSerializerSnapshot.lambda$restoreSerializer$0(PojoSerializerSnapshot.java:144)
    at org.apache.flink.util.LinkedOptionalMap.forEach(LinkedOptionalMap.java:159)
    at org.apache.flink.api.java.typeutils.runtime.PojoSerializerSnapshot.restoreSerializer(PojoSerializerSnapshot.java:141)
    at java.base/java.util.stream.ReferencePipeline$3$1.accept(Unknown Source)
    at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Unknown Source)
    at java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source)
    at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source)
    at java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source)
    at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source)
    at java.base/java.util.stream.ReferencePipeline.toArray(Unknown Source)
    at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.snapshotsToRestoreSerializers(NestedSerializersSnapshotDelegate.java:221)
    at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.getRestoredNestedSerializers(NestedSerializersSnapshotDelegate.java:80)
    at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:219)
    at org.apache.flink.api.java.typeutils.runtime.PojoSerializerSnapshot.lambda$restoreSerializer$0(PojoSerializerSnapshot.java:148)
    at org.apache.flink.util.LinkedOptionalMap.forEach(LinkedOptionalMap.java:159)
    at org.apache.flink.api.java.typeutils.runtime.PojoSerializerSnapshot.restoreSerializer(PojoSerializerSnapshot.java:141)
    at java.base/java.util.stream.ReferencePipeline$3$1.accept(Unknown Source)
    at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Unknown Source)
    at java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source)
    at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source)
    at java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source)
    at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source)
    at java.base/java.util.stream.ReferencePipeline.toArray(Unknown Source)
    at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.snapshotsToRestoreSerializers(NestedSerializersSnapshotDelegate.java:221)
    at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.getRestoredNestedSerializers(NestedSerializersSnapshotDelegate.java:80)
    at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:219)
    at org.apache.flink.runtime.state.StateSerializerProvider.previousSchemaSerializer(StateSerializerProvider.java:186)
    at org.apache.flink.runtime.state.StateSerializerProvider.currentSchemaSerializer(StateSerializerProvider.java:161)
    at org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.getStateSerializer(RegisteredKeyValueStateBackendMetaInfo.java:143)
    at org.apache.flink.contrib.streaming.state.ttl.RocksDbTtlCompactFiltersManager.setAndRegisterCompactFilterIfStateTtl(RocksDbTtlCompactFiltersManager.java:73)
    at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.createColumnFamilyDescriptor(RocksDBOperationUtils.java:157)
    at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.createStateInfo(RocksDBOperationUtils.java:134)
    at org.apache.flink.contrib.streaming.state.restore.RocksDBHandle.getOrRegisterStateColumnFamilyHandle(RocksDBHandle.java:163)
    at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.applyRestoreResult(RocksDBFullRestoreOperation.java:122)
    at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:102)
    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:314)
    ... 18 more;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalExecutorITCase.testBatchQueryExecutionMultipleTimes[Planner: old] fails,FLINK-21748,13364160,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,dwysakowicz,dwysakowicz,12/Mar/21 11:04,28/May/21 09:04,13/Jul/23 08:07,13/Apr/21 05:57,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14520&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=8800

{code}
[ERROR] testBatchQueryExecutionMultipleTimes[Planner: old](org.apache.flink.table.client.gateway.local.LocalExecutorITCase)  Time elapsed: 0.438 s  <<< ERROR!
org.apache.flink.table.client.gateway.SqlExecutionException: Error while retrieving result.
	at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:79)
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.processRecord(MaterializedCollectBatchResult.java:48)
	at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:76)

{code}",,dwysakowicz,fsk119,jark,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 05:57:57 UTC 2021,,,,,,,,,,"0|z0olwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/21 11:05;dwysakowicz;cc [~jark][~fsk119];;;","29/Mar/21 18:59;mapohl;[This one|https://dev.azure.com/mapohl/flink/_build/results?buildId=369&view=logs&j=d0dc8a09-802e-543a-1851-c31096d61b33&t=5952d6c2-ad1d-5ad4-5bfc-48bb7f31ebd9] failed in {{LocalExecutorITCase.testBatchQueryExecution[Planner: old]}}. Different test but same stacktrace;;;","30/Mar/21 02:31;jark;Will take a look. ;;;","12/Apr/21 13:36;fsk119;Thanks for Kurt's tips. The problem is because of multi-thread.

{{MaterializedCollectBatchResult}} extends {{CollectResultBase}}. {{CollectResultBase}} has a thread periodically fetch data from the remote and invoke the {{processRecord}} to add data into the `buffer`. When {{processRecord}}, the thread needs to visit the `buffer`. The `buffer` is created when init the child class {{MaterializedCollectBatchResult}} but the thread is started in the init in the parent class {{CollectResultBase}}. 

 

The solution is simple. I think we can start the thread when init {{MaterializedCollectBatchResult}};;;","13/Apr/21 05:57;ykt836;fixed: b78ce3b9713c7b2792629d509a3951a96829012d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink sql fields in row access error about scalarfunction,FLINK-21746,13364108,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,Peihui,Peihui,12/Mar/21 10:46,15/Apr/21 06:46,13/Jul/23 08:07,15/Apr/21 06:46,1.12.1,,,,,,,,1.12.3,1.13.0,,,,,,Table SQL / Planner,,,,,0,flinksql,planner,,,,"i defined the scalar function below:

{code:scala}
class Test extends ScalarFunction{
 @DataTypeHint(""ROW<a String, b String, c String>"")
 def eval(): Row ={
 Row.of(""a"", ""b"", ""c"")
 }
}
{code}

where  run the sql '*select Test().a from t1*',  i will get the error;

{code}
java.io.IOException: Fail to run stream sql job at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:172) at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:105) at org.apache.zeppelin.flink.FlinkStreamSqlInterpreter.callInnerSelect(FlinkStreamSqlInterpreter.java:89) at org.apache.zeppelin.flink.FlinkSqlInterrpeter.callSelect(FlinkSqlInterrpeter.java:494) at org.apache.zeppelin.flink.FlinkSqlInterrpeter.callCommand(FlinkSqlInterrpeter.java:257) at org.apache.zeppelin.flink.FlinkSqlInterrpeter.runSqlList(FlinkSqlInterrpeter.java:151) at org.apache.zeppelin.flink.FlinkSqlInterrpeter.internalInterpret(FlinkSqlInterrpeter.java:111) at org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47) at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110) at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:852) at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:744) at org.apache.zeppelin.scheduler.Job.run(Job.java:172) at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132) at org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: scala.MatchError: Test() (of class org.apache.calcite.rex.RexCall) at org.apache.flink.table.planner.plan.utils.NestedSchemaExtractor.internalVisit$1(NestedProjectionUtil.scala:273) at org.apache.flink.table.planner.plan.utils.NestedSchemaExtractor.visitFieldAccess(NestedProjectionUtil.scala:283) at org.apache.flink.table.planner.plan.utils.NestedSchemaExtractor.visitFieldAccess(NestedProjectionUtil.scala:269) at org.apache.calcite.rex.RexFieldAccess.accept(RexFieldAccess.java:92) at org.apache.flink.table.planner.plan.utils.NestedProjectionUtil$$anonfun$build$1.apply(NestedProjectionUtil.scala:112) at org.apache.flink.table.planner.plan.utils.NestedProjectionUtil$$anonfun$build$1.apply(NestedProjectionUtil.scala:111) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.flink.table.planner.plan.utils.NestedProjectionUtil$.build(NestedProjectionUtil.scala:111) at org.apache.flink.table.planner.plan.utils.NestedProjectionUtil.build(NestedProjectionUtil.scala) at org.apache.flink.table.planner.plan.rules.logical.ProjectWatermarkAssignerTransposeRule.getUsedFieldsInTopLevelProjectAndWatermarkAssigner(ProjectWatermarkAssignerTransposeRule.java:155) at org.apache.flink.table.planner.plan.rules.logical.ProjectWatermarkAssignerTransposeRule.matches(ProjectWatermarkAssignerTransposeRule.java:65) at org.apache.calcite.plan.volcano.VolcanoRuleCall.matchRecurse(VolcanoRuleCall.java:284) at org.apache.calcite.plan.volcano.VolcanoRuleCall.matchRecurse(VolcanoRuleCall.java:411) at org.apache.calcite.plan.volcano.VolcanoRuleCall.match(VolcanoRuleCall.java:268) at org.apache.calcite.plan.volcano.VolcanoPlanner.fireRules(VolcanoPlanner.java:985) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1245) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:84) at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:268) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1132) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604) at org.apache.calcite.plan.volcano.VolcanoPlanner.changeTraits(VolcanoPlanner.java:486) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:309) at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1329) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1321) at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1276) at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:161) ... 16 more
{code}",flinksql 1.12.1,fsk119,jark,libenchao,Peihui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22082,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 06:46:09 UTC 2021,,,,,,,,,,"0|z0oll4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 15:05;jark;The exception stack looks similar to FLINK-22082. Maybe a bug in {{NestedProjectionUtil}}.;;;","15/Apr/21 06:46;fsk119;Fix in FLINK-22082. Verify the case in local environment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterTest.testReconnectionAfterDisconnect hangs on azure,FLINK-21745,13364104,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,dwysakowicz,dwysakowicz,12/Mar/21 10:33,25/Mar/21 06:24,13/Jul/23 08:07,25/Mar/21 06:24,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14500&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8884

{code}

{code}",,dwysakowicz,maguowei,mapohl,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21866,,,,,,FLINK-21135,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 25 06:24:27 UTC 2021,,,,,,,,,,"0|z0olk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/21 12:28;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14567&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8693


{code:java}
""main"" #1 prio=5 os_prio=0 tid=0x00007f799000b800 nid=0x1c38 waiting on condition [0x00007f7997fb9000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000086502cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
	at org.apache.flink.runtime.jobmaster.JobMasterTest.testReconnectionAfterDisconnect(JobMasterTest.java:826)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)

{code}
;;;","14/Mar/21 12:47;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14599&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8526;;;","15/Mar/21 03:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14637&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8521;;;","17/Mar/21 04:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14845&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8972;;;","17/Mar/21 12:40;mapohl;The test is failing due to a change in [bc08cff|https://github.com/apache/flink/commit/bc08cff1620e65cde64bb1b5158749aa652fd393] (FLINK-21135) that removed the overwriting of the default parallelism {{-1}}. This caused [SlotSharingSlotAllocator.calculateRequiredSlots|https://github.com/apache/flink/blob/50eca3182cad284990fd8c48b50e74582995d99f/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/allocator/SlotSharingSlotAllocator.java#L61] to fail as there's only on {{JobVertex}} having the default parallelism of {{-1}}.;;;","18/Mar/21 03:04;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14920&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8972;;;","19/Mar/21 07:22;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14990&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8605;;;","19/Mar/21 08:19;dwysakowicz;Are the failures in this run also related to the issue?: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14990&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4438;;;","19/Mar/21 10:22;mapohl;{quote}Are the failures in this run also related to the issue?:{quote}
Yes, they are.

FYI: The relevant stacktrace is
{code}
Caused by: java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
	at org.apache.flink.runtime.util.ResourceCounter.withResource(ResourceCounter.java:244)
	at org.apache.flink.runtime.scheduler.adaptive.allocator.SlotSharingSlotAllocator.calculateRequiredSlots(SlotSharingSlotAllocator.java:61)
	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.calculateDesiredResources(AdaptiveScheduler.java:728)
	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToWaitingForResources(AdaptiveScheduler.java:720)
	at org.apache.flink.runtime.scheduler.adaptive.Created.startScheduling(Created.java:72)
	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.startScheduling(AdaptiveScheduler.java:308)
	at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:960)
	at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:873)
	at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:382)
	... 20 more
{code}

But I hope that we finalize it today.;;;","22/Mar/21 02:31;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9148

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15115&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9458;;;","22/Mar/21 03:23;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15115&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=13549;;;","22/Mar/21 06:12;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15137&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=13549;;;","22/Mar/21 06:13;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15137&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8833;;;","23/Mar/21 04:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15215&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9014;;;","23/Mar/21 04:10;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15215&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=13856;;;","24/Mar/21 04:05;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15316&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9378;;;","24/Mar/21 04:05;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15316&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=13547;;;","25/Mar/21 06:24;rmetzger;Resolved in https://github.com/apache/flink/commit/18a32e8d172f99b60dcfd4c36efe653a2f1d2922;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 CliTableauResultViewTest.testCancelBatchResult fails,FLINK-21740,13364012,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,maguowei,maguowei,12/Mar/21 05:38,15/Mar/21 15:30,13/Jul/23 08:07,15/Mar/21 15:30,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14500&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=e5682198-9e22-5770-69f6-7551182edea8
{code:java}
[ERROR]   CliTableauResultViewTest.testCancelBatchResult:236 expected:<...-03-01 18:39:14.1 |
[|    true |  2147483647 |               (NULL) |              abcdefg |     1234567890 |     2020-03-01 18:39:14.12 |
|   false | -2147483648 |  9223372036854775807 |               (NULL) |    12345.06789 |    2020-03-01 18:39:14.123 |
Query terminated, received a total of 4 rows]
> but was:<...-03-01 18:39:14.1 |
[Query terminated, received a total of 1 rows
|    true |  2147483647 |               (NULL) |              abcdefg |     1234567890 |     2020-03-01 18:39:14.12 |
|   false | -2147483648 |  9223372036854775807 |               (NULL) |    12345.06789 |    2020-03-01 18:39:14.123 |]
>

{code}
",,dwysakowicz,fsk119,jark,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 15 15:30:14 UTC 2021,,,,,,,,,,"0|z0okzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/21 10:30;dwysakowicz;cc [~godfreyhe][~jark];;;","12/Mar/21 14:48;jark;It seems this is introduced by FLINK-18550. ;;;","12/Mar/21 14:49;jark;We can temporary disable this test if see more failed builds. ;;;","15/Mar/21 03:51;fsk119;Hi, all.

Before explain the reason why test fails, I will just describe the logic in the test. There are 3 parts in the test: {{TestingExecutor}}, {{Terminal}} and {{CliTableauResultView}}. The {{CliTableauResultView}} will requst the data from the {{TestingExecutor}} and send the data to {{Terminal}} to print. When {{CliTableauResultView}} request data from {{TestingExecutor}}, the {{TestingExecutor}} will memorize how many times {{CliTableauResultView}} reads. For simplicty, we call this variable as {{numRetrieveResultChancesCalls.}}

In the test, if {{numRetrieveResultChancesCalls}} is larger than 0, we will cancel the {{CliTableauResultView}}. It's possible {{CliTableauResultView}} read the data from the {{TestingExecutor}} but the terminal haven't printed.

Therefore, we just need to modify the logic to wait the terminal finish its job and then cancel the job.

I am willing to fix this problem. Please assign it to me.;;;","15/Mar/21 15:30;jark;Fixed in master: f6d6fa5eff02dcba8c3d9c38a5632ded1886814e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WatermarkAssigner incorrectly recomputing the rowtime index which may cause ArrayIndexOutOfBoundsException,FLINK-21733,13363798,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,11/Mar/21 09:42,12/Mar/21 14:30,13/Jul/23 08:07,12/Mar/21 14:30,1.11.3,1.12.2,,,,,,,1.12.3,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"WatermarkAssigner incorrectly recomputing the rowtime index in copy method which may cause ArrayIndexOutOfBoundsException in such case:
{code}
@Test
  def testProjectTransposeWatermarkAssigner(): Unit = {
    val sourceDDL =
      s""""""
         |CREATE TEMPORARY TABLE `t1` (
         |  `a`  VARCHAR,
         |  `b`  VARCHAR,
         |  `c`  VARCHAR,
         |  `d`  INT,
         |  `t`  TIMESTAMP(3),
         |  `ts` AS `t`,
         |  WATERMARK FOR `ts` AS `ts`  - INTERVAL '10' SECOND
         |) WITH (
         |  'connector' = 'values',
         |  'enable-watermark-push-down' = 'true',
         |  'bounded' = 'false',
         |  'disable-lookup' = 'true'
         |)
       """""".stripMargin
    util.tableEnv.executeSql(sourceDDL)

    val sql =
      s""""""
         |select a, b, ts
         |from t1
         |"""""".stripMargin
    util.verifyPlan(sql)
  }
{code}

exception stack
{code}
java.lang.ArrayIndexOutOfBoundsException: Index -1 out of bounds for length 3

	at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:75)
	at org.apache.calcite.util.Util$TransformingList.get(Util.java:2732)
	at org.apache.flink.table.planner.plan.nodes.calcite.WatermarkAssigner.copy(WatermarkAssigner.scala:68)
	at org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:805)
	at org.apache.calcite.plan.hep.HepPlanner.setRoot(HepPlanner.java:158)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:60)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:282)

{code}

",,jark,libenchao,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 12 07:20:42 UTC 2021,,,,,,,,,,"0|z0ojo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/21 07:20;jark;Fixed in 
 - master: c2d9e69450e0a1bf14aa866ecdd8f5c38072923d
 - release-1.12: dfe55bcf15e82fa63e85551fba2e0b415bba8dae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataTypeExtractor extracts wrong fields ordering for Tuple12,FLINK-21725,13363717,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,jark,jark,11/Mar/21 03:56,15/Mar/21 12:48,13/Jul/23 08:07,15/Mar/21 07:57,1.11.3,1.12.2,1.13.0,,,,,,1.11.4,1.12.3,1.13.0,,,,,Table SQL / Planner,,,,,1,pull-request-available,,,,,"The following test can reproduce the problem:

{code:java}
 /** Emit Tuple12 result. */
    public static class JavaTableFuncTuple12
            extends TableFunction<
                    Tuple12<
                            String,
                            String,
                            String,
                            String,
                            String,
                            String,
                            Integer,
                            Integer,
                            Integer,
                            Integer,
                            Integer,
                            Integer>> {
        private static final long serialVersionUID = -8258882510989374448L;

        public void eval(String str) {
            collect(
                    Tuple12.of(
                            str + ""_a"",
                            str + ""_b"",
                            str + ""_c"",
                            str + ""_d"",
                            str + ""_e"",
                            str + ""_f"",
                            str.length(),
                            str.length() + 1,
                            str.length() + 2,
                            str.length() + 3,
                            str.length() + 4,
                            str.length() + 5));
        }
    }
{code}

{code:scala}
@Test
  def testCorrelateTuple12(): Unit = {
    val util = streamTestUtil()
    util.addTableSource[(Int, Long, String)](""MyTable"", 'a, 'b, 'c)
    val function = new JavaTableFuncTuple12
    util.addTemporarySystemFunction(""func1"", function)
    val sql =
      """"""
        |SELECT *
        |FROM MyTable, LATERAL TABLE(func1(c)) AS T
        |"""""".stripMargin

    util.verifyExecPlan(sql)
  }
{code}

{code}
// output plan
Correlate(invocation=[func1($cor0.c)], correlate=[table(func1($cor0.c))], select=[a,b,c,f0,f1,f10,f11,f2,f3,f4,f5,f6,f7,f8,f9], rowType=[RecordType(INTEGER a, BIGINT b, VARCHAR(2147483647) c, VARCHAR(2147483647) f0, VARCHAR(2147483647) f1, INTEGER f10, INTEGER f11, VARCHAR(2147483647) f2, VARCHAR(2147483647) f3, VARCHAR(2147483647) f4, VARCHAR(2147483647) f5, INTEGER f6, INTEGER f7, INTEGER f8, INTEGER f9)], joinType=[INNER])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}

Note that there is no problem if using the legacy {{tEnv.registerFunction}} to register function, becuase it uses {{TypeInformation}}. However, it has problem if using  {{tEnv.createTemporaryFunction}} or {{CREATE FUNCTION}} syntax, because it uses {{TypeInference}}.

Note this problem exists in latest 1.11, 1.12, and master branch. 

I think the problem might lay in this line: https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/extraction/DataTypeExtractor.java#L562

because it orders field names by alphabetical.",,gsavl,jark,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 15 07:57:13 UTC 2021,,,,,,,,,,"0|z0oj68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/21 03:56;jark;cc [~twalthr];;;","15/Mar/21 07:57;twalthr;Fixed in 1.13.0: 2434ad544c1a21b780e5fc08bfa6bff9b7b0bc23 90c297db503b3da5f6496db7068c58c748b20845 bebf3b5a105dd4bc21882116570c6d71299269a6

 

Fixed in 1.12.3: 9051df378ebf1d217cbbeed8f66ba458bf024b4c 0df64fb27f216e4cca5f55371a4f5b887dae45bd 7383416dfaf59f0aeebc77450b6c3dc9b495789a

 

Fixed in 1.11.4: 085a0f3441c939257ab706868dc1f229cff2bda8 d5dd1d777ce947f45afb8b2e63b53b0e01756023 90c70e183440c6278ea31f7315cf718f339f6f59;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove type annotation from python message.as_type,FLINK-21724,13363643,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,10/Mar/21 19:55,10/Mar/21 22:28,13/Jul/23 08:07,10/Mar/21 22:28,,,,,,,,,statefun-3.0.0,,,,,,,Stateful Functions,,,,,0,pull-request-available,,,,,message.as_type's return type is dynamic based on the given type parameter. It should not have a type annotation.,,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 10 22:27:53 UTC 2021,,,,,,,,,,"0|z0oiq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/21 22:27;sjwiesman;fixed in master: a9f6ebcb89b8c258720b02e17d2577f41deab474;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop integration link is broken on download page,FLINK-21723,13363632,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,trohrmann,trohrmann,trohrmann,10/Mar/21 18:24,12/Mar/21 14:28,13/Jul/23 08:07,12/Mar/21 14:28,,,,,,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,,,"A community member has reported that the Hadoop integration link is broken on the download page.

I suggest to remove it because Yarn is no longer our only resource provider integration.

https://lists.apache.org/thread.html/r91b4c23330c1135d57112da8f8ce923b21e05581e574acceaa26992f%40%3Cuser.flink.apache.org%3E",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 12 14:28:52 UTC 2021,,,,,,,,,,"0|z0oink:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/21 14:28;trohrmann;Fixed via 9badc757159f16ccf4bf143abb3adef0ddbd372c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong class name used in 'Generating Watermarks' documentation page for the Scala examples,FLINK-21722,13363621,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Thelgis,Thelgis,Thelgis,10/Mar/21 17:39,10/Mar/21 22:41,13/Jul/23 08:07,10/Mar/21 22:41,,,,,,,,,1.13.0,,,,,,,Documentation,,,,,0,documentation,pull-request-available,,,,"The 'Generating Watermarks' documentation page has examples on 'Periodic WatermarkGenerator' and 'Punctuated WatermarkGenerator'. 

The Java examples implement the class 'WatermarkGenerator', whereas the Scala examples extend the 'AssignerWithPeriodicWatermarks'. Both examples should be using 'WatermarkGenerator' which is the correct class for the example. ",,sjwiesman,Thelgis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 10 22:40:34 UTC 2021,,,,,,,,,,"0|z0oil4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/21 17:40;Thelgis;I plan to create a pull request for this in a while. ;;;","10/Mar/21 22:40;sjwiesman;fixed in master: e8e951c74c6c8e28b6a6b885bcf280f84bb1c08b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManagerTest.testNotificationAboutNotEnoughResources fails,FLINK-21718,13363549,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,10/Mar/21 13:20,11/Mar/21 10:50,13/Jul/23 08:07,11/Mar/21 10:50,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,test-stability,,,,,"[Build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7961] is (not exclusively) failing due to a failure in  {{FineGrainedSlotManagerTest.testNotificationAboutNotEnoughResources}}
{code:java}
[ERROR] Failures: 
[ERROR]   FineGrainedSlotManagerTest.testNotificationAboutNotEnoughResources:493->testNotificationAboutNotEnoughResources:507 
Expected: a collection with size <1>
     but: collection size was <0> {code}",,guoyangze,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21587,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 09:18:33 UTC 2021,,,,,,,,,,"0|z0oi54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/21 09:18;guoyangze;I think this is fixed in FLINK-21587. We can close it atm.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Links to Java/Python docs are Broken,FLINK-21717,13363539,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,10/Mar/21 12:35,11/Mar/21 15:49,13/Jul/23 08:07,11/Mar/21 15:49,1.13.0,,,,,,,,1.13.0,,,,,,,Documentation,,,,,0,pull-request-available,,,,,"After migrating to Hugo, all the hyperlinks refer to Python docs became unreachable.

E.g. {{docs/dev/python/table_environment.md}} 
{code:html}
<a href=""{{ site.pythondocs_baseurl }}/api/python/pyflink.table.html#pyflink.table.TableEnvironment.from_pandas"">link</a>
{code}
is generating [http://localhost:1313/docs/dev/python/table/table_environment/%7B%7B%20site.pythondocs_baseurl%20%7D%7D/api/python/pyflink.table.html#pyflink.table.TableEnvironment.from_elements]

 Another problem is the param defined in the config.toml is *JavaDocs* while the shortcode javadoc.html has a typo(*JavaDoc*), which rendered broken links either.

E.g. {{docs/connectors/datastream/jdbc.md}}

!image-2021-03-10-20-31-52-502.png!

is generating [http://api/java/org/apache/flink/connector/jdbc/JdbcSink.html]

 ",,dian.fu,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/21 12:22;qingyue;image-2021-03-10-20-22-09-425.png;https://issues.apache.org/jira/secure/attachment/13022006/image-2021-03-10-20-22-09-425.png","10/Mar/21 12:31;qingyue;image-2021-03-10-20-31-52-502.png;https://issues.apache.org/jira/secure/attachment/13022005/image-2021-03-10-20-31-52-502.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 15:49:21 UTC 2021,,,,,,,,,,"0|z0oi2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/21 15:49;chesnay;master: 3ae97bf4416ccd1c354098d438318fef090e401a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkRelMdUniqueKeys gets incorrect result on TableScan after project push-down,FLINK-21710,13363482,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,10/Mar/21 09:30,25/Jun/21 02:39,13/Jul/23 08:07,15/Mar/21 02:36,,,,,,,,,1.12.3,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"FlinkRelMdUniqueKeys#getTableUniqueKeys uses the original columns to find the index of primary keys. But after project push-down, the indexes is incorrect based on original columns, and the new columns should be used.",,godfreyhe,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23135,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 15 02:36:06 UTC 2021,,,,,,,,,,"0|z0ohq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/21 02:36;godfreyhe;Fixed in 1.13.0: 0297b849ee2ab0c96bb60f585a71a04c6b9c0f97
Fixed in 1.12.3: feaa9a6b10553fd44f11987cb564fb808a7a53b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job is possible to hang when restarting a FINISHED task with POINTWISE BLOCKING consumers,FLINK-21707,13363447,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhuzh,zhuzh,zhuzh,10/Mar/21 07:54,17/Mar/21 04:20,13/Jul/23 08:07,16/Mar/21 06:12,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Job is possible to hang when restarting a FINISHED task with POINTWISE BLOCKING consumers. This is because {{PipelinedRegionSchedulingStrategy#onExecutionStateChange()}} will try to schedule all the consumer tasks/regions of the finished *ExecutionJobVertex*, even though the regions are not the exact consumers of the finished *ExecutionVertex*. In this case, some of the regions can be in non-CREATED state because they are not connected to nor affected by the restarted tasks. However, {{PipelinedRegionSchedulingStrategy#maybeScheduleRegion()}} does not allow to schedule a non-CREATED region and will throw an Exception and breaks the scheduling of all the other regions. One example to show this problem case can be found at [PipelinedRegionSchedulingITCase#testRecoverFromPartitionException |https://github.com/zhuzhurk/flink/commit/1eb036b6566c5cb4958d9957ba84dc78ce62a08c].

To fix the problem, we can add a filter in {{PipelinedRegionSchedulingStrategy#onExecutionStateChange()}} to only trigger the scheduling of regions in CREATED state.",,bupt_ljy,mapohl,Thesharing,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21734,FLINK-21735,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 06:12:31 UTC 2021,,,,,,,,,,"0|z0ohig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/21 07:56;zhuzh;cc [~trohrmann] [~mapohl];;;","10/Mar/21 09:10;mapohl;Thanks for bringing this up, [~zhuzh]. This is related to how we handled the stop-with-savepoint issue in FLINK-21030, isn't it?

I managed to reproduce the hanging job locally with the patch [~zhuzh] suggest. We run into the following exception:
{code:java}
java.lang.reflect.InvocationTargetException: null
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_265]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_265]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_265]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_265]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[classes/:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
Caused by: java.lang.IllegalStateException: BUG: trying to schedule a region which is not in CREATED state
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.maybeScheduleRegion(PipelinedRegionSchedulingStrategy.java:162) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.maybeScheduleRegions(PipelinedRegionSchedulingStrategy.java:153) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.onExecutionStateChange(PipelinedRegionSchedulingStrategy.java:141) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:201) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:699) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:433) ~[classes/:?]
	... 26 more
{code};;;","10/Mar/21 09:56;trohrmann;Thanks for reporting this issue [~zhuzh]. Why do we have to wait for a blocking intermediate result to be fully produced before scheduling independent consumers? Not having to do this could allow us to get rid of the {{PipelinedRegionSchedulingStrategy.correlatedResultPartitions}} and avoid the problem you are describing. Is it because we want to support stage wise scheduling?

If this is strictly required, then your proposal should work. However, I am bit hesitant because it feels as if we are complicating things a bit and that these special cases makes everything a bit more brittle.

As a side comment: We should make sure that these kind of {{IllegalStateException}} don't only show up when the DEBUG log is enabled. Instead, we should fail hard.;;;","10/Mar/21 12:55;zhuzh;[~mapohl] Sorry for the confusing. It is not related to stop-with-savepoint. It is a BUG of {{PipelinedRegionSchedulingStrategy}}. And thanks a lot that the exception you post clearly shows the problem. (I did not tried debug log so it take me quite some time to figure out the cause)

[~trohrmann] 
>> Why do we have to wait for a blocking intermediate result to be fully produced before scheduling independent consumers?
I think it is not strictly required. But Flink was scheduling batch jobs in stage-wise pattern so pipelined region scheduling was designed to be aligned with it.
In Blink version the scheduler was already reworked to trigger the scheduling of BLOCKING partition downstream vertices individually. It has been working well so I think it's safe to also do this for Flink.
One known side effect is the computing complexity will increase because we now need to check all the consumers for each finished partition. However, with the improvement FLINK-21328 the performance will no longer be a problem.

>> We should make sure that these kind of IllegalStateException don't only show up when the DEBUG log is enabled. Instead, we should fail hard.
Big +1. The trouble shooting took me quite some time because the root error was not printed in INFO logs. What I am thinking is to wrap all the invocations on SchedulingStrategy methods with a try-catch and fail globally when any exception is caught. This is because the call stack can be quite deep and complex considering the underlying {{DefaultScheduler}} and {{ExecutionSlotAllocator}}. And we actually do not expect any exception to be thrown directly in these invocations. WDYT?;;;","10/Mar/21 14:45;trohrmann;Whether to schedule it stage-wise or not could be a concern of the scheduler and not the underlying {{IntermediateResult}} and {{IntermediateResultPartition}}. Hence, I would be in favour of removing this logic from the {{IntermediateResult}}/{{IntermediateResultPartition}}.

Maybe we can create a {{FatallyFailingSchedulerNGWrapper}} which does the failing if an exception occurs. That way it would work for all {{SchedulerNG}} implementations.;;;","11/Mar/21 03:56;zhuzh;Agreed to ""removing this logic from the IntermediateResult/IntermediateResultPartition"". However, given that the problem of this ticket also exists in 1.11/1.12, how about we do a simple fix for this ticket first as the initial proposal and open a separate 1.13 ticket to change the step-wise scheduling logic? 

Regarding {{FatallyFailingSchedulerNGWrapper}}, I can see that currently exceptions are acceptable for some of the RPCs, e.g. {{requestNextInputSplit(), requestPartitionState()}} and I do not have confidence to find out all this kind of RPCs at the moment. If we always fail the job on exceptions, Flink might become more unstable than it is. Currently what I can only say that direct exceptions from {{updateTaskExecutionState()}} are not expected. So how about we just do try-catch for {{updateTaskExecutionState()}} in JobMaster? It can also work for all SchedulerNG implementations. And I also prefer it to be a separate task for 1.13 only to give it more exposure time.;;;","11/Mar/21 09:43;trohrmann;Both proposals sound good to me. Do you have time to work on it [~zhuzh]?;;;","11/Mar/21 12:43;zhuzh;I have a fix for it already so I will take this ticket.
I opened 2 other tickets FLINK-21734 and FLINK-21735 according to the discussion but may not be able to work on them soon. Anyone else who are interested can take them, or I can also get back to them one or two weeks later.;;;","16/Mar/21 06:12;zhuzh;fixed via:
master(1.13): c1aa6b41841342ddf6e168f326a183db8a8edcac
1.12: f935621fb357613a501cbd285a88b0a470758878;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink JobManager failed to restart from checkpoint in kubernetes HA setup,FLINK-21685,13363190,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,petrizhang,petrizhang,09/Mar/21 07:22,28/Aug/21 11:13,13/Jul/23 08:07,01/Apr/21 11:29,1.12.1,1.12.2,,,,,,,1.12.3,1.13.0,,,,,,Deployment / Kubernetes,,,,,0,k8s-ha,pull-request-available,,,,"We use Flink K8S session cluster with HA mode (1 JobManager and 4 TaskManagers). When jobs are running in Flink, and JobManager restarted, Flink JobManager failed to recover job from checkpoint


{code}
2021-03-08 13:16:42,962 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Trying to fetch 1 checkpoints from storage. 
2021-03-08 13:16:42,962 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Trying to fetch 1 checkpoints from storage. 
2021-03-08 13:16:42,962 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Trying to retrieve checkpoint 1. 
2021-03-08 13:16:43,014 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 9a534b2e309b24f78866b65d94082ead from Checkpoint 1 @ 1615208258041 for 9a534b2e309b24f78866b65d94082ead located at s3a://zalando-stellar-flink-state-eu-central-1-staging/checkpoints/9a534b2e309b24f78866b65d94082ead/chk-1. 
2021-03-08 13:16:43,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No master state to restore 
2021-03-08 13:16:43,024 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@58d927d2 for BrandCollectionTrackingJob (9a534b2e309b24f78866b65d94082ead). 
2021-03-08 13:16:43,046 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] - JobManager runner for job BrandCollectionTrackingJob (9a534b2e309b24f78866b65d94082ead) was granted leadership with session id c258d8ce-69d3-49df-8bee-1b748d5bbe74 at akka.tcp://flink@10.2.179.12:6123/user/rpc/jobmanager_2. 
2021-03-08 13:16:43,060 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [null] failed with java.net.NoRouteToHostException: No route to host 
2021-03-08 13:16:43,060 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@10.2.174.188:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@10.2.174.188:6123]] Caused by: [java.net.NoRouteToHostException: No route to host]
{code}


Attached is the log, and our configuration.

 ",,aitozi,dailw,petrizhang,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21902,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21382,,,,,,,,,,"16/Mar/21 07:44;wangyang0918;jstack.jm.1;https://issues.apache.org/jira/secure/attachment/13022355/jstack.jm.1",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 11:29:19 UTC 2021,,,,,,,,,,"0|z0ofxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/21 09:50;dailw;You need create Job Manager RPC service on k8s,like :

apiVersion: v1
kind: Service
metadata:
 name: flink-jm-rpc-service
spec:
 clusterIP: None
 selector:
 role: jobmanager
 ports:
 - protocol: TCP
 port: 6123
 targetPort: 6123;;;","09/Mar/21 12:22;wangyang0918;The internal Kubernetes service is unnecessary when HA enabled.

 

From your attached logs, I find ""java.net.SocketTimeoutException: sent ping but didn't receive pong within"" many times. Could you please check whether the network connection between JobManager pod and Kubernetes APIServer is stable?

 

By the way, I have verified the Kubernetes standalone session cluster with HA enabled in my minikube. When I kill the JobManager, a new one will be launched and take over the leadership. After it becomes the leader, it could recover all the jobs from latest checkpoint successfully. Actually, I could not reproduce your issues.;;;","10/Mar/21 07:37;petrizhang;[~fly_in_gis] we are running in a production K8S cluster, so it is unlikely that there is network issue between JobManager pod and Kubernetes APIServer. Would you be able to try in a K8S cluster (not in minikube since the K8S behaviour might be different)?

 

And when you said that you could reproduce the issue, could you explain more if there is an issue or how you work around it?

 

 ;;;","10/Mar/21 08:41;trohrmann;Could you share a bit more details about the job and the exact setup you are running [~petrizhang]? 

In the attached logs, I could see three JobManager instances. One instance only living from 12:05: to 13:01. It received a SIGTERM at 12:31 but only shut down 13:01. This is strange.

At 12:42 a second instance is started which becomes the leader. A bit later at (12:54) we see the following snippet in the logs:

{code}
1615207818477197056,Will wait for AWS credentials for up to 60 s.
1615207818921065984,Successfully obtained AWS credentials. Caller: arn:aws:sts::637664033771:assumed-role/stellar-flink-staging-iam-role/bf1a2c86-stellar-flink-staging-iam-role
1615207817604473088,""Shutting down, got signal: Terminated""
{code}

From this point onwards the second JobManager seems to be no longer working. Only at 13:15, the instance receives the SIGTERM signal to shut itself down.

In the meantime a third JobManager is started 13:01. The third JobManager was granted the leadership but it never starts the job.

I am suspecting that somewhere in https://github.com/apache/flink/blob/release-1.12/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunnerImpl.java#L337 it fails.

Could you maybe run the deployment with DEBUG logs? This could help us further pinning down the problem.;;;","10/Mar/21 11:20;wangyang0918;[~petrizhang] I am just saying that I could not reproduce this issue even on a real K8s cluster. Note that I am using the aliyun Kubernetes cluster and aliyun oss as the HA storage.;;;","10/Mar/21 11:26;wangyang0918;Could you please run the example job with HA enabled by using the following yamls? This is exactly what I tried to reproduce your issue.

 

jobmanager.job
{code:java}
apiVersion: batch/v1
kind: Job
metadata:
  name: flink-jobmanager
spec:
  template:
    metadata:
      labels:
        app: flink
        component: jobmanager
    spec:
      restartPolicy: OnFailure
      initContainers:
        - name: artifacts-fetcher
          image: apache/flink:1.12.2
          imagePullPolicy: IfNotPresent
          # Use wget or other tools to get user jars from remote storage
          command: [ 'cp', '/opt/flink/examples/streaming/StateMachineExample.jar', '/flink-artifact/myjob.jar' ]
          volumeMounts:
            - mountPath: /flink-artifact
              name: flink-artifact
      containers:
        - name: jobmanager
          image: apache/flink:1.12.2
          imagePullPolicy: Always
          env:
          - name: ENABLE_BUILT_IN_PLUGINS
            value: flink-oss-fs-hadoop-1.12.2.jar
          - name: _POD_IP_ADDRESS
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          args: [""standalone-job"", ""--job-classname"", ""org.apache.flink.streaming.examples.statemachine.StateMachineExample"", ""--host"", ""$(_POD_IP_ADDRESS)""]
          ports:
            - containerPort: 6123
              name: rpc
            - containerPort: 6124
              name: blob-server
            - containerPort: 8081
              name: webui
          livenessProbe:
            tcpSocket:
              port: 6123
            initialDelaySeconds: 30
            periodSeconds: 60
          volumeMounts:
            - name: flink-config-volume
              mountPath: /opt/flink/conf
            - mountPath: /opt/flink/usrlib
              name: flink-artifact
          securityContext:
            runAsUser: 9999  # refers to user _flink_ from official flink image, change if necessary
      volumes:
        - name: flink-artifact
          emptyDir: { }
        - name: flink-config-volume
          configMap:
            name: flink-config
            items:
              - key: flink-conf.yaml
                path: flink-conf.yaml
              - key: log4j-console.properties
                path: log4j-console.properties
{code}
taskmanager.yaml
{code:java}
// code placeholder
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flink-taskmanager
spec:
  replicas: 2
  selector:
    matchLabels:
      app: flink
      component: taskmanager
  template:
    metadata:
      labels:
        app: flink
        component: taskmanager
    spec:
      initContainers:
        - name: artifacts-fetcher
          image: apache/flink:1.12.2
          imagePullPolicy: IfNotPresent
          # Use wget or other tools to get user jars from remote storage
          command: [ 'cp', '/opt/flink/examples/streaming/StateMachineExample.jar', '/flink-artifact/myjob.jar' ]
          volumeMounts:
            - mountPath: /flink-artifact
              name: flink-artifact
      containers:
      - name: taskmanager
        image: apache/flink:1.12.2
        imagePullPolicy: Always
        env:
        - name: ENABLE_BUILT_IN_PLUGINS
          value: flink-oss-fs-hadoop-1.12.2.jar
        args: [""taskmanager""]
        ports:
        - containerPort: 6122
          name: rpc
        - containerPort: 6125
          name: query-state
        livenessProbe:
          tcpSocket:
            port: 6122
          initialDelaySeconds: 30
          periodSeconds: 60
        volumeMounts:
        - name: flink-config-volume
          mountPath: /opt/flink/conf/
        - mountPath: /opt/flink/usrlib
          name: flink-artifact
        securityContext:
          runAsUser: 9999  # refers to user _flink_ from official flink image, change if necessary
      volumes:
      - name: flink-artifact
        emptyDir: { }
      - name: flink-config-volume
        configMap:
          name: flink-config
          items:
          - key: flink-conf.yaml
            path: flink-conf.yaml
          - key: log4j-console.properties
            path: log4j-console.properties
{code}
flink-configuration-configmap.yaml
{code:java}
apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-config
  labels:
    app: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager
    taskmanager.numberOfTaskSlots: 2
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    queryable-state.proxy.ports: 6125
    jobmanager.memory.process.size: 1600m
    taskmanager.memory.process.size: 1728m
    parallelism.default: 2
    kubernetes.cluster-id: standalone-k8s-ha-app1
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
    high-availability.storageDir: oss://flink-debug-yiqi/flink-ha
    restart-strategy: fixed-delay
    restart-strategy.fixed-delay.attempts: 10
    fs.oss.endpoint: oss-cn-beijing.aliyuncs.com
    fs.oss.accessKeyId: xxxx
    fs.oss.accessKeySecret: yyyy
  log4j-console.properties: |+
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n    # Log all infos in the given rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF    logger.rest.name = org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint
    logger.rest.level = ERROR
    logger.minirest.name = org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint
    logger.minirest.level = ERROR
{code}
 

jobmanager-rest-service.yaml
{code:java}
apiVersion: v1
kind: Service
metadata:
  name: flink-jobmanager-rest
spec:
  type: LoadBalancer
  ports:
  - name: rest
    port: 8081
  selector:
    app: flink
    component: jobmanager
{code};;;","11/Mar/21 09:40;petrizhang;[~trohrmann] We have a company wide K8S cluster, where we set up Flink cluster HA. We wanner to set up one Flink JobManager and 4 TaskManagers, and we run Flink in session cluster mode. And we submit jobs to Flink via Rest API. When no jobs are not running in Flink, restarting Flink JobManager can recover. However, when jobs are running in Flink, after I deleted Flink JobManager pod by using `kubectl delete pod <jobmanager-pod-id>`, a new JobManager pod is started, but then Flink cannot recover properly.

I found that `stellar-flink-cluster-resourcemanager-leader` is not updated to use the IP address to the new JobManager pod. Then, TaskManagers cannot find the new JobManager. However, it is unclear why the cluster is not updated to use the new JobManager.

I saw this issue [https://stackoverflow.com/questions/66219093/flink-fencing-errors-in-k8-ha-mode/66228073#66228073] and tried to configure via `FLINK_PROPERTIES` while not using configmap for flink-config.yaml. But this does not help.

 

[~fly_in_gis] I am trying your example, and will let you know the result. Meanwhile, your example starts Flink as `standalone-job` and our case just start Flink in standalone session mode. I wonder if this would matter, and would you be able to try to start Flink in standalone session mode and submit a job, and then delete the JobManager pod (which will restart a new JobManager), and see if there is an issue. Thanks!;;;","11/Mar/21 09:46;trohrmann;I think your configuration also sets the pod's IP as {{jobmanager.rpc.address}}. Hence, this should not be the problem. It might be a problem of the Kubernetes HA services not being able to update the new leader information. Do you have the possibility to run the deployment with DEBUG logs enabled?;;;","11/Mar/21 12:52;petrizhang;[~trohrmann] I uploaded the debug file [^scalyr-logs.txt.zip]

At about 12:30, Flink starts Job Manager

At about 12:38:30, I deleted the JobManager pod, then K8S started a new JobManager. Then, after a while, the error happened

 

I also uploaded the role definition files [^01-role.yaml] [^02-role-binding.yaml];;;","13/Mar/21 12:31;petrizhang;[~trohrmann] [~fly_in_gis] Do you have any findings from the logs?

From the debug log, I saw that the new JobManager pod was granted leadship

{quote}2021-03-11 12:39:43,140 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: DefaultDispatcherRunner with session ID 28ab74d0-83bf-44a4-afc3-691884ef0d98.	
2021-03-11 12:39:43,993 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender http://10.2.19.254:8081 with session ID 726e06fe-59a0-4985-a069-4f5334c76da1.	
2021-03-11 12:39:46,520 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: JobManagerRunnerImpl with session ID 1afde9ba-6119-4c4d-8735-c3115e7ad8d6.	
2021-03-11 12:39:48,841 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: StandaloneResourceManager with session ID 6de9cf53-9d5c-44a1-bc65-1b34e55a6b38.	
2021-03-11 12:39:49,322 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: JobManagerRunnerImpl with session ID 41fe8896-d00a-4ff0-a866-e4fd9f7e023e.	
2021-03-11 12:39:54,182 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: JobManagerRunnerImpl with session ID bee3d4f6-e857-46d0-9766-53fc47747413.{quote}
 
but no logs on 

{quote}Successfully wrote leader information{quote}

so, something went wrong after the new JobManager was granted leadership and tried to confirm the leadership and update the leadership into configmap. What do you think? Thanks!


 ;;;","15/Mar/21 03:30;wangyang0918;[~petrizhang] I think your analysis is right. The new leader did not write the leader information to the ConfigMap successfully. This is the root cause why it could not recover the jobs from latest successful checkpoints.

 

It is really strange that JobManager could still receive the ""MODIFIED"" event for leader ConfigMap, which means the leader elector could update the annotation successfully. Maybe you could double check the annotation of leader ConfigMap for verification. However, we could not write the leader information to the content of ConfigMap. And we also do not have any exceptions(e.g. resource conflicts).

 

I am curious which version of Kubernetes you are using? Is it a standard Kubernetes cluster or with some internal changes?

I have tested in the minikube and a real K8s cluster with version 1.18. It just works well and I could not reproduce such issue.;;;","15/Mar/21 08:22;petrizhang;[~fly_in_gis] We are using standard Kubernetes, and version is 1.19.8. Would you be able to try on this version in minikube?

Server Version: version.Info\{Major:""1"", Minor:""19"", GitVersion:""v1.19.8"", GitCommit:""fd5d41537aee486160ad9b5356a9d82363273721"", GitTreeState:""clean"", BuildDate:""2021-02-17T12:33:08Z"", GoVersion:""go1.15.8"", Compiler:""gc"", Platform:""linux/amd64""}

 

As you said, it is strange that JobManager could still receive the ""MODIFIED"" event for leader ConfigMap, which means the leader elector could update the annotation successfully. I checked the configmap and it seems it is kept being updated, but I do not know why. Do you have any idea how we could debug the issue?

 

 ;;;","16/Mar/21 04:50;wangyang0918;[~petrizhang] After submitting more jobs in the session cluster, I could reproduce your issue. I think the root cause might be the io-executor pool size is not big enough. In my real K8s cluster, all the cpus(64 cores) are bound to pods. The io-executor pool size is 256(64 * 4). However, in the minikube, I just have one cpus and pool size is 4. Currently, we may have some blocking operations in the io-executor. This causes the ""confirmLeadership"" could not be done timely.

 

Could you please increase {{cluster.io-pool.size}} to 32 or bigger and have a try again? Moreover, I will dive into more and try to find out what is blocking the io-executor.;;;","16/Mar/21 07:48;wangyang0918;I have attached a jstack of JobManager process. It reveals the root cause. All the cluster-io theads(only 4) are waiting for the result of completableFuture while the futures are also needed to be executed in the io-executor.

I believe increase the size of {{cluster.io-pool.size}} could help.;;;","16/Mar/21 08:57;petrizhang;[~fly_in_gis] I confirm that increasing cluster.io-pool.size solved the problem, and JobManager can recover. Thanks a lot!

BTW, do you think Flink could be improved to handle this case (when cluster.io-pool.size is not big and the futures are blocked)? Anyway, I think it is fine for my case now.;;;","16/Mar/21 11:13;wangyang0918;[~petrizhang] Yes, I think the current default value of {{cluster.io-pool.size}} does not make sense when running in the containerized environment. Since the CPU cores may be just 1, and then the pool size(1 * 4) will be too small.

 

cc [~trohrmann], do you think we could make the {{cluster.io-pool.size}} always than 16? It is enough to run more than 10 jobs in a session cluster.

{{Math.max(16, 4 * the number of CPU cores)}};;;","17/Mar/21 17:48;trohrmann;Thanks for the analysis [~petrizhang] and [~fly_in_gis]. Do we know why we wait on {{CompletableFuture}} in the io-executor? I think this does not sound right and we might wanna fix this.;;;","17/Mar/21 19:24;trohrmann;I think the problem is that we use the same executor for {{KubernetesRunningJobsRegistry.writeJobStatusToConfigMap}} and for the {{Fabric8FlinkKubeClient}}. If we give the latter its own executor, it should all work.;;;","18/Mar/21 06:07;wangyang0918;[~trohrmann] At the very beginning, we have dedicated thread pool for {{Fabric8FlinkKubeClient}}. The default value is 4 and I think it is enough for almost cases. But we make Kubernetes Client in KubernetesResourceManagerDriver use io-executor in FLINK-19700.

So are you suggesting to change back to use the dedicated thread pool for {{Fabric8FlinkKubeClient}}? After then, I think we also need to introduce a new config option to configure the pool size.

 

Beyond this change, I still have the concern about how we calculate the default io-executor pool size based on hardware information. In containerize environment, it will cause the value too small.;;;","18/Mar/21 09:14;trohrmann;Yes, I think we need to introduce a separate thread pool here. The problem is if we have two actions A, B and A waits on B but both actions use the same thread pool, then it is possible that multiple A actions deadlock the B actions.

Theoretically, if we design everything correctly, then Flink should work with a single thread for each available thread pool. We should try to build Flink like this. Admittedly, this might not give us the best performance based on the available resources. Hence, I think it makes sense to make the thread pool size of the {{Fabric8FlinkKubeClient}} configurable.

How would you like to change the default size for these pools?;;;","18/Mar/21 09:55;wangyang0918;I think 2 is enough for the default pool size of the {{Fabric8FlinkKubeClient}}. And I will have a test via starting a Flink application, which has more than 1000 TaskManager and enable the Kubernetes HA service.;;;","23/Mar/21 04:49;wangyang0918;I have tested in a real K8s cluster and having the following result.
 * If I configure the dedicated thread pool for Kubernetes client to 2, requesting all 1000 TaskManagers needs 24 seconds.
 * The requesting time will down to 11 seconds if we have 4 threads.

 So I will introduce a new config option for Kubernetes client thread pool with default value 4.;;;","01/Apr/21 11:29;trohrmann;Fixed via

1.13.0: ade268ec1bce2d784b9c1c70fb0fdcf6fae91498
1.12.3: 94f69ee2ac546f446478769a4944779d1096b3ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC sink can't get valid connection after 5 minutes using Oracle JDBC driver,FLINK-21674,13363084,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,fuyaoli,fuyaoli,08/Mar/21 19:56,15/Mar/21 08:19,13/Jul/23 08:07,13/Mar/21 01:12,1.12.1,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,,,,,,"I use JDBCSink.sink() method to sink data to Oracle Autonomous Data Warehousr with Oracle JDBC driver. I can sink data into Oracle Autonomous database sucessfully. If there is IDLE time of over 5 minutes, then do a insertion, the retry mechanism can't reestablish the JDBC and it will run into the error below. I have set the retry to be 3 times, even after retry, it will still fail. Only restart the application(an automatic process) could solve the issue from checkpoint.


 11:41:04,872 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat - JDBC executeBatch error, retry times = 0
 java.sql.BatchUpdateException: IO Error: Broken pipe

It will fail the application and restart from checkpoint. After restarting from checkpoint, the JDBC connection can be established correctly.

The connection timeout can be configured by

alter system set MAX_IDLE_TIME=1440; // Connection will get timeout after 1440 minutes.

Such timeout parameter behavior change can be verified by SQL developer. However, Flink still got connection error after 5 minutes configuring this.

I suspect this is some issues in reading some configuration problems from Flink side to establish to sucessful connection.

Full log:
{code:java}
11:41:04,872 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat  - JDBC executeBatch error, retry times = 0
java.sql.BatchUpdateException: IO Error: Broken pipe
	at oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9711)
	at oracle.jdbc.driver.T4CPreparedStatement.executeLargeBatch(T4CPreparedStatement.java:1447)
	at oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9487)
	at oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:237)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.executeBatch(SimpleBatchStatementExecutor.java:73)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:216)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:184)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.writeRecord(JdbcBatchingOutputFormat.java:167)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.invoke(GenericJdbcSinkFunction.java:54)
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:75)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:50)
	at org.myorg.quickstart.processor.InvoiceBoProcessFunction.onTimer(InvoiceBoProcessFunction.java:613)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.invokeUserFunction(KeyedProcessOperator.java:91)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.onEventTime(KeyedProcessOperator.java:70)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.advanceWatermark(InternalTimerServiceImpl.java:302)
	at org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.advanceWatermark(InternalTimeServiceManagerImpl.java:194)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:626)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitWatermark(OneInputStreamTask.java:193)
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:196)
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:206)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:174)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:395)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:609)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
11:41:05,725 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat  - JDBC connection is not valid, and reestablish connection failed.
java.sql.SQLRecoverableException: Closed Connection
	at oracle.jdbc.driver.PhysicalConnection.needLine(PhysicalConnection.java:3525)
	at oracle.jdbc.driver.OracleStatement.closeOrCache(OracleStatement.java:1478)
	at oracle.jdbc.driver.OracleStatement.close(OracleStatement.java:1461)
	at oracle.jdbc.driver.OracleStatementWrapper.close(OracleStatementWrapper.java:122)
	at oracle.jdbc.driver.OraclePreparedStatementWrapper.close(OraclePreparedStatementWrapper.java:98)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.closeStatements(SimpleBatchStatementExecutor.java:81)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:195)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.writeRecord(JdbcBatchingOutputFormat.java:167)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.invoke(GenericJdbcSinkFunction.java:54)
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:75)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:50)
	at org.myorg.quickstart.processor.InvoiceBoProcessFunction.onTimer(InvoiceBoProcessFunction.java:613)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.invokeUserFunction(KeyedProcessOperator.java:91)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.onEventTime(KeyedProcessOperator.java:70)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.advanceWatermark(InternalTimerServiceImpl.java:302)
	at org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.advanceWatermark(InternalTimeServiceManagerImpl.java:194)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:626)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitWatermark(OneInputStreamTask.java:193)
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:196)
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:206)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:174)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:395)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:609)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
11:41:05,729 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat  - JDBC executeBatch error, retry times = 0
java.sql.SQLRecoverableException: Closed Connection
	at oracle.jdbc.driver.OracleStatement.ensureOpen(OracleStatement.java:4204)
	at oracle.jdbc.driver.OracleStatement.getQueryTimeout(OracleStatement.java:3093)
	at oracle.jdbc.driver.OracleStatementWrapper.getQueryTimeout(OracleStatementWrapper.java:183)
	at org.myorg.quickstart.boconstruction.stream.BusinessObjectConstruction.lambda$processFunction$16ee7a3d$1(BusinessObjectConstruction.java:103)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.executeBatch(SimpleBatchStatementExecutor.java:70)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:216)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:184)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.close(JdbcBatchingOutputFormat.java:232)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.close(GenericJdbcSinkFunction.java:67)
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:783)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:762)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:681)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
11:41:05,770 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat  - JDBC executeBatch error, retry times = 1
java.sql.SQLRecoverableException: Closed Connection
	at oracle.jdbc.driver.OracleStatement.ensureOpen(OracleStatement.java:4204)
	at oracle.jdbc.driver.OracleStatement.getQueryTimeout(OracleStatement.java:3093)
	at oracle.jdbc.driver.OracleStatementWrapper.getQueryTimeout(OracleStatementWrapper.java:183)
	at org.myorg.quickstart.boconstruction.stream.BusinessObjectConstruction.lambda$processFunction$16ee7a3d$1(BusinessObjectConstruction.java:103)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.executeBatch(SimpleBatchStatementExecutor.java:70)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:216)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:184)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.close(JdbcBatchingOutputFormat.java:232)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.close(GenericJdbcSinkFunction.java:67)
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:783)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:762)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:681)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
11:41:06,820 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat  - JDBC executeBatch error, retry times = 2
java.sql.SQLRecoverableException: Closed Connection
	at oracle.jdbc.driver.OracleStatement.ensureOpen(OracleStatement.java:4204)
	at oracle.jdbc.driver.OracleStatement.getQueryTimeout(OracleStatement.java:3093)
	at oracle.jdbc.driver.OracleStatementWrapper.getQueryTimeout(OracleStatementWrapper.java:183)
	at org.myorg.quickstart.boconstruction.stream.BusinessObjectConstruction.lambda$processFunction$16ee7a3d$1(BusinessObjectConstruction.java:103)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.executeBatch(SimpleBatchStatementExecutor.java:70)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:216)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:184)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.close(JdbcBatchingOutputFormat.java:232)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.close(GenericJdbcSinkFunction.java:67)
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:783)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:762)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:681)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
11:41:08,865 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat  - JDBC executeBatch error, retry times = 3
java.sql.SQLRecoverableException: Closed Connection
	at oracle.jdbc.driver.OracleStatement.ensureOpen(OracleStatement.java:4204)
	at oracle.jdbc.driver.OracleStatement.getQueryTimeout(OracleStatement.java:3093)
	at oracle.jdbc.driver.OracleStatementWrapper.getQueryTimeout(OracleStatementWrapper.java:183)
	at org.myorg.quickstart.boconstruction.stream.BusinessObjectConstruction.lambda$processFunction$16ee7a3d$1(BusinessObjectConstruction.java:103)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.executeBatch(SimpleBatchStatementExecutor.java:70)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:216)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:184)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.close(JdbcBatchingOutputFormat.java:232)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.close(GenericJdbcSinkFunction.java:67)
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:783)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:762)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:681)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
11:41:08,866 WARN  org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat  - Writing records to JDBC failed.
java.io.IOException: java.sql.SQLRecoverableException: Closed Connection
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:190)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.close(JdbcBatchingOutputFormat.java:232)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.close(GenericJdbcSinkFunction.java:67)
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:783)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:762)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:681)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.sql.SQLRecoverableException: Closed Connection
	at oracle.jdbc.driver.OracleStatement.ensureOpen(OracleStatement.java:4204)
	at oracle.jdbc.driver.OracleStatement.getQueryTimeout(OracleStatement.java:3093)
	at oracle.jdbc.driver.OracleStatementWrapper.getQueryTimeout(OracleStatementWrapper.java:183)
	at org.myorg.quickstart.boconstruction.stream.BusinessObjectConstruction.lambda$processFunction$16ee7a3d$1(BusinessObjectConstruction.java:103)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.executeBatch(SimpleBatchStatementExecutor.java:70)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:216)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:184)
	... 11 more
11:41:08,866 WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Error closing producer.
java.lang.NoSuchMethodError: org.apache.kafka.clients.producer.KafkaProducer.close(Ljava/time/Duration;)V
	at org.apache.flink.streaming.connectors.kafka.internals.FlinkKafkaInternalProducer.close(FlinkKafkaInternalProducer.java:172)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:949)
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:783)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:762)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:681)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
11:41:08,868 WARN  org.apache.flink.runtime.taskmanager.Task                     - ProcessTableOutput -> (Sink: adwSink, Sink: Print to Std. Out, Sink: invoice-notification, Sink: Print to Std. Out, Sink: header-notification, Sink: Print to Std. Out, Sink: lines-notification, Sink: Print to Std. Out, Sink: distributions-notification) (1/1)#0 (95b16d216f03759f0f0c131ba188b338) switched from RUNNING to FAILED.
java.io.IOException: Writing records to JDBC failed.
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.writeRecord(JdbcBatchingOutputFormat.java:170)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.invoke(GenericJdbcSinkFunction.java:54)
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:75)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:50)
	at org.myorg.quickstart.processor.InvoiceBoProcessFunction.onTimer(InvoiceBoProcessFunction.java:613)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.invokeUserFunction(KeyedProcessOperator.java:91)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.onEventTime(KeyedProcessOperator.java:70)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.advanceWatermark(InternalTimerServiceImpl.java:302)
	at org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.advanceWatermark(InternalTimeServiceManagerImpl.java:194)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:626)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitWatermark(OneInputStreamTask.java:193)
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:196)
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:206)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:174)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:395)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:609)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.IOException: Reestablish JDBC connection failed
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:202)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.writeRecord(JdbcBatchingOutputFormat.java:167)
	... 29 more
Caused by: java.sql.SQLRecoverableException: Closed Connection
	at oracle.jdbc.driver.PhysicalConnection.needLine(PhysicalConnection.java:3525)
	at oracle.jdbc.driver.OracleStatement.closeOrCache(OracleStatement.java:1478)
	at oracle.jdbc.driver.OracleStatement.close(OracleStatement.java:1461)
	at oracle.jdbc.driver.OracleStatementWrapper.close(OracleStatementWrapper.java:122)
	at oracle.jdbc.driver.OraclePreparedStatementWrapper.close(OraclePreparedStatementWrapper.java:98)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.closeStatements(SimpleBatchStatementExecutor.java:81)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:195)
	... 30 more
11:41:08,869 INFO  org.apache.flink.runtime.taskmanager.Task                     - Freeing task resources for ProcessTableOutput -> (Sink: adwSink, Sink: Print to Std. Out, Sink: invoice-notification, Sink: Print to Std. Out, Sink: header-notification, Sink: Print to Std. Out, Sink: lines-notification, Sink: Print to Std. Out, Sink: distributions-notification) (1/1)#0 (95b16d216f03759f0f0c131ba188b338).
11:41:08,876 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Un-registering task and sending final execution state FAILED to JobManager for task ProcessTableOutput -> (Sink: adwSink, Sink: Print to Std. Out, Sink: invoice-notification, Sink: Print to Std. Out, Sink: header-notification, Sink: Print to Std. Out, Sink: lines-notification, Sink: Print to Std. Out, Sink: distributions-notification) (1/1)#0 95b16d216f03759f0f0c131ba188b338.
11:41:08,880 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - ProcessTableOutput -> (Sink: adwSink, Sink: Print to Std. Out, Sink: invoice-notification, Sink: Print to Std. Out, Sink: header-notification, Sink: Print to Std. Out, Sink: lines-notification, Sink: Print to Std. Out, Sink: distributions-notification) (1/1) (95b16d216f03759f0f0c131ba188b338) switched from RUNNING to FAILED on ac2c0e70-42f9-4d5d-820a-19f6561a6297 @ localhost (dataPort=-1).
java.io.IOException: Writing records to JDBC failed.
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.writeRecord(JdbcBatchingOutputFormat.java:170)
	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.invoke(GenericJdbcSinkFunction.java:54)
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:75)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:50)
	at org.myorg.quickstart.processor.InvoiceBoProcessFunction.onTimer(InvoiceBoProcessFunction.java:613)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.invokeUserFunction(KeyedProcessOperator.java:91)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.onEventTime(KeyedProcessOperator.java:70)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.advanceWatermark(InternalTimerServiceImpl.java:302)
	at org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.advanceWatermark(InternalTimeServiceManagerImpl.java:194)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:626)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitWatermark(OneInputStreamTask.java:193)
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:196)
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:206)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:174)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:395)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:609)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.IOException: Reestablish JDBC connection failed
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:202)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.writeRecord(JdbcBatchingOutputFormat.java:167)
	... 29 more
Caused by: java.sql.SQLRecoverableException: Closed Connection
	at oracle.jdbc.driver.PhysicalConnection.needLine(PhysicalConnection.java:3525)
	at oracle.jdbc.driver.OracleStatement.closeOrCache(OracleStatement.java:1478)
	at oracle.jdbc.driver.OracleStatement.close(OracleStatement.java:1461)
	at oracle.jdbc.driver.OracleStatementWrapper.close(OracleStatementWrapper.java:122)
	at oracle.jdbc.driver.OraclePreparedStatementWrapper.close(OraclePreparedStatementWrapper.java:98)
	at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.closeStatements(SimpleBatchStatementExecutor.java:81)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:195)
	... 30 more
11:41:08,886 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy  - Calculating tasks to restart to recover the failed task 91f695b4d2df74b06fe58043ee03541f_0.
11:41:08,887 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy  - 9 tasks should be restarted to recover the failed task 91f695b4d2df74b06fe58043ee03541f_0. 
{code}","Flink version: 1.12.1 Scala version: 2.11 Java version: 1.11 Flink System parallelism: 1 JDBC Driver: Oracle ojdbc10 Database: Oracle Autonomous Database on Oracle Cloud Infrastructure version 19c(You can regard this as an cloud based Oracle Database)

 

Flink user mailing list: http://mail-archives.apache.org/mod_mbox/flink-user/202103.mbox/%3CCH2PR10MB402466373B33A3BBC5635A0AEC8C9%40CH2PR10MB4024.namprd10.prod.outlook.com%3E",fuyaoli,jark,michael ran,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 15 08:19:40 UTC 2021,,,,,,,,,,"0|z0of9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/21 12:49;roman;I'd suppose that the connection is closed on the network level.

But if maxRetries > 1 then JdbcBatchingOutputFormat will try to reconnect and that could solve the problem (a proper solution would be keep-alive though).

Do you have maxRetries > 1? If not, could you try increasing it?

cc: @wanglijie95, @jark;;;","11/Mar/21 15:39;fuyaoli;[~roman_khachatryan] Hi Roman, I already set the max retry to be 3. I think the major issue is that the retry mechanism is not working at all here. It will always fail. However, restart the application will reestablish the connection. You can check my logs for details. The code below is used to submit the query. One more question, JDBC is not able to do select query, right? It is supposed to do UPDATE or INSERT or DELETE, right?

[~jark]
{code:java}
businessObjectDataStream.addSink(
 JdbcSink.sink(
 ""INSERT INTO ADW_DEMO_TABLE (invoice_id, last_update_time, invoice_deleted_flag, json_doc) values(?, ?, ?, ?)"",
 (preparedStatement, testInvoiceBo) -> {
 try {
 Gson gson = new GsonBuilder()
 .excludeFieldsWithoutExposeAnnotation()
 .create();
 String invoiceId = testInvoiceBo.getINVOICE_ID();
 Timestamp lastUpdateTime = testInvoiceBo.getHEADER().getOp_ts();
 String invoiceDeletedFlag = ""0"";
 String json = gson.toJson(testInvoiceBo);
 log.info(""insertion information: {}"", invoiceId);
 log.info(""insertion information: {}"", lastUpdateTime);
 log.info(""insertion information: {}"", invoiceDeletedFlag);
 log.info(""insertion information: {}"", json);
 preparedStatement.setString(1, invoiceId);
 preparedStatement.setTimestamp(2, lastUpdateTime);
 preparedStatement.setString(3, invoiceDeletedFlag);
 preparedStatement.setString(4, json); } catch (JsonIOException e)
{ log.error(""Failed to parse JSON"", e); }
},
 new JdbcExecutionOptions.Builder()
 .withBatchIntervalMs(0)
 .withBatchSize(1)
 .withMaxRetries(3)
 .build(),
 new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
 .withUrl(DBCredentials.DB_URL)
 .withDriverName(DBCredentials.DB_DRIVER)
 .withUsername(DBCredentials.DB_USER)
 .withPassword(DBCredentials.DB_PASSWORD)
 .build()))
 .name(""adwSink"")
 .uid(""adwSink"")
 .setParallelism(1);
{code}
The jar libraries I used for JDBC connection. This is an oracle JDBC driver.

 
{code:java}
<dependency>
 <groupId>com.oracle.database.jdbc</groupId>
 <artifactId>ojdbc10</artifactId>
 <version>19.3.0.0</version>
 </dependency>
 <dependency>
 <groupId>com.oracle.database.jdbc</groupId>
 <artifactId>ucp</artifactId>
 <version>19.3.0.0</version>
 </dependency>
 <dependency>
 <groupId>com.oracle.database.security</groupId>
 <artifactId>osdt_core</artifactId>
 <version>19.3.0.0</version>
 </dependency>
 <dependency>
 <groupId>com.oracle.database.security</groupId>
 <artifactId>oraclepki</artifactId>
 <version>19.3.0.0</version>
 </dependency>
 <dependency>
 <groupId>org.apache.flink</groupId>
 <artifactId>flink-connector-jdbc_2.11</artifactId>
 <version>${flink.version}</version>
 </dependency>
{code};;;","11/Mar/21 19:02;roman;I see, retry does actually happen but it fails with ""Closed Connection"" exception.

Probably, connection pool returns the same (broken) connection to Flink? In that case, [validating connection|https://docs.oracle.com/cd/E11882_01/java.112/e12265/connect.htm#CHDIDJGH] when returning from the pool should help (in addition to validation query).

I would first try to verify it by removing UCP jar from pom/classpath.

 
{quote}One more question, JDBC is not able to do select query, right? It is supposed to do UPDATE or INSERT or DELETE, right?
{quote}
Right, at least the API exposed in Flink JdbcSink (GenericJdbcSinkFunction can be constructed with a custom AbstractJdbcOutputFormat).;;;","13/Mar/21 01:11;fuyaoli;Hi Roman, thanks for your help. 

I was able to figure out the root cause.

I am an employee in Oracle. I need to be within OCI VPN instead of Oracle VPN to get access to ADW. I used to use proxy to access it. However, proxy will be pose a 5 min hard limit for connection. After 5 min, the JDBC connection will be cut off no matter what you configured from DB side and client side.

 

With OCI VPN, I am able to get over the issue. Thanks for your help.;;;","13/Mar/21 01:13;fuyaoli;I was not aware of this earlier, finally I find someone who knows some configurations. It is a little bit tricky here.;;;","15/Mar/21 08:19;roman;Thanks for the clarification [~fuyaoli]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standby RM might remove resources from Kubernetes,FLINK-21667,13362984,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,08/Mar/21 11:01,08/Feb/22 08:38,13/Jul/23 08:07,03/Jun/21 11:02,1.12.2,,,,,,,,1.14.0,,,,,,,Deployment / Kubernetes,Runtime / Coordination,,,,1,pull-request-available,,,,,"Currently, on initialization {{KubernetesResourceManagerDriver}} starts a watch for receiving pod events. It could happen that it starts to receive events before obtaining leadership. Consequently, a standby RM may remove terminated pods from Kubernetes during handling the events.

This is not very damaging atm, since the removed pods are already terminated anyway. However, it would still be good for a standby RM to strictly following the contract and make no modifications before obtaining leadership. We might consider to postpone starting of the watch to when the leadership is granted.",,aitozi,dlorych,klion26,nicholasjiang,trohrmann,wangyang0918,xtsong,yunta,,,,,,,,,,,,,,,,,,,,FLINK-17707,,,,,,,,,,,,,,,,,FLINK-23240,FLINK-24038,FLINK-25885,FLINK-22816,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 03 11:02:24 UTC 2021,,,,,,,,,,"0|z0oens:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/21 12:30;nicholasjiang;[~xintongsong], do you mean that this could postpone the `watchTaskManagerPods` caller to KubernetesLeaderElector#hasLeadership?;;;","09/Mar/21 06:31;xtsong;I haven't check this in detail yet. But yes, that's what I meant in general.;;;","09/Mar/21 14:13;trohrmann;I would suggest to only start a {{ResourceManager}} after obtaining leadership (similar to how we do it for the {{Dispatcher}} and the {{TaskExecutor}}).;;;","10/Mar/21 03:05;xtsong;Thanks [~trohrmann], I think that's a really good idea.

The lifecycle management and leadership management are coupled for the {{ResourceManager}}. It should simplify things a lot if we only start the {{ResourceManager}} when leadership is obtained, so that the {{ResourceManager}} and its components can always assume having the leadership.

I'll take a closer look into this.

BTW, I assume what you meant is {{Dispatcher}} and {{JobMaster}}? We don't have leader election for {{TaskExecutor}}.;;;","10/Mar/21 09:01;trohrmann;Yes, sorry I meant {{JobMaster}} instead of {{TaskExecutor}}.;;;","16/Apr/21 22:43;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","19/Apr/21 01:35;xtsong;A PR has been opened for this ticket. We are waiting for the release 1.13 branch cut.;;;","03/Jun/21 11:02;xtsong;Fixed via
- master (1.14): a6c1e3a59880ba10e06dedbd4717444afa97da34;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHARD_GETRECORDS_INTERVAL_MILLIS  wrong use?,FLINK-21661,13362960,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dannycranmer,jiamo,jiamo,08/Mar/21 09:32,25/Mar/21 13:52,13/Jul/23 08:07,25/Mar/21 13:52,1.12.2,,,,,,,,1.12.3,1.13.0,,,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,,"kinesis `SHARD_GETRECORDS_INTERVAL_MILLIS`

mean between normal getRecords sleep.

But at end . The value was used in exception.
{code:java}
 
// sleep for the fetch interval before the next getRecords attempt with the
// refreshed iterator
if (expiredIteratorBackoffMillis != 0) {
 Thread.sleep(expiredIteratorBackoffMillis);
}
{code}
 

 
{code:java}
return new PollingRecordPublisher(
        startingPosition,
        streamShardHandle,
        metricsReporter,
        kinesisProxy,
        configuration.getMaxNumberOfRecordsPerFetch(),
        configuration.getFetchIntervalMillis());

{code}
But the last arg was *expiredIteratorBackoffMillis.*

 

Is this a problem?

 

 

 

 

 ",,dannycranmer,jiamo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18512,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 10 10:55:14 UTC 2021,,,,,,,,,,"0|z0oeig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/21 06:35;jiamo;In [https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kinesis.html#polling-default-record-publisher-1] 

while the latter modifies the sleep interval between each fetch (default is 200);;;","10/Mar/21 10:53;dannycranmer;Thank you for the bug report, I agree with your findings and will work towards a fix.

Thanks;;;","10/Mar/21 10:55;dannycranmer;If this is blocking you, consider using adaptive reads {{SHARD_USE_ADAPTIVE_READS}}
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE LIKE cannot reference tables in HiveCatalog,FLINK-21660,13362943,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,leoliu,leoliu,08/Mar/21 08:13,28/May/21 09:07,13/Jul/23 08:07,15/Apr/21 01:12,1.12.1,,,,,,,,1.13.0,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"create a table like with a hive catalog table

{color:#cc7832}CREATE TABLE {color}temp_order
{color:#cc7832}WITH {color}(
 {color:#6a8759}'properties.group.id' {color}= {color:#6a8759}'test1'{color}{color:#cc7832},
{color} {color:#6a8759}'properties.bootstrap.servers' {color}= {color:#6a8759}'kafka.address'
{color})
{color:#cc7832}LIKE {color}hive.ods.flink_order

It has a error, log is :

 

Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.test_taos_customer_agent_cinema'.

Table options are:

'connector'='upsert-kafka'
'is_generic'='true'
'key.format'='json'
'properties.bootstrap.servers'='kafka.address'
'properties.group.id'='test1'
'topic'='topic1'
'value.format'='json'
...
Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for connector 'upsert-kafka'.

Unsupported options:

is_generic

 

the ’is_generic‘ option is  added by hive catalog,  but  `kafka` connector  does not support this  option.

 ",,jark,leoliu,libenchao,lirui,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17922,,,FLINK-21680,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 01:12:36 UTC 2021,,,,,,,,,,"0|z0oeeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/21 08:18;leoliu;I have a solution:

{color:#cc7832}CREATE TABLE {color}temp_order
{color:#cc7832}WITH {color}(
 {color:#6a8759}'connector'{color}={color:#6a8759}'upsert-kafka'{color}{color:#cc7832},
{color} {color:#6a8759}'key.format'{color}={color:#6a8759}'json'{color}{color:#cc7832},
{color} {color:#6a8759}'topic'{color}={color:#6a8759}'topic1'{color}{color:#cc7832},
{color} {color:#6a8759}'value.format'{color}={color:#6a8759}'json'{color}{color:#cc7832},
{color} {color:#6a8759}'properties.group.id' {color}= {color:#6a8759}'test1'{color}{color:#cc7832},
{color} {color:#6a8759}'properties.bootstrap.servers' {color}= {color:#6a8759}'kafka.address'
{color})
{color:#cc7832}LIKE {color}hive.ods.flink_order
(
    EXCLUDING OPTIONS
)

 

It's not perfect, you  must to  overwrite  all options

 ;;;","08/Mar/21 08:39;jark;cc [~lirui];;;","08/Mar/21 12:12;lirui;I'd prefer to deprecate the {{""is_generic""}} flag, and follow flink's convention to use {{""connector""}} to distinguish hive and non-hive tables. I guess it'll make the logic simpler and avoid more issues in the future.

If we do that, we need to keep backward compatibility in mind -- tables created by an older HiveCatalog should continue to be usable after this change.;;;","08/Mar/21 12:45;jark;sounds good to me. ;;;","15/Apr/21 01:12;ykt836;fixed: 5b9e7882207357120717966d8bf7efd53c53ede5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointSettings not properly exposed for initializing jobs,FLINK-21659,13362933,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,maguowei,maguowei,08/Mar/21 07:37,28/May/21 09:13,13/Jul/23 08:07,23/Apr/21 14:06,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,Runtime / REST,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14232&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a

“Completed checkpoint” is more than two times(42 times in the ""flink-vsts-standalonejob-2-fv-az83-563.log"") but the test still fail. 
 !screenshot-1.png! 



",,maguowei,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/21 09:01;maguowei;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13023603/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 23 14:06:11 UTC 2021,,,,,,,,,,"0|z0oecg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/21 03:56;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15573&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=1733
;;;","09/Apr/21 09:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16056&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=1589

;;;","09/Apr/21 09:13;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15486&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=1583;;;","11/Apr/21 04:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16326&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=1639
we can find 38 completed checkpoints in the flink-vsts-standalonejob-2-fv-az227-558.log



;;;","12/Apr/21 03:38;maguowei;we could find 22 completed checkpoints in the flink-vsts-standalonejob-2-fv-az101-42.log.

I suspect the following code does not check the  flink-vsts-standalonejob-2-fv-az101-42.log.

{code:bash}
       N=$(grep -o ""${text}"" $FLINK_DIR/log/*${logs}*.log | wc -l)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16332&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=1583

;;;","15/Apr/21 06:50;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16566&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=1601;;;","20/Apr/21 11:24;chesnay;There is some regex parsing being applied to the result of a REST request, but we don't check that the request was actually successful. We end up trying to parse a checkpoint id from a stacktrace:
{code:java}
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/common_ha.sh: line 151: [: 58)\n\tat org.apache.flink.runtime.rest.handler.job.AbstractAccessExecutionGraphHandler.handleRequest(AbstractAccessExecutionGraphHandler.java: integer expression expected {code}
The reason this request fails is because, if a job is in the INITIALIZING state, then the ArchivedExecutionGraph is created with a nulled CheckpointStatsSnapshot, which is mistakenly interpreted by the handler as checkpointing not being enabled.

This is not logged, because for this specific case the logging was disabled in https://github.com/apache/flink/pull/14129.;;;","23/Apr/21 14:06;chesnay;master: ef03c9d82fd2df5a53f4e1f553df79972e825e2d

1.13: 7578b1d5a9f10e7360f2d75f2b2dbd2339864d4b ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align flink-benchmarks with job graph builder,FLINK-21658,13362929,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,08/Mar/21 07:14,08/Mar/21 12:04,13/Jul/23 08:07,08/Mar/21 12:02,,,,,,,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,,"After FLINK-21401 resolved, previous job graph constructor has been removed:
{code:java}
public JobGraph(JobVertex... vertices) {
    this(null, vertices);
}
{code}
However, this method is still used in flink-benchmarks, we need to adopt code to latest Flink master.",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21524,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-08 07:14:37.0,,,,,,,,,,"0|z0oebk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect simplification for coalesce call on a groupingsets' result,FLINK-21655,13362917,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,08/Mar/21 06:06,24/Mar/21 13:25,13/Jul/23 08:07,24/Mar/21 13:25,1.11.3,1.12.2,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Currently the planner will do an incorrectly simplification for `coalesce` call on a `groupingsets`'s result, see the following query example:

{code:scala}

val sqlQuery =
 """"""
 |SELECT
 | a,
 | b,
 | coalesce(c, 'empty'),
 | avg(d)
 |FROM t1
 |GROUP BY GROUPING SETS ((a, b), (a, b, c))
 """""".stripMargin

{code}

will generate a wrong plan which lost the `coalesce` call:

{code}

Calc(select=[a, b, c AS EXPR$2, EXPR$3])
+- HashAggregate(isMerge=[true], groupBy=[a, b, c, $e], select=[a, b, c, $e, Final_AVG(sum$0, count$1) AS EXPR$3])
 +- Exchange(distribution=[hash[a, b, c, $e]])
 +- LocalHashAggregate(groupBy=[a, b, c, $e], select=[a, b, c, $e, Partial_AVG(d) AS (sum$0, count$1)])
 +- Expand(projects=[\{a, b, c, d, 0 AS $e}, \{a, b, null AS c, d, 1 AS $e}])
 +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, c, d])

{code}",,godfreyhe,jark,jingzhang,libenchao,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 13:25:14 UTC 2021,,,,,,,,,,"0|z0oe8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/21 13:25;godfreyhe;Fixed in 1.13.0: b975fdaf741e1f02008a9593ffb25aadccd5b165;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionCapacitySchedulerITCase.testStartYarnSessionClusterInQaTeamQueue fail because of NullPointerException,FLINK-21654,13362910,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,maguowei,maguowei,08/Mar/21 04:33,19/Mar/21 12:30,13/Jul/23 08:07,19/Mar/21 12:30,1.12.2,,,,,,,,1.11.4,1.12.3,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14265&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code:java}
2021-03-07T23:00:44.6390668Z [ERROR] testStartYarnSessionClusterInQaTeamQueue(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase)  Time elapsed: 7.338 s  <<< ERROR!
2021-03-07T23:00:44.6391415Z java.lang.NullPointerException: 
2021-03-07T23:00:44.6403594Z java.lang.NullPointerException
2021-03-07T23:00:44.6404575Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
2021-03-07T23:00:44.6405710Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
2021-03-07T23:00:44.6406830Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
2021-03-07T23:00:44.6407970Z 	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
2021-03-07T23:00:44.6409075Z 	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
2021-03-07T23:00:44.6412848Z 	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
2021-03-07T23:00:44.6417313Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2021-03-07T23:00:44.6421872Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2021-03-07T23:00:44.6423676Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
2021-03-07T23:00:44.6424387Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
2021-03-07T23:00:44.6424997Z 	at java.security.AccessController.doPrivileged(Native Method)
2021-03-07T23:00:44.6425608Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-03-07T23:00:44.6426513Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2021-03-07T23:00:44.6427351Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
2021-03-07T23:00:44.6427767Z 
2021-03-07T23:00:44.6428196Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2021-03-07T23:00:44.6428975Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2021-03-07T23:00:44.6429888Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2021-03-07T23:00:44.6442419Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2021-03-07T23:00:44.6445364Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
2021-03-07T23:00:44.6644429Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateRuntimeException(RPCUtil.java:85)
2021-03-07T23:00:44.6658468Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:122)
2021-03-07T23:00:44.6669171Z 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:291)
2021-03-07T23:00:44.6680027Z 	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
2021-03-07T23:00:44.6690713Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-07T23:00:44.6701085Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-07T23:00:44.6708626Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2021-03-07T23:00:44.6709488Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2021-03-07T23:00:44.6710261Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2021-03-07T23:00:44.6711051Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2021-03-07T23:00:44.6711864Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2021-03-07T23:00:44.6729939Z 	at com.sun.proxy.$Proxy111.getApplications(Unknown Source)
2021-03-07T23:00:44.6746044Z 	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:528)
2021-03-07T23:00:44.6747093Z 	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:505)
2021-03-07T23:00:44.6748256Z 	at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:293)
2021-03-07T23:00:44.6749122Z 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:274)
2021-03-07T23:00:44.6750975Z 	at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.testStartYarnSessionClusterInQaTeamQueue(YARNSessionCapacitySchedulerITCase.java:164)
2021-03-07T23:00:44.6751907Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-07T23:00:44.6753574Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-07T23:00:44.6754504Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-07T23:00:44.6755270Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-07T23:00:44.6757569Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-03-07T23:00:44.6758434Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-03-07T23:00:44.6759463Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-03-07T23:00:44.6760279Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-03-07T23:00:44.6761082Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-03-07T23:00:44.6762689Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-03-07T23:00:44.6763589Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6764601Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6765295Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-03-07T23:00:44.6765968Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-07T23:00:44.6766601Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-03-07T23:00:44.6767442Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-03-07T23:00:44.6768248Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-03-07T23:00:44.6768968Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-07T23:00:44.6769645Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-07T23:00:44.6770337Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-07T23:00:44.6771022Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-07T23:00:44.6771746Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-07T23:00:44.6772483Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-03-07T23:00:44.6773298Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-03-07T23:00:44.6774046Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6774773Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6775435Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-07T23:00:44.6776067Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-07T23:00:44.6776794Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-03-07T23:00:44.6777694Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-03-07T23:00:44.6778543Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-03-07T23:00:44.6779362Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-03-07T23:00:44.6780201Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-03-07T23:00:44.6781085Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-03-07T23:00:44.6781878Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-03-07T23:00:44.6782634Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-03-07T23:00:44.6783545Z Caused by: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException
2021-03-07T23:00:44.6784609Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
2021-03-07T23:00:44.6785745Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
2021-03-07T23:00:44.6786821Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
2021-03-07T23:00:44.6787886Z 	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
2021-03-07T23:00:44.6794073Z 	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
2021-03-07T23:00:44.6796335Z 	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
2021-03-07T23:00:44.6798877Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2021-03-07T23:00:44.6801068Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2021-03-07T23:00:44.6803385Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
2021-03-07T23:00:44.6805566Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
2021-03-07T23:00:44.6807902Z 	at java.security.AccessController.doPrivileged(Native Method)
2021-03-07T23:00:44.6810554Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-03-07T23:00:44.6813030Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2021-03-07T23:00:44.6815324Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
2021-03-07T23:00:44.6817630Z 
2021-03-07T23:00:44.6819989Z 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
2021-03-07T23:00:44.6820646Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
2021-03-07T23:00:44.6821270Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2021-03-07T23:00:44.6822571Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2021-03-07T23:00:44.6823429Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2021-03-07T23:00:44.6825060Z 	at com.sun.proxy.$Proxy110.getApplications(Unknown Source)
2021-03-07T23:00:44.6827746Z 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:288)
2021-03-07T23:00:44.6828489Z 	... 50 more
2021-03-07T23:00:44.6828728Z 
2021-03-07T23:01:15.2299964Z [INFO] Running org.apache.flink.yarn.YARNITCase
2021-03-07T23:01:20.5439420Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5442231Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
2021-03-07T23:01:20.5442949Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5443688Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
2021-03-07T23:01:20.5444176Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5444733Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2021-03-07T23:01:20.5471230Z Mar 07, 2021 11:01:20 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2021-03-07T23:01:20.5473577Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2021-03-07T23:01:20.6368864Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getC
{code}
",,maguowei,mapohl,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15534,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 19 12:30:30 UTC 2021,,,,,,,,,,"0|z0oe7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/21 07:52;mapohl;Thanks for reporting this [~maguowei]. This is a known issue of the {{YARNSessionCapacitySchedulerITCase}}. It's discussed in FLINK-15534. I'm gonna add the {{NullPointerException}} documentation to the remaining tests of this class to make people aware of it in the future.;;;","10/Mar/21 13:10;mapohl;There's another proposal using retries to cover this issue. I'm working on it now.;;;","19/Mar/21 12:30;trohrmann;Fixed via

1.13.0: 5e08e55caede0c81100d7032257133854de1155c
1.12.3: e3273ef3f513ce83661696a69c1b73d695d7a495
1.11.4: 9e29a221440169bdd264c6114091580ad09592f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Run kubernetes session test (default input)' failed on Azure,FLINK-21647,13362850,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,jark,jark,07/Mar/21 14:28,01/Oct/21 11:08,13/Jul/23 08:07,02/Apr/21 01:53,1.13.0,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,0,,,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14236&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=2247,,jark,lake,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21687,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 01 11:08:48 UTC 2021,,,,,,,,,,"0|z0odu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/21 14:31;jark;Another instance:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14199&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","07/Mar/21 14:34;jark;Another instance: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14236&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","08/Mar/21 02:29;wangyang0918;[~jark] We have changed a bit about the native K8s start commands in FLINK-21128. Could you please rebase the latest master and try again?;;;","26/Mar/21 13:44;trohrmann;[~jark] have you had the chance to validate whether this problem has been fixed?;;;","01/Apr/21 16:28;trohrmann;Ping [~jark].;;;","02/Apr/21 01:53;jark;I haven't seen this test failed for a long time. I think it has been fixed. Will close it. ;;;","10/Sep/21 01:06;lake;Hello, as I face this issue currently with Flink 1.13.2 on Amazone Kubernetes Service 1.20.7 (AKS) when starting a flink session (./bin/kubernetes-session.sh -Dkubernetes.cluster-id=my-first-flink-cluster): What was the outcome. How can I solve this issue on my cluster? Thanks

[~jark], [~trohrmann], [~wangyang0918];;;","10/Sep/21 09:08;wangyang0918;[~lake] Could you post your problem and share the JobManager logs in the user ML?;;;","10/Sep/21 15:27;lake;Hello [~wangyang0918], thanks for you reply!

My goal is to run Kafka 2.8.0 and Flink 1.13.2 on Amazone Kubernetes Service 1.20.7 (AKS). AKS seems to run fine and Kafka (which I deployed via Strimzi) can produce and consume message via console. However, I now struggle with staring a Flink session with following command:
{code:java}
./bin/kubernetes-session.sh -Dkubernetes.cluster-id=my-first-flink-cluster{code}
{{Do the logs provide meangingful insights to you?}}

{{BR,}}
 {{Kevin}}

 
----
{{Checking status of pods:}}
{code:java}
kevin@road-condition-vm-flink-client:~$ kubectl get pods
NAME READY STATUS RESTARTS AGE
my-first-flink-clustercd-6d59756c7c-9fb7s 0/1 CrashLoopBackOff 167 14h
road-condition-kafka-kafka-0 1/1 Running 0 11d
road-condition-kafka-zookeeper-0 1/1 Running 0 11d
strimzi-cluster-operator-687fdd6f77-24ccn 1/1 Running 7 11d
{code}
Checking status of services (flink monitoring service is not accessible even both is provided, internal & external IP) – I have masked the external IP in the code below:
{code:java}
kevin@road-condition-vm-flink-client:~$ kubectl get services
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubernetes ClusterIP 10.0.0.1 <none> 443/TCP 31d
my-first-flink-clustercd ClusterIP None <none> 6123/TCP,6124/TCP 14h
my-first-flink-clustercd-rest LoadBalancer 10.0.138.102 20.XX.XX.0 8081:32603/TCP 14h
road-condition-kafka-kafka-bootstrap ClusterIP 10.0.99.147 <none> 9091/TCP,9092/TCP 11d
road-condition-kafka-kafka-brokers ClusterIP None <none> 9090/TCP,9091/TCP,9092/TCP 11d
road-condition-kafka-zookeeper-client ClusterIP 10.0.59.247 <none> 2181/TCP 11d
road-condition-kafka-zookeeper-nodes ClusterIP None <none> 2181/TCP,2888/TCP,3888/TCP 11d{code}
Checking logs of flink session pod:
{code:java}
kevin@road-condition-vm-flink-client:~$ kubectl logs my-first-flink-clustercd-6d59756c7c-9fb7s
sed: couldn't open temporary file /opt/flink/conf/sed0NT8SI: Read-only file system
sed: couldn't open temporary file /opt/flink/conf/sed5rHenH: Read-only file system
/docker-entrypoint.sh: line 73: /opt/flink/conf/flink-conf.yaml: Read-only file system
sed: couldn't open temporary file /opt/flink/conf/sedi357bL: Read-only file system
/docker-entrypoint.sh: line 88: /opt/flink/conf/flink-conf.yaml.tmp: Read-only file system
Starting kubernetes-session as a console application on host my-first-flink-clustercd-6d59756c7c-9fb7s.
2021-09-10 15:14:04,524 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - --------------------------------------------------------------------------------
2021-09-10 15:14:04,527 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Preconfiguration:
2021-09-10 15:14:04,527 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] -

RESOURCE_PARAMS extraction logs:
jvm_params: -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456
dynamic_configs: -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b
logs: INFO [] - Loading configuration property: blob.server.port, 6124
INFO [] - Loading configuration property: taskmanager.memory.process.size, 1728m
INFO [] - Loading configuration property: kubernetes.internal.jobmanager.entrypoint.class, org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint
INFO [] - Loading configuration property: jobmanager.execution.failover-strategy, region
INFO [] - Loading configuration property: jobmanager.rpc.address, my-first-flink-clustercd.default
INFO [] - Loading configuration property: execution.target, kubernetes-session
INFO [] - Loading configuration property: jobmanager.memory.process.size, 1600m
INFO [] - Loading configuration property: jobmanager.rpc.port, 6123
INFO [] - Loading configuration property: kubernetes.cluster-id, my-first-flink-clustercd
INFO [] - Loading configuration property: taskmanager.rpc.port, 6122
INFO [] - Loading configuration property: internal.cluster.execution-mode, NORMAL
INFO [] - Loading configuration property: parallelism.default, 1
INFO [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
INFO [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
INFO [] - Final Master Memory configuration:
INFO [] - Total Process Memory: 1.563gb (1677721600 bytes)
INFO [] - Total Flink Memory: 1.125gb (1207959552 bytes)
INFO [] - JVM Heap: 1024.000mb (1073741824 bytes)
INFO [] - Off-heap: 128.000mb (134217728 bytes)
INFO [] - JVM Metaspace: 256.000mb (268435456 bytes)
INFO [] - JVM Overhead: 192.000mb (201326592 bytes)
2021-09-10 15:14:04,528 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - --------------------------------------------------------------------------------
2021-09-10 15:14:04,528 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Starting KubernetesSessionClusterEntrypoint (Version: 1.13.2, Scala: 2.11, Rev:5f007ff, Date:2021-07-23T04:35:55+02:00)
2021-09-10 15:14:04,529 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - OS current user: flink
2021-09-10 15:14:04,529 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Current Hadoop/Kerberos user: <no hadoop dependency found>
2021-09-10 15:14:04,529 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - JVM: OpenJDK 64-Bit Server VM - Oracle Corporation - 1.8/25.302-b08
2021-09-10 15:14:04,529 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Maximum heap size: 989 MiBytes
2021-09-10 15:14:04,529 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - JAVA_HOME: /usr/local/openjdk-8
2021-09-10 15:14:04,530 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - No Hadoop Dependency available
2021-09-10 15:14:04,530 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - JVM Options:
2021-09-10 15:14:04,530 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Xmx1073741824
2021-09-10 15:14:04,530 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Xms1073741824
2021-09-10 15:14:04,530 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -XX:MaxMetaspaceSize=268435456
2021-09-10 15:14:04,530 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Dlog.file=/opt/flink/log/flink--kubernetes-session-0-my-first-flink-clustercd-6d59756c7c-9fb7s.log
2021-09-10 15:14:04,530 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
2021-09-10 15:14:04,531 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
2021-09-10 15:14:04,531 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
2021-09-10 15:14:04,531 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Program Arguments:
2021-09-10 15:14:04,532 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
2021-09-10 15:14:04,532 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.off-heap.size=134217728b
2021-09-10 15:14:04,532 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
2021-09-10 15:14:04,532 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.jvm-overhead.min=201326592b
2021-09-10 15:14:04,533 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
2021-09-10 15:14:04,533 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.jvm-metaspace.size=268435456b
2021-09-10 15:14:04,533 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
2021-09-10 15:14:04,533 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.heap.size=1073741824b
2021-09-10 15:14:04,533 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
2021-09-10 15:14:04,533 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.jvm-overhead.max=201326592b
2021-09-10 15:14:04,533 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Classpath: /opt/flink/lib/flink-csv-1.13.2.jar:/opt/flink/lib/flink-json-1.13.2.jar:/opt/flink/lib/flink-shaded-zookeeper-3.4.14.jar:/opt/flink/lib/flink-table-blink_2.11-1.13.2.jar:/opt/flink/lib/flink-table_2.11-1.13.2.jar:/opt/flink/lib/log4j-1.2-api-2.12.1.jar:/opt/flink/lib/log4j-api-2.12.1.jar:/opt/flink/lib/log4j-core-2.12.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.12.1.jar:/opt/flink/lib/flink-dist_2.11-1.13.2.jar:::
2021-09-10 15:14:04,534 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - --------------------------------------------------------------------------------
2021-09-10 15:14:04,535 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Registered UNIX signal handlers for [TERM, HUP, INT]
2021-09-10 15:14:04,547 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: blob.server.port, 6124
2021-09-10 15:14:04,547 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2021-09-10 15:14:04,547 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: kubernetes.internal.jobmanager.entrypoint.class, org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint
2021-09-10 15:14:04,548 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2021-09-10 15:14:04,548 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.rpc.address, my-first-flink-clustercd.default
2021-09-10 15:14:04,548 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: execution.target, kubernetes-session
2021-09-10 15:14:04,548 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.memory.process.size, 1600m
2021-09-10 15:14:04,548 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.rpc.port, 6123
2021-09-10 15:14:04,548 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: kubernetes.cluster-id, my-first-flink-clustercd
2021-09-10 15:14:04,549 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: taskmanager.rpc.port, 6122
2021-09-10 15:14:04,549 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: internal.cluster.execution-mode, NORMAL
2021-09-10 15:14:04,549 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: parallelism.default, 1
2021-09-10 15:14:04,549 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2021-09-10 15:14:04,619 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Starting KubernetesSessionClusterEntrypoint.
2021-09-10 15:14:04,659 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Install default filesystem.
2021-09-10 15:14:04,709 INFO org.apache.flink.core.fs.FileSystem [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
2021-09-10 15:14:04,743 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Install security context.
2021-09-10 15:14:04,754 INFO org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
2021-09-10 15:14:04,758 INFO org.apache.flink.runtime.security.modules.JaasModule [] - Jaas file will be created as /tmp/jaas-5459458947931074872.conf.
2021-09-10 15:14:04,810 INFO org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
2021-09-10 15:14:04,811 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Initializing cluster services.
2021-09-10 15:14:04,833 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils [] - Trying to start actor system, external address my-first-flink-clustercd.default:6123, bind address 0.0.0.0:6123.
2021-09-10 15:14:05,635 INFO akka.event.slf4j.Slf4jLogger [] - Slf4jLogger started
2021-09-10 15:14:05,708 INFO akka.remote.Remoting [] - Starting remoting
2021-09-10 15:14:05,921 INFO akka.remote.Remoting [] - Remoting started; listening on addresses :[akka.tcp://flink@my-first-flink-clustercd.default:6123]
2021-09-10 15:14:06,036 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils [] - Actor system started at akka.tcp://flink@my-first-flink-clustercd.default:6123
2021-09-10 15:14:06,116 INFO org.apache.flink.configuration.Configuration [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
2021-09-10 15:14:06,124 INFO org.apache.flink.runtime.blob.BlobServer [] - Created BLOB server storage directory /tmp/blobStore-951adff6-057f-4c4e-a15e-150904156cc0
2021-09-10 15:14:06,131 INFO org.apache.flink.runtime.blob.BlobServer [] - Started BLOB server at 0.0.0.0:6124 - max concurrent requests: 50 - max backlog: 1000
2021-09-10 15:14:06,162 INFO org.apache.flink.runtime.metrics.MetricRegistryImpl [] - No metrics reporter configured, no metrics will be exposed/reported.
2021-09-10 15:14:06,207 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils [] - Trying to start actor system, external address my-first-flink-clustercd.default:0, bind address 0.0.0.0:0.
2021-09-10 15:14:06,244 INFO akka.event.slf4j.Slf4jLogger [] - Slf4jLogger started
2021-09-10 15:14:06,251 INFO akka.remote.Remoting [] - Starting remoting
2021-09-10 15:14:06,313 INFO akka.remote.Remoting [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@my-first-flink-clustercd.default:36611]
2021-09-10 15:14:06,324 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils [] - Actor system started at akka.tcp://flink-metrics@my-first-flink-clustercd.default:36611
2021-09-10 15:14:06,344 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2021-09-10 15:14:06,430 INFO org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStore [] - Initializing FileExecutionGraphInfoStore: Storage directory /tmp/executionGraphStore-388e1bbd-4263-4a3b-a921-4afd2164ab27, expiration time 3600000, maximum cache size 52428800 bytes.
2021-09-10 15:14:06,534 INFO org.apache.flink.configuration.Configuration [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
2021-09-10 15:14:06,535 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Upload directory /tmp/flink-web-820a0acb-c0ad-4bc0-871b-0fb546f21993/flink-web-upload does not exist.
2021-09-10 15:14:06,536 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Created directory /tmp/flink-web-820a0acb-c0ad-4bc0-871b-0fb546f21993/flink-web-upload for file uploads.
2021-09-10 15:14:06,537 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Starting rest endpoint.
2021-09-10 15:14:07,046 INFO org.apache.flink.runtime.webmonitor.WebMonitorUtils [] - Determined location of main cluster component log file: /opt/flink/log/flink--kubernetes-session-0-my-first-flink-clustercd-6d59756c7c-9fb7s.log
2021-09-10 15:14:07,046 INFO org.apache.flink.runtime.webmonitor.WebMonitorUtils [] - Determined location of main cluster component stdout file: /opt/flink/log/flink--kubernetes-session-0-my-first-flink-clustercd-6d59756c7c-9fb7s.out
2021-09-10 15:14:07,278 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Rest endpoint listening at my-first-flink-clustercd.default:8081
2021-09-10 15:14:07,279 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - http://my-first-flink-clustercd.default:8081 was granted leadership with leaderSessionID=00000000-0000-0000-0000-000000000000
2021-09-10 15:14:07,306 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Web frontend listening at http://my-first-flink-clustercd.default:8081.
2021-09-10 15:14:07,330 INFO org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
2021-09-10 15:14:08,109 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: blob.server.port, 6124
2021-09-10 15:14:08,110 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2021-09-10 15:14:08,110 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: kubernetes.internal.jobmanager.entrypoint.class, org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint
2021-09-10 15:14:08,110 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2021-09-10 15:14:08,111 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.rpc.address, my-first-flink-clustercd.default
2021-09-10 15:14:08,112 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: execution.target, kubernetes-session
2021-09-10 15:14:08,113 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.memory.process.size, 1600m
2021-09-10 15:14:08,113 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.rpc.port, 6123
2021-09-10 15:14:08,114 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: kubernetes.cluster-id, my-first-flink-clustercd
2021-09-10 15:14:08,115 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: taskmanager.rpc.port, 6122
2021-09-10 15:14:08,116 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: internal.cluster.execution-mode, NORMAL
2021-09-10 15:14:08,117 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: parallelism.default, 1
2021-09-10 15:14:08,117 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2021-09-10 15:14:08,122 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager at akka://flink/user/rpc/resourcemanager_0 .
2021-09-10 15:14:08,143 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
2021-09-10 15:14:08,148 INFO org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Starting the resource manager.
2021-09-10 15:14:08,208 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
2021-09-10 15:14:08,209 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
2021-09-10 15:14:08,245 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2021-09-10 15:14:09,146 WARN io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager [] - Exec Failure: HTTP 403, Status: 403 - pods is forbidden: User ""system:serviceaccount:default:default"" cannot watch resource ""pods"" in API group """" in the namespace ""default""
java.net.ProtocolException: Expected HTTP 101 response but was '403 Forbidden'
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket.checkResponse(RealWebSocket.java:229) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:196) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.RealCall$AsyncCall.execute(RealCall.java:206) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_302]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_302]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_302]
2021-09-10 15:14:09,207 INFO org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcher [] - The watcher is closing.
2021-09-10 15:14:09,208 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
2021-09-10 15:14:09,209 ERROR org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Fatal error occurred in ResourceManager.
org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not start the ResourceManager akka.tcp://flink@my-first-flink-clustercd.default:6123/user/rpc/resourcemanager_0
 at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:239) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:605) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:180) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Cannot initialize resource provider.
 at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.initialize(ActiveResourceManager.java:156) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.ResourceManager.startResourceManagerServices(ResourceManager.java:251) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 ... 20 more
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: pods is forbidden: User ""system:serviceaccount:default:default"" cannot watch resource ""pods"" in API group """" in the namespace ""default""
 at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onFailure(WatchConnectionManager.java:203) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:198) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.RealCall$AsyncCall.execute(RealCall.java:206) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_302]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_302]
 at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_302]
 Suppressed: java.lang.Throwable: waiting here
 at io.fabric8.kubernetes.client.utils.Utils.waitUntilReady(Utils.java:144) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager.waitUntilReady(WatchConnectionManager.java:341) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:755) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:739) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:70) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.watchPodsAndDoCallback(Fabric8FlinkKubeClient.java:227) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.watchTaskManagerPods(KubernetesResourceManagerDriver.java:331) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.initializeInternal(KubernetesResourceManagerDriver.java:103) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.active.AbstractResourceManagerDriver.initialize(AbstractResourceManagerDriver.java:81) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.initialize(ActiveResourceManager.java:154) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.ResourceManager.startResourceManagerServices(ResourceManager.java:251) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:605) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:180) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.2.jar:1.13.2]
2021-09-10 15:14:09,214 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Fatal error occurred in the cluster entrypoint.
org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not start the ResourceManager akka.tcp://flink@my-first-flink-clustercd.default:6123/user/rpc/resourcemanager_0
 at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:239) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:605) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:180) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Cannot initialize resource provider.
 at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.initialize(ActiveResourceManager.java:156) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.ResourceManager.startResourceManagerServices(ResourceManager.java:251) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 ... 20 more
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: pods is forbidden: User ""system:serviceaccount:default:default"" cannot watch resource ""pods"" in API group """" in the namespace ""default""
 at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onFailure(WatchConnectionManager.java:203) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:198) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.RealCall$AsyncCall.execute(RealCall.java:206) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_302]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_302]
 at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_302]
 Suppressed: java.lang.Throwable: waiting here
 at io.fabric8.kubernetes.client.utils.Utils.waitUntilReady(Utils.java:144) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager.waitUntilReady(WatchConnectionManager.java:341) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:755) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:739) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:70) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.watchPodsAndDoCallback(Fabric8FlinkKubeClient.java:227) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.watchTaskManagerPods(KubernetesResourceManagerDriver.java:331) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.initializeInternal(KubernetesResourceManagerDriver.java:103) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.active.AbstractResourceManagerDriver.initialize(AbstractResourceManagerDriver.java:81) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.initialize(ActiveResourceManager.java:154) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.ResourceManager.startResourceManagerServices(ResourceManager.java:251) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:605) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:180) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.2.jar:1.13.2]
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.2.jar:1.13.2]
Exception in thread ""OkHttp Dispatcher"" java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@393ed1 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1fd81363[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
 at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
 at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
 at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326)
 at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
 at java.util.concurrent.ScheduledThreadPoolExecutor.submit(ScheduledThreadPoolExecutor.java:632)
 at java.util.concurrent.Executors$DelegatedExecutorService.submit(Executors.java:678)
 at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager.scheduleReconnect(WatchConnectionManager.java:305)
 at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager.access$800(WatchConnectionManager.java:50)
 at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onFailure(WatchConnectionManager.java:218)
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571)
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:198)
 at org.apache.flink.kubernetes.shaded.okhttp3.RealCall$AsyncCall.execute(RealCall.java:206)
 at org.apache.flink.kubernetes.shaded.okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2021-09-10 15:14:09,227 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting KubernetesSessionClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2021-09-10 15:14:09,229 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Shutting down rest endpoint.
2021-09-10 15:14:09,229 INFO org.apache.flink.runtime.blob.BlobServer [] - Stopped BLOB server at 0.0.0.0:6124
{code}
 
{code:java}
kevin@road-condition-vm-flink-client:~$ kubectl get po my-first-flink-clustercd-6d59756c7c-9fb7s -o yaml
apiVersion: v1
kind: Pod
metadata:
 creationTimestamp: ""2021-09-10T00:45:02Z""
 generateName: my-first-flink-clustercd-6d59756c7c-
 labels:
 app: my-first-flink-clustercd
 component: jobmanager
 pod-template-hash: 6d59756c7c
 type: flink-native-kubernetes
 name: my-first-flink-clustercd-6d59756c7c-9fb7s
 namespace: default
 ownerReferences:
 - apiVersion: apps/v1
 blockOwnerDeletion: true
 controller: true
 kind: ReplicaSet
 name: my-first-flink-clustercd-6d59756c7c
 uid: d500b8e0-df0d-4544-b219-b36c83a3c1a5
 resourceVersion: ""3569128""
 uid: c4fb95a0-b4c8-4dd5-828b-7f0f9077c09f
spec:
 containers:
 - args:
 - bash
 - -c
 - kubernetes-jobmanager.sh kubernetes-session
 command:
 - /docker-entrypoint.sh
 env:
 - name: _POD_IP_ADDRESS
 valueFrom:
 fieldRef:
 apiVersion: v1
 fieldPath: status.podIP
 image: apache/flink:1.13.2-scala_2.11
 imagePullPolicy: IfNotPresent
 name: flink-main-container
 ports:
 - containerPort: 8081
 name: rest
 protocol: TCP
 - containerPort: 6123
 name: jobmanager-rpc
 protocol: TCP
 - containerPort: 6124
 name: blobserver
 protocol: TCP
 resources:
 limits:
 cpu: ""1""
 memory: 1600Mi
 requests:
 cpu: ""1""
 memory: 1600Mi
 terminationMessagePath: /dev/termination-log
 terminationMessagePolicy: File
 volumeMounts:
 - mountPath: /opt/flink/conf
 name: flink-config-volume
 - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
 name: default-token-5crsm
 readOnly: true
 dnsPolicy: ClusterFirst
 enableServiceLinks: true
 nodeName: aks-agentpool-43117142-vmss000002
 preemptionPolicy: PreemptLowerPriority
 priority: 0
 restartPolicy: Always
 schedulerName: default-scheduler
 securityContext: {}
 serviceAccount: default
 serviceAccountName: default
 terminationGracePeriodSeconds: 30
 tolerations:
 - effect: NoExecute
 key: node.kubernetes.io/not-ready
 operator: Exists
 tolerationSeconds: 300
 - effect: NoExecute
 key: node.kubernetes.io/unreachable
 operator: Exists
 tolerationSeconds: 300
 - effect: NoSchedule
 key: node.kubernetes.io/memory-pressure
 operator: Exists
 volumes:
 - configMap:
 defaultMode: 420
 items:
 - key: logback-console.xml
 path: logback-console.xml
 - key: log4j-console.properties
 path: log4j-console.properties
 - key: flink-conf.yaml
 path: flink-conf.yaml
 name: flink-config-my-first-flink-clustercd
 name: flink-config-volume
 - name: default-token-5crsm
 secret:
 defaultMode: 420
 secretName: default-token-5crsm
status:
 conditions:
 - lastProbeTime: null
 lastTransitionTime: ""2021-09-10T00:47:00Z""
 status: ""True""
 type: Initialized
 - lastProbeTime: null
 lastTransitionTime: ""2021-09-10T15:19:31Z""
 message: 'containers with unready status: [flink-main-container]'
 reason: ContainersNotReady
 status: ""False""
 type: Ready
 - lastProbeTime: null
 lastTransitionTime: ""2021-09-10T15:19:31Z""
 message: 'containers with unready status: [flink-main-container]'
 reason: ContainersNotReady
 status: ""False""
 type: ContainersReady
 - lastProbeTime: null
 lastTransitionTime: ""2021-09-10T00:47:00Z""
 status: ""True""
 type: PodScheduled
 containerStatuses:
 - containerID: containerd://005e86da1e020dec3313103c1231bb916921545fd7b091007e7ef412d88917cd
 image: docker.io/apache/flink:1.13.2-scala_2.11
 imageID: docker.io/apache/flink@sha256:2da83bf5f7437769ba1f04caed217d20bb02494a018162bfc6b2467e1914ad77
 lastState:
 terminated:
 containerID: containerd://005e86da1e020dec3313103c1231bb916921545fd7b091007e7ef412d88917cd
 exitCode: 239
 finishedAt: ""2021-09-10T15:19:30Z""
 reason: Error
 startedAt: ""2021-09-10T15:19:17Z""
 name: flink-main-container
 ready: false
 restartCount: 168
 started: false
 state:
 waiting:
 message: back-off 5m0s restarting failed container=flink-main-container pod=my-first-flink-clustercd-6d59756c7c-9fb7s_default(c4fb95a0-b4c8-4dd5-828b-7f0f9077c09f)
 reason: CrashLoopBackOff
 hostIP: 10.240.0.5
 phase: Running
 podIP: 10.244.2.3
 podIPs:
 - ip: 10.244.2.3
 qosClass: Guaranteed
 startTime: ""2021-09-10T00:47:00Z""{code};;;","01/Oct/21 11:08;trohrmann;[~lake], the problem seems to be {{pods is forbidden: User ""system:serviceaccount:default:default"" cannot watch resource ""pods"" in API group """" in the namespace ""default""}}. I think you need to give the default service account watch rights in the default namespace.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RequestReplyFunction recovery fails with a remote SDK,FLINK-21642,13362803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,igal,igal,06/Mar/21 20:28,10/Mar/21 06:46,13/Jul/23 08:07,10/Mar/21 06:46,,,,,,,,,statefun-3.0.0,,,,,,,Stateful Functions,,,,,0,pull-request-available,,,,,"While extending our smoke e2e test to use the remote SDKS I've stumbled upon a bug in the RequestReplyFunction. We get a unknown state exception after recovery.

The exact scenario that trigger that bug is:
 # There was  request in flight.
 # A  failure occurs that causes the job to restart.
 # On restore, we start with no managed state
 # But we try to re-send to the SDK exactly the same ToFunction message.
 # That ToFunction contains state definitions from the previous attempt. (before the failure)
 # The SDK processes this message normally (it has all the state definitions that it knows)
 # The SDK responds with a state mutation.
 # The PersistedRemoteFunctionValues fails with unknown state. 

 

We need to treat the ToFunction messages as a retryBatch, instead of sending it as-is.

 ",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 10 06:46:31 UTC 2021,,,,,,,,,,"0|z0odjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/21 06:46;tzulitai;flink-statefun/master: d46a4511ecdc8ad6bf16d977b51d3ced85f403b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When you insert multiple inserts with statementSet, you modify multiple inserts with OPTIONS('table-name '=' XXX '), but only the first one takes effect",FLINK-21627,13362522,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,apache22@163.com,apache22@163.com,05/Mar/21 07:47,28/May/21 09:10,13/Jul/23 08:07,18/Apr/21 05:42,1.12.0,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"{code:java}
//代码占位符
StatementSet statementSet = tableEnvironment.createStatementSet();
String sql1 = ""insert into test select a,b,c from test_a_12342 /*+
OPTIONS('table-name'='test_a_1')*/"";
String sql2 = ""insert into test select a,b,c from test_a_12342 /*+
OPTIONS('table-name'='test_a_2')*/"";
statementSet.addInsertSql(sql1);
statementSet.addInsertSql(sql2);
statementSet.execute();
{code}
Sql code as above, in the final after the insert is put test_a_1 table data into the two times, and test_a_2 data did not insert, is excuse me this bug",,apache22@163.com,godfreyhe,jark,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 18 05:42:14 UTC 2021,,,,,,,,,,"0|z0obtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/21 07:53;jark;cc [~godfreyhe];;;","11/Mar/21 01:00;yr;[~apache22@163.com] what is the connector?;;;","12/Apr/21 04:54;godfreyhe;[~apache22@163.com] thanks for reporting this, The reason is the digest of TableScan nodes do not contain table hints, and the planner will reuse two TableScans if their digest are same. I have created a pr to fix it.
btw, please check whether {{table.dynamic-table-options.enabled}} is true, which default value is false.;;;","18/Apr/21 05:42;godfreyhe;Fixed in 1.13.0: c6147f2df2e81c1b1db6aa43b3a827a8c4148af1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5.03.2021 Benchmarks are not compiling,FLINK-21625,13362512,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Thesharing,pnowojski,pnowojski,05/Mar/21 06:31,05/Mar/21 09:00,13/Jul/23 08:07,05/Mar/21 09:00,1.13.0,,,,,,,,1.13.0,,,,,,,Benchmarks,Runtime / Coordination,,,,0,pull-request-available,,,,,"http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/7531/console


{code:java}
[INFO] Compiling 1 Scala source and 63 Java sources to /Users/pnowojski/flink-benchmark/target/classes ...
[ERROR] /Users/pnowojski/flink-benchmark/src/main/java/org/apache/flink/benchmark/StreamGraphUtils.java:22:40:  error: cannot find symbol
[ERROR] /Users/pnowojski/flink-benchmark/src/main/java/org/apache/flink/scheduler/benchmark/JobConfiguration.java:24:40:  error: cannot find symbol
[ERROR] /Users/pnowojski/flink-benchmark/src/main/java/org/apache/flink/scheduler/benchmark/JobConfiguration.java:45:15:  error: cannot find symbol
[ERROR] /Users/pnowojski/flink-benchmark/src/main/java/org/apache/flink/scheduler/benchmark/JobConfiguration.java:51:3:  error: cannot find symbol
[ERROR] /Users/pnowojski/flink-benchmark/src/main/java/org/apache/flink/scheduler/benchmark/JobConfiguration.java:71:8:  error: cannot find symbol
[ERROR] /Users/pnowojski/flink-benchmark/src/main/java/org/apache/flink/scheduler/benchmark/topology/BuildExecutionGraphBenchmark.java:22:46:  error: cannot find symbol
{code}
",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21347,FLINK-21580,,,,,,,,,,,,,,,,,,,,FLINK-21524,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 05 09:00:31 UTC 2021,,,,,,,,,,"0|z0obr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/21 09:00;pnowojski;merged commit d0ec358 into flink-benchmarks:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Japicmp fails with ""Could not resolve org.apache.flink:flink-core:jar:1.12.0""",FLINK-21611,13362385,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dwysakowicz,dwysakowicz,04/Mar/21 18:41,05/Mar/21 07:27,13/Jul/23 08:07,05/Mar/21 07:27,1.13.0,,,,,,,,1.13.0,,,,,,,Build System,,,,,0,,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14139&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=9b1a0f88-517b-5893-fc93-76f4670982b4

{code}
[ERROR] Failed to execute goal com.github.siom79.japicmp:japicmp-maven-plugin:0.11.0:cmp (default) on project flink-core: Could not resolve org.apache.flink:flink-core:jar:1.12.0 -> [Help 1]
{code}",,dwysakowicz,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21571,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 05 07:27:36 UTC 2021,,,,,,,,,,"0|z0oayw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/21 18:41;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14142&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=9b1a0f88-517b-5893-fc93-76f4670982b4;;;","04/Mar/21 18:42;dwysakowicz;Hey [~chesnay] could you check if it is related to FLINK-21571 ? Thank you!;;;","04/Mar/21 19:29;chesnay;Well, now that doesn't make any sense. Let's observe it a bit more; in that same linked run it worked fine in the compile stage.

On a tangential note, we shouldn't be running those checks in the e2e stage.;;;","04/Mar/21 19:30;chesnay;It is rather curious that in both instances it failed in the e2e stage...;;;","05/Mar/21 00:57;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14147&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=9b1a0f88-517b-5893-fc93-76f4670982b4
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14151&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=9b1a0f88-517b-5893-fc93-76f4670982b4;;;","05/Mar/21 06:57;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14155&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=9b1a0f88-517b-5893-fc93-76f4670982b4;;;","05/Mar/21 06:58;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14162&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=9b1a0f88-517b-5893-fc93-76f4670982b4;;;","05/Mar/21 07:22;chesnay;I'll revert the change for the time being, but we should figure out why it so persistently only occurs in the e2e stage.;;;","05/Mar/21 07:27;chesnay;master: 7738ea8ed45edf9ec7525aff05784fa6ed7f0845;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProcessFailureCancelingITCase failed on azure,FLINK-21610,13362381,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dwysakowicz,dwysakowicz,04/Mar/21 18:36,17/Mar/21 11:51,13/Jul/23 08:07,17/Mar/21 11:51,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14115&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

{code}
2021-03-04T11:38:44.9321152Z [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 16.463 s <<< FAILURE! - in org.apache.flink.test.recovery.ProcessFailureCancelingITCase
2021-03-04T11:38:44.9323887Z [ERROR] testCancelingOnProcessFailure(org.apache.flink.test.recovery.ProcessFailureCancelingITCase)  Time elapsed: 15.692 s  <<< ERROR!
2021-03-04T11:38:44.9334234Z java.lang.NullPointerException
2021-03-04T11:38:44.9335222Z 	at org.apache.flink.test.recovery.ProcessFailureCancelingITCase.testCancelingOnProcessFailure(ProcessFailureCancelingITCase.java:247)
2021-03-04T11:38:44.9335994Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-04T11:38:44.9336640Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-04T11:38:44.9337423Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-04T11:38:44.9341553Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-04T11:38:44.9342587Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-03-04T11:38:44.9343392Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-03-04T11:38:44.9343896Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-03-04T11:38:44.9344396Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-03-04T11:38:44.9345092Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-04T11:38:44.9345529Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-04T11:38:44.9345954Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-04T11:38:44.9346374Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-03-04T11:38:44.9346896Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-03-04T11:38:44.9347636Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-04T11:38:44.9350380Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-03-04T11:38:44.9351222Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-03-04T11:38:44.9352002Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-03-04T11:38:44.9353195Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-04T11:38:44.9353879Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-04T11:38:44.9354538Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-04T11:38:44.9355220Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-04T11:38:44.9355890Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-04T11:38:44.9356548Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-04T11:38:44.9357252Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-03-04T11:38:44.9358062Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-03-04T11:38:44.9358869Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-03-04T11:38:44.9359742Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-03-04T11:38:44.9360593Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-03-04T11:38:44.9361416Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-03-04T11:38:44.9362156Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-03-04T11:38:44.9362977Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 17 11:51:14 UTC 2021,,,,,,,,,,"0|z0oay0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/21 11:51;chesnay;master: 8bdb286bc6838025fe96227310c2f5651093156c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleRecoveryITCaseBase.testRestartMultipleTimes fails on azure,FLINK-21609,13362378,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,dwysakowicz,dwysakowicz,04/Mar/21 18:34,28/Aug/21 11:11,13/Jul/23 08:07,01/Apr/21 07:47,1.13.0,,,,,,,,1.11.4,1.12.3,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14115&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

{code}
2021-03-04T11:39:35.8958609Z [ERROR] testRestartMultipleTimes(org.apache.flink.test.recovery.SimpleRecoveryExponentialDelayRestartStrategyITBase)  Time elapsed: 1.753 s  <<< FAILURE!
2021-03-04T11:39:35.8959196Z java.lang.AssertionError: expected:<55> but was:<143>
2021-03-04T11:39:35.8959667Z 	at org.junit.Assert.fail(Assert.java:88)
2021-03-04T11:39:35.8959989Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2021-03-04T11:39:35.8962924Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2021-03-04T11:39:35.8964175Z 	at org.junit.Assert.assertEquals(Assert.java:631)
2021-03-04T11:39:35.8964980Z 	at org.apache.flink.test.recovery.SimpleRecoveryITCaseBase.testRestartMultipleTimes(SimpleRecoveryITCaseBase.java:165)
2021-03-04T11:39:35.8965750Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-04T11:39:35.8966503Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-04T11:39:35.8967221Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-04T11:39:35.8967902Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-04T11:39:35.8968526Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-03-04T11:39:35.8969842Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-03-04T11:39:35.8970492Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-03-04T11:39:35.8971209Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-03-04T11:39:35.8971878Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-03-04T11:39:35.8972603Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-03-04T11:39:35.8973268Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-03-04T11:39:35.8973874Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-04T11:39:35.8974416Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-04T11:39:35.8987184Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-04T11:39:35.8987899Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-04T11:39:35.8988531Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-04T11:39:35.8989129Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-04T11:39:35.8989801Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-04T11:39:35.8990307Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-04T11:39:35.8991100Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-03-04T11:39:35.8991907Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-03-04T11:39:35.8993251Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-03-04T11:39:35.8993978Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-03-04T11:39:35.8994705Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-03-04T11:39:35.8995465Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-03-04T11:39:35.8996117Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-03-04T11:39:35.8996759Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21729,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21993,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 07:47:08 UTC 2021,,,,,,,,,,"0|z0oaxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/21 18:34;dwysakowicz;I was not sure if it is the right component. Please correct it if I was wrong.;;;","26/Mar/21 08:24;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15504&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9693;;;","26/Mar/21 10:18;trohrmann;The problem seems to be that the reduce operator sometimes sends some results to the {{LocalCollectionOutputFormat}} even though one of the combiners fails. This smells like a bug in Flink.;;;","26/Mar/21 11:18;trohrmann;Additionally, the {{LocalCollectionOutputFormat}} simply records all values it receives independent of failovers. I think this is wrong.;;;","01/Apr/21 07:47;trohrmann;Fixed via

1.13.0: d5ac263d6c176951a50c7c1cd4c2decb996f0f6f
1.12.3: 90e4b2b6f39793cd95e59630cea9ef282054b418
1.11.4: efd694af18383e6efb9415a79a38b7db8c7fb90f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManager connected to invalid JobManager leading to TaskSubmissionException,FLINK-21606,13362328,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,rmetzger,rmetzger,04/Mar/21 15:09,15/Mar/21 09:51,13/Jul/23 08:07,12/Mar/21 18:27,1.13.0,,,,,,,,1.11.4,1.12.3,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"While testing reactive mode, I had to start my JobManager a few times to get the configuration right. While doing that, I had at least on TaskManager (TM6), which was first connected to the first JobManager (with a running job), and then to the second one.

On the second JobManager, I was able to execute my test job (on another TaskManager (TMx)), once TM6 reconnected, and reactive mode tried to utilize all available resources, I repeatedly ran into this issue:

{code}
2021-03-04 15:49:36,322 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (5/7) (ae8f39c8dd88148aff93c8f811fab22e) switched from DEPLOYING to FAILED on 192.168.2.173:64041-4f7521 @ macbook-pro-2.localdomain (dataPort=64044).
java.util.concurrent.CompletionException: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: Could not submit task because there is no JobManager associated for the job bbe8634736b5b1d813dd322cfaaa08ea.
	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_252]
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_252]
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1064) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:263) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:261) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:101) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:999) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:458) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: Could not submit task because there is no JobManager associated for the job bbe8634736b5b1d813dd322cfaaa08ea.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$submitTask$3(TaskExecutor.java:523) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.Optional.orElseThrow(Optional.java:290) ~[?:1.8.0_252]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:514) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	... 9 more
{code}

I will upload all logs to this ticket and post my initial analysis.
",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10407,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/21 15:10;rmetzger;FLINK-21606-logs.tgz;https://issues.apache.org/jira/secure/attachment/13021589/FLINK-21606-logs.tgz",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 12 18:27:58 UTC 2021,,,,,,,,,,"0|z0oam8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/21 15:24;rmetzger;I assume the issue is the following:

Job 1 (bf6a3e21d2f0aa92b9ee522d816380aa) gets submitted on JobManager 1 (flink-robert-standalonejob-2-MacBook-Pro-2.localdomain.log.1) and executes with a parallelism of 1 only on TaskManager 6 (flink-robert-taskexecutor-6-MacBook-Pro-2.localdomain.log).

Then JobManager 1 gets killed, TaskManager 6 stops executing the respective tasks.

Job 2 (bbe8634736b5b1d813dd322cfaaa08ea) gets submitted on JobManager 2 (flink-robert-standalonejob-2-MacBook-Pro-2.localdomain.log) and executes with a parallelism 1 on a TaskManager.

TaskManager 6 connects to JobManager 2, even though it doesn't execute bf6a3e21d2f0aa92b9ee522d816380aa.
{code}
2021-03-04 15:49:04,129 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job bf6a3e21d2f0aa92b9ee522d816380aa.
{code}

Probably TaskManager 6 assumes everything is fine (happily heartbeating with the wrong JobManager).

I will investigate this as soon as I have time, but I'm also happy if somebody else takes a look ;) 
;;;","05/Mar/21 16:16;trohrmann;I think the problem is that Flink does not support {{JobManager}} failovers w/o HA. So what happens is that you start a new {{JobManager}} process for a new job under the same address of the first instance of the {{JobManager}}. That's why the {{TaskManager}} can connect to the second {{JobManager}} assuming that it is still responsible for the old job. Since we don't check that the {{JobManager}} is responsible for a given job when connecting to it, the {{TaskManager}} will simply offer its slots but does not accept any task submissions.

I think what we can do to prevent this situation is to send the expected {{JobID}} together with the registration request from the {{TaskExecutor}} to the {{JobMaster}}. If the {{JobMaster}} is not responsible for the specified job, then the {{TaskExecutor}} should free all slots which have been allocated for this job.;;;","12/Mar/21 18:27;trohrmann;Fixed via

1.13.0:
ab2f89940d 
42b94df19d 
0bd837c4fd 
691f87e00f
b24c5e6768

1.12.3:
76fdf33da7 
b025ed27b9 
3f97d0e988 
685baca1bf 
2bcc56ef04

1.11.4:
7edd724e13
2f094da996
db0a5f5b64
969fda2525
172fdfed4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed fail,FLINK-21596,13362233,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,maguowei,maguowei,04/Mar/21 06:57,16/Mar/21 17:18,13/Jul/23 08:07,16/Mar/21 17:18,1.12.2,1.13.0,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107

{code:java}
[ERROR] testAsyncCheckpointFailureTriggerJobFailed(org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase)  Time elapsed: 38.623 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:62)
	at org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed(CheckpointFailureManagerITCase.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
",,dwysakowicz,gaoyunhaii,maguowei,mapohl,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 17:18:24 UTC 2021,,,,,,,,,,"0|z0oa14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/21 09:18;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14214&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","10/Mar/21 13:18;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107;;;","15/Mar/21 14:56;dwysakowicz;I think the problem is that the timeout is too aggressive for azure. TaskManager cannot start in time to take a checkpoint and perform the test. You can see in the failed logs that very close to the actual timeout the jobs switches to {{RUNNING}}:

{code}
*01:14:14,988 [Source: Custom Source -> Sink: Unnamed (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (1/1)#0 (8bfcc29007eca8cef72ff256cd6ff37a) switched from DEPLOYING to RUNNING.
01:14:15,027 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (8bfcc29007eca8cef72ff256cd6ff37a) switched from DEPLOYING to RUNNING.*
01:14:15,186 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1614820455048 for job 470600686ea2d957e6a81620d925566e.
01:14:15,858 [                main] ERROR org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase [] - 
--------------------------------------------------------------------------------
Test testAsyncCheckpointFailureTriggerJobFailed(org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase) failed with:
org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
        at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
        at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
        at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:62)
        at org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed(CheckpointFailureManagerITCase.java:103)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.lang.Thread.run(Thread.java:748)

{code}

I will increase the timeout for the test and slightly the checkpointing interval.;;;","16/Mar/21 17:18;dwysakowicz;Increased the time out in:
* master
** 7adeacd3e3f61d5bbfc3e10551a5cb126bad19de
* 1.12.3
** 1fafa1a8123c3e06a1a1198cdb4ba18f19d59c2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoveSingleAggregateRule fails due to nullability mismatch,FLINK-21592,13362217,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,04/Mar/21 06:18,28/May/21 09:06,13/Jul/23 08:07,14/Apr/21 09:52,1.12.2,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The test case to reproduce the issue:
{code}
    @Test
    public void test() throws Exception {
        tableEnv.executeSql(""create table foo(x int,y int)"");
        tableEnv.executeSql(""create table bar(i int,s string)"");
        System.out.println(tableEnv.explainSql(""select (select count(x)-1 from foo where foo.y=bar.i) from bar""));
    }
{code}
Error stack trace is:
{noformat}
java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(BIGINT NOT NULL $f0) NOT NULL
expression type is RecordType(BIGINT $f0) NOT NULL
set is rel#94:LogicalAggregate.NONE.any.[](input=HepRelVertex#93,group={},agg#0=SINGLE_VALUE($0))
expression is LogicalProject($f0=[CAST(-($0, 1)):BIGINT])
  LogicalAggregate(group=[{}], agg#0=[COUNT($0)])
    LogicalProject(x=[$0])
      LogicalFilter(condition=[=($1, $cor0.i)])
        LogicalTableScan(table=[[test-catalog, default, foo]])


	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:58)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.calcite.sql2rel.RelDecorrelator$RemoveSingleAggregateRule.onMatch(RelDecorrelator.java:1881)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.calcite.sql2rel.RelDecorrelator.removeCorrelationViaRule(RelDecorrelator.java:346)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateQuery(RelDecorrelator.java:192)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateQuery(RelDecorrelator.java:169)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.optimize(FlinkDecorrelateProgram.scala:41)
{noformat}",,jark,libenchao,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 09:52:11 UTC 2021,,,,,,,,,,"0|z0o9xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/21 06:29;lirui;I think the root cause is because {{RemoveSingleAggregateRule::onMatch}} assumes {{SqlSingleValueAggFunction}} 
 returns a [nullable|https://github.com/apache/flink/blob/release-1.12.2/flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql2rel/RelDecorrelator.java#L1872] value, which is wrong because {{SqlSingleValueAggFunction}}'s return type inference is {{ReturnTypes.ARG0}} and doesn't guarantee nullability.;;;","02/Apr/21 06:07;ykt836;[~godfreyhe] could you help to review?;;;","14/Apr/21 09:52;ykt836;fixed: b22bc62ae59d3ccaef95507897c7725970e4e5c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManagerTest.testNotificationAboutNotEnoughResources is unstable,FLINK-21587,13362126,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,guoyangze,rmetzger,rmetzger,03/Mar/21 15:04,31/May/21 08:04,13/Jul/23 08:07,04/Mar/21 09:15,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"Happened in my WIP branch, but most likely unrelated to my change: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8925&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b

Also note that the error is reproducable locally with DEBUG log level, but not with INFO:
{code}
[ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.169 s <<< FAILURE! - in org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest
[ERROR] testNotificationAboutNotEnoughResources(org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest)  Time elapsed: 0.029 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: a collection with size <1>
     but: collection size was <0>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest$9.lambda$new$5(FineGrainedSlotManagerTest.java:548)
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase$Context.runTest(FineGrainedSlotManagerTestBase.java:197)
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest$9.<init>(FineGrainedSlotManagerTest.java:521)
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testNotificationAboutNotEnoughResources(FineGrainedSlotManagerTest.java:507)
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testNotificationAboutNotEnoughResources(FineGrainedSlotManagerTest.java:493)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}
",,guoyangze,maguowei,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21718,FLINK-21779,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 09:15:04 UTC 2021,,,,,,,,,,"0|z0o9dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/21 02:09;guoyangze;Thanks for the report [~rmetzger]. I think this issue might come from the following snippet:
{code:java}
resourceActionsBuilder.setNotEnoughResourcesConsumer(
                        (jobId1, acquiredResources) -> {
                            notifyNotEnoughResourceFuture.complete(null);
                            notEnoughResourceNotifications.add(
                                    Tuple2.of(jobId1, acquiredResources));
                        });
{code}
The {{notifyNotEnoughResourceFuture}} should be completed after the notification added to the list.

I've looped the test 10,000 times to verify it.

;;;","04/Mar/21 06:48;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392;;;","04/Mar/21 09:15;xtsong;Fixed via
* master (1.13): 2794f34c37d666753060d8c9896729a742c88f4e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COALESCE not works when cast a variable return null,FLINK-21582,13362066,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,Jerry4free,Jerry4free,03/Mar/21 10:00,22/Mar/22 16:05,13/Jul/23 08:07,22/Mar/22 16:05,1.11.1,,,,,,,,1.15.0,,,,,,,Table SQL / API,,,,,1,auto-unassigned,,,,,"select COALESCE(cast('aa' as int), 0);return NULL 

select COALESCE(NULL, 0); return 0 

The first case works failed, but the second case works successful

 ","Flink1.11.1

 ",gkgkgk,godfreyhe,jark,Jerry4free,libenchao,martijnvisser,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/21 11:48;Jerry4free;image-2021-03-03-19-48-21-306.png;https://issues.apache.org/jira/secure/attachment/13021509/image-2021-03-03-19-48-21-306.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 22 16:05:39 UTC 2022,,,,,,,,,,"0|z0o900:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/21 10:21;twalthr;[~Jerry4free] could you post the full stack trace? It could help to identify the problem quickly.;;;","03/Mar/21 11:48;Jerry4free;There is no exception in sql-client command line. And there is no ERROR log in sql-client.log, taskexecutor-0.log, standalonesession-0.log, 

I run sql-client.sh in embedded mode.

In IDE, it also not works:

!image-2021-03-03-19-48-21-306.png!;;;","03/Mar/21 14:19;jark;I tested in SQL CLI v1.12.1 and got the same result:

{code}
Flink SQL> select COALESCE(cast('aa' as int), 0);
+-----+-------------+
| +/- |      EXPR$0 |
+-----+-------------+
|   + |      (NULL) |
+-----+-------------+
Received a total of 1 rows

Flink SQL> select COALESCE(NULL, 0);
+-----+-------------+
| +/- |      EXPR$0 |
+-----+-------------+
|   + |           0 |
+-----+-------------+
Received a total of 1 rows

{code};;;","05/Mar/21 08:40;yr;[~twalthr] [~jark]

I debug and found the coalesce function is parsed to *case-when*,because the sqlType's nullable of CAST REXCALL is false

,so the case-when result is *cast('aa' as int) not null*,however the CAST ExprCodeGen resultTerm is null.

I think if the nullable of cast('aa' as int)  can be true.
 ;;;","07/Mar/21 14:03;jark;I think we know the root cause, but it's hard to fix. Because all the literals are not-null. And Calcite think the return value of operationns on literal are also not-null. 

We may need to fix {{org.apache.calcite.sql.fun.SqlCastFunction#inferReturnType}} if the operand is literal and the cast-to-type is illegal. However, how many other function do we need to fix? And this rule may change in the future if we support implicit type conversion (FLIP-154 [1]). That why I said it may not easy to fix. 


[1]: https://cwiki.apache.org/confluence/display/FLINK/FLIP-154%3A+SQL+Implicit+Type+Coercion;;;","08/Mar/21 01:56;yr;so we need require more planning,and do it  in the future?

if it is possible ,I would like to contribute to the imporvement:D;;;","09/Mar/21 10:42;twalthr;[~jark] I would rather say that the runtime code of CAST is wrong in this case. It should throw an exception instead of returning NULL. Because of this wrong behavior there is a mismatch between logical type and runtime result. I don't think that fixing {{SqlCastFunction#inferReturnType}} helps because you would need to evaluate the function in this method.;;;","09/Mar/21 11:54;jark;Hi [~twalthr], I agree throwing exception is a more standard compliant behavior. However, it is on purpose to return NULL in Flink SQL to avoid runtime exception for a long running job. There are also many other functions returning NULL when processing invalid input. ;;;","09/Mar/21 11:57;twalthr;I understand the reasoning behind it but this arbitrary behavior has consequences that we should address in the mid term. In theory, the behavior should be configurable. And based on this config flag all type inferences would behave differently. Or even better: We modify the plan to wrap an nullable expression around it.;;;","16/Apr/21 10:45;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:46;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","24/Sep/21 13:09;slinkydeveloper;I tried the query in the issue description with the patch from https://issues.apache.org/jira/browse/FLINK-23385 applied, and it still fails because the cast function return value nullability is defined as follows:
{code:java}
.outputTypeStrategy(
        nullableIfArgs(ConstantArgumentCount.to(0), TypeStrategies.argument(1)))
{code}

while it should probably be forceNullable for the way the cast function behaves now. Still changing to forceNullable doesn't work, I'm investigating on it.

On a side note, shouldn't cast raise an exception more than returning NULL? Perhaps should we consider changing this behaviour and introduce TRY_CAST?;;;","27/Sep/21 08:23;slinkydeveloper;Links to https://issues.apache.org/jira/browse/FLINK-24385;;;","22/Mar/22 16:05;slinkydeveloper;I think this can be closed, as it has been resolved with https://issues.apache.org/jira/browse/FLINK-24385;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SimpleType.simpleTypeFrom(...) complains with ""Collection is empty""",FLINK-21577,13362023,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,03/Mar/21 07:04,10/Mar/21 06:46,13/Jul/23 08:07,10/Mar/21 06:46,,,,,,,,,statefun-3.0.0,,,,,,,Stateful Functions,,,,,0,pull-request-available,,,,,"This is caused by the {{EnumSet.copyOf}} method call at:
https://github.com/apache/flink-statefun/blob/master/statefun-sdk-java/src/main/java/org/apache/flink/statefun/sdk/java/types/SimpleType.java#L57

That expects the collection to be non-empty.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 10 06:46:50 UTC 2021,,,,,,,,,,"0|z0o8qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/21 06:46;tzulitai;flink-statefun/master: 76858f526b58b13fe178422006c5220e65c93741;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowDistinctAggregateITCase.testCumulateWindow_GroupingSets unstable,FLINK-21574,13362011,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,maguowei,maguowei,03/Mar/21 06:10,01/Apr/21 15:10,13/Jul/23 08:07,01/Apr/21 15:10,1.13.0,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14014&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53
{code:java}
[ERROR]   WindowDistinctAggregateITCase.testCumulateWindow_GroupingSets:660 expected:<...:10,7,21.09,6.0,1.0,[4
1,null,2020-10-10T00:00,2020-10-10T00:00:15,7,21.09,6.0,1.0,4]
1,null,2020-10-10T0...> but was:<...:10,7,21.09,6.0,1.0,[5
1,null,2020-10-10T00:00,2020-10-10T00:00:15,7,21.09,6.0,1.0,5]
1,null,2020-10-10T0...>

{code}
",,jark,maguowei,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 15:10:57 UTC 2021,,,,,,,,,,"0|z0o8ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/21 09:00;roman;A similar failure:
{code}
2021-03-09T21:32:46.4286014Z [ERROR] Failures:
 2021-03-09T21:32:46.4286413Z [ERROR] WindowDistinctAggregateITCase.testHopWindow_Rollup:584 expected <...:10,7,21.09,6.0,1.0,[4]
 2021-03-09T21:32:46.4287592Z 1,null,2020-10-10T0...> but was:<...:10,7,21.09,6.0,1.0,[5]
 2021-03-09T21:32:46.4289629Z 1,null,2020-10-10T0...>
 2021-03-09T21:32:46.4289839Z [INFO]
 2021-03-09T21:32:46.4290145Z [ERROR] Tests run: 2992, Failures: 1, Errors: 0, Skipped: 22
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14361&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29;;;","01/Apr/21 15:10;jark;Has been fixed by FLINK-21553.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Japicmp reference version does not exist,FLINK-21571,13361971,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,02/Mar/21 23:37,04/Mar/21 18:41,13/Jul/23 08:07,04/Mar/21 15:21,1.13.0,,,,,,,,1.13.0,,,,,,,Build System,,,,,0,pull-request-available,,,,,"In FLINK-21570 the japicmp reference version was set to 1.13.0 . Since this version does not exist the japicmp checks have effectively been disabled.

{code}
[INFO] --- japicmp-maven-plugin:0.11.0:cmp (default) @ flink-metrics-core ---
[WARNING] Could not resolve org.apache.flink:flink-metrics-core:jar:1.13.0
[WARNING] Could not resolve dependency with descriptor 'org.apache.flink:flink-metrics-core:1.13.0'.
[WARNING] Please provide at least one resolvable old version using one of the configuration elements <oldVersion/> or <oldVersions/>.
{code}",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21570,,,,,,,,,,,,,,,,,,,,,FLINK-21611,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 15:21:03 UTC 2021,,,,,,,,,,"0|z0o8ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/21 15:21;chesnay;master: 5d8c4c5ed5549dec2714302ba1f5a99d53e849bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL with CSV file input job hangs,FLINK-21569,13361926,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,TsReaper,nkruber,nkruber,02/Mar/21 17:47,15/Dec/21 01:40,13/Jul/23 08:07,03/Aug/21 02:06,1.12.1,,,,,,,,1.12.8,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Runtime,,,,0,auto-deprioritized-major,pull-request-available,,,,"In extension to FLINK-21567, I actually also got the job to be stuck on cancellation by doing the following in the SQL client:

* configure SQL client defaults to run with parallelism 2
* execute the following statement

{code}
CREATE TABLE `airports` (
  `IATA_CODE` CHAR(3),
  `AIRPORT` STRING,
  `CITY` STRING,
  `STATE` CHAR(2),
  `COUNTRY` CHAR(3),
  `LATITUDE` DOUBLE NULL,
  `LONGITUDE` DOUBLE NULL,
  PRIMARY KEY (`IATA_CODE`) NOT ENFORCED
) WITH (
  'connector' = 'filesystem',
  'path' = 'file:///tmp/kaggle-flight-delay/airports.csv',
  'format' = 'csv',
  'csv.allow-comments' = 'true',
  'csv.ignore-parse-errors' = 'true',
  'csv.null-literal' = ''
);

CREATE TABLE `flights` (
  `_YEAR` CHAR(4),
  `_MONTH` CHAR(2),
  `_DAY` CHAR(2),
  `_DAY_OF_WEEK` TINYINT,
  `AIRLINE` CHAR(2),
  `FLIGHT_NUMBER` SMALLINT,
  `TAIL_NUMBER` CHAR(6),
  `ORIGIN_AIRPORT` CHAR(3),
  `DESTINATION_AIRPORT` CHAR(3),
  `_SCHEDULED_DEPARTURE` CHAR(4),
  `SCHEDULED_DEPARTURE` AS TO_TIMESTAMP(`_YEAR` || '-' || `_MONTH` || '-' || `_DAY` || ' ' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 0 FOR 2) || ':' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 3) || ':00'),
  `_DEPARTURE_TIME` CHAR(4),
  `DEPARTURE_DELAY` SMALLINT,
  `DEPARTURE_TIME` AS TIMESTAMPADD(MINUTE, CAST(`DEPARTURE_DELAY` AS INT), TO_TIMESTAMP(`_YEAR` || '-' || `_MONTH` || '-' || `_DAY` || ' ' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 0 FOR 2) || ':' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 3) || ':00')),
  `TAXI_OUT` SMALLINT,
  `WHEELS_OFF` CHAR(4),
  `SCHEDULED_TIME` SMALLINT,
  `ELAPSED_TIME` SMALLINT,
  `AIR_TIME` SMALLINT,
  `DISTANCE` SMALLINT,
  `WHEELS_ON` CHAR(4),
  `TAXI_IN` SMALLINT,
  `SCHEDULED_ARRIVAL` CHAR(4),
  `ARRIVAL_TIME` CHAR(4),
  `ARRIVAL_DELAY` SMALLINT,
  `DIVERTED` BOOLEAN,
  `CANCELLED` BOOLEAN,
  `CANCELLATION_REASON` CHAR(1),
  `AIR_SYSTEM_DELAY` SMALLINT,
  `SECURITY_DELAY` SMALLINT,
  `AIRLINE_DELAY` SMALLINT,
  `LATE_AIRCRAFT_DELAY` SMALLINT,
  `WEATHER_DELAY` SMALLINT
) WITH (
  'connector' = 'filesystem',
  'path' = 'file:///tmp/kaggle-flight-delay/flights-small2.csv',
  'format' = 'csv',
  'csv.null-literal' = ''
);

SELECT `ORIGIN_AIRPORT`, `AIRPORT`, `STATE`, `NUM_DELAYS`
FROM (
  SELECT `ORIGIN_AIRPORT`, `AIRPORT`, `STATE`, COUNT(*) AS `NUM_DELAYS`,
    ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC) AS rownum
  FROM flights, airports
  WHERE `ORIGIN_AIRPORT` = `IATA_CODE` AND `DEPARTURE_DELAY` > 0
  GROUP BY `ORIGIN_AIRPORT`, `AIRPORT`, `STATE`)
WHERE rownum <= 10;
{code}

Results are shown in the CLI but after quitting the result view, the job seems stuck in CANCELLING until (at least) one of the TMs shuts itself down because a task wouldn't react to the cancelling signal. This appears in its TM logs:

{code}
2021-03-02 18:39:19,451 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Task 'Source: TableSourceScan(table=[[default_catalog, default_database, airports, project=[IATA_CODE, AIRPORT, STATE]]], fields=[IATA_CODE, AIRPORT, STATE]) (2/2)#0' did not react to cancelling signal for 30 seconds, but is stuck in method:
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:653)
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
java.lang.Thread.run(Thread.java:748)

...

2021-03-02 18:39:49,447 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Task did not exit gracefully within 180 + seconds.
org.apache.flink.util.FlinkRuntimeException: Task did not exit gracefully within 180 + seconds.
	at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1685) [flink-dist_2.12-1.12.1.jar:1.12.1]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
2021-03-02 18:39:49,448 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Fatal error occurred while executing the TaskManager. Shutting it down...
org.apache.flink.util.FlinkRuntimeException: Task did not exit gracefully within 180 + seconds.
	at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1685) [flink-dist_2.12-1.12.1.jar:1.12.1]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{code}",,alpinegizmo,jark,lzljs3620320,nkruber,rmetzger,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/21 17:42;nkruber;airports.csv;https://issues.apache.org/jira/secure/attachment/13021472/airports.csv","02/Mar/21 17:42;nkruber;flights-small2.csv;https://issues.apache.org/jira/secure/attachment/13021471/flights-small2.csv",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 03 02:06:18 UTC 2021,,,,,,,,,,"0|z0o84w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 10:48;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 22:56;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","27/Jul/21 07:37;TsReaper;Hi all!

I've looked into this issue today and found that this is caused by [a bug|https://github.com/FasterXML/jackson-dataformats-text/issues/191] in Jackson 2.10. This bug will be triggered when the 4000th character is the '\n' in a csv input split.

As Jackson 2.11 and above solves this issue and in Flink 1.13 we've updated Jackson to 2.12, I'd like to also update Jackson version here in Flink 1.12. However I see in FLINK-21020 we only update Jackson version in Flink 1.11 and 1.12 from 2.10.1 to 2.10.5, not a higher version. Is there any consideration when updating dependency version just for bug fixes? [~chesnay] [~rmetzger] I'd like to seek for your advice. Thanks.

P.S.: If we do not allow parsing errors then we'll see the following stack trace, which points to that Jackson bug.
{code:java}
java.lang.RuntimeException: Failed to fetch next result

	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:117)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:350)
	at org.apache.flink.table.utils.PrintUtils.printAsTableauForm(PrintUtils.java:149)
	at org.apache.flink.table.api.internal.TableResultImpl.print(TableResultImpl.java:154)
	at org.apache.flink.table.client.MyTest.myTest(MyTest.java:42)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:169)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:118)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	... 28 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:167)
	... 30 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:117)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:614)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1983)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:114)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:166)
	... 30 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:118)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:80)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:233)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:224)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:215)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:666)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:446)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive$$$capture(Actor.scala:517)
	at akka.actor.Actor$class.aroundReceive(Actor.scala)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.io.IOException: Failed to deserialize CSV row.
	at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:261)
	at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:162)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:90)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:66)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:267)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 4000
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.impl.CsvDecoder.skipLinesWhenNeeded(CsvDecoder.java:527)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.impl.CsvDecoder.startNewLine(CsvDecoder.java:499)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvParser._handleObjectRowEnd(CsvParser.java:1067)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvParser._handleNextEntry(CsvParser.java:858)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvParser.nextFieldName(CsvParser.java:665)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:250)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:68)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:15)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator.nextValue(MappingIterator.java:280)
	at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:252)
	... 5 more
{code};;;","29/Jul/21 06:51;rmetzger;Thanks a lot for looking into this. We try to avoid major version updates (in this case from Jackson 2.10 to 2.12) in bugfix Flink releases (say Flink 1.11.1 to 1.11.2). The reason is that we want to avoid that users need to change their dependency management or that such an upgrade causes major differences between bugfix releases.

However in this case, Jackson is a shaded dependency, and I'd consider it a stable project. In my opinion, we can bump Jackson to 2.12 in Flink 1.12.;;;","30/Jul/21 03:13;TsReaper;According to the discussion with Chesnay offline, I'm upgrading flink-shaded-jackson version to 2.12.1-13.0 instead of bumping its version in the flink-shaded project.;;;","03/Aug/21 02:06;lzljs3620320;release-1.12: e621c70d530d52cdf8e8b638aaf02efdd3a19eb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV Format exception while parsing: ArrayIndexOutOfBoundsException: 4000,FLINK-21567,13361920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,nkruber,nkruber,02/Mar/21 17:15,15/Dec/21 01:40,13/Jul/23 08:07,02/Nov/21 06:05,1.11.3,1.12.1,,,,,,,1.12.8,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,auto-deprioritized-major,stale-minor,,,,"I've been trying to play a bit with the data available at https://www.kaggle.com/usdot/flight-delays and got the following exception:

{code}
2021-02-16 18:57:37,913 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: TableSourceScan(table=[[default_catalog, default_database, flights, filter=[], project=[ORIGIN_AIRPORT, DEPARTURE_DELAY]]], fields=[ORIGIN_AIRPORT, DEPARTURE_DELAY]) -> Calc(select=[ORIGIN_AIRPORT], where=[(DEPARTURE_DELAY > 0)]) -> LocalHashAggregate(groupBy=[ORIGIN_AIRPORT], select=[ORIGIN_AIRPORT, Partial_COUNT(*) AS count1$0]) (1/1)#0 (ebbf1204d875a5a4ace529df0d5ba719) switched from RUNNING to FAILED.
java.io.IOException: Failed to deserialize CSV row.
	at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:257) ~[flink-csv-1.12.1.jar:1.12.1]
	at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:162) ~[flink-csv-1.12.1.jar:1.12.1]
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:90) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:66) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:241) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
Caused by: java.lang.ArrayIndexOutOfBoundsException: 4000
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.impl.CsvDecoder.skipLinesWhenNeeded(CsvDecoder.java:527) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.impl.CsvDecoder.startNewLine(CsvDecoder.java:499) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvParser._handleObjectRowEnd(CsvParser.java:1067) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvParser._handleNextEntry(CsvParser.java:858) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvParser.nextFieldName(CsvParser.java:665) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:250) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:68) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:15) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator.nextValue(MappingIterator.java:280) ~[flink-dist_2.12-1.12.1.jar:1.12.1]
	at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:250) ~[flink-csv-1.12.1.jar:1.12.1]
	... 5 more
{code}

h1. Fully working example:

Using the attached file (derived from the data on flight delays, linked above) and the SQL CLI:
{code}
CREATE TABLE `flights` (
  `_YEAR` CHAR(4),
  `_MONTH` CHAR(2),
  `_DAY` CHAR(2),
  `_DAY_OF_WEEK` TINYINT,
  `AIRLINE` CHAR(2),
  `FLIGHT_NUMBER` SMALLINT,
  `TAIL_NUMBER` CHAR(6),
  `ORIGIN_AIRPORT` CHAR(3),
  `DESTINATION_AIRPORT` CHAR(3),
  `_SCHEDULED_DEPARTURE` CHAR(4),
  `SCHEDULED_DEPARTURE` AS TO_TIMESTAMP(`_YEAR` || '-' || `_MONTH` || '-' || `_DAY` || ' ' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 0 FOR 2) || ':' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 3) || ':00'),
  `_DEPARTURE_TIME` CHAR(4),
  `DEPARTURE_DELAY` SMALLINT,
  `DEPARTURE_TIME` AS TIMESTAMPADD(MINUTE, CAST(`DEPARTURE_DELAY` AS INT), TO_TIMESTAMP(`_YEAR` || '-' || `_MONTH` || '-' || `_DAY` || ' ' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 0 FOR 2) || ':' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 3) || ':00')),
  `TAXI_OUT` SMALLINT,
  `WHEELS_OFF` CHAR(4),
  `SCHEDULED_TIME` SMALLINT,
  `ELAPSED_TIME` SMALLINT,
  `AIR_TIME` SMALLINT,
  `DISTANCE` SMALLINT,
  `WHEELS_ON` CHAR(4),
  `TAXI_IN` SMALLINT,
  `SCHEDULED_ARRIVAL` CHAR(4),
  `ARRIVAL_TIME` CHAR(4),
  `ARRIVAL_DELAY` SMALLINT,
  `DIVERTED` BOOLEAN,
  `CANCELLED` BOOLEAN,
  `CANCELLATION_REASON` CHAR(1),
  `AIR_SYSTEM_DELAY` SMALLINT,
  `SECURITY_DELAY` SMALLINT,
  `AIRLINE_DELAY` SMALLINT,
  `LATE_AIRCRAFT_DELAY` SMALLINT,
  `WEATHER_DELAY` SMALLINT
) WITH (
  'connector' = 'filesystem',
  'path' = 'file:///tmp/kaggle-flight-delay/flights-small.csv',
  'format' = 'csv',
  'csv.allow-comments' = 'true',
  'csv.null-literal' = ''
);

SELECT * FROM `flights` LIMIT 10;
{code}",,alpinegizmo,jark,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21569,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/21 17:06;nkruber;flights-small.csv;https://issues.apache.org/jira/secure/attachment/13021467/flights-small.csv",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 28 10:40:23 UTC 2021,,,,,,,,,,"0|z0o83k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/21 17:23;nkruber;In this particular case, removing both the commented-line in the csv file AND also removing {{'csv.allow-comments' = 'true',}} from the Table DDL above solves this. Not sure whether the problem lies deeper though and applies to more scenarios.;;;","22/Apr/21 10:49;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 22:56;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","28/Oct/21 10:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingKafkaITCase hangs on azure,FLINK-21556,13361829,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,dwysakowicz,dwysakowicz,02/Mar/21 10:40,19/Sep/22 03:09,13/Jul/23 08:07,19/Sep/22 03:09,1.13.0,,,,,,,,1.16.0,,,,,,,Connectors / Kafka,Tests,,,,0,auto-deprioritized-major,pull-request-available,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13966&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529,,dwysakowicz,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21103,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 07 10:58:01 UTC 2021,,,,,,,,,,"0|z0o7jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/21 10:47;dwysakowicz;Disabled in 6f8f307e8a196fca8564dbec71ae0e71e34b3200;;;","02/Mar/21 14:39;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13966&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","31/Mar/21 14:41;dwysakowicz;Reenabled test in 08c57d493d8a7f6cc268e6866a7f6909369bdfa0 . Added timeout in 7716f78dd0a40f7af63d9ad658ad95c021c57a35. Decreasing the priority for the time being.;;;","13/Apr/21 08:16;dwysakowicz;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16411&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27709]
{code:java}
Apr 13 07:56:14 [ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 180.039 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase
Apr 13 07:56:14 [ERROR] testKafka[0: kafka-version:2.4.1](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 180.016 s  <<< ERROR!
Apr 13 07:56:14 org.junit.runners.model.TestTimedOutException: test timed out after 3 minutes
Apr 13 07:56:14 	at java.lang.Object.wait(Native Method)
Apr 13 07:56:14 	at java.lang.Object.wait(Object.java:460)
Apr 13 07:56:14 	at java.util.concurrent.TimeUnit.timedWait(TimeUnit.java:348)
Apr 13 07:56:14 	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:410)
Apr 13 07:56:14 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:126)
Apr 13 07:56:14 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)
Apr 13 07:56:14 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.queryBrokerStatus(LocalStandaloneKafkaResource.java:259)
Apr 13 07:56:14 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.isKafkaRunning(LocalStandaloneKafkaResource.java:239)
Apr 13 07:56:14 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.shutdownResource(LocalStandaloneKafkaResource.java:185)
Apr 13 07:56:14 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.afterTestSuccess(LocalStandaloneKafkaResource.java:168)
Apr 13 07:56:14 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:57)
Apr 13 07:56:14 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
Apr 13 07:56:14 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
Apr 13 07:56:14 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Apr 13 07:56:14 	at java.lang.Thread.run(Thread.java:748)
Apr 13 07:56:14 
Apr 13 07:56:14 [ERROR] testKafka[0: kafka-version:2.4.1](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 180.016 s  <<< ERROR!
Apr 13 07:56:14 java.lang.Exception: Appears to be stuck in thread Thread-45
Apr 13 07:56:14 	at java.io.FileInputStream.readBytes(Native Method)
Apr 13 07:56:14 	at java.io.FileInputStream.read(FileInputStream.java:255)
Apr 13 07:56:14 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
Apr 13 07:56:14 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
Apr 13 07:56:14 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
Apr 13 07:56:14 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
Apr 13 07:56:14 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
Apr 13 07:56:14 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
Apr 13 07:56:14 	at java.io.BufferedReader.fill(BufferedReader.java:161)
Apr 13 07:56:14 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
Apr 13 07:56:14 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
Apr 13 07:56:14 	at org.apache.flink.tests.util.AutoClosableProcess.lambda$consumeOutput$0(AutoClosableProcess.java:202)
Apr 13 07:56:14 	at org.apache.flink.tests.util.AutoClosableProcess$$Lambda$398/1268743402.run(Unknown Source)
Apr 13 07:56:14 	at java.lang.Thread.run(Thread.java:748)
 {code};;;","30/May/21 11:27;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","07/Jun/21 10:58;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HA does not work when using mesos scheduler,FLINK-21555,13361803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,q.xu,q.xu,02/Mar/21 09:35,25/Jun/21 07:00,13/Jul/23 08:07,25/Jun/21 07:00,1.12.0,,,,,,,,,,,,,,,Deployment / Mesos,,,,,0,auto-deprioritized-major,auto-unassigned,,,,"When trying to start a flink session cluster in mesos, it's crashing because of jackson dependency issue:
{code:java}
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint MesosSessionClusterEntrypoint. at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:191) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:529) [flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.main(MesosSessionClusterEntrypoint.java:103) [flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002]Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.enable([Lcom/fasterxml/jackson/core/JsonParser$Feature;)Lcom/fasterxml/jackson/databind/ObjectMapper; at com.amazonaws.partitions.PartitionsLoader.<clinit>(PartitionsLoader.java:54) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:64) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:52) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.regions.EndpointToRegion.guessRegionOrRegionNameForEndpoint(EndpointToRegion.java:101) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.regions.EndpointToRegion.guessRegionNameForEndpoint(EndpointToRegion.java:41) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.regions.EndpointToRegion.guessRegionNameForEndpointWithDefault(EndpointToRegion.java:48) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.AmazonWebServiceClient.computeSignerByURI(AmazonWebServiceClient.java:429) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.AmazonWebServiceClient.setEndpoint(AmazonWebServiceClient.java:318) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.amazonaws.services.s3.AmazonS3Client.setEndpoint(AmazonS3Client.java:728) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.facebook.presto.hive.s3.PrestoS3FileSystem.createAmazonS3Client(PrestoS3FileSystem.java:661) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at com.facebook.presto.hive.s3.PrestoS3FileSystem.initialize(PrestoS3FileSystem.java:216) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:126) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:468) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:389) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.core.fs.Path.getFileSystem(Path.java:292) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:100) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:89) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:117) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:309) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:272) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.initializeServices(MesosSessionClusterEntrypoint.java:56) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:212) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:173) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_275] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_275] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) ~[flink-s3-fs-presto-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:172) ~[flink-dist_2.11-1.12-criteo-1605205002.jar:1.12-criteo-1605205002] ... 2 more
{code}",,mapohl,q.xu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 22:42:35 UTC 2021,,,,,,,,,,"0|z0o7ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/21 09:37;q.xu;I would like to work on a fix for it.;;;","02/Mar/21 13:19;mapohl;Thanks [~q.xu]. I'm gonna assign the issue to you.;;;","14/Mar/21 18:05;q.xu;It turns out it's a conflict with some third party libraries that depend on a different version of jackson (and we also see a similar issue with guava after fixing the jackson issue).

Is it worth to relocate jackson (and guava) package in flink mesos to align with flink-shaded, so that such conflicts can be avoided?;;;","17/Mar/21 12:47;mapohl;Hi [~q.xu], it looks like you've put the presto file system plugin jar into the {{./lib}} folder instead of the {{./plugin}} folder as suggested in the [docs for pluggable file systems|https://ci.apache.org/projects/flink/flink-docs-stable/deployment/filesystems/#pluggable-file-systems]. Can you verify that? It's advised to not do that because the risks of running into dependency conflicts is high as you seem to experienced.;;;","16/Apr/21 10:45;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","20/Apr/21 10:33;mapohl;Hi [~q.xu], any updates on that issue? Can you verify [my statement above|https://issues.apache.org/jira/browse/FLINK-21555?focusedCommentId=17303377&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17303377]?;;;","27/Apr/21 22:46;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","28/Apr/21 09:03;q.xu;Hello Matthias
Sorry for the late response. Yes, indeed we put presto jar together with other libraries in `/lib` directory.

I'll give it a try to put presto to the plugin directory.;;;","28/May/21 23:09;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Jun/21 07:26;mapohl;[~q.xu] any updates in this matter? Can I close this issue?;;;","08/Jun/21 22:42;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowDistinctAggregateITCase#testHopWindow_Cube is unstable,FLINK-21553,13361759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingzhang,jark,jark,02/Mar/21 06:40,01/Apr/21 15:10,13/Jul/23 08:07,04/Mar/21 12:19,,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,,"See https://dev.azure.com/imjark/Flink/_build/results?buildId=422&view=logs&j=d1352042-8a7d-50b6-3946-a85d176b7981&t=b2322052-d503-5552-81e2-b3a532a1d7e8

 !screenshot-1.png! 
",,dwysakowicz,jark,jingzhang,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/21 04:06;jingzhang;image-2021-03-04-12-05-59-802.png;https://issues.apache.org/jira/secure/attachment/13021538/image-2021-03-04-12-05-59-802.png","04/Mar/21 04:07;jingzhang;image-2021-03-04-12-07-53-566.png;https://issues.apache.org/jira/secure/attachment/13021539/image-2021-03-04-12-07-53-566.png","04/Mar/21 04:08;jingzhang;image-2021-03-04-12-08-07-097.png;https://issues.apache.org/jira/secure/attachment/13021540/image-2021-03-04-12-08-07-097.png","02/Mar/21 06:41;jark;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13021429/screenshot-1.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 12:19:00 UTC 2021,,,,,,,,,,"0|z0o740:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/21 21:25;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14013&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29;;;","03/Mar/21 03:02;jingzhang;Could you assign the issue to me ? I would like to find out the root cause.;;;","03/Mar/21 05:47;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14013&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29;;;","03/Mar/21 07:54;dwysakowicz;Is FLINK-21482 the root cause? The test fails quite frequently. How do you feel about reverting the change?;;;","03/Mar/21 08:58;jingzhang;[~dwysakowicz] [~maguowei], FLINK-21482  is not the root cause, it does only fix a minor bug about WindowProperties inference about `Expand` node and add more test cases to cover groupsets/cube/rollup syntax in wtf. The unstable ITCase may reflect something went wrong in the physical operator. If the issue is a blocker, reverting the change is OK for me. I would continue to find root cause to prevent a deep bug in the Operator.;;;","03/Mar/21 09:38;dwysakowicz;I mean if you are saying the FLINK-21482 does not cause the test to fail it doesn't make sense to revert it.;;;","03/Mar/21 14:05;jark;We can disable this test temporarily if it fails frequently.;;;","04/Mar/21 04:10;jingzhang;[~jark] [~dwysakowicz] [~guoweima] Now when flush buffer to state, `CombineRecordsFunction` only copy window key because window key is reused. However, it forgets to copy record which is also reused. I think it's the root cause of the failure case. For above failed case, there exists `count(distinct distinctKey)` in sql,  distinctKey is the UK of MapState. If pushed object directly to  state when flush buffer to state, it may be updated after it is pushed into HeapStateBackend because it is a reused object in `AbstractBytesMultiMap`.

 

 
{code:java}
//代码占位符
@Test
def testHopWindow_Cube(): Unit = {
  System.setProperty(""org.codehaus.janino.source_debugging.enable"", ""true"")
  System.setProperty(""org.codehaus.janino.source_debugging.dir"",
    ""/Users/zhangjing/IdeaProjects/flink/flink-table/flink-table-planner-blink/src/main/java"")
  val inputData: Seq[Row] = List(
    row(""2020-10-10 00:00:01"", ""Hi"", ""a""),
    row(""2020-10-10 00:00:03"", ""Comment#1"", ""a""),
    row(""2020-10-10 00:00:04"", null, ""a""),

    row(""2020-10-10 00:00:07"", ""Hello"", ""b""),
    row(""2020-10-10 00:00:06"", ""Hi"", ""b""), // out of order
    row(""2020-10-10 00:00:08"", ""Comment#2"", ""a"")
  )
  val dataId = TestValuesTableFactory.registerData(inputData)
  tEnv.executeSql(
    s""""""
       |CREATE TABLE T2 (
       | `ts` STRING,
       | `string` STRING,
       | `name` STRING,
       | `rowtime` AS TO_TIMESTAMP(`ts`),
       | WATERMARK for `rowtime` AS `rowtime` - INTERVAL '1' SECOND
       |) WITH (
       | 'connector' = 'values',
       | 'data-id' = '$dataId',
       | 'failing-source' = 'true'
       |)
       |"""""".stripMargin)
  val sql =
    """"""
      |SELECT
      |  GROUPING_ID(`name`),
      |  `name`,
      |  window_start,
      |  window_end,
      |  COUNT(DISTINCT `string`)
      |FROM TABLE(
      |   HOP(TABLE T2, DESCRIPTOR(rowtime), INTERVAL '5' SECOND, INTERVAL '10' SECOND))
      |GROUP BY CUBE(`name`), window_start, window_end
    """""".stripMargin

  val sink = new TestingAppendSink
  tEnv.sqlQuery(sql).toAppendStream[Row].addSink(sink)
  env.execute()

  val data = Seq(
    ""0,a,2020-10-09T23:59:55,2020-10-10T00:00:05,2"",
    ""0,a,2020-10-10T00:00,2020-10-10T00:00:10,3"",
    ""0,a,2020-10-10T00:00:05,2020-10-10T00:00:15,1"",
    ""0,b,2020-10-10T00:00,2020-10-10T00:00:10,2"",
    ""0,b,2020-10-10T00:00:05,2020-10-10T00:00:15,2"",
    ""1,null,2020-10-09T23:59:55,2020-10-10T00:00:05,2"",
    ""1,null,2020-10-10T00:00,2020-10-10T00:00:10,4"",
    ""1,null,2020-10-10T00:00:05,2020-10-10T00:00:15,3""
  )
  assertEquals(
    data.sorted.mkString(""\n""),
    sink.getAppendResults.sorted.mkString(""\n""))
}
{code}
 

I simplify the failure case as above case, which could fails frequently (1 per 3~4 times based on *HeapStateBackend + SplitDistinct: false*) on my local machine. The state in HeapStateBackend is following pic1 when test pass, while state are pic2 or pic3 when test failure.
  
 !image-2021-03-04-12-05-59-802.png|width=1308,height=292!

 

!image-2021-03-04-12-07-53-566.png|width=1420,height=361!

 

!image-2021-03-04-12-08-07-097.png|width=1297,height=211!
  
 After copy record, the case could always be passed.;;;","04/Mar/21 12:19;jark;Fixed in master: 40e5c27b2d621a946a82a4371f829491d14f4c42;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The managed memory was not released if exception was thrown in createPythonExecutionEnvironment,FLINK-21552,13361758,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,dian.fu,dian.fu,02/Mar/21 06:39,04/Mar/21 06:04,13/Jul/23 08:07,04/Mar/21 06:04,1.12.0,,,,,,,,1.12.3,1.13.0,,,,,,API / Python,Runtime / Coordination,,,,0,pull-request-available,,,,,"If there is exception thrown in [createPythonExecutionEnvironment|https://github.com/apache/flink/blob/3796e59f79a90bd8ad5e6fc37458e2d6cce23139/flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java#L248], the job will failed with the following exception:
{code:java}
org.apache.flink.runtime.memory.MemoryAllocationException: Could not created the shared memory resource of size 611948962. Not enough memory left to reserve from the slot's managed memory.
at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$5(MemoryManager.java:536)
at org.apache.flink.runtime.memory.SharedResources.createResource(SharedResources.java:126)
at org.apache.flink.runtime.memory.SharedResources.getOrAllocateSharedResource(SharedResources.java:72)
at org.apache.flink.runtime.memory.MemoryManager.getSharedMemoryResourceForManagedMemory(MemoryManager.java:555)
at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:250)
at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:113)
at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:116)
at org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:88)
at org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.open(AbstractRowDataPythonScalarFunctionOperator.java:70)
at org.apache.flink.table.runtime.operators.python.scalar.RowDataPythonScalarFunctionOperator.open(RowDataPythonScalarFunctionOperator.java:59)
at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:428)
at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:543)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:533)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
at java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.flink.runtime.memory.MemoryReservationException: Could not allocate 611948962 bytes, only 0 bytes are remaining. This usually indicates that you are requesting more memory than you have reserved. However, when running an old JVM version it can also be caused by slow garbage collection. Try to upgrade to Java 8u72 or higher if running on an old Java version.
at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:170)
at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:84)
at org.apache.flink.runtime.memory.MemoryManager.reserveMemory(MemoryManager.java:423)
at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$5(MemoryManager.java:534)
... 17 more
{code}
The reason is that the reserved managed memory was not added back to the MemoryManager when Job failed because of exceptions thrown in createPythonExecutionEnvironment. This causes that there is no managed memory to allocate during failover.",,dian.fu,highfly,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 06:04:44 UTC 2021,,,,,,,,,,"0|z0o73s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/21 06:55;xtsong;Thanks [~dian.fu],

I think the problem in the following snippet from {{MemoryManager#getSharedMemoryResourceForManagedMemory}}. We probably should unreserve the budget if {{initializer.apply}} fail with an error.

{code:java}
final LongFunctionWithException<T, Exception> reserveAndInitialize =
        (size) -> {
            try {
                reserveMemory(type, size);
            } catch (MemoryReservationException e) {
                throw new MemoryAllocationException(
                        ""Could not created the shared memory resource of size ""
                                + size
                                + "". Not enough memory left to reserve from the slot's managed memory."",
                        e);
            }

            return initializer.apply(size);
        };
{code}

cc [~sewen];;;","02/Mar/21 06:59;xtsong;BTW, I think this issue only affects when the job tries to reuse a cached slot. As long as the slot is freed, memory manager will be destroyed and the corrupt states will be discarded.

Since it doesn't affect other jobs in the cluster, and for the current job the {{initializer}} is failing anyway, I think we can downgrade it to major priority.;;;","02/Mar/21 07:57;dian.fu;Thanks [~xintongsong] for the analysis. Agree with you. I think we should handle properly the case when exception was thrown in initializer.apply(). 

cc [~sewen];;;","04/Mar/21 06:04;xtsong;Fixed via
* master (1.13): 1e71b1c35c25063ce80abd35d1ac59e437c858c3
* release-1.12: 64647490f3e96bdbdfe535654c667f1ead0b026c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperHaServicesTest.testSimpleClose fail,FLINK-21550,13361748,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,maguowei,maguowei,02/Mar/21 05:53,23/Mar/21 13:42,13/Jul/23 08:07,23/Mar/21 13:42,1.12.3,1.13.0,,,,,,,1.11.4,1.12.3,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13956&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034
{code:java}
[ERROR] testSimpleClose(org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest) Time elapsed: 9.265 s <<< ERROR! java.util.concurrent.TimeoutException: Listener was not notified about a new leader within 2000ms at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151) at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:136) at org.apache.flink.runtime.leaderelection.TestingRetrievalBase.waitForNewLeader(TestingRetrievalBase.java:53) at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.runCleanupTest(ZooKeeperHaServicesTest.java:195) at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.testSimpleClose(ZooKeeperHaServicesTest.java:100) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
{code}",,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 13:42:07 UTC 2021,,,,,,,,,,"0|z0o71k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/21 03:06;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14637&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&l=6595

{code:java}
java.util.concurrent.TimeoutException: Listener was not notified about a new leader within 2000ms
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:136)
	at org.apache.flink.runtime.leaderelection.TestingRetrievalBase.waitForNewLeader(TestingRetrievalBase.java:53)
	at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.runCleanupTest(ZooKeeperHaServicesTest.java:195)
	at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.testSimpleClose(ZooKeeperHaServicesTest.java:100)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)

{code}
;;;","22/Mar/21 21:16;trohrmann;The logs show that there are gaps of more than 2 seconds. The test has a timeout of 2 seconds for the leader election to happen. Hence, I suggest to increase the timeout to 10s to be on the safe side.;;;","22/Mar/21 21:20;trohrmann;Actually I suggest to replace busy loop waiting with using the {{LeaderRetrievalUtils.LeaderConnectionInfoListener}} which offers a {{CompletableFuture}} to wait on.;;;","23/Mar/21 13:42;trohrmann;Fixed via

1.13.0: 241db3f6a5e736eb89c15f221dd2db0c57efbea6
1.12.3: 56e557bf2c9421b5407068097c551ec46e7f98e7
1.11.4: c3992df220b3fc7b0623ff3cae130b230c2804c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running Kerberized YARN per-job on Docker test stalls on azure,FLINK-21545,13361625,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,dwysakowicz,dwysakowicz,01/Mar/21 13:46,30/Mar/21 07:58,13/Jul/23 08:07,30/Mar/21 07:58,1.13.0,,,,,,,,1.13.0,,,,,,,Deployment / YARN,Tests,,,,0,test-stability,,,,,"For some reason the test started taking 10x more time than before

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13921&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13920&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13918&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13919&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
[PASS] 'Running Kerberized YARN per-job on Docker test (default input)' passed after 40 minutes and 34 seconds! Test exited with exit code 0.
{code}
",,dwysakowicz,liyu,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21561,,,,,,,,,,,,,,,FLINK-21561,,,,,,,,,,,,,,,,FLINK-21103,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 30 07:57:53 UTC 2021,,,,,,,,,,"0|z0o6a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/21 14:24;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13928&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","01/Mar/21 14:25;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13929&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
;;;","01/Mar/21 14:46;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13931&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","01/Mar/21 14:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13931&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","01/Mar/21 14:55;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13930&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","01/Mar/21 15:25;dwysakowicz;I tried running the test locally. It turns out that for some reason the download speed from archive.apache.org is terribly slow.;;;","01/Mar/21 16:23;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13937&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","01/Mar/21 16:56;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13938&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","02/Mar/21 13:55;dwysakowicz;Temporarily disabled in 7463ac55585039d97b7b770f7aef339b8a1edaad and 6a51827fb515feb2561d592eb9a1bfcb30030f88;;;","02/Mar/21 14:39;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13971&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","02/Mar/21 14:43;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13980&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","02/Mar/21 14:45;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13981&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","02/Mar/21 14:48;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13984&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","02/Mar/21 14:56;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13986&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","02/Mar/21 19:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14002&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=41;;;","02/Mar/21 19:55;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13994&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","30/Mar/21 07:57;dwysakowicz;Reenabled tests in 59832e11cb85099c658cbc413120247b1aa46a75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'SQL Client end-to-end test (Blink planner) Elasticsearch (v6.3.1)' fails during download,FLINK-21539,13361553,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,dwysakowicz,dwysakowicz,01/Mar/21 09:14,15/Dec/21 01:44,13/Jul/23 08:07,01/Dec/21 16:31,1.11.3,1.13.0,1.14.0,,,,,,1.14.3,1.15.0,,,,,,Table SQL / Ecosystem,Tests,,,,0,auto-deprioritized-major,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13906&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=03dbd840-5430-533d-d1a7-05d0ebe03873

{code}
Downloading Elasticsearch from https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.1.tar.gz ...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0 87.1M    0   997    0     0   3298      0  7:42:02 --:--:--  7:42:02  3290
 17 87.1M   17 15.6M    0     0  11.6M      0  0:00:07  0:00:01  0:00:06 11.6M
 31 87.1M   31 27.8M    0     0  12.1M      0  0:00:07  0:00:02  0:00:05 12.1M
 49 87.1M   49 42.9M    0     0  12.9M      0  0:00:06  0:00:03  0:00:03 12.9M
 67 87.1M   67 58.9M    0     0  13.5M      0  0:00:06  0:00:04  0:00:02 13.5M
 87 87.1M   87 75.8M    0     0  14.3M      0  0:00:06  0:00:05  0:00:01 15.1M
 87 87.1M   87 75.9M    0     0  14.2M      0  0:00:06  0:00:05  0:00:01 15.1M
curl: (56) GnuTLS recv error (-110): The TLS connection was non-properly terminated.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.
{code}",,dwysakowicz,godfreyhe,joemoe,maguowei,martijnvisser,trohrmann,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25092,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 01 16:31:33 UTC 2021,,,,,,,,,,"0|z0o5u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/21 08:52;twalthr;I will take another look at this. But actually, I really tried my best to make this downloading as stable as possible.;;;","02/Mar/21 08:53;twalthr;Is this problem for 1.12 as well or only 1.11?;;;","02/Mar/21 08:56;dwysakowicz;So far I've seen only a single occurrence on 1.11 branch.;;;","22/Mar/21 03:29;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15136&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=18382;;;","22/Mar/21 05:08;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15138&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=d47e27f5-9721-5d5f-1cf3-62adbf3d115d&l=9199;;;","25/Mar/21 03:34;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15411&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=7d4f7375-52df-5ce0-457f-b2ffbb2289a4&l=6628;;;","27/Apr/21 23:28;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","19/May/21 22:55;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/21 10:21;joemoe;[~twalthr] will have another look at it.;;;","11/Nov/21 08:33;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26329&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=1fdd9d50-31f7-5383-5578-49e27385b5f1&l=9456

[~twalthr] will you take a look at the problem?;;;","29/Nov/21 11:25;trohrmann;[~twalthr] can you give an update for this ticket?;;;","29/Nov/21 13:33;twalthr;Sorry, I forgot about this one. I will give an update shortly.;;;","29/Nov/21 15:18;martijnvisser;[~trohrmann] I actually didn't know about this ticket, but I have created https://issues.apache.org/jira/browse/FLINK-25092 with PR https://github.com/apache/flink/pull/17941/ to address the download issue already. It's pending a review. CC [~twalthr];;;","01/Dec/21 16:31;martijnvisser;This should now be resolved via https://issues.apache.org/jira/browse/FLINK-25092;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase fails on azure,FLINK-21537,13361543,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dwysakowicz,dwysakowicz,01/Mar/21 08:20,10/Mar/21 14:21,13/Jul/23 08:07,10/Mar/21 14:21,1.13.0,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13866&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228

{code}
2021-02-26T23:25:17.5041521Z [ERROR] testStopWithSavepointFailingInSnapshotCreation(org.apache.flink.test.checkpointing.SavepointITCase)  Time elapsed: 0.359 s  <<< FAILURE!
2021-02-26T23:25:17.5042304Z java.lang.AssertionError
2021-02-26T23:25:17.5042938Z 	at org.junit.Assert.fail(Assert.java:86)
2021-02-26T23:25:17.5043637Z 	at org.junit.Assert.assertTrue(Assert.java:41)
2021-02-26T23:25:17.5046831Z 	at org.junit.Assert.assertTrue(Assert.java:52)
2021-02-26T23:25:17.5047567Z 	at org.apache.flink.test.checkpointing.SavepointITCase.lambda$assertInSnapshotCreationFailure$4(SavepointITCase.java:604)
2021-02-26T23:25:17.5048194Z 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithFailingSourceInOnePipeline(SavepointITCase.java:684)
2021-02-26T23:25:17.5049245Z 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointFailingInSnapshotCreation(SavepointITCase.java:564)
2021-02-26T23:25:17.5049751Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-26T23:25:17.5050168Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-26T23:25:17.5051317Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-26T23:25:17.5052136Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-02-26T23:25:17.5052931Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-02-26T23:25:17.5053700Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-02-26T23:25:17.5054466Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-02-26T23:25:17.5055163Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-02-26T23:25:17.5055865Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-02-26T23:25:17.5056560Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-02-26T23:25:17.5057240Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-02-26T23:25:17.5057906Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-02-26T23:25:17.5058488Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-02-26T23:25:17.5059193Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-02-26T23:25:17.5059935Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-02-26T23:25:17.5060685Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-02-26T23:25:17.5061305Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-26T23:25:17.5061940Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-26T23:25:17.5062717Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-26T23:25:17.5063355Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-26T23:25:17.5064011Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-26T23:25:17.5064648Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-26T23:25:17.5065227Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-02-26T23:25:17.5065750Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-02-26T23:25:17.5066719Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-26T23:25:17.5067330Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-26T23:25:17.5067988Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-26T23:25:17.5068659Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-26T23:25:17.5069424Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-26T23:25:17.5070052Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-26T23:25:17.5070710Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-02-26T23:25:17.5071469Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-02-26T23:25:17.5072312Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-02-26T23:25:17.5073373Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-02-26T23:25:17.5074172Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-02-26T23:25:17.5074941Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-02-26T23:25:17.5075766Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-02-26T23:25:17.5076827Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-02-26T23:25:17.5077549Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-02-26T23:25:17.5078239Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-02-26T23:25:17.5078643Z 
2021-02-26T23:25:17.5079442Z [ERROR] testStopWithSavepointFailingAfterSnapshotCreation(org.apache.flink.test.checkpointing.SavepointITCase)  Time elapsed: 0.31 s  <<< FAILURE!
2021-02-26T23:25:17.5080168Z java.lang.AssertionError
2021-02-26T23:25:17.5080585Z 	at org.junit.Assert.fail(Assert.java:86)
2021-02-26T23:25:17.5081026Z 	at org.junit.Assert.assertTrue(Assert.java:41)
2021-02-26T23:25:17.5081469Z 	at org.junit.Assert.assertTrue(Assert.java:52)
2021-02-26T23:25:17.5087193Z 	at org.apache.flink.test.checkpointing.SavepointITCase.lambda$assertAfterSnapshotCreationFailure$3(SavepointITCase.java:590)
2021-02-26T23:25:17.5087941Z 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithFailingSourceInOnePipeline(SavepointITCase.java:684)
2021-02-26T23:25:17.5088590Z 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointFailingAfterSnapshotCreation(SavepointITCase.java:576)
2021-02-26T23:25:17.5089179Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-26T23:25:17.5089599Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-26T23:25:17.5090081Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-26T23:25:17.5090491Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-02-26T23:25:17.5091366Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-02-26T23:25:17.5092037Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-02-26T23:25:17.5092689Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-02-26T23:25:17.5093180Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-02-26T23:25:17.5093686Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-02-26T23:25:17.5094219Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-02-26T23:25:17.5094669Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-02-26T23:25:17.5095097Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-02-26T23:25:17.5095458Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-02-26T23:25:17.5096043Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-02-26T23:25:17.5096475Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-02-26T23:25:17.5096930Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-02-26T23:25:17.5097359Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-26T23:25:17.5097761Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-26T23:25:17.5098153Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-26T23:25:17.5098570Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-26T23:25:17.5099063Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-26T23:25:17.5099447Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-26T23:25:17.5099812Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-02-26T23:25:17.5100159Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-02-26T23:25:17.5100514Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-26T23:25:17.5100908Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-26T23:25:17.5101400Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-26T23:25:17.5101795Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-26T23:25:17.5102617Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-26T23:25:17.5103222Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-26T23:25:17.5103843Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-02-26T23:25:17.5104582Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-02-26T23:25:17.5105368Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-02-26T23:25:17.5106146Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-02-26T23:25:17.5106935Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-02-26T23:25:17.5107734Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-02-26T23:25:17.5108540Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-02-26T23:25:17.5109553Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-02-26T23:25:17.5110302Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-02-26T23:25:17.5111005Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,maguowei,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 10 14:21:57 UTC 2021,,,,,,,,,,"0|z0o5s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/21 09:19;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13905&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228;;;","02/Mar/21 05:29;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13954&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228;;;","04/Mar/21 07:16;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228;;;","10/Mar/21 14:21;mapohl;This failure happens due to the stop-with-savepoint functionality not being implemented for the adaptive scheduler, yet. A hotfix was added in [ef6dc2c9|https://github.com/apache/flink/commit/ef6dc2c9]. I'm closing this issue as the tests should not be executed anymore in the adaptive-scheduling enabled stages.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UnalignedCheckpointITCase.execute failed with ""OutOfMemoryError: Java heap space""",FLINK-21535,13361540,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,dwysakowicz,dwysakowicz,01/Mar/21 08:16,22/Jun/21 14:05,13/Jul/23 08:07,16/Mar/21 17:48,1.13.0,,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13866&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56

{code}
2021-02-27T02:11:41.5659201Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-02-27T02:11:41.5659947Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-02-27T02:11:41.5660794Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-02-27T02:11:41.5661618Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-02-27T02:11:41.5662356Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-02-27T02:11:41.5663104Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-02-27T02:11:41.5664016Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-02-27T02:11:41.5664817Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
2021-02-27T02:11:41.5665638Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-02-27T02:11:41.5666405Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-02-27T02:11:41.5667609Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-02-27T02:11:41.5668358Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-02-27T02:11:41.5669218Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1066)
2021-02-27T02:11:41.5669928Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2021-02-27T02:11:41.5670540Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-02-27T02:11:41.5671268Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-02-27T02:11:41.5671881Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-02-27T02:11:41.5672512Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-02-27T02:11:41.5673219Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-02-27T02:11:41.5674085Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-02-27T02:11:41.5674794Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-02-27T02:11:41.5675466Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-02-27T02:11:41.5676181Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2021-02-27T02:11:41.5676977Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-02-27T02:11:41.5677717Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-02-27T02:11:41.5678409Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-02-27T02:11:41.5679071Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-02-27T02:11:41.5679776Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-02-27T02:11:41.5680576Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-02-27T02:11:41.5681383Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-02-27T02:11:41.5682167Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-02-27T02:11:41.5683040Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-02-27T02:11:41.5683759Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-02-27T02:11:41.5684493Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-02-27T02:11:41.5685238Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-02-27T02:11:41.5686193Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-02-27T02:11:41.5686901Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-02-27T02:11:41.5687621Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-02-27T02:11:41.5688337Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-02-27T02:11:41.5689199Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
2021-02-27T02:11:41.5690155Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:130)
2021-02-27T02:11:41.5691115Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:81)
2021-02-27T02:11:41.5692140Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)
2021-02-27T02:11:41.5693174Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212)
2021-02-27T02:11:41.5694037Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203)
2021-02-27T02:11:41.5694882Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:696)
2021-02-27T02:11:41.5695679Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
2021-02-27T02:11:41.5696679Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:433)
2021-02-27T02:11:41.5697369Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-27T02:11:41.5698136Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-27T02:11:41.5699013Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-27T02:11:41.5699822Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-02-27T02:11:41.5700518Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
2021-02-27T02:11:41.5701297Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
2021-02-27T02:11:41.5702092Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-02-27T02:11:41.5702872Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-02-27T02:11:41.5703579Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-02-27T02:11:41.5704234Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-02-27T02:11:41.5704897Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-02-27T02:11:41.5705584Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-02-27T02:11:41.5706268Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-02-27T02:11:41.5706950Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-27T02:11:41.5707627Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-27T02:11:41.5708276Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-02-27T02:11:41.5708920Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-02-27T02:11:41.5709572Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-02-27T02:11:41.5710192Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-02-27T02:11:41.5710808Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-02-27T02:11:41.5711511Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-02-27T02:11:41.5712074Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-02-27T02:11:41.5712561Z 	... 4 more
2021-02-27T02:11:41.5713267Z Caused by: java.lang.OutOfMemoryError: Java heap space
2021-02-27T02:11:41.5713740Z 	at java.util.Arrays.copyOf(Arrays.java:3236)
2021-02-27T02:11:41.5714369Z 	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
2021-02-27T02:11:41.5714949Z 	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
2021-02-27T02:11:41.5715639Z 	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
2021-02-27T02:11:41.5716279Z 	at com.esotericsoftware.kryo.io.Output.flush(Output.java:163)
2021-02-27T02:11:41.5716902Z 	at com.esotericsoftware.kryo.io.Output.require(Output.java:142)
2021-02-27T02:11:41.5717398Z 	at com.esotericsoftware.kryo.io.Output.writeLong(Output.java:501)
2021-02-27T02:11:41.5717941Z 	at com.twitter.chill.java.BitSetSerializer.write(BitSetSerializer.java:79)
2021-02-27T02:11:41.5718501Z 	at com.twitter.chill.java.BitSetSerializer.write(BitSetSerializer.java:35)
2021-02-27T02:11:41.5719419Z 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:505)
2021-02-27T02:11:41.5720114Z 	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:266)
2021-02-27T02:11:41.5720879Z 	at org.apache.flink.runtime.state.ArrayListSerializer.copy(ArrayListSerializer.java:75)
2021-02-27T02:11:41.5721624Z 	at org.apache.flink.runtime.state.PartitionableListState.<init>(PartitionableListState.java:64)
2021-02-27T02:11:41.5722507Z 	at org.apache.flink.runtime.state.PartitionableListState.deepCopy(PartitionableListState.java:76)
2021-02-27T02:11:41.5723353Z 	at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.syncPrepareResources(DefaultOperatorStateBackendSnapshotStrategy.java:77)
2021-02-27T02:11:41.5724425Z 	at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.syncPrepareResources(DefaultOperatorStateBackendSnapshotStrategy.java:36)
2021-02-27T02:11:41.5725434Z 	at org.apache.flink.runtime.state.SnapshotStrategyRunner.snapshot(SnapshotStrategyRunner.java:82)
2021-02-27T02:11:41.5726280Z 	at org.apache.flink.runtime.state.DefaultOperatorStateBackend.snapshot(DefaultOperatorStateBackend.java:230)
2021-02-27T02:11:41.5727152Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:220)
2021-02-27T02:11:41.5728043Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:163)
2021-02-27T02:11:41.5728921Z 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:371)
2021-02-27T02:11:41.5729844Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:691)
2021-02-27T02:11:41.5730849Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:612)
2021-02-27T02:11:41.5731913Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:575)
2021-02-27T02:11:41.5733435Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:298)
2021-02-27T02:11:41.5734991Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$9(StreamTask.java:1020)
2021-02-27T02:11:41.5735786Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$663/1514851121.run(Unknown Source)
2021-02-27T02:11:41.5736587Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
2021-02-27T02:11:41.5737425Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1004)
2021-02-27T02:11:41.5738230Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:960)
2021-02-27T02:11:41.5739112Z 	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:115)
2021-02-27T02:11:41.5740088Z 	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.handleBarrier(SingleCheckpointBarrierHandler.java:182)
{code}",,AHeise,dwysakowicz,maguowei,mapohl,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21689,FLINK-21540,FLINK-21599,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 17:48:37 UTC 2021,,,,,,,,,,"0|z0o5rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/21 07:45;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14081&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","05/Mar/21 08:16;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=310&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4323;;;","08/Mar/21 17:45;dwysakowicz;cc [~AHeise] [~pnowojski] Could you take a look at the issue?;;;","11/Mar/21 09:41;mapohl;Here's another one: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8962&view=logs&j=3ee594b8-fa28-5d05-4dd6-9b78308b361f&t=7198886a-7547-54df-c98b-08bc30f41e3f&l=9313;;;","11/Mar/21 09:54;rmetzger;The one Matthias posted is actually not a UC failure, it's a failure from a test I introduced.;;;","11/Mar/21 09:59;mapohl;Not sure whether I understand: The {{UnalignedCheckpointICase}} is actually failing due to a OOM in the build. ...independent of the build timing out. Or do I miss something here?;;;","11/Mar/21 10:37;rmetzger;Oh, yes, you are right. I was mislead by the stacktraces printed at the bottom of the log output, where {{	at org.apache.flink.test.scheduling.ReactiveModeITCase.testScaleUpOnAdditionalTaskManager(ReactiveModeITCase.java:104)
}} was failing.
Then ignore my comment.

Afaik Arvid has debugged some memory leak in that test.;;;","11/Mar/21 18:52;arvid;A quick assessment of the 3 cases: the test is just running too long and some test implementations that track all records are running OOM. The root cause however is rather that the test take >10 min when they should finish <<1min. I'll investigate further.

Quite possible that FLINK-21689 is a duplicate.;;;","12/Mar/21 08:39;arvid;I verified that FLINK-21540, FLINK-21599, and FLINK-21689 are duplicates and closed them as such. It's also ""only"" a test issue. Fix for test coming today.;;;","15/Mar/21 18:26;arvid;Merged as c177d15323d5025f0cf737b98bb051efbc08a149 into master.;;;","16/Mar/21 17:48;arvid;Merged as 755a8d8214554c64e9db0271a827485208185b8d into 1.12.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException occurs while run a hive streaming job with partitioned table source ,FLINK-21523,13361443,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zouyunhe,zouyunhe,zouyunhe,28/Feb/21 15:23,13/Mar/21 06:19,13/Jul/23 08:07,12/Mar/21 10:28,1.12.1,,,,,,,,1.12.3,1.13.0,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"we have two hive table, the ddl as below
{code:java}
//test_tbl5
create table test.test_5
 (dpi int,     
  uid bigint) 
partitioned by( day string, hour string) stored as parquet;

//test_tbl3
create table test.test_3(
  dpi int,
 uid bigint,    
 itime timestamp) stored as parquet;{code}
 then add a partiton to test_tbl5, 
{code:java}
alter table test_tbl5 add partition(day='2021-02-27',hour='12');
{code}
we start a flink streaming job to read hive table test_tbl5 , and write the data into test_tbl3, the job's  sql as 
{code:java}
set test_tbl5.streaming-source.enable = true;
insert into hive.test.test_tbl3 select dpi, uid, cast(to_timestamp('2020-08-09 00:00:00') as timestamp(9)) from hive.test.test_tbl5 where `day` = '2021-02-27';
{code}
and we seen the exception throws
{code:java}
2021-02-28 22:33:16,553 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext - Exception while handling result from async call in SourceCoordinator-Source: HiveSource-test.test_tbl5. Triggering job failover.org.apache.flink.connectors.hive.FlinkHiveException: Failed to enumerate files    at org.apache.flink.connectors.hive.ContinuousHiveSplitEnumerator.handleNewSplits(ContinuousHiveSplitEnumerator.java:152) ~[flink-connector-hive_2.12-1.12.1.jar:1.12.1]    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$4(ExecutorNotifier.java:136) ~[flink-dist_2.12-1.12.1.jar:1.12.1]    at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40) [flink-dist_2.12-1.12.1.jar:1.12.1]    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]Caused by: java.lang.ArrayIndexOutOfBoundsException: -1    at org.apache.flink.connectors.hive.util.HivePartitionUtils.toHiveTablePartition(HivePartitionUtils.java:184) ~[flink-connector-hive_2.12-1.12.1.jar:1.12.1]    at org.apache.flink.connectors.hive.HiveTableSource$HiveContinuousPartitionFetcherContext.toHiveTablePartition(HiveTableSource.java:417) ~[flink-connector-hive_2.12-1.12.1.jar:1.12.1]    at org.apache.flink.connectors.hive.ContinuousHiveSplitEnumerator$PartitionMonitor.call(ContinuousHiveSplitEnumerator.java:237) ~[flink-connector-hive_2.12-1.12.1.jar:1.12.1]    at org.apache.flink.connectors.hive.ContinuousHiveSplitEnumerator$PartitionMonitor.call(ContinuousHiveSplitEnumerator.java:177) ~[flink-connector-hive_2.12-1.12.1.jar:1.12.1]    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$5(ExecutorNotifier.java:133) ~[flink-dist_2.12-1.12.1.jar:1.12.1]    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_60]    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) ~[?:1.8.0_60]    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_60]    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) ~[?:1.8.0_60]    ... 3 more{code}
it seems the partitoned field is not found in the source table field list.
  ",,jark,leonard,libenchao,lirui,ZhuShang,zouyunhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 12 10:28:47 UTC 2021,,,,,,,,,,"0|z0o55s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/21 02:39;zouyunhe;[~lzljs3620320]  hello, please take a look at this issue。 ;;;","03/Mar/21 05:47;lirui;I guess it's because the projection pushdown doesn't include the partition column so that we don't have it in the field name/type arrays.;;;","03/Mar/21 06:05;zouyunhe;I print some debug log , the partition field is not found in the projected fields. just like you said [~lirui];;;","03/Mar/21 06:14;lirui;So if {{HivePartitionFetcherContextBase}} needs the full field names/types, we should call {{HiveTableSource::getTableSchema}} rather than {{getProducedTableSchema}} when constructing the context. [~Leonard Xu] what do you think?;;;","04/Mar/21 08:34;leonard;Make sense to me;;;","12/Mar/21 10:28;jark;Fixed in 
 - master: 63a6aba6722ae0e3d17381aaeb2fa464ea15d2f5
 - release-1.12: 621d7c223c5e8044ecb7965faf086431500f7d63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointCoordinatorTest.testMinCheckpointPause fails fatally on AZP,FLINK-21518,13361187,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,trohrmann,trohrmann,26/Feb/21 13:08,01/Mar/21 08:49,13/Jul/23 08:07,27/Feb/21 15:11,1.12.1,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"The {{CheckpointCoordinatorTest.testMinCheckpointPause}} fails fatally on AZP. The problem seems to be that the test fails first:

{code}
Test testMinCheckpointPause(org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest) failed with:
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.testMinCheckpointPause(CheckpointCoordinatorTest.java:190)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

and then the process fails fatally because we try to enqueue something into the {{CheckpointCoordinator}} after the {{ExecutorService}} has been shut down.

{code}
12:20:17,785 [     pool-3-thread-1] ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'pool-3-thread-1' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@219d5491 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@649fd666[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 8]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$8(CheckpointCoordinator.java:652) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:898) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2209) ~[?:1.8.0_275]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.startTriggeringCheckpoint(CheckpointCoordinator.java:649) ~[classes/:?]
	at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_275]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint(CheckpointCoordinator.java:533) [classes/:?]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint(CheckpointCoordinator.java:515) [classes/:?]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$ScheduledTrigger.run(CheckpointCoordinator.java:1841) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_275]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_275]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_275]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_275]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_275]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_275]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_275]
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@219d5491 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@649fd666[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 8]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:851) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.handleAsync(CompletableFuture.java:2178) ~[?:1.8.0_275]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.startTriggeringCheckpoint(CheckpointCoordinator.java:597) ~[classes/:?]
	... 11 more
Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@219d5491 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@649fd666[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 8]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_275]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_275]
	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_275]
	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_275]
	at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622) ~[?:1.8.0_275]
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668) ~[?:1.8.0_275]
	at org.apache.flink.runtime.concurrent.ScheduledExecutorServiceAdapter.execute(ScheduledExecutorServiceAdapter.java:64) ~[classes/:?]
	at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:826) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:851) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.handleAsync(CompletableFuture.java:2178) ~[?:1.8.0_275]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.startTriggeringCheckpoint(CheckpointCoordinator.java:597) ~[classes/:?]
	... 11 more
{code}

Looking at the code, the problem should also be present in the current master.

cc [~roman_khachatryan]",,gaoyunhaii,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 01 08:49:05 UTC 2021,,,,,,,,,,"0|z0o3kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/21 14:04;roman;I think this is a test stability issue:
 # The last assertion (the one that fails) depends on timing; increasing wait time would solve this
 # RejectionExecutionException is caused by the shutdown of executor (in test finally block) without shutting down CheckpointCoordinator;;;","01/Mar/21 08:49;roman;Merged into master as 8c610fcd0719569dbc6479e133442003032eaa6d,
into release-1.12 as a15df72b215b9a8a08df3c31602d8f7c6a71effc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test harnesses are bypassing serialization stack for events,FLINK-21517,13361185,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,pnowojski,pnowojski,26/Feb/21 12:54,27/Feb/21 06:21,13/Jul/23 08:07,27/Feb/21 06:21,1.12.1,1.13.0,,,,,,,1.13.0,,,,,,,Runtime / Network,Tests,,,,0,pull-request-available,,,,,"Since FLINK-19297 (https://github.com/apache/flink/pull/13447/files#diff-e5c3ecec28e8d4c2f7f62bc8a4c9ed88c30141d44a570059849a3b2120e4d2b50) we accidentally removed a test coverage for event (de)serialization from a lot of the unit tests, that were/are using test harnesses. For example because of that I almost broke 1.12.2 release, since `stop-with-savepoint --drain` was only tested using test harnesses (didn't have an ITCases and/or end to end test). ",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 27 06:21:18 UTC 2021,,,,,,,,,,"0|z0o3kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/21 06:21;pnowojski;merged commit 01352aa into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceStreamTaskTest.testStopWithSavepointShouldNotInterruptTheSource is failing,FLINK-21515,13361179,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,mapohl,mapohl,26/Feb/21 12:39,19/Apr/21 14:24,13/Jul/23 08:07,30/Mar/21 19:50,1.12.2,1.13.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,,"We experience a test instability with {{SourceStreamTaskTest.testStopWithSavepointShouldNotInterruptTheSource}}. The test is occassionally timing out.
See [this build|https://dev.azure.com/mapohl/flink/_build/results?buildId=290&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=7846] being related to FLINK-21030.

{noformat}
""main"" #1 prio=5 os_prio=0 tid=0x00007f72fc00b800 nid=0x2133 runnable [0x00007f73046ed000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.waitForTaskCompletion(StreamTaskMailboxTestHarness.java:147)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.testStopWithSavepointShouldNotInterruptTheSource(SourceStreamTaskTest.java:604)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}

This failure was reproducible on {{master}}.",,dwysakowicz,liyu,maguowei,mapohl,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21780,,,,,,FLINK-21028,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 30 19:50:48 UTC 2021,,,,,,,,,,"0|z0o3j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/21 12:40;mapohl;The test was added in FLINK-21028. Hence, I assume it's caused by related changes.;;;","26/Feb/21 14:45;roman;This is a test stability issue: latch in test is supposed to be reset before the test starts, but it can actually be reset after triggering.

cc: [~pnowojski];;;","27/Feb/21 09:59;roman;Merged to master as c02efc60bbf5a73ff12c60ac9acc7f9775d8d506

and to release-1.12 as b768a21df21e48f76df2216e6486c10c3ab27dbd.;;;","02/Mar/21 04:07;liyu;Change fix version from 1.12.2 to 1.12.3 since the latest 1.12.2 release candidate (rc2) is cut one commit (e9af362) before this one (b768a21). We can change the fix version back if any new RC produced for 1.12.2;;;","04/Mar/21 06:44;maguowei;another case from 1.11  branch。
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14080&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708
{code:java}
2021-03-03T22:43:01.0479311Z ""main"" #1 prio=5 os_prio=0 cpu=1038548.77ms elapsed=1039.83s tid=0x00007f89fc028000 nid=0x61bc runnable  [0x00007f8a057a9000]
2021-03-03T22:43:01.0479743Z    java.lang.Thread.State: RUNNABLE
2021-03-03T22:43:01.0480269Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.waitForTaskCompletion(StreamTaskMailboxTestHarness.java:150)
2021-03-03T22:43:01.0481230Z 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.testStopWithSavepointShouldNotInterruptTheSource(SourceStreamTaskTest.java:606)
2021-03-03T22:43:01.0481813Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@11.0.9.1/Native Method)
2021-03-03T22:43:01.0482346Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@11.0.9.1/NativeMethodAccessorImpl.java:62)
2021-03-03T22:43:01.0483002Z 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@11.0.9.1/DelegatingMethodAccessorImpl.java:43)
2021-03-03T22:43:01.0483465Z 	at java.lang.reflect.Method.invoke(java.base@11.0.9.1/Method.java:566)
2021-03-03T22:43:01.0483900Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-03-03T22:43:01.0484928Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-03-03T22:43:01.0485657Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-03-03T22:43:01.0486360Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-03-03T22:43:01.0487063Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-03-03T22:43:01.0487959Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-03-03T22:43:01.0488707Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-03-03T22:43:01.0489318Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-03T22:43:01.0490038Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-03T22:43:01.0490658Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-03T22:43:01.0491332Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-03T22:43:01.0491964Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-03T22:43:01.0492594Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-03T22:43:01.0493165Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-03-03T22:43:01.0493650Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-03-03T22:43:01.0494201Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-03T22:43:01.0495008Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-03T22:43:01.0495651Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-03T22:43:01.0496332Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-03T22:43:01.0496971Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-03T22:43:01.0497558Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-03T22:43:01.0498770Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-03-03T22:43:01.0499616Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-03-03T22:43:01.0500443Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-03-03T22:43:01.0501256Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-03-03T22:43:01.0501832Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-03-03T22:43:01.0502400Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-03-03T22:43:01.0502928Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-03-03T22:43:01.0503513Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-03-03T22:43:01.0504069Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-03-03T22:43:01.0504833Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
;;;","10/Mar/21 02:23;maguowei;another case from 1.11 branch 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14363&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708
Do you think it is ok that pick the fix to the 1.11? [~roman_khachatryan];;;","10/Mar/21 08:20;roman;Yes, it should be fine, I don't think there will be any conflicts.;;;","30/Mar/21 07:31;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15719&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708&l=8907
;;;","30/Mar/21 09:21;roman;Re-opening to fix in 1.11;;;","30/Mar/21 19:50;roman;Merged into 1.11 as 311a2f327fd50defc7694e7f9fe1da903f380b2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26.02.2021 Benchmarks are not compiling,FLINK-21514,13361174,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,pnowojski,pnowojski,26/Feb/21 12:21,05/Mar/21 06:32,13/Jul/23 08:07,01/Mar/21 08:40,1.13.0,,,,,,,,,,,,,,,Benchmarks,Runtime / Coordination,,,,0,,,,,,"http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/7497/console


{code:java}
08:46:27  [ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/scheduler/benchmark/SchedulerBenchmarkUtils.java:152:32:  error: incompatible types: JobID cannot be converted to ExecutionAttemptID
08:46:27  [ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/scheduler/benchmark/SchedulerBenchmarkUtils.java:169:44:  error: incompatible types: JobID cannot be converted to ExecutionAttemptID
{code}
",,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21451,,,,,,,,,,,,,,,,,,,,,FLINK-21524,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 01 08:40:49 UTC 2021,,,,,,,,,,"0|z0o3i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/21 08:36;trohrmann;cc [~chesnay], [~Thesharing];;;","01/Mar/21 08:40;chesnay;benchmarks-master: cf3f5f1ff344390c2de9a21c453fd74064899d1a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionGraph metrics collide on restart,FLINK-21510,13361121,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Feb/21 07:48,22/Nov/21 21:39,13/Jul/23 08:07,22/Nov/21 21:39,,,,,,,,,1.15.0,,,,,,,Runtime / Coordination,,,,,0,reactive,,,,,"The ExecutionGraphBuilder registers several metrics directly on the JobManagerJobMetricGroup, which are never cleaned up.

These include upTime/DownTime/restartingTime as well as various checkpointing metrics (see the CheckpointStatsTracker for details; examples are number of checkpoints, checkpoint sizes etc).

When the AdaptiveScheduler re-creates the EG these will collide with metrics of prior attempts.

Essentially we either need to create a separate metric group that we pass to the EG or refactor the metrics to be based on some mutable EG reference.",,aitozi,rmetzger,Thesharing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21855,,,FLINK-21513,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21075,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 10 09:36:40 UTC 2021,,,,,,,,,,"0|z0o368:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 10:45;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:46;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","04/Jun/21 23:22;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","12/Jun/21 22:39;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","01/Sep/21 08:00;rmetzger;How much effort would it be fix this issue?

These are some important metrics, that are missing when using Adaptive Scheduler. ;;;","10/Sep/21 09:36;chesnay;A fair amount because we have to completely decouple these metrics from the ExecutionGraph (and thus, checkpoint coordinator) instances. Otherwise we lose these metrics temporarily when the AdaptiveScheduler restarts a job (i.e., they are not exposed for some time and we lose the current state).

Essentially we will need to do similar things like we do for task IO metrics; create a set of metrics that should be updated, and pass them to the component that should require them.

For the checkpoint coordinator this should work reasonably well (but we have to double-check the semantics of all metrics).

The job status metrics currently poll information from the ExecutionGraph. Ideally they would instead ask the Scheduler instead, so the AdaptiveScheduler needs it's own timetamp data-structure. Depending on how FLINK-21513 turns out the scheduler may also need to accumulate the durations of previous attempts (so you can for example get do totalUptime/totalRunTime).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""w.start"" returns ""1970-01-01"" when used with Pandas UDAF after grouping by slide window with processing time",FLINK-21509,13361119,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,zhongwei,zhongwei,26/Feb/21 07:37,02/Mar/21 03:29,13/Jul/23 08:07,02/Mar/21 03:29,1.13.0,,,,,,,,1.13.0,,,,,,,API / Python,,,,,0,pull-request-available,,,,,"""w.start"" returns ""1970-01-01"" when used with Pandas UDAF after grouping by slide window with processing time. Reproduce code:
{code:java}
t_env.get_config().get_configuration().set_string(""parallelism.default"", ""1"")
from pyflink.table.window import Slide
t_env.register_function(""mean_udaf"", mean_udaf)

source_table = """"""
    create table source_table(
        a INT,
        proctime as PROCTIME()
    ) with(
        'connector' = 'datagen',
        'rows-per-second' = '1',
        'fields.a.kind' = 'sequence',
        'fields.a.start' = '1',
        'fields.a.end' = '10'
    )
""""""
t_env.execute_sql(source_table)
t = t_env.from_path(""source_table"")
iterator = t.select(""a, proctime"") \
    .window(Slide.over(""1.seconds"").every(""1.seconds"").on(""proctime"").alias(""w"")) \
    .group_by(""a, w"") \
    .select(""mean_udaf(a) as b, w.start"").execute().collect()
result = [i for i in iterator]
{code}",,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 02 03:28:30 UTC 2021,,,,,,,,,,"0|z0o35s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/21 03:28;zhongwei;Fixed via f00da5561075990ea200f168bcdd0fa5b084bb41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests time out on azure,FLINK-21503,13360958,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,dwysakowicz,dwysakowicz,25/Feb/21 13:55,09/Mar/21 10:29,13/Jul/23 08:07,26/Feb/21 00:22,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / State Backends,Tests,,,,0,pull-request-available,test-stability,,,,"Both test_ci legacy_slot_management and test_ci finegrained_resource_management time out on azure

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13731&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9",,dwysakowicz,maguowei,roman,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21508,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 09 10:29:09 UTC 2021,,,,,,,,,,"0|z0o260:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/21 13:56;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13753&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","25/Feb/21 14:22;dwysakowicz;I think it might be caused by https://issues.apache.org/jira/browse/FLINK-19463 It is the biggest batch of changes after last successful run. Moreover I compared tests times of the failed case and the last successful one and I see some long running tests introduced in FLINK-19463 such as e.g. {{org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest}}

[~sjwiesman] Could you take a look?;;;","25/Feb/21 15:02;sjwiesman;[~dwysakowicz] I'll take a look today;;;","25/Feb/21 16:24;sjwiesman;Commenting as I go for anyone watching:

{{org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest}} is {{org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest. It was renamed to better align with the new public API but is functionally the same test suite. Its execution time appears to be on par with previous runs of RocksDBStateBackendTest}}{{.}}

 

Looking through the logs org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase appears to be the test that never finished. Continuing to investigate.;;;","25/Feb/21 16:39;sjwiesman;Can confirm {color:#000000}UnalignedCheckpointCompatibilityITCase hangs which is causing the failure. Looking into how my change interacts with this test. {color}

 

 ;;;","25/Feb/21 17:32;sjwiesman;{color:#000000}UnalignedCheckpointCompatibilityITCase successfully deploys the job and then hangs waiting for the checkpoint metadata to go appear. I have verified it is using HashMapStateBackend and FileSystemCheckpointStorage which proxies the same runtime data structures as FsStateBackend, the state backend used before FLINK-19463  was merged in. Continuing to investigate
{color};;;","26/Feb/21 10:27;roman;The fix merged to master as 6d2a05a72fc73c60473078a050b4ea4f79b61a27.;;;","08/Mar/21 04:03;maguowei;I found the 1.12 branch test is hanged by a `org.apache.flink.test.checkpointing.UnalignedCheckpointITCase`
Is it the same reason?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14234&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","09/Mar/21 10:29;roman;No, UnalignedCheckpointITCase is not related to this issue, [~maguowei].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobLeaderIdService completes leader future despite no leader being elected,FLINK-21497,13360876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,maguowei,maguowei,25/Feb/21 04:08,03/Mar/21 09:23,13/Jul/23 08:07,03/Mar/21 09:22,1.13.0,,,,,,,,1.11.4,1.12.3,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13722&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699
{code:java}
2021-02-24T22:47:55.4844360Z java.lang.RuntimeException: Failed to fetch next result
2021-02-24T22:47:55.4847421Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2021-02-24T22:47:55.4848395Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-02-24T22:47:55.4849262Z 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testBoundedTextFileSource(FileSourceTextLinesITCase.java:148)
2021-02-24T22:47:55.4850030Z 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testBoundedTextFileSourceWithJobManagerFailover(FileSourceTextLinesITCase.java:108)
2021-02-24T22:47:55.4850780Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-24T22:47:55.4851322Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-24T22:47:55.4858977Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-24T22:47:55.4860737Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-02-24T22:47:55.4861855Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-02-24T22:47:55.4862873Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-02-24T22:47:55.4863598Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-02-24T22:47:55.4864289Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-02-24T22:47:55.4864937Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-02-24T22:47:55.4865570Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-02-24T22:47:55.4866152Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-02-24T22:47:55.4866670Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-02-24T22:47:55.4867172Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-02-24T22:47:55.4867765Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-02-24T22:47:55.4868588Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-02-24T22:47:55.4869683Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-24T22:47:55.4886595Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-24T22:47:55.4887656Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-24T22:47:55.4888451Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-24T22:47:55.4889199Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-24T22:47:55.4889845Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-02-24T22:47:55.4890447Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-02-24T22:47:55.4891037Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-24T22:47:55.4891604Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-02-24T22:47:55.4892235Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-02-24T22:47:55.4892959Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-24T22:47:55.4893573Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-24T22:47:55.4894216Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-24T22:47:55.4894824Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-24T22:47:55.4895425Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-24T22:47:55.4896027Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-24T22:47:55.4896638Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-02-24T22:47:55.4897378Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-02-24T22:47:55.4898342Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-02-24T22:47:55.4899204Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-02-24T22:47:55.4899965Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-02-24T22:47:55.4900709Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-02-24T22:47:55.4901503Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-02-24T22:47:55.4902315Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-02-24T22:47:55.4903260Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-02-24T22:47:55.4903941Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-02-24T22:47:55.4904586Z Caused by: java.io.IOException: Failed to fetch job execution result
2021-02-24T22:47:55.4905378Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:169)
2021-02-24T22:47:55.4906286Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:118)
2021-02-24T22:47:55.4907230Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2021-02-24T22:47:55.4907998Z 	... 44 more
2021-02-24T22:47:55.4908618Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-02-24T22:47:55.4909533Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-02-24T22:47:55.4910233Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-02-24T22:47:55.4911035Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:167)
2021-02-24T22:47:55.4911650Z 	... 46 more
2021-02-24T22:47:55.4912141Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-02-24T22:47:55.4912942Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-02-24T22:47:55.4913955Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-02-24T22:47:55.4914902Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-02-24T22:47:55.4915610Z 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-02-24T22:47:55.4916316Z 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-02-24T22:47:55.4917123Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
2021-02-24T22:47:55.4918172Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:166)
2021-02-24T22:47:55.4918901Z 	... 46 more
2021-02-24T22:47:55.4919463Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Not enough resources available for scheduling.
2021-02-24T22:47:55.4920378Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$determineParallelismAndAssignResources$18(AdaptiveScheduler.java:585)
2021-02-24T22:47:55.4921197Z 	at java.util.Optional.orElseThrow(Optional.java:290)
2021-02-24T22:47:55.4921999Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.determineParallelismAndAssignResources(AdaptiveScheduler.java:582)
2021-02-24T22:47:55.4923217Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphWithAvailableResources(AdaptiveScheduler.java:598)
2021-02-24T22:47:55.4924249Z 	at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.createExecutionGraphWithAvailableResources(WaitingForResources.java:105)
2021-02-24T22:47:55.4925252Z 	at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.resourceTimeout(WaitingForResources.java:99)
2021-02-24T22:47:55.4926126Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:898)
2021-02-24T22:47:55.4926986Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$20(AdaptiveScheduler.java:913)
2021-02-24T22:47:55.4927768Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-02-24T22:47:55.4928488Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-02-24T22:47:55.4929180Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-02-24T22:47:55.4929946Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-02-24T22:47:55.4930725Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-02-24T22:47:55.4931493Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-02-24T22:47:55.4932187Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-02-24T22:47:55.4932944Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-02-24T22:47:55.4933558Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-02-24T22:47:55.4934198Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-02-24T22:47:55.4934856Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-02-24T22:47:55.4935498Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-24T22:47:55.4936136Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-24T22:47:55.4936744Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-02-24T22:47:55.4937321Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-02-24T22:47:55.4938023Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-02-24T22:47:55.4938569Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-02-24T22:47:55.4939154Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-02-24T22:47:55.4939701Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-02-24T22:47:55.4940205Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-02-24T22:47:55.4940958Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-02-24T22:47:55.4941788Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-02-24T22:47:55.4942569Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-02-24T22:47:55.4943274Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-02-24T22:47:55.4943685Z 
{code}",,dwysakowicz,maguowei,mapohl,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 09:22:53 UTC 2021,,,,,,,,,,"0|z0o1ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/21 07:24;mapohl;Thanks [~maguowei] for reporting this test instability. I'm moving this issue into FLINK-21075 where we collect issues related to the current FLIP-160 work;;;","25/Feb/21 10:48;chesnay;The underlying issue also occurs with the default scheduler.;;;","01/Mar/21 09:21;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13905&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699;;;","03/Mar/21 09:22;chesnay;master: 4542abba8b0743c5b121bf5fbbdcb2f492021aea
1.12: 8be24ee5c8b750765c0da0558f4928497d09a651
1.11: b61e01ac1644151ba02a4833181adb1383b1a9da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Testcontainers to 1.15.1 in Stateful Functions,FLINK-21496,13360870,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tzulitai,tzulitai,tzulitai,25/Feb/21 03:28,25/Feb/21 05:13,13/Jul/23 08:07,25/Feb/21 05:13,,,,,,,,,statefun-3.0.0,,,,,,,Stateful Functions,,,,,0,,,,,,"The E2E tests in CI is currently failing for StateFun, started failing recently due to Github Actions upgrading their Docker version to 20.10.2+.
Due to this upgrade, our current Testcontainers version 1.12.x is no longer compatible since that version relies on a deprecated Docker API that no longer exists in Docker version 10.10.2 (API version 1.41).

Full description of the issue: https://github.com/testcontainers/testcontainers-java/issues/3574",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 05:13:36 UTC 2021,,,,,,,,,,"0|z0o1mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/21 05:13;tzulitai;flink-statefun/master: ec762a7413c3f94470ee13d57edc47350feb1569;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ActiveResourceManager swallows exception stack trace,FLINK-21492,13360718,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,24/Feb/21 18:28,25/Feb/21 16:26,13/Jul/23 08:07,25/Feb/21 16:26,1.12.1,1.13.0,,,,,,,1.12.2,1.13.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The {{ActiveResourceManager}} swallows the exception stack trace [here|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/active/ActiveResourceManager.java#L297].",,guoyangze,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 16:26:45 UTC 2021,,,,,,,,,,"0|z0o16o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/21 16:26;trohrmann;Fixed via

1.13.0: 0834d6c099c2ab35f00facdfc793a32f66829557
1.12.2: a76ddef1aced79b92648957de7db81368465640e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase fails on azure,FLINK-21490,13360672,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,dwysakowicz,dwysakowicz,24/Feb/21 16:02,22/Jun/21 14:05,13/Jul/23 08:07,25/Feb/21 18:44,1.12.1,1.13.0,,,,,,,1.12.2,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13682&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2

{code}
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1066)
	at akka.dispatch.OnComplete.internal(Future.scala:264)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:130)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:81)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:692)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:434)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
Caused by: java.lang.IllegalArgumentException: Stream corrupted
	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.checkHeader(UnalignedCheckpointTestBase.java:903)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.withoutHeader(UnalignedCheckpointTestBase.java:897)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$MinEmittingFunction.flatMap2(UnalignedCheckpointITCase.java:459)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$MinEmittingFunction.flatMap2(UnalignedCheckpointITCase.java:429)
	at org.apache.flink.streaming.api.operators.co.CoStreamFlatMap.processElement2(CoStreamFlatMap.java:59)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.processRecord2(StreamTwoInputProcessorFactory.java:208)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.lambda$create$1(StreamTwoInputProcessorFactory.java:177)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessorFactory.java:278)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:205)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:175)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:97)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:407)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:190)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:623)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:587)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)


{code}",,AHeise,dwysakowicz,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 18:43:55 UTC 2021,,,,,,,,,,"0|z0o0wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/21 16:02;dwysakowicz;cc [~AHeise] [~pnowojski];;;","25/Feb/21 07:30;arvid;The error is probably test-only:
For some reason the test does not terminate after 10 successful checkpoints (to be investigated).

{noformat}
12:21:43,173 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 3165 (type=CHECKPOINT) @ 1614169303172 for job 78d8cb678ee2304d517c9e42bff43aea.
{noformat}

I suspect that we overflow {{MAX_INT}} in {{value}}, and then {{checkHeader}} fails as it uses the upper 4 bytes of the long. We have already hardened that part to give a meaningful exception in the {{UCRescaleITCase}}, but it might be a good idea to extract that to this ticket as that test will only go into master.

So for now I'd harden the test. There is also a related issue with unions that I initially suspected.;;;","25/Feb/21 18:43;arvid;Merged as 29abccd4cb7d4905fa168f8d7b68a113e9640fca^ in master and as 0c1b20d2119463d4571d17de607aebfff1b4b17f^ in 1.12.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hugo docs add two anchor links to headers,FLINK-21489,13360652,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,24/Feb/21 15:15,24/Feb/21 15:48,13/Jul/23 08:07,24/Feb/21 15:48,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 24 15:48:51 UTC 2021,,,,,,,,,,"0|z0o0s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/21 15:48;sjwiesman;fixed in master: 9b84132bece85ab663da1820f1bc23d18cbbd0e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jdbc XA sink - XID generation conflicts between jobs,FLINK-21488,13360640,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mobuchowski,mobuchowski,mobuchowski,24/Feb/21 14:39,11/Mar/21 13:17,13/Jul/23 08:07,11/Mar/21 13:17,1.13.0,,,,,,,,1.13.0,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,,"I'm using Flink 1.13's JDBC XA sink to write data to oracle DB using exactly once semantics.
I want to have two jobs doing this. One is working right now. When starting second one, I encountered errors:

org.apache.flink.util.FlinkRuntimeException: unable to start XA transaction, xid: 201:0600000000000000:9b1d1b84e8ce79bb, error -3: resource manager error has occurred. [XAErr (-3): A resource manager error has occured in the transaction branch. ORA-2079 SQLErr (0)]

Oracle description:
ORA-02079: cannot join a committing distributed transaction
    Cause: Once a transaction branch is prepared, no more new transaction branches are allowed to start, nor is the prepared transaction branch allowed to be joined.
    Action: Check the application code as this is an XA protocol violation.

I've looked at the implementation of XID generation and noticed following line:

private transient byte[] gtridBuffer; // globalTransactionId = checkpoint id (long)

My hypothesis is that second job generated xid that referred to global transaction id that the first job created. If I'm right, then I'd suppose fix would rely on embedding part of job id inside of gtridBuffer.",,mobuchowski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 13:17:56 UTC 2021,,,,,,,,,,"0|z0o0pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/21 13:53;mobuchowski;Turns out getting jobId from runtime context is not that easy. In my local fix I've used random 4 bytes generated by job manager on job startup.

If this solution is acceptable, I can provide PR.;;;","28/Feb/21 17:41;mobuchowski;Sorry for bothering, [~roman] but can you take a look at this issue?;;;","28/Feb/21 21:54;roman;Thanks for reporting this issue [~mobuchowski].

XID generation is apparently incorrect.

I think swapping global and local branch IDs would work.

Job ID, which is 128 bits, unfortunately doesn't fit into the global branch identifier (64 bits).;;;","28/Feb/21 22:24;mobuchowski;XID global/branch identifiers max sizes are defined as 64 bytes, not bits, so it would fit. Not sure if there are any performance implications of using longer or shorter ids.

However, seems to me that swapping those would be a simpler solution. 
I'll test it tomorrow and will be able to provide PR after.;;;","28/Feb/21 22:29;roman;Yes, you are right, thanks!;;;","01/Mar/21 11:07;mobuchowski;Unfortunately, swapping those does not work. Using statically generated bytes as global transaction id makes second checkpoint fail - as Oracle treats this as starting second transaction branch with already prepared transaction.  

So, I believe that global transaction id has to be well, globally unique. And branch id is kind of irrelevant here as every transaction that this sink makes has only one branch.

My proposed solutions right now are:
1) Keep ""semantic"" semantics of current generator, and make gtrId combination of job id, task index and checkpoint id. Problem with this solution is that we're adding 16 bytes to gtrId, and that getting job id in runtime is not trivial: [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/How-to-get-flink-JobId-in-runtime-td36756.html] 
2) Add 4 or 8 byte static random component to gtrId. 
3) Make gtrId random on each generateXid call.;;;","01/Mar/21 20:34;roman;The 1st options seems preferable to me as it guarantees uniqueness in most cases (not all, however).

I published a PR to simplify access to JobID via RuntimeContext: [https://github.com/apache/flink/pull/15053];;;","02/Mar/21 11:19;mobuchowski;Thanks. Will push PR as soon as it's merged.;;;","11/Mar/21 13:17;roman;Merged into master as `cb987a114a5a496ef3399507a547f83140c20d9f` .. `46546c4887c226a32da8ecfaee42ede0749ef4b6`.

Thanks for the fix [~mobuchowski] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Internal enum CheckpointType is exposed via REST API, making it part of Public API",FLINK-21484,13360604,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,24/Feb/21 12:07,19/Apr/21 07:10,13/Jul/23 08:07,19/Apr/21 07:10,1.12.1,1.13.0,,,,,,,1.12.2,1.13.0,,,,,,Runtime / REST,,,,,0,stale-assigned,,,,,{{org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics}} is using and returning/exposing the internal {{CheckpointType}} enum freezing any development and changes to ti. It has to be changed by providing a translation layer. This was caused by FLINK-18851,,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18851,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 16 10:45:32 UTC 2021,,,,,,,,,,"0|z0o0hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/21 17:58;pnowojski;Merged to master as fbdc2c0166b
Merged to release-1.12 as e3657ab91d0;;;","16/Apr/21 10:45;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop-with-savepoint --drain doesn't advance watermark for sources chained to MultipleInputStreamTask,FLINK-21469,13360390,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ym,pnowojski,pnowojski,23/Feb/21 17:53,28/Aug/21 11:21,13/Jul/23 08:07,12/May/21 12:22,1.12.1,1.13.0,,,,,,,1.12.5,1.13.1,1.14.0,,,,,API / DataStream,Runtime / Task,,,,0,pull-request-available,,,,,{{StreamTask#advanceToEndOfEventTime()}} is used to bump the watermark to {{MAX_WATERMARK}}. However this method is only implemented for the source tasks (legacy and FLIP-27). Because of that watermarks are not advanced to end of time if job has sources chained to {{MultipleInputStreamTask}}.,,dwysakowicz,kezhuw,klion26,pnowojski,stevenz3wu,trohrmann,ym,,,,,,,,,,,,,,,,,,,,FLINK-21133,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 12 12:22:21 UTC 2021,,,,,,,,,,"0|z0nzaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 12:24;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","27/Apr/21 07:50;ym;working on this. Mark to prevent deprioritizing.;;;","10/May/21 04:55;ym;For stop with save point, `StreamTask#advanceToEndOfEventTime()`  is called (in source tasks) to advance to the max watermark; However, this is not the case for chained sources in `MultipleInputStreamTask` since `advanceToEndOfEventTime` is not implemented. This PR is to fix this problem.

*The fix is as follows:*

Consider `MultipleInputStreamTask` as a chained task with some inputs are sources(StreamTaskSourceInput) and the rest from network(StreamTaskNetworkInput). `MAX_WATERMARK` is injected from the source when advanceToEndOfEventTime is called, together with `MAX_WATERMARK` propagated from the NetworkInput, `MultipleInputStreamTask` emits `MAX_WATERMARK`.;;;","12/May/21 12:22;dwysakowicz;Fixed in:
* master
** 62f91de993eb1c74ef92a42608e688f5cb711724
* 1.13.1
** 64af77b8e239c9f0442d1200f1d36147633bbe5c
* 1.12.5
** 9541b4c72a1b7d0bcf8b0a5720d7ac17883b9a8a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedOneInput.endInput is NOT called when doing stop with savepoint WITH drain,FLINK-21453,13360289,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,23/Feb/21 08:58,26/Feb/21 09:02,13/Jul/23 08:07,25/Feb/21 10:32,1.11.4,1.12.2,1.13.0,,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,,"In FLINK-21132 we disable {{endInput}} calls when stopping with savepoint. However as discussed in [FLINK-21133|https://issues.apache.org/jira/browse/FLINK-21133?focusedCommentId=17288467&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17288467], stop with savepoint with drain (*stop-with-savepoint --drain*), should be calling {{endOfInput()}}.",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21132,,,,,,,,,,,,,,,,,,,,,FLINK-21133,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 09:02:46 UTC 2021,,,,,,,,,,"0|z0nyog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/21 17:59;pnowojski;Merged to master as f322de701a5^, f322de701a5 and 63dc7f7a558;;;","26/Feb/21 09:02;pnowojski;merged to release-1.12 as 9642555fed5 and d0126f67b09
merged to release-1.11 as 7cb2154d7fe and 1196dff8f16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-27 sources cannot reliably downscale,FLINK-21452,13360280,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,arvid,AHeise,23/Feb/21 08:27,22/Jun/21 14:07,13/Jul/23 08:07,25/Feb/21 19:13,1.12.1,1.13.0,,,,,,,1.12.2,1.13.0,,,,,,Connectors / Common,,,,,0,pull-request-available,,,,,"Sources currently store their registered readers into the snapshot. However, when downscaling, there are unmatched readers we violate a couple of invariants.

The solution is to not store registered readers - they are re-registered anyways on restart.

To keep it backward compatible, the best option is to always store an empty set of readers while writing the snapshot and discard any recovered readers from the snapshot.",,AHeise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 19:12:57 UTC 2021,,,,,,,,,,"0|z0nymg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/21 19:12;arvid;Merged as fb99ce2e22ca84dece1f7a431a92a4cecb6a71f2^ in 1.12 and as 81cfe465c9e4a17e563e1b4c02cd60a63b984de5^ in master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application mode does not set the configuration when building PackagedProgram,FLINK-21445,13360241,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,wangyang0918,wangyang0918,23/Feb/21 06:00,18/Apr/23 13:05,13/Jul/23 08:07,07/Jul/21 18:28,1.11.3,1.12.1,1.13.0,,,,,,1.12.5,1.13.2,1.14.0,,,,,Deployment / Kubernetes,Deployment / Scripts,Deployment / YARN,,,0,auto-unassigned,pull-request-available,,,,"Application mode uses {{ClassPathPackagedProgramRetriever}} to create the {{PackagedProgram}}. However, it does not set the configuration. This will cause some client configurations not take effect. For example, {{classloader.resolve-order}}.

I think we just forget to do this since we have done the similar thing in {{CliFrontend}}.",,klion26,mapohl,nicholasjiang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,FLINK-22490,,,,,,,,,,FLINK-31844,FLINK-22490,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 06 11:08:02 UTC 2021,,,,,,,,,,"0|z0nye0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/21 07:14;mapohl;Thanks for creating this issue. Your finding makes sense after looking into the code. I assume that the configuration would be necessary in all three cases which are currently handled by {{ClassPathPackagedProgramRetriever}}. Taking this into account I am wondering whether we should refactor {{ClassPathPackagedProgramRetriever}} a bit to have separate classes for the Python-based {{PackagedProgram}} and the {{PackagedProgram}} with and without a jar. This way, we would avoid redundant test implementation. Right now, there is no testing for the Python-based implementation as far as I can see.

[~fly_in_gis] does that make sense?;;;","23/Feb/21 07:56;wangyang0918;I think it is a good idea to do this minor refactor. Just like you said, we have three cases to build the {{PackagedProgram}}. And the refactor will make the code easier to maintain and test.
 * Python based
 * Specified jar file
 * Scan the classpath for job class;;;","24/Feb/21 05:56;nicholasjiang;[~fly_in_gis], [~mapohl], I would like to do a bit refactoring for ClassPathPackagedProgramRetriever as [~mapohl] mentioned. If [~mapohl] aren't available, I will work for this issue.;;;","24/Feb/21 06:41;mapohl;Thanks for taking over here, [~nicholasjiang]. It was on my list but I'm still busy with FLINK-21030. I assigned the ticket to you.;;;","24/May/21 22:50;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","09/Jun/21 22:41;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","22/Jun/21 02:30;wangyang0918;Thanks [~mapohl] for taking this ticket over. I would like to help with reviewing.;;;","06/Jul/21 11:08;chesnay;master:
a8ec5991e780833ffa9b5ca21a2516d6a6dd67ea
012dc6a9b800bae0cfa5250d38de992ccbabc015
1.13:
848d4baf7054781de82d731d6ed60bb10d44d8e0 
1.12:
bb29f5e3b155f8667f67d65bd6d6bbc8307c0c2b ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jdbc XA sink - restore state serialization error,FLINK-21442,13360125,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mobuchowski,mobuchowski,mobuchowski,22/Feb/21 14:33,25/Feb/21 11:15,13/Jul/23 08:07,25/Feb/21 11:15,1.13.0,,,,,,,,1.13.0,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,,"There are state restoration errors connected to XaSinkStateSerializer with it's implementation of SNAPSHOT using anonymous inner class, which is not restorable due to not being public.",,mobuchowski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 11:15:02 UTC 2021,,,,,,,,,,"0|z0nxo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/21 11:15;roman;Merged to master as eb0c19dac1644f11430da7b30b4fb9f828be7464.
Thanks for the fix [~mobuchowski];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When UDAF return ROW type, and the number of fields is more than 14, the crash happend",FLINK-21434,13360058,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhongwei,awayne,awayne,22/Feb/21 09:53,04/Mar/21 06:28,13/Jul/23 08:07,04/Mar/21 06:28,1.12.1,,,,,,,,1.12.3,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,," Code(a simple udaf to return a Row containing 15 fields):
{code:python}
from pyflink.common import Row
from pyflink.table.udf import AggregateFunction, udaf
from pyflink.table import DataTypes, EnvironmentSettings, StreamTableEnvironment

class Test(AggregateFunction):

  def create_accumulator(self):
    return Row(0, 0)

  def get_value(self, accumulator):
    return Row(1.23, 1.23, 1.23, 1.23, 1.23, 1.23, 1.23,
               1.23, 1.23, 1.23, 1.23, 1.23, 1.23, 1.23, 1.23)

  def accumulate(self, accumulator, a, b):
    pass

  def get_result_type(self):
    return DataTypes.ROW([
        DataTypes.FIELD(""f1"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f2"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f3"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f4"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f5"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f6"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f7"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f8"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f9"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f10"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f11"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f12"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f13"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f14"", DataTypes.FLOAT()),
        DataTypes.FIELD(""f15"", DataTypes.FLOAT())
    ])

  def get_accumulator_type(self):
    return DataTypes.ROW([
        DataTypes.FIELD(""f1"", DataTypes.BIGINT()),
        DataTypes.FIELD(""f2"", DataTypes.BIGINT())])


def udaf_test():
  env_settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
  table_env = StreamTableEnvironment.create(environment_settings=env_settings)
  test = udaf(Test())
  table_env.execute_sql(""""""
      CREATE TABLE print_sink (
          `name` STRING,
          `agg` ROW<f1 FLOAT, f2 FLOAT, f3 FLOAT, f4 FLOAT,
                    f5 FLOAT, f6 FLOAT, f7 FLOAT, f8 FLOAT,
                    f9 FLOAT, f10 FLOAT, f11 FLOAT, f12 FLOAT,
                    f13 FLOAT, f14 FLOAT, f15 FLOAT>
      ) WITH (
          'connector' = 'print'
      )
  """""")
  table = table_env.from_elements([(1, 2, ""Lee"")], ['value', 'count', 'name'])
  result_table = table.group_by(table.name)\
                      .select(table.name, test(table.value, table.count))
  result_table.execute_insert(""print_sink"").wait()


if __name__ == ""__main__"":
  udaf_test()

{code}
Exception:
{code:java}
Caused by: java.io.EOFException
	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)
	at java.base/java.io.DataInputStream.readFloat(DataInputStream.java:451)
	at org.apache.flink.api.common.typeutils.base.FloatSerializer.deserialize(FloatSerializer.java:72)
	at org.apache.flink.api.common.typeutils.base.FloatSerializer.deserialize(FloatSerializer.java:30)
	at org.apache.flink.table.runtime.typeutils.serializers.python.RowDataSerializer.deserialize(RowDataSerializer.java:106)
	at org.apache.flink.table.runtime.typeutils.serializers.python.RowDataSerializer.deserialize(RowDataSerializer.java:49)
	at org.apache.flink.table.runtime.typeutils.serializers.python.RowDataSerializer.deserialize(RowDataSerializer.java:106
{code}","python 3.7.5

pyflink 1.12.1",awayne,dian.fu,hxbks2ks,Leo Zhou,libenchao,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 06:28:17 UTC 2021,,,,,,,,,,"0|z0nx9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/21 06:28;zhongwei;Fixed via:

 - master: 1b0e592cf56005bff430252b0d59b8b6da1347f8

 - release-1.12: 591cd3cf5e65d6726178652107c6751c5705141a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerSlotSharingITCase.testSchedulingOfJobRequiringSlotSharing fail,FLINK-21428,13359971,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,maguowei,maguowei,22/Feb/21 02:43,10/Mar/21 10:51,13/Jul/23 08:07,10/Mar/21 10:51,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13510&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865
{code:java}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 23.313 s <<< FAILURE! - in org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase [ERROR] testSchedulingOfJobRequiringSlotSharing(org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase) Time elapsed: 20.683 s <<< ERROR! org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) at org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase.runJob(DeclarativeSchedulerSlotSharingITCase.java:83) at org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase.testSchedulingOfJobRequiringSlotSharing(DeclarativeSchedulerSlotSharingITCase.java:71) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
{code}",,maguowei,rmetzger,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21536,,,FLINK-21653,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/21 08:05;mapohl;FLINK-21428-failure.trace.log;https://issues.apache.org/jira/secure/attachment/13021551/FLINK-21428-failure.trace.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 10 10:51:50 UTC 2021,,,,,,,,,,"0|z0nwqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/21 12:34;rmetzger;I observed this too: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8903&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b;;;","23/Feb/21 07:13;rmetzger;Is this the same issue ? https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8910&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b {{HistoryServerTest.testRemainExpiredJob}} runs a job 3 times in a row, and fails with:

{code}
2021-02-22T21:07:03.2332599Z [ERROR] testRemainExpiredJob[Flink version less than 1.4: true](org.apache.flink.runtime.webmonitor.history.HistoryServerTest)  Time elapsed: 13.125 s  <<< ERROR!
2021-02-22T21:07:03.2333649Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-02-22T21:07:03.2334391Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-02-22T21:07:03.2335245Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-02-22T21:07:03.2336066Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-02-22T21:07:03.2336801Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-02-22T21:07:03.2337748Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-02-22T21:07:03.2338484Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-02-22T21:07:03.2339273Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
2021-02-22T21:07:03.2340063Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-02-22T21:07:03.2340818Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-02-22T21:07:03.2341556Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-02-22T21:07:03.2342278Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-02-22T21:07:03.2343004Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1066)
2021-02-22T21:07:03.2343682Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2021-02-22T21:07:03.2345261Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-02-22T21:07:03.2345779Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-02-22T21:07:03.2346297Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-02-22T21:07:03.2346814Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-02-22T21:07:03.2347412Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-02-22T21:07:03.2348039Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-02-22T21:07:03.2348624Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-02-22T21:07:03.2349174Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-02-22T21:07:03.2349766Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2021-02-22T21:07:03.2350434Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-02-22T21:07:03.2351157Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-02-22T21:07:03.2351712Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-02-22T21:07:03.2352251Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-02-22T21:07:03.2352829Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-02-22T21:07:03.2353496Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-02-22T21:07:03.2354173Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-02-22T21:07:03.2354832Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-02-22T21:07:03.2355444Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-02-22T21:07:03.2356028Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-02-22T21:07:03.2356606Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-02-22T21:07:03.2357222Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-02-22T21:07:03.2357846Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-02-22T21:07:03.2358415Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-02-22T21:07:03.2359157Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-02-22T21:07:03.2359744Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-02-22T21:07:03.2360383Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Not enough resources available for scheduling.
2021-02-22T21:07:03.2361194Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$determineParallelismAndAssignResources$18(AdaptiveScheduler.java:584)
2021-02-22T21:07:03.2361867Z 	at java.util.Optional.orElseThrow(Optional.java:290)
2021-02-22T21:07:03.2362598Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.determineParallelismAndAssignResources(AdaptiveScheduler.java:581)
2021-02-22T21:07:03.2363423Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphWithAvailableResources(AdaptiveScheduler.java:597)
2021-02-22T21:07:03.2366031Z 	at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.createExecutionGraphWithAvailableResources(WaitingForResources.java:105)
2021-02-22T21:07:03.2367062Z 	at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.resourceTimeout(WaitingForResources.java:99)
2021-02-22T21:07:03.2367924Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:897)
2021-02-22T21:07:03.2368768Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$20(AdaptiveScheduler.java:912)
2021-02-22T21:07:03.2369554Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-02-22T21:07:03.2370238Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-02-22T21:07:03.2371437Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-02-22T21:07:03.2372210Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-02-22T21:07:03.2372998Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-02-22T21:07:03.2373911Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-02-22T21:07:03.2374808Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-02-22T21:07:03.2376929Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-02-22T21:07:03.2378639Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-02-22T21:07:03.2379351Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-02-22T21:07:03.2380046Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-02-22T21:07:03.2380905Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-22T21:07:03.2381574Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-22T21:07:03.2383110Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-02-22T21:07:03.2383786Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-02-22T21:07:03.2384446Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-02-22T21:07:03.2385113Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-02-22T21:07:03.2385758Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-02-22T21:07:03.2386361Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-02-22T21:07:03.2386926Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-02-22T21:07:03.2387435Z 	... 4 more
{code};;;","02/Mar/21 05:27;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13954&view=logs&j=77009337-525b-5cd6-c533-94b7ca20542a&t=7ab53e5a-ac96-590c-10c5-9a31a346f79d;;;","03/Mar/21 06:03;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14014&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107;;;","08/Mar/21 03:29;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14263&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9

;;;","10/Mar/21 10:51;chesnay;master:
a4e0efdf6e0cabdd0c477c76ca656ca22ad7a685
0f02ac186b18674e89a648e29a7aaa8f6f20d9ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileBufferReaderITCase.testSequentialReading fails on azure,FLINK-21416,13359539,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,dwysakowicz,dwysakowicz,19/Feb/21 08:28,26/Mar/21 15:39,13/Jul/23 08:07,26/Mar/21 15:39,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13473&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a

{code}
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:811)
	at org.apache.flink.runtime.io.network.partition.FileBufferReaderITCase.testSequentialReading(FileBufferReaderITCase.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:117)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:79)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:650)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:81)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer

{code}",,aitozi,dwysakowicz,gaoyunhaii,kevin.cyj,maguowei,mapohl,pnowojski,rmetzger,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19925,,,,,,,,,,,,,,,,FLINK-21967,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 15:39:47 UTC 2021,,,,,,,,,,"0|z0nu2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/21 14:17;pnowojski;[~kevin.cyj], could you or someone from your team take a look?;;;","26/Feb/21 12:12;mapohl;Another build failed due to this issue: https://dev.azure.com/mapohl/flink/_build/results?buildId=290&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b&l=8167;;;","08/Mar/21 02:53;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14263&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a
;;;","08/Mar/21 02:58;kevin.cyj;Sorry for the late reply, I think it is the same issue with FLINK-19925. Previously, I spent some time looking into the issue, but no suspicion was found. I guess the cause is purely unstable network connection. I can speed more time to verify that. But I think it is not a blocker issue.;;;","09/Mar/21 20:40;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=327&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a;;;","10/Mar/21 07:17;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14362&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107;;;","11/Mar/21 12:04;rmetzger;{code}
Caused by: org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandshakeTimeoutException: handshake timed out after 10000ms
	at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$5.run(SslHandler.java:2054)
	... 8 more

{code}
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8966&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b;;;","12/Mar/21 05:34;maguowei;another case:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14500&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a;;;","12/Mar/21 06:30;yunta;Another instance but failed with timeout

https://myasuka.visualstudio.com/flink/_build/results?buildId=272&view=logs&j=cc649950-03e9-5fae-832;;;","12/Mar/21 06:45;mapohl;Another one:
https://dev.azure.com/mapohl/flink/_build/results?buildId=337&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a&l=6879;;;","12/Mar/21 10:29;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14500&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a&l=6787;;;","14/Mar/21 12:46;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14599&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a&l=6911;;;","16/Mar/21 12:57;dwysakowicz;Hey [~reswqa] any progress on the issue?;;;","16/Mar/21 13:12;pnowojski;The amount of those failures is a bit suspicious. Has something changed recently either in the tests setup or the blocking partition that could be related?

I have suspicion, especially after:
{noformat}
Caused by: org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandshakeTimeoutException: handshake timed out after 10000ms
	at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$5.run(SslHandler.java:2054)
	... 8 more
{noformat}
that this might be cause by us doing blocking io in the netty threads when using blocking partition. This handshake time out could be easily explained by that. Also in the 5 reported failures that I checked, ""connection reset by peer"" happens around 60s into the test. Maybe this is also a problem where server frozen for x seconds, causing the client to timeout (error got lost? maybe it's in the logs?), and server side detected this and failed with ""connection reset by peer""?

Another pointer, it seems like all of the failures happened with SSL enabled.

I don't know why has it started to fail now so frequently. Maybe something changed in the test setup or in the environment. I was always afraid that doing a blocking IO in the netty threads can cause problems, but that's not something we can easily change (assuming this is causing those issues). Maybe we can speed up the test? Make it lighter? Or maybe we can increase some timeouts (related to ssl?) either in Netty or in tcp stack? What worries me is that apparently this issue is happening not only in the ITCase here, but also on (cluster?) benchmarks?;;;","17/Mar/21 02:48;kevin.cyj;> Maybe we can speed up the test? Make it lighter?

I think [~pnowojski]'s suggestion is worth trying. This test blocking shuffle several gigabytes of data with parallelism of 8. Besides, SSL encryption can also consume lots of CPU time. As mentioned by [~pnowojski], maybe something changed in the test environment and after the change it's too heavy for the test to run?;;;","17/Mar/21 04:06;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14845&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a&l=6948;;;","17/Mar/21 07:53;trohrmann;Another instance: https://dev.azure.com/tillrohrmann/flink/_build/results?buildId=525&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a;;;","22/Mar/21 03:19;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15115&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=6999;;;","22/Mar/21 10:33;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8998&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a;;;","22/Mar/21 14:16;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=355&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b&l=8354;;;","24/Mar/21 04:02;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15316&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=8425;;;","24/Mar/21 12:12;gaoyunhaii;Hi all, based on the suggestions in some offline discussion, I tried the case with HDD device and now I mainly suspect it is caused by the handshake timeout of the _SSLHandler:_ after the sink tasks started, it starts to connect to, send request and read buffers from the upstream taskmanagers (in the minicluster). Each TM only has one Netty thread, but has to serve 7 tasks. Since different tasks might send request in different time and Netty also need to read buffers from disk, so it is possible when one task send request and start SSL handshake, the Netty thread is loaded with disk IO and thus finally cause handshake timeout.

 

The evidences for the suspicion are that
 # For all the above failed azure tests, the failover happens about 10s after the log ""Converting recovered input channels"", which is right before the downstream tasks send requests, and 10s is the default handshake timeout of SSLHandler
 # On my Mac by manually set handshake timeout to 10 milliseconds via explicitly set SecurityOptions.SSL_INTERNAL_HANDSHAKE_TIMEOUT, it could reproduce the ""connection reset by peer"" case (but since both client and server side will check the timeout, sometime the client first detect timeout and the job failed with ""handshake timeout""), and in one ubuntu docker it could reproduce the CloseChannelException of the last case attached by [~maguowei].
 # If we manually add sleep to buffer reader after all the `PartitionRequest` are processed, there would not any failover no matter how long the sleep are set, which indicates that it is unlike due to timeout in data transmission.

To fix this issue, I think we could first try to increase SecurityOptions.SSL_INTERNAL_HANDSHAKE_TIMEOUT. ;;;","26/Mar/21 15:39;pnowojski;I've merged SSL timeout bump to master
> merged commit 6834ecd into apache:master

let's see if it solves the problem for this test. Further more let's document this issue and wait if it will ever becomes a problem for real world users.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TtlMapState and TtlListState cannot be clean completely with Filesystem StateBackend,FLINK-21413,13359494,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wind_ljy,wind_ljy,wind_ljy,19/Feb/21 03:13,28/Aug/21 11:11,13/Jul/23 08:07,31/Mar/21 10:22,1.9.0,,,,,,,,1.13.0,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,,"Take the #TtlMapState as an example,

 
{code:java}
public Map<UK, TtlValue<UV>> getUnexpiredOrNull(@Nonnull Map<UK, TtlValue<UV>> ttlValue) {
        Map<UK, TtlValue<UV>> unexpired = new HashMap<>();
        TypeSerializer<TtlValue<UV>> valueSerializer =
                ((MapSerializer<UK, TtlValue<UV>>) original.getValueSerializer()).getValueSerializer();
        for (Map.Entry<UK, TtlValue<UV>> e : ttlValue.entrySet()) {
                if (!expired(e.getValue())) {
                        // we have to do the defensive copy to update the value
                        unexpired.put(e.getKey(), valueSerializer.copy(e.getValue()));
                }
        }
        return ttlValue.size() == unexpired.size() ? ttlValue : unexpired;
}
{code}
 

The returned value will never be null and the #StateEntry will exists forever, which leads to memory leak if the key's range of the stream is very large. Below we can see that 20+ millison uncleared TtlStateMap could take up several GB memory.

 

!image-2021-02-19-11-13-58-672.png!",,fanrui,hackergin,klion26,libenchao,liyu,Paul Lin,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/21 03:14;wind_ljy;image-2021-02-19-11-13-58-672.png;https://issues.apache.org/jira/secure/attachment/13020673/image-2021-02-19-11-13-58-672.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 31 10:22:23 UTC 2021,,,,,,,,,,"0|z0ntsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/21 06:10;liyu;Indeed, thanks for reporting the issue [~wind_ljy]. Would you like to work on this and supply a PR to fix it? Thanks.;;;","20/Feb/21 08:59;wind_ljy;Sure. I'm still thinking about the solution. I've come up with two proposals:

1. #TtlMapState#getUnexpiredOrNull returns null directly if the {{unexpired.size() == 0}}, but this will also clean the empty map. 
2. Add a lastAccessTimestamp(long) in #TtlMapState like what Flink does in TtlValue, and use the timestamp to check the map's lifecyle. But this may break the state compatibility.

What do you think? [~liyu];;;","21/Feb/21 07:33;liyu;Checking the [state TTL FLIP document|https://cwiki.apache.org/confluence/display/FLINK/FLIP-25%3A+Support+User+State+TTL+Natively#FLIP25:SupportUserStateTTLNatively-TTLbehaviour] I cannot find description on whether TTL for a whole map is supported for {{MapState}}, but according to the current implementation the answer is no (TTL is only checked against value of each map entry). What's more, in [HeapMapState#remove|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapMapState.java#L130-L132] we could see the whole map will be removed if become empty, so I don't think {{TtlIncrementalCleanup}} need to take care of the empty map case.

Accordingly, I think we should have a fast path in {{TtlMapState#getUnexpiredOrNull}} to check whether {{ttlValue}} is empty and return it directly (instead of returning {{NULL}}) if so, and returning {{NULL}} iif {{ttlValue}} is not empty but all values expired ({{unexpired.size()}} is zero).

And similar logic should be applied to {{TtlListState#getUnexpiredOrNull}}.

Please let me know your thoughts [~wind_ljy]. Thanks.;;;","22/Feb/21 03:11;wind_ljy;[~liyu] Oops... I didn't notice that the #remove(UK) method would clear the empty map. I think you're right, we can expire the {{StateEntry}} once all {{ttlValue}} are expired. ;;;","22/Feb/21 07:57;liyu;[~wind_ljy] would you like to work on the PR following the above solution? I will assign this JIRA to you if so and help review the PR, or just feel free to let me know if you're not available at this moment.;;;","22/Feb/21 09:03;wind_ljy;[~liyu] Yes. I'll submit a PR soon. ;;;","22/Feb/21 14:57;liyu;Thanks [~wind_ljy], JIRA assigned.;;;","31/Mar/21 10:22;liyu;Merged into master via 1e17621e65bfa8f4f3aff379118654ed77a82bd2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyflink DataTypes.DECIMAL is not available,FLINK-21412,13359492,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dian.fu,awayne,awayne,19/Feb/21 03:07,26/Feb/21 23:00,13/Jul/23 08:07,22/Feb/21 01:55,1.12.0,,,,,,,,1.12.2,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,"when i use DataTypes.DECIMAL in udaf
File ""/home/ubuntu/pyflenv/lib/python3.7/site-packages/pyflink/table/types.py"", line 2025, in _to_java_data_type
 _to_java_data_type(data_type._element_type))
 File ""/home/ubuntu/pyflenv/lib/python3.7/site-packages/pyflink/table/types.py"", line 1964, in _to_java_data_type
 j_data_type = JDataTypes.Decimal(data_type.precision, data_type.scale)
 File ""/home/ubuntu/pyflenv/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1516, in __getattr__
 ""\{0}.\{1} does not exist in the JVM"".format(self._fqn, name))
py4j.protocol.Py4JError: org.apache.flink.table.api.DataTypes.Decimal does not exist in the JVM

 

in pyflink\table\types.py

line 1963-1964

elif isinstance(data_type, DecimalType):
    j_data_type = JDataTypes.{color:#FF0000}Decimal{color}(data_type.precision, data_type.scale)

in java should be called ""DECIMAL""","python 3.7.5

pyflink 1.12.1",awayne,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 01:55:48 UTC 2021,,,,,,,,,,"0|z0nts0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/21 01:55;dian.fu;Fixed in 
- master via d86cf91908ecbd5c3d3bbd9377b041af8525e581
- release-1.12 via 698cfdb95fffc600c1bbb10f8e097a51ec60c132;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet DECIMAL logical type is not properly supported in ParquetSchemaConverter,FLINK-21388,13359024,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,17/Feb/21 09:47,15/Mar/21 02:33,13/Jul/23 08:07,15/Mar/21 02:33,,,,,,,,,1.12.3,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"For a parquet field such as this one 
{code:java}
optional int32 ss_wholesale_cost (DECIMAL(7,2));{code}
_ParquetSchemaConverter_ raises the exception:
{code:java}
Unsupported original type : DECIMAL for primitive type INT32{code}

whereas it should be mapped to flink _DECIMAL_ Datatype (_BIG_DEC_ type information) owing to https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/formats/parquet.html#data-type-mapping 
and https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/types.html#old-planner",,echauchot,libenchao,lzljs3620320,rmetzger,ZhenqiuHuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 15 02:33:01 UTC 2021,,,,,,,,,,"0|z0nqw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/21 10:10;echauchot;Hi [~rmetzger], can you assign this ticket to me please ?;;;","17/Feb/21 10:38;rmetzger;Assigned. Happy coding!;;;","17/Feb/21 11:02;echauchot;Thanks Robert !;;;","17/Feb/21 11:03;echauchot;can you also assign to me [the other|https://issues.apache.org/jira/browse/FLINK-21389] related ticket ?;;;","17/Feb/21 12:06;echauchot;For the reference: [parquet DECIMAL logical type definition |https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#decimal];;;","17/Feb/21 19:24;ZhenqiuHuang;[~echauchot]
Thanks for reporting this bug.;;;","12/Mar/21 02:56;lzljs3620320;master (1.13): cde2cf102ae865702e624d89387665cc2d9f3763;;;","15/Mar/21 02:33;lzljs3620320;release-1.12: 43f7364b027ac20c1458b3245867cc3476e72681;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherTest.testInvalidCallDuringInitialization times out on azp,FLINK-21387,13359010,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,dwysakowicz,dwysakowicz,17/Feb/21 08:29,26/Mar/21 11:32,13/Jul/23 08:07,26/Mar/21 11:32,1.13.0,,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13388&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
2021-02-16T23:46:56.7270697Z [ERROR] testInvalidCallDuringInitialization(org.apache.flink.runtime.dispatcher.DispatcherTest)  Time elapsed: 6.691 s  <<< ERROR!
2021-02-16T23:46:56.7271801Z org.junit.runners.model.TestTimedOutException: test timed out after 5000 milliseconds
2021-02-16T23:46:56.7272538Z 	at sun.misc.Unsafe.park(Native Method)
2021-02-16T23:46:56.7273100Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2021-02-16T23:46:56.7273854Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2021-02-16T23:46:56.7274939Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2021-02-16T23:46:56.7275654Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2021-02-16T23:46:56.7287739Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-02-16T23:46:56.7289295Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest.lambda$testInvalidCallDuringInitialization$1(DispatcherTest.java:438)
2021-02-16T23:46:56.7290531Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest$$Lambda$156/1546789696.get(Unknown Source)
2021-02-16T23:46:56.7291284Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2021-02-16T23:46:56.7292120Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:129)
2021-02-16T23:46:56.7293172Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest.testInvalidCallDuringInitialization(DispatcherTest.java:436)
2021-02-16T23:46:56.7293966Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-16T23:46:56.7294581Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-16T23:46:56.7295338Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-16T23:46:56.7296014Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-02-16T23:46:56.7296653Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-02-16T23:46:56.7297426Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-02-16T23:46:56.7298200Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-02-16T23:46:56.7298961Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-02-16T23:46:56.7299906Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-02-16T23:46:56.7300910Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-02-16T23:46:56.7301585Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-02-16T23:46:56.7302118Z 	at java.lang.Thread.run(Thread.java:748)
{code}",,dwysakowicz,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 11:32:07 UTC 2021,,,,,,,,,,"0|z0nqsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/21 21:31;trohrmann;My suspicion is that the test simply failed because the timeout of 5s is too low for AZP. In the logs one can see twice gaps of 2s and once a gap of 1 second. I would suggest to increase the test timeout a bit and to see whether the problem re-occurs.;;;","26/Mar/21 11:32;trohrmann;Fixed via

1.13.0: cd02aff8e446d5d1c13d89ecb1a4ed2bbae94317
1.12.3: 435a037668fb60d2489ccbe25433b6dc1c8181c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FromElements ignores returns(),FLINK-21386,13358964,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kezhuw,chesnay,chesnay,16/Feb/21 23:14,28/May/21 09:00,13/Jul/23 08:07,08/Apr/21 07:59,1.13.0,,,,,,,,1.13.0,,,,,,,API / DataStream,,,,,0,pull-request-available,usability,,,,"StreamExEnv#fromElements eagerly serializes data with the automatically determined serializer. This can result in errors for example when generic avro records are used, because it will default to Kryo.

Subsequent calls to returns() have no effect because the typeinformation is never forwarded to the function.

Annoyingly the fact that it serializes data isn't logged anywhere. and there doesn't seem to be a way to change the serializer except by using fromCollection() instead.",,dwysakowicz,kezhuw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 07:59:13 UTC 2021,,,,,,,,,,"0|z0nqio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/21 14:37;kezhuw;I think implementing {{OutputTypeConfigurable}} for {{FromElementsFunction}} could solve this. {{AbstractUdfStreamOperator}} will redirect {{OutputTypeConfigurable}} to user defined function. Here is my proposed steps:
 # Store a {{transient}} {{elements}} field for original elements.
 # Keep eagerly serialization but re-serialize if serializer from {{setOutputType}} differs from existing one.
 # Add new constructors with no {{serializer}} to postpone serialization to {{setOutputType}}.

[~chesnay] [~dwysakowicz] [~trohrmann] What do you think ?;;;","01/Mar/21 08:53;chesnay;How does that address the issue of the function using kryo in the first place (aka, not automatically determining that avro should be used)?;;;","01/Mar/21 09:25;kezhuw; [~chesnay] Combination of #1 and #3 could solve this. In construction of {{FromElementsFunction}}, only {{elements}} will pass to constructor. In stream graph generation, {{setOutputType}} will be called with correct serializer and we can generate serialized data in that stage. In case that incorrect serializer will success in serialization(eg. kryo case as I tested), combination of #1 and #2 could also solve this. But there might be some waste and indeterminate for other incorrect serializer, I plan to no serializer in construction in first place.;;;","01/Mar/21 11:11;chesnay;{{OutputTypeConfigurable}} only works for operators, not functions.;;;","01/Mar/21 11:35;kezhuw;[~chesnay] It works for function also, I have verified that. {{AbstractUdfStreamOperator}} will redirect {{setOutputType}} to its {{userFunction}}. I also planed to write a test in {{StreamGraphGeneratorTest}} to assert this, there are tests for operators already. I just pushed a preview [branch|https://github.com/kezhuw/flink/commit/a48df8a13ae75da954bd3cbbdf1ffc68c7ae135c#diff-8a1048680bd7aefdaf6d1007f9065b4f85a04c8a8ff9e936ad202b979df0874fR212] if you are unsure with this.;;;","23/Mar/21 13:56;kezhuw;Hi [~chesnay] could I take over this ?;;;","07/Apr/21 07:48;dwysakowicz;Hi [~kezhuw],
Would you still like to work on this for 1.13?;;;","07/Apr/21 09:09;kezhuw;Hi [~dwysakowicz], I could give it a try.;;;","08/Apr/21 07:59;dwysakowicz;Fixed in 0fc03e408cb80f37699e116230a5876dd7ff03e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix baseurl in documentation,FLINK-21363,13358167,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,11/Feb/21 14:20,11/Feb/21 15:25,13/Jul/23 08:07,11/Feb/21 15:25,,,,,,,,,1.13.0,,,,,,,Documentation,,,,,0,pull-request-available,,,,,,,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 11 14:51:06 UTC 2021,,,,,,,,,,"0|z0nllk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/21 14:51;sjwiesman;fixed in master: cf00676243c1b0b3dd10e06ebab239188a4880e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkRelMdUniqueKeys matches on AbstractCatalogTable instead of CatalogTable,FLINK-21361,13358137,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,airblader,airblader,airblader,11/Feb/21 10:14,11/Feb/21 16:21,13/Jul/23 08:07,11/Feb/21 16:21,1.12.0,1.12.1,,,,,,,1.12.2,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,In FlinkRelMdUniqueKeys there's a match on AbstractCatalogTable rather than the underlying interface CatalogTable. This causes exceptions e.g. during temporal table joins when using alternative catalog table implementations.,,airblader,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 11 16:21:15 UTC 2021,,,,,,,,,,"0|z0nlf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/21 16:21;twalthr;Fixed in 1.12.2: a3ec04128dc27b63be13671357c4ffb0f853e749
Fixed in 1.13.0: 3f7db36fe0ac1196fd33db48e4a0ac9729b02012;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental checkpoint data would be lost once a non-stop savepoint completed,FLINK-21351,13357954,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,yunta,yunta,10/Feb/21 15:16,03/Mar/21 21:49,13/Jul/23 08:07,23/Feb/21 15:59,1.11.3,1.12.1,1.13.0,,,,,,1.12.2,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"FLINK-10354 counted savepoint as retained checkpoint so that job could failover from latest position. I think this operation is reasonable, however, current implementation would let incremental checkpoint data lost immediately once a non-stop savepoint completed.

Current general phase of incremental checkpoints: once a newer checkpoint completed, it would be added to checkpoint store. And if the size of completed checkpoints larger than max retained limit, it would subsume the oldest one. This lead to the reference of incremental data decrease one and data would be deleted once reference reached to zero. As we always ensure to register newer checkpoint and then unregister older checkpoint, current phase works fine as expected.

However, if a non-stop savepoint (a median manual trigger savepoint) is completed, it would be also added into checkpoint store and just subsume previous added checkpoint (in default retain one checkpoint case), which would unregister older checkpoint without newer checkpoint registered, leading to data lost.

Thanks for [~banmoy] reporting this problem first.",,aitozi,banmoy,dwysakowicz,Joyce.Li,kaibo.zhou,pnowojski,roman,trohrmann,uce,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 21:49:34 UTC 2021,,,,,,,,,,"0|z0nkag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/21 16:21;trohrmann;The problem seems to be that the TaskManager side has its own bookkeeping of already uploaded SST files in {{RocksIncrementalSnapshotStrategy}} which can diverge from the {{JM}} side in the case of savepoints.;;;","10/Feb/21 16:25;dwysakowicz;To be clear, RocksDB incremental checkpoints + savepoints without cancel is the only faulty combination, right?;;;","10/Feb/21 16:30;trohrmann;I think so yes.;;;","11/Feb/21 05:55;yunta;[~dwysakowicz] yes, once a savepoint completed, the previous added incremental checkpoint would be subsumed. If the job is cancelled and savepoint would be used to restore for next running, everything is fine. However, if the job is still running and still depends on checkpoint mechanism for failover, this would cause unrecoverable data lost.;;;","11/Feb/21 10:29;trohrmann;Would it work if we don't count savepoints towards the number of maximum retained checkpoints? That way we would only remove checkpoints if a non-savepoint is being added. Consequently, we would maintain the assumption on which the current implementation of incremental checkpoints are built that SST files are only deleted if there is no other checkpoint relying on it.

Implementation wise this could be done by introducing a separate checkpoint counter to the {{DefaultCompletedCheckpointStore}}. It adds a bit of logic to always keep this counter correctly set but it should be a rather small change.;;;","17/Feb/21 18:06;roman;I think that would work but would keep savepoints not subsumed unnecessarily.

A little bit different approach would allow to subsume savepoints too:
1. Iterate through the completed checkpoints starting from the earliest 
2. Subsume a checkpoint if it's earlier than the last checkpoint-not-savepoint
3. Subsume a savepoint if it's not the last one
4. Break whenever checkpoints.size <= maxRetain

I've published a PR with this change, could you take a look: https://github.com/apache/flink/pull/14953?;;;","18/Feb/21 12:42;trohrmann;I think that this should work a bit better at the cost of higher complexity.;;;","18/Feb/21 12:42;trohrmann;[~pnowojski] can you help with the review?;;;","03/Mar/21 20:11;uce;[~roman_khachatryan] The ticket includes fixVersion 1.11.4. Are you planning to open a PR against the release-1.11 branch?;;;","03/Mar/21 21:49;roman;No, I don't plan to backport the fix to 1.11 [~uce] 

I've removed 1.11.4 fixVersion.

Sorry for confusion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala:157,FLINK-21345,13357869,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zicat,zicat,zicat,10/Feb/21 05:31,15/Dec/21 01:44,13/Jul/23 08:07,09/Oct/21 08:45,1.12.1,,,,,,,,1.14.3,1.15.0,,,,,,Table SQL / Planner,,,,,0,auto-unassigned,pull-request-available,,,,"First Step: Create 2 Source Tables as below:
{code:java}
CREATE TABLE test_streaming(
 vid BIGINT,
 ts BIGINT,
 proc AS proctime()
) WITH (
 'connector' = 'kafka',
 'topic' = 'test-streaming',
 'properties.bootstrap.servers' = '127.0.0.1:9092',
 'scan.startup.mode' = 'latest-offset',
 'format' = 'json'
);
CREATE TABLE test_streaming2(
 vid BIGINT,
 ts BIGINT,
 proc AS proctime()
) WITH (
 'connector' = 'kafka',
 'topic' = 'test-streaming2',
 'properties.bootstrap.servers' = '127.0.0.1:9092',
 'scan.startup.mode' = 'latest-offset',
 'format' = 'json'
);
{code}
Second Step: Create a TEMPORARY Table Function, function name:dim, key:vid, timestamp:proctime()

Third Step: test_streaming union all  test_streaming2 join dim like below:
{code:java}
SELECT r.vid,d.name,timestamp_from_long(r.ts)
FROM (
    SELECT * FROM test_streaming UNION ALL SELECT * FROM test_streaming2
) AS r,
    LATERAL TABLE (dim(r.proc)) AS d
WHERE r.vid = d.vid;
{code}
Exception Detail: (if only use test-streaming or test-streaming2 join temporary table function, the program run ok)
{code:java}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.getRelOptSchema(LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala:157)
	at org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.onMatch(LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala:99)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155)
	at scala.collection.Iterator$class.foreach(Iterator.scala:742)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155)
	at scala.collection.immutable.Range.foreach(Range.scala:166)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155)
	at scala.collection.Iterator$class.foreach(Iterator.scala:742)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57){code}","Planner: BlinkPlanner
Flink Version: 1.12.1_2.11
Java Version: 1.8
OS: mac os",godfreyhe,jark,jingzhang,leonard,libenchao,zicat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/21 08:00;zicat;image-2021-02-10-16-00-45-553.png;https://issues.apache.org/jira/secure/attachment/13020278/image-2021-02-10-16-00-45-553.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 09 08:45:28 UTC 2021,,,,,,,,,,"0|z0njrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/21 07:36;jark;cc [~Leonard Xu];;;","10/Feb/21 08:01;zicat;[~jark] [~Leonard Xu] 
 !image-2021-02-10-16-00-45-553.png!

I try to add the code of the method getRelOptSchema in LogicalCorrelateToJoinFromTemporalTableFunctionRule.java, It seems run ok.;;;","13/Apr/21 11:31;leonard;Hi [~zicat]  Do you have time to update this PR?

;;;","10/May/21 06:00;zicat;Hi [~Leonard Xu], I have updated this PR.;;;","10/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","18/Jun/21 22:39;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","09/Oct/21 08:45;godfreyhe;Fixed in 1.15.0: 24e6121d5f882e55dfc0616b1da81dc0b46f2d34
Fixed in 1.14.1: 3598a16f4d2d46b75f15a4eb01610ecfe2640f1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Local recovery and sticky scheduling end-to-end test"" does not finish within 600 seconds",FLINK-21329,13357694,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,rmetzger,rmetzger,09/Feb/21 07:16,28/Aug/21 11:16,13/Jul/23 08:07,19/May/21 12:30,1.13.0,,,,,,,,1.12.5,1.13.0,1.14.0,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13118&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=38515

{code}
Feb 08 22:25:46 ==============================================================================
Feb 08 22:25:46 Running 'Local recovery and sticky scheduling end-to-end test'
Feb 08 22:25:46 ==============================================================================
Feb 08 22:25:46 TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-46881214821
Feb 08 22:25:47 Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.13-SNAPSHOT-bin/flink-1.13-SNAPSHOT
Feb 08 22:25:47 Running local recovery test with configuration:
Feb 08 22:25:47         parallelism: 4
Feb 08 22:25:47         max attempts: 10
Feb 08 22:25:47         backend: rocks
Feb 08 22:25:47         incremental checkpoints: false
Feb 08 22:25:47         kill JVM: false
Feb 08 22:25:47 Starting zookeeper daemon on host fv-az127-394.
Feb 08 22:25:47 Starting HA cluster with 1 masters.
Feb 08 22:25:48 Starting standalonesession daemon on host fv-az127-394.
Feb 08 22:25:49 Starting taskexecutor daemon on host fv-az127-394.
Feb 08 22:25:49 Waiting for Dispatcher REST endpoint to come up...
Feb 08 22:25:50 Waiting for Dispatcher REST endpoint to come up...
Feb 08 22:25:51 Waiting for Dispatcher REST endpoint to come up...
Feb 08 22:25:53 Waiting for Dispatcher REST endpoint to come up...
Feb 08 22:25:54 Dispatcher REST endpoint is up.
Feb 08 22:25:54 Started TM watchdog with PID 28961.
Feb 08 22:25:58 Job has been submitted with JobID e790e85a39040539f9386c0df7ca4812
Feb 08 22:35:47 Test (pid: 27970) did not finish after 600 seconds.
Feb 08 22:35:47 Printing Flink logs and killing it:

{code}

and

{code}

	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.unhandledError(ZooKeeperLeaderRetrievalDriver.java:184)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$6.apply(CuratorFrameworkImpl.java:713)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$6.apply(CuratorFrameworkImpl.java:709)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100)
	at org.apache.flink.shaded.curator4.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.logError(CuratorFrameworkImpl.java:708)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:874)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:990)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:943)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:66)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:346)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:862)
	... 10 more

{code}",,dwysakowicz,maguowei,mapohl,rmetzger,roman,sjwiesman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22045,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 19 12:30:49 UTC 2021,,,,,,,,,,"0|z0nioo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/21 08:26;dwysakowicz;Similar outcome: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13326&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610

{code}
2021-02-15T01:02:52.3770907Z ------------------------------------------------------------
2021-02-15T01:02:52.3772409Z  The program finished with the following exception:
2021-02-15T01:02:52.3773214Z 
2021-02-15T01:02:52.3774604Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.TimeoutException
2021-02-15T01:02:52.3775820Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:366)
2021-02-15T01:02:52.3776889Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:219)
2021-02-15T01:02:52.3778111Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
2021-02-15T01:02:52.3778824Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
2021-02-15T01:02:52.3779712Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
2021-02-15T01:02:52.3780328Z 	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
2021-02-15T01:02:52.3780963Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
2021-02-15T01:02:52.3781663Z 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
2021-02-15T01:02:52.3782340Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
2021-02-15T01:02:52.3782958Z Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
2021-02-15T01:02:52.3783619Z 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
2021-02-15T01:02:52.3784257Z 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
2021-02-15T01:02:52.3784973Z 	at org.apache.flink.client.program.StreamContextEnvironment.getJobExecutionResult(StreamContextEnvironment.java:123)
2021-02-15T01:02:52.3785740Z 	at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:80)
2021-02-15T01:02:52.3786561Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1782)
2021-02-15T01:02:52.3787396Z 	at org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob.main(StickyAllocationAndLocalRecoveryTestJob.java:138)
2021-02-15T01:02:52.3788110Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-15T01:02:52.3788757Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-15T01:02:52.3791150Z 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-15T01:02:52.3791827Z 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2021-02-15T01:02:52.3792706Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:349)
2021-02-15T01:02:52.3793248Z 	... 8 more
2021-02-15T01:02:52.3793651Z Caused by: java.util.concurrent.TimeoutException
2021-02-15T01:02:52.3794206Z 	at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1220)
2021-02-15T01:02:52.3794897Z 	at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:217)
2021-02-15T01:02:52.3795593Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$orTimeout$15(FutureUtils.java:582)
2021-02-15T01:02:52.3796266Z 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
2021-02-15T01:02:52.3796880Z 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2021-02-15T01:02:52.3797574Z 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
2021-02-15T01:02:52.3798805Z 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2021-02-15T01:02:52.3800632Z 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2021-02-15T01:02:52.3801242Z 	at java.base/java.lang.Thread.run(Thread.java:834)
2021-02-15T01:54:04.9335391Z ==========================================================================================
2021-02-15T01:54:04.9336579Z === WARNING: This E2E Run took already 80% of the allocated time budget of 250 minutes ===
2021-02-15T01:54:04.9338033Z ==========================================================================================
2021-02-15T02:33:04.9292547Z ======================================================================================================
2021-02-15T02:33:04.9293721Z === WARNING: This E2E Run will time out in the next few minutes. Starting to upload the log output ===
2021-02-15T02:33:04.9294789Z ======================================================================================================
{code};;;","11/Mar/21 06:02;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14422&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610

{code:java}
2021-03-10T23:56:19.2456160Z  The program finished with the following exception:
2021-03-10T23:56:19.2456749Z 
2021-03-10T23:56:19.2469726Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Unhandled error in ZooKeeperLeaderRetrievalDriver:Background operation retry gave up
2021-03-10T23:56:19.2472547Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
2021-03-10T23:56:19.2473760Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
2021-03-10T23:56:19.2474822Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
2021-03-10T23:56:19.2476172Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
2021-03-10T23:56:19.2477146Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
2021-03-10T23:56:19.2478086Z 	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
2021-03-10T23:56:19.2479065Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
2021-03-10T23:56:19.2480129Z 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
2021-03-10T23:56:19.2486127Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
2021-03-10T23:56:19.2487084Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Unhandled error in ZooKeeperLeaderRetrievalDriver:Background operation retry gave up
2021-03-10T23:56:19.2487873Z 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
2021-03-10T23:56:19.2488470Z 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
2021-03-10T23:56:19.2489106Z 	at org.apache.flink.client.program.StreamContextEnvironment.getJobExecutionResult(StreamContextEnvironment.java:123)
2021-03-10T23:56:19.2489773Z 	at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:80)
2021-03-10T23:56:19.2490443Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1839)
2021-03-10T23:56:19.2491167Z 	at org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob.main(StickyAllocationAndLocalRecoveryTestJob.java:139)
2021-03-10T23:56:19.2491807Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-10T23:56:19.2492391Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-10T23:56:19.2493322Z 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-10T23:56:19.2494397Z 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2021-03-10T23:56:19.2494976Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
2021-03-10T23:56:19.2495419Z 	... 8 more
2021-03-10T23:56:19.2495958Z Caused by: org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Unhandled error in ZooKeeperLeaderRetrievalDriver:Background operation retry gave up
2021-03-10T23:56:19.2496708Z 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.unhandledError(ZooKeeperLeaderRetrievalDriver.java:184)
2021-03-10T23:56:19.2497400Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$6.apply(CuratorFrameworkImpl.java:713)
2021-03-10T23:56:19.2498080Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$6.apply(CuratorFrameworkImpl.java:709)
2021-03-10T23:56:19.2498749Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100)
2021-03-10T23:56:19.2499441Z 	at org.apache.flink.shaded.curator4.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
2021-03-10T23:56:19.2500135Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92)
2021-03-10T23:56:19.2500932Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.logError(CuratorFrameworkImpl.java:708)
2021-03-10T23:56:19.2501636Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:874)
2021-03-10T23:56:19.2502420Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:990)
2021-03-10T23:56:19.2503156Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:943)
2021-03-10T23:56:19.2503866Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:66)
2021-03-10T23:56:19.2504599Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:346)
2021-03-10T23:56:19.2505190Z 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2021-03-10T23:56:19.2505985Z 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
2021-03-10T23:56:19.2506603Z 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2021-03-10T23:56:19.2507161Z 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2021-03-10T23:56:19.2507654Z 	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-10T23:56:19.2508336Z Caused by: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
2021-03-10T23:56:19.2508994Z 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
2021-03-10T23:56:19.2509662Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:862)
2021-03-10T23:56:19.2510188Z 	... 9 more
{code}
;;;","30/Mar/21 07:19;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15701&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=41327;;;","30/Mar/21 07:34;dwysakowicz;This might be related: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15720&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610&l=29594;;;","30/Mar/21 07:35;mapohl;[~dwysakowicz]: What makes you assume that [this failure|https://issues.apache.org/jira/browse/FLINK-21329?focusedCommentId=17284596&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17284596] is related to the ZooKeeper issue of the other build failures reported in this Jira issue?;;;","30/Mar/21 07:38;dwysakowicz;Ofc, I might be wrong, but I thought the wall of zookeeper logs looks similar in both cases:

{code}
2021-02-15T01:02:11.3132966Z Feb 15 01:01:58 2021-02-15 00:59:30,586 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x10000accbb00000 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3169981Z Feb 15 01:01:58 2021-02-15 00:59:30,586 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x10000accbb00000 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3172987Z Feb 15 01:01:58 2021-02-15 00:59:31,732 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x10000accbb00001 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3174898Z Feb 15 01:01:58 2021-02-15 00:59:31,732 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x10000accbb00001 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3176779Z Feb 15 01:01:58 2021-02-15 00:59:33,661 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x10000accbb00004 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3178828Z Feb 15 01:01:58 2021-02-15 00:59:33,755 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x10000accbb00004 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3180699Z Feb 15 01:01:58 2021-02-15 00:59:37,826 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x10000accbb00003 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3182543Z Feb 15 01:01:58 2021-02-15 00:59:37,826 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x10000accbb00003 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3184497Z Feb 15 01:01:58 2021-02-15 00:59:39,257 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x10000accbb00005 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3186337Z Feb 15 01:01:58 2021-02-15 00:59:39,257 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x10000accbb00005 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3188198Z Feb 15 01:01:58 2021-02-15 00:59:41,054 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x10000accbb00006 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3190043Z Feb 15 01:01:58 2021-02-15 00:59:41,054 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x10000accbb00006 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3191917Z Feb 15 01:01:58 2021-02-15 00:59:43,931 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x10000accbb00000 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3214820Z Feb 15 01:01:58 2021-02-15 00:59:43,931 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x10000accbb00000 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3216765Z Feb 15 01:01:58 2021-02-15 00:59:45,079 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x10000accbb00001 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3218632Z Feb 15 01:01:58 2021-02-15 00:59:45,079 DEBUG org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x10000accbb00001 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2021-02-15T01:02:11.3220492Z Feb 15 01:01:58 2021-02-15 00:59:48,851 WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.persistence.FileTxnLog [] - fsync-ing the write ahead log in SyncThread:0 took 2298ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide
{code};;;","30/Mar/21 09:15;mapohl;true, thanks for clarification;;;","30/Mar/21 10:36;trohrmann;Are you looking into this problem [~mapohl]? If yes, then please assign yourself and set the ticket to in progress.;;;","30/Mar/21 11:25;mapohl;The extensive ZooKeeper logs are a result of a wrong log4j setup. I created FLINK-22045 to cover this.;;;","30/Mar/21 20:13;mapohl;[~trohrmann] you're right. I forgot to assign the issue.

I looked into the different failures with the help of [~chesnay]. It appears that the job failures are caused by reaching the timeout of the build. This triggers the killing of child processes which results in random failures (e.g. ZooKeeper becoming unavailable). Considering that we're close to the release we might want to increase the timeout rather than putting effort into fixing the build times, I guess?;;;","31/Mar/21 08:46;trohrmann;Sounds good to me as a temporary fix. I've assigned the ticket to you now because I accidentally assigned myself yesterday.;;;","31/Mar/21 09:55;mapohl;[~trohrmann] thanks for assigning me.

[~dwysakowicz] [~maguowei] [~rmetzger] what's your stand on increasing the timeout considering the release? I'm just not sure how we would manage the efforts on improving the runtime of the overall test pipeline afterwards.;;;","31/Mar/21 11:36;dwysakowicz;Are we sure the problem is with just the test timing out? If I compare execution times from the failed run from description with a last successful build (https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15854&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529) the execution time for the failed test doubles:

succesful run:
{code}
804;PyFlink YARN per-job on Docker test
48;Local recovery and sticky scheduling end-to-end test
129;Local recovery and sticky scheduling end-to-end test
248;Local recovery and sticky scheduling end-to-end test
87;Local recovery and sticky scheduling end-to-end test
358;Local recovery and sticky scheduling end-to-end test
328;Local recovery and sticky scheduling end-to-end test
{code}

failed run:
{code}
848;PyFlink YARN per-job on Docker test
53;Local recovery and sticky scheduling end-to-end test
132;Local recovery and sticky scheduling end-to-end test
601;Local recovery and sticky scheduling end-to-end test <- failed test
{code}

BTW by increasing timeout of the build you mean the timeout ~5h timeout for running all e2e tests? There is also something interesting I spotted that I can not explain myself. If you sum up individual execution times of all tests the successful build takes longer. However the Azure dashboard shows that the failed build took ~30 min longer. Do we know why is that? 
;;;","31/Mar/21 16:51;mapohl;You have a point. (y) Comparing it to the successful build is a good idea. ...and I'm back on the whiteboard...;;;","01/Apr/21 07:46;mapohl;For [build #13326|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13326&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610] I had a look at the timestamps. First of all, in all cases the test doesn't fail for the {{HashMapStateBackend}}. Every failure is caused by the {{4 10 rocks false false}} setup (i.e. {{parallism=4}}, {{maxAttempts=10}}, {{EmbeddedRocksDBStateBackend}} used, {{incrementalCheckpoints=false}}, {{killJvm=false}}).

It looks like the checkpoint takes up quite some time for checkpoints later in the test:
{code}
$ grep ""Received ack\|attempt #\|Triggering checkpoin"" flink-vsts-standalonesession-0-fv-az127-111.log
2021-02-15 00:52:20,703 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #0) with attempt id 4f4ad74ee089d4dd970ec49c30c04bd9 to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:52:20,720 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #0) with attempt id 82981af733cb7b98a6a5887afe6ddd9f to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:52:20,725 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #0) with attempt id 449a2c47d0757dcea7dd388e48f85e45 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:52:20,726 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #0) with attempt id 7973e2cbeb1c92794d732674a4b61416 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:52:20,743 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #0) with attempt id 880f6ba3112ff89dc036928542878ce2 to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:52:20,763 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #0) with attempt id 55bd0597a2f983d09dfc97ff7bd9e825 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:52:20,767 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #0) with attempt id 82f8ea6e74f36cfd316a288372811f68 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:52:20,769 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #0) with attempt id 0c95f9805ca3eb643e9dcac59c43f0e7 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:52:22,930 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1613350342913 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:52:23,900 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 1 from task 7973e2cbeb1c92794d732674a4b61416 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:52:23,908 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 1 from task 449a2c47d0757dcea7dd388e48f85e45 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:23,949 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 1 from task 4f4ad74ee089d4dd970ec49c30c04bd9 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:52:24,009 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 1 from task 82981af733cb7b98a6a5887afe6ddd9f of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:52:25,593 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 1 from task 880f6ba3112ff89dc036928542878ce2 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:52:25,706 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 1 from task 55bd0597a2f983d09dfc97ff7bd9e825 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:52:25,734 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 1 from task 0c95f9805ca3eb643e9dcac59c43f0e7 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:52:25,859 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 1 from task 82f8ea6e74f36cfd316a288372811f68 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:26,027 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2 (type=CHECKPOINT) @ 1613350346023 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:52:26,076 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 2 from task 82981af733cb7b98a6a5887afe6ddd9f of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:52:26,109 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 2 from task 7973e2cbeb1c92794d732674a4b61416 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:52:26,326 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 2 from task 449a2c47d0757dcea7dd388e48f85e45 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:27,388 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #1) with attempt id 788bb45fc691f093c8fa38aedf4c7c5e to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:52:27,389 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #1) with attempt id a484233ad0362559effe8505022cd7bb to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:52:27,389 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #1) with attempt id 405efa0b99caf483b2ff3fbde9626142 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:52:27,412 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #1) with attempt id ceaba47bd65a109d5ae70453ee6d55d3 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:52:27,426 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #1) with attempt id 9eef2db0894ded24070d711200cef00b to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:52:27,430 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #1) with attempt id 11a87877544346f024394efdf94f61a5 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:52:27,434 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #1) with attempt id bfeb96193c8b4b1a7720523d934ae148 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:52:27,436 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #1) with attempt id b8789dbe8cb8f86c2a34cbcb1994937e to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:52:28,312 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 3 (type=CHECKPOINT) @ 1613350348309 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:52:29,130 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 3 from task 405efa0b99caf483b2ff3fbde9626142 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:29,230 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 3 from task 788bb45fc691f093c8fa38aedf4c7c5e of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:52:30,378 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 3 from task a484233ad0362559effe8505022cd7bb of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:52:30,969 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 3 from task ceaba47bd65a109d5ae70453ee6d55d3 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:52:31,783 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 3 from task 11a87877544346f024394efdf94f61a5 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:52:31,928 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 3 from task 9eef2db0894ded24070d711200cef00b of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:52:31,985 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 3 from task b8789dbe8cb8f86c2a34cbcb1994937e of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:52:32,078 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 3 from task bfeb96193c8b4b1a7720523d934ae148 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:32,229 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 4 (type=CHECKPOINT) @ 1613350352189 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:52:32,297 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 4 from task ceaba47bd65a109d5ae70453ee6d55d3 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:52:32,304 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 4 from task 405efa0b99caf483b2ff3fbde9626142 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:32,376 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 4 from task a484233ad0362559effe8505022cd7bb of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:52:35,478 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #2) with attempt id 048a1a9d6f2a992a191a13daccb1948c to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:52:35,479 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #2) with attempt id b0c96b67fcb504a094ad8d0bcbe3e77e to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:52:35,494 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #2) with attempt id 3b6dfaaa1ca850325860f3862ee79c16 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:52:35,495 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #2) with attempt id ecfb6e5e16de6b8bc8ebcb4123c01eb0 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:52:35,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #2) with attempt id a98bdd545328575af264df54ec5c26a5 to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:52:35,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #2) with attempt id 8b41c40b397ce1c3c4dedf229e317810 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:52:35,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #2) with attempt id efe3eb687b17f610e031d1f30b4703e9 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:52:35,510 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #2) with attempt id 59fd2b7377356729a937d5a7f1765256 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:52:36,409 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 5 (type=CHECKPOINT) @ 1613350356406 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:52:41,052 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 5 from task 048a1a9d6f2a992a191a13daccb1948c of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:52:42,306 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 5 from task ecfb6e5e16de6b8bc8ebcb4123c01eb0 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:52:44,067 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 5 from task b0c96b67fcb504a094ad8d0bcbe3e77e of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:52:44,937 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 5 from task 3b6dfaaa1ca850325860f3862ee79c16 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:46,971 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 5 from task efe3eb687b17f610e031d1f30b4703e9 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:47,053 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 5 from task 59fd2b7377356729a937d5a7f1765256 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:52:47,081 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 5 from task 8b41c40b397ce1c3c4dedf229e317810 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:52:47,359 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 5 from task a98bdd545328575af264df54ec5c26a5 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:52:47,404 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 6 (type=CHECKPOINT) @ 1613350367400 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:52:47,463 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 6 from task 3b6dfaaa1ca850325860f3862ee79c16 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:52:47,551 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 6 from task 048a1a9d6f2a992a191a13daccb1948c of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:52:50,769 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #3) with attempt id 42505d412d2fdbfd25b2d2f96185c8cc to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:52:50,769 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #3) with attempt id 691e4d66555626abcd504ae697a97617 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:52:50,769 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #3) with attempt id 383562c09b5e1bfd12434ed7e2a17110 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:52:50,779 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #3) with attempt id 702f75f4ee6a8fbd626272ee4ee69b4d to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:52:50,779 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #3) with attempt id 016fdf521f8ab31ddbf825098358738b to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:52:50,780 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #3) with attempt id 73add6ac94057d25dd721ceec2edd0fb to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:52:50,783 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #3) with attempt id 02018111834d3bc0ba89bf4a180b6f4e to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:52:50,783 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #3) with attempt id 144e393b9e369541eeb7bed89b79952a to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:52:51,070 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 7 (type=CHECKPOINT) @ 1613350371067 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:53:06,270 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 7 from task 691e4d66555626abcd504ae697a97617 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:53:06,331 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 7 from task 42505d412d2fdbfd25b2d2f96185c8cc of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:53:06,959 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 7 from task 702f75f4ee6a8fbd626272ee4ee69b4d of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:53:07,213 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 7 from task 383562c09b5e1bfd12434ed7e2a17110 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:53:08,936 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 7 from task 016fdf521f8ab31ddbf825098358738b of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:53:09,125 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 7 from task 73add6ac94057d25dd721ceec2edd0fb of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:53:09,243 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 7 from task 02018111834d3bc0ba89bf4a180b6f4e of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:53:09,602 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 7 from task 144e393b9e369541eeb7bed89b79952a of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:53:09,644 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 8 (type=CHECKPOINT) @ 1613350389632 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:53:14,233 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #4) with attempt id 6f78f2945af7ac21eb1dd3fcf2689ebc to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:53:14,234 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #4) with attempt id f026bd2bbea6227dc80d89b5c6eda695 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:53:14,235 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #4) with attempt id 86a2c8ba4a03b89a2c591cc5d3cd9c6f to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:53:14,235 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #4) with attempt id 58a49f79cc22df7897aed0bbcf83cbf8 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:53:14,235 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #4) with attempt id 78d3ce338fb1e2c27a67fe5eec73fd9a to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:53:14,235 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #4) with attempt id 7900b3067e9211096fa2b9b2ad4db1e3 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:53:14,235 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #4) with attempt id 2beb71300bcfaed2ec16c88312b2bb24 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:53:14,235 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #4) with attempt id 941eb8f47142721812ec335fdac77e66 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:53:15,262 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 9 (type=CHECKPOINT) @ 1613350395260 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:53:32,027 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 9 from task f026bd2bbea6227dc80d89b5c6eda695 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:53:32,276 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 9 from task 6f78f2945af7ac21eb1dd3fcf2689ebc of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:53:37,030 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 9 from task 58a49f79cc22df7897aed0bbcf83cbf8 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:53:42,168 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 9 from task 86a2c8ba4a03b89a2c591cc5d3cd9c6f of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:53:47,214 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 9 from task 941eb8f47142721812ec335fdac77e66 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:53:47,530 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 9 from task 2beb71300bcfaed2ec16c88312b2bb24 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:53:47,641 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 9 from task 7900b3067e9211096fa2b9b2ad4db1e3 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:53:47,944 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 9 from task 78d3ce338fb1e2c27a67fe5eec73fd9a of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:53:48,914 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 10 (type=CHECKPOINT) @ 1613350428898 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:53:48,964 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 10 from task f026bd2bbea6227dc80d89b5c6eda695 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:53:49,135 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 10 from task 6f78f2945af7ac21eb1dd3fcf2689ebc of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:53:51,678 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #5) with attempt id 910ca61f7469c8ec7949ec14e2c0e239 to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:53:51,678 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #5) with attempt id dfe8fff4ae0da75df98be7d9d6590e45 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:53:51,678 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #5) with attempt id c50f803c83979afd49ea479f653ba8af to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:53:51,678 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #5) with attempt id 8c50f0659c3e6541784fdd4c2474333e to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:53:51,678 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #5) with attempt id 029e69c47b7824deb136eec7728441ef to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:53:51,678 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #5) with attempt id cb23e8fdefac39a50d3bd5ffb3bc3e86 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:53:51,678 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #5) with attempt id 95ac0f1a6bf2c2281db6972028d3f957 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:53:51,678 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #5) with attempt id 1d1805b290593d54313a8a7e33d69ab7 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:53:52,142 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 11 (type=CHECKPOINT) @ 1613350432135 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:54:33,436 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 11 from task c50f803c83979afd49ea479f653ba8af of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:54:33,540 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 11 from task dfe8fff4ae0da75df98be7d9d6590e45 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:54:34,314 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 11 from task 910ca61f7469c8ec7949ec14e2c0e239 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:54:35,783 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 11 from task 8c50f0659c3e6541784fdd4c2474333e of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:54:41,022 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 11 from task 1d1805b290593d54313a8a7e33d69ab7 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:54:41,137 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 11 from task cb23e8fdefac39a50d3bd5ffb3bc3e86 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:54:41,179 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 11 from task 029e69c47b7824deb136eec7728441ef of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:54:42,511 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 11 from task 95ac0f1a6bf2c2281db6972028d3f957 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:54:43,941 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 12 (type=CHECKPOINT) @ 1613350483847 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:54:43,959 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 12 from task dfe8fff4ae0da75df98be7d9d6590e45 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:54:44,017 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 12 from task 910ca61f7469c8ec7949ec14e2c0e239 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:54:44,149 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 12 from task c50f803c83979afd49ea479f653ba8af of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:54:48,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #6) with attempt id 8f89a8ab1e618df1b940f7b47dfaf55f to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:54:48,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #6) with attempt id e427bc378c8a4ccef08266f513ba04b3 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:54:48,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #6) with attempt id 3019aa338dcb1c5da07b68f3d2053e8d to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:54:48,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #6) with attempt id fbc19f708a93a3ddcbb4448037af25eb to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:54:48,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #6) with attempt id 1e4fa709d4cae44602d2d0f269f663e6 to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:54:48,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #6) with attempt id 5413a5bf4812076e4bcf91a660b2c8df to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:54:48,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #6) with attempt id 7f3cf7692ebb98e99a485fe1c4ca0616 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:54:48,999 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #6) with attempt id c541f3d355a456d9d68456b3393e3109 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:54:49,354 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 13 (type=CHECKPOINT) @ 1613350489351 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:55:34,111 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 13 from task 8f89a8ab1e618df1b940f7b47dfaf55f of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:55:37,156 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 13 from task fbc19f708a93a3ddcbb4448037af25eb of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:55:37,666 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 13 from task 3019aa338dcb1c5da07b68f3d2053e8d of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:55:39,720 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 13 from task e427bc378c8a4ccef08266f513ba04b3 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:55:45,909 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 13 from task c541f3d355a456d9d68456b3393e3109 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:55:46,107 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 13 from task 7f3cf7692ebb98e99a485fe1c4ca0616 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:55:46,798 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 13 from task 1e4fa709d4cae44602d2d0f269f663e6 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:55:47,181 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 13 from task 5413a5bf4812076e4bcf91a660b2c8df of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:55:49,045 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 14 (type=CHECKPOINT) @ 1613350549043 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:55:49,245 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 14 from task 3019aa338dcb1c5da07b68f3d2053e8d of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:55:50,396 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 14 from task 8f89a8ab1e618df1b940f7b47dfaf55f of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:55:51,134 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 14 from task fbc19f708a93a3ddcbb4448037af25eb of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:55:54,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #7) with attempt id ec5ea040bc73c307ab416a8a1fcb208c to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:55:54,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #7) with attempt id f3f009ae9a8b62967d9cfbe5b3b5f3ad to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:55:54,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #7) with attempt id c67144c6d3b24a2ca98959981dca7507 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:55:54,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #7) with attempt id 9b4d2495d74966513b62f57682ec1b0c to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:55:54,998 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #7) with attempt id 225fc04f7a84880fc83266d12e457d1d to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:55:54,999 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #7) with attempt id a90fc3b1d8f9cf189c769bb82aa9212e to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:55:54,999 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #7) with attempt id 37c71ff6a787d67a389372c90a3ed7cd to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:55:54,999 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #7) with attempt id ed1ee0ebefc90ecc5d49f5bf93cd928b to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:55:55,270 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 15 (type=CHECKPOINT) @ 1613350555266 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:56:47,653 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 15 from task ec5ea040bc73c307ab416a8a1fcb208c of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:56:50,392 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 15 from task 9b4d2495d74966513b62f57682ec1b0c of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:56:50,866 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 15 from task c67144c6d3b24a2ca98959981dca7507 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:56:52,394 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 15 from task f3f009ae9a8b62967d9cfbe5b3b5f3ad of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:57:01,563 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 15 from task 37c71ff6a787d67a389372c90a3ed7cd of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:57:01,813 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 15 from task ed1ee0ebefc90ecc5d49f5bf93cd928b of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:57:02,145 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 15 from task 225fc04f7a84880fc83266d12e457d1d of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:57:03,166 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 15 from task a90fc3b1d8f9cf189c769bb82aa9212e of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:57:03,840 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 16 (type=CHECKPOINT) @ 1613350623794 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:57:03,897 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 16 from task ec5ea040bc73c307ab416a8a1fcb208c of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:57:03,916 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 16 from task 9b4d2495d74966513b62f57682ec1b0c of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:57:04,000 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 16 from task c67144c6d3b24a2ca98959981dca7507 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:57:06,478 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 16 from task f3f009ae9a8b62967d9cfbe5b3b5f3ad of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:57:07,038 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #8) with attempt id e9d9b82afa186c411e6bdf881873e25d to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:57:07,038 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #8) with attempt id b35b8153a9f2c0ba60b51ecf243ea73f to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:57:07,038 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #8) with attempt id 39993b609c57b6226c58c199bd722159 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:57:07,038 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #8) with attempt id f979490d52ab8fe2ff77b2932a0bfbef to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:57:07,038 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #8) with attempt id 7e12451b9faa438f6f06e995e474ed71 to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:57:07,038 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #8) with attempt id edc0da7ea739f999e8a3c2850d89e5ed to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:57:07,038 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #8) with attempt id 8f96e7dc7d586e7ea101cb16e3facf29 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:57:07,045 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #8) with attempt id 5d7bc3d6c1df759348a120a702e76aa9 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:57:07,423 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 17 (type=CHECKPOINT) @ 1613350627420 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:58:07,870 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 17 from task 39993b609c57b6226c58c199bd722159 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:58:07,871 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 17 from task b35b8153a9f2c0ba60b51ecf243ea73f of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:58:07,886 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 17 from task e9d9b82afa186c411e6bdf881873e25d of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:58:08,957 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 17 from task f979490d52ab8fe2ff77b2932a0bfbef of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:58:19,243 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 17 from task 5d7bc3d6c1df759348a120a702e76aa9 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:58:19,685 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 17 from task 7e12451b9faa438f6f06e995e474ed71 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:58:19,962 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 17 from task 8f96e7dc7d586e7ea101cb16e3facf29 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:58:20,276 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 17 from task edc0da7ea739f999e8a3c2850d89e5ed of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:58:21,869 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 18 (type=CHECKPOINT) @ 1613350701857 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:58:21,897 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 18 from task e9d9b82afa186c411e6bdf881873e25d of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:58:21,936 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 18 from task f979490d52ab8fe2ff77b2932a0bfbef of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:58:23,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #9) with attempt id 3e8bb0f0ec3899f62304a8dffa103faf to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:58:23,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #9) with attempt id c6864d5e4339977fec5e5b10b8cb9db7 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:58:23,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #9) with attempt id e49c76a882344632c0d9a84d456e2700 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:58:23,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #9) with attempt id 7b95374e5523ec14be73a17668467420 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:58:23,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #9) with attempt id 4bbd2a3aac0de6593f8199b0ef8eaaff to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:58:23,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #9) with attempt id 22514f0ff054d425950b839e92aad3da to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:58:23,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #9) with attempt id adc4277261c074dc802e9f4c5ff80717 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:58:23,363 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #9) with attempt id b3f43cadfe0d2bd2eeff7ba8d2210f10 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:58:23,846 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 19 (type=CHECKPOINT) @ 1613350703843 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:59:26,678 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 19 from task 3e8bb0f0ec3899f62304a8dffa103faf of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:59:27,346 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 19 from task c6864d5e4339977fec5e5b10b8cb9db7 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:59:35,173 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 19 from task 7b95374e5523ec14be73a17668467420 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:59:36,477 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 19 from task e49c76a882344632c0d9a84d456e2700 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:59:44,876 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 19 from task b3f43cadfe0d2bd2eeff7ba8d2210f10 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 00:59:45,223 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 19 from task 22514f0ff054d425950b839e92aad3da of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 00:59:45,824 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 19 from task adc4277261c074dc802e9f4c5ff80717 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 00:59:46,549 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 19 from task 4bbd2a3aac0de6593f8199b0ef8eaaff of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 00:59:48,874 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 20 (type=CHECKPOINT) @ 1613350788857 for job dfa837311853a69cdca82db082270dff.
2021-02-15 00:59:50,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #10) with attempt id ad1ef05bb02038bab84960023be2139f to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:59:50,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #10) with attempt id 1eb939470ab93f6a50c6df4e0d5757a1 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:59:50,796 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #10) with attempt id 41b6ec89308b04922c9f043faa83c7b9 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:59:50,796 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #10) with attempt id 89878b144856fead9161dafeed1b416d to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:59:50,796 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #10) with attempt id cc83d9b2bb53ac446aad4ed344259223 to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 00:59:50,797 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #10) with attempt id 957a638e36686032cb9f9bd7144915f3 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 00:59:50,797 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #10) with attempt id 728e40516877edb97748f12a56b9a4c4 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 00:59:50,809 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #10) with attempt id 072aa4c8afc4326e05d926468125cfac to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 00:59:51,077 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 21 (type=CHECKPOINT) @ 1613350791074 for job dfa837311853a69cdca82db082270dff.
2021-02-15 01:01:02,644 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 21 from task 1eb939470ab93f6a50c6df4e0d5757a1 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 01:01:02,652 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 21 from task ad1ef05bb02038bab84960023be2139f of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 01:01:11,154 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 21 from task 89878b144856fead9161dafeed1b416d of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 01:01:12,482 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 21 from task 41b6ec89308b04922c9f043faa83c7b9 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 01:01:21,515 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 21 from task cc83d9b2bb53ac446aad4ed344259223 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 01:01:22,282 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 21 from task 728e40516877edb97748f12a56b9a4c4 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525).
2021-02-15 01:01:23,136 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 21 from task 957a638e36686032cb9f9bd7144915f3 of job dfa837311853a69cdca82db082270dff at 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557).
2021-02-15 01:01:23,385 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 21 from task 072aa4c8afc4326e05d926468125cfac of job dfa837311853a69cdca82db082270dff at 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803).
2021-02-15 01:01:25,380 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 22 (type=CHECKPOINT) @ 1613350885367 for job dfa837311853a69cdca82db082270dff.
2021-02-15 01:01:25,631 DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received acknowledge message for checkpoint 22 from task ad1ef05bb02038bab84960023be2139f of job dfa837311853a69cdca82db082270dff at 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283).
2021-02-15 01:01:28,495 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (1/4) (attempt #11) with attempt id 19a29174acd3e16276da75a060a45f89 to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 01:01:28,498 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (2/4) (attempt #11) with attempt id 8ac7c1c87b1809407eec0d64bd20a7cb to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 01:01:28,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (3/4) (attempt #11) with attempt id 97f2b0292ba8efb63aea4b44e217f382 to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 01:01:28,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom Source (4/4) (attempt #11) with attempt id 31d061561664d3b35d4ffb51060783e3 to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
2021-02-15 01:01:28,510 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (1/4) (attempt #11) with attempt id a68642cf1f5c0d64625c07ee07dbb57c to 10.1.0.4:36079-9ecc1c @ fv-az127-111.internal.cloudapp.net (dataPort=43283) with allocation id 34ba1021819900f400145f4eeb68cf98
2021-02-15 01:01:28,511 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (2/4) (attempt #11) with attempt id 73854761d80223752d9e973e49899574 to 10.1.0.4:46341-c100f5 @ fv-az127-111.internal.cloudapp.net (dataPort=40557) with allocation id 7dbde52bef472bf82dfe6b9c51030bbe
2021-02-15 01:01:28,513 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (3/4) (attempt #11) with attempt id cf6c2d52b4929d79db4a1db09ae7279d to 10.1.0.4:42443-4be16a @ fv-az127-111.internal.cloudapp.net (dataPort=40525) with allocation id 6bd434b58457e3ede2b354f5ec514bda
2021-02-15 01:01:28,514 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Unnamed (4/4) (attempt #11) with attempt id ebb41ec1aaa5ba8ddf134b3f9ade46ac to 10.1.0.4:44619-9da455 @ fv-az127-111.internal.cloudapp.net (dataPort=40803) with allocation id 9e93afdbf0a591fe019a790882e49405
{code}

[~sjwiesman] do you have a guess here considering your work on FLINK-19467?;;;","01/Apr/21 08:02;mapohl;{quote}
BTW by increasing timeout of the build you mean the timeout ~5h timeout for running all e2e tests? There is also something interesting I spotted that I can not explain myself. If you sum up individual execution times of all tests the successful build takes longer. However the Azure dashboard shows that the failed build took ~30 min longer. Do we know why is that?
{quote}
Yes, I meant increasing the time until the e2e times out. But as you pointed out - there must be another issue with this test.

About the difference in execution time vs the time which is shown on the Azure dashboard: How did you come up with that? I checked [the successful build you shared above|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15854&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529]:
{code}
$ paste -sd+ <(grep -F ""[PASS]"" 171-azureci.log | grep -o ""passed after [0-9]* minutes and [0-9]*"" | cut -d' ' -f 6) | bc
2597
$ paste -sd+ <(grep -F ""[PASS]"" 171-azureci.log | grep -o ""passed after [0-9]* minutes and [0-9]*"" | cut -d' ' -f 3) | bc
157
{code}
Which means 157mins and 2597secs which boils down to 3h 20mins 17secs in contrast to the AzureCI dashboard providing 3h 36mins 56secs for the overall e2e runtime.;;;","06/Apr/21 13:40;sjwiesman;I looked at the test and I don't see anything obvious that would point to FLINK-19467. Just to be safe I kicked off a build that reverts the test to use the old RocksDBStateBackend class. If this passes without issue then my change was the problem. You can follow the build here.

 

https://dev.azure.com/sjwiesman/Flink/_build/results?buildId=56&view=results;;;","07/Apr/21 05:57;mapohl;{quote}
I looked at the test and I don't see anything obvious that would point to FLINK-19467. Just to be safe I kicked off a build that reverts the test to use the old RocksDBStateBackend class. If this passes without issue then my change was the problem. You can follow the build here.
{quote}

I guess that's not what we can conclude, unfortunately, since it's not constantly failing. [~pnowojski] could someone of you have a look at this?;;;","08/Apr/21 18:19;mapohl;I unassigned myself after verifying with [~AHeise]. It's in the runtime team's backlog.;;;","15/Apr/21 10:33;roman;From what I see in the logs:
 1. Test times out because the needed 10 checkpoints take too much time each (seconds in PASS case vs tens of seconds to minutes in FAIL case)
 2. Checkpoints take longer either because of either longer alignment (~20s) or  longer time to trigger checkpoint at the source (40-55s). Both indicate high back-pressure:
{code:java}
2021-02-08 22:27:13,093 DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Flat Map -> Sink: Unnamed (1/4)#3 - finished synchronous part of checkpoint 7. Alignment duration: 21525 ms, snapshot duration -1 ms, is unaligned checkpoint : false
{code}
{code:java}
2021-02-08 22:27:19,560 DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Trigger checkpoint 9@1612823239549 for 4343c6f1810b6f077dd688312b88e60c.
2021-02-08 22:28:06,724 DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Source: Custom Source (2/4)#4 - finished synchronous part of checkpoint 9. Alignment duration: 0 ms, snapshot duration -1 ms, is unaligned checkpoint : false
{code}
Nor sync neither async phases take more than 5s.

 

Sources generate data without any delay currently, so back-pressure can happen very likely if the machine is slow. The test tries to perform 10 checkpoints in 10 minutes; if each takes a bit more than 1 minute then it's likely to timeout. This can also explain why RocksDB fails and HashTable doesn't (the latter works faster).

 

So propose:
 1. Increase test timeout from 10m to 15m
 2. Induce delay between emitting elements in sources (currently none,  change to 100ms)
 3. Log time from receiving checkpoint RPC to actually executing it if it exceeds some threshold (say 10s, WARN level)

Turning on UC could eliminate long alignment but not the delay of triggering checkpoint at the sources. So wouldn't enable it and wait for it to become a default.
 ;;;","20/Apr/21 20:00;roman;Merged into master as https://github.com/apache/flink/commit/2f4d9d8d3cf781ee03e60cc21db0addb9ebcc0f4;;;","19/May/21 03:00;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18107&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c&l=37432
Maybe we could merge the fix to the 1.12？;;;","19/May/21 12:30;roman;Also merged into 1.12 as e09c919103a19b97abd169732e44ef7231fab1be

and into 1.13 as 2f1e7928258876970ac9859e7afe070c9f03fa96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
statefun-testutil can't assert messages function sends to itself,FLINK-21324,13357635,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Yarosh,Yarosh,Yarosh,08/Feb/21 22:11,01/Mar/21 16:38,13/Jul/23 08:07,01/Mar/21 16:38,statefun-2.2.2,,,,,,,,statefun-3.0.0,,,,,,,Stateful Functions,,,,,0,pull-request-available,,,,,"Assertions don't work for messages sent by functions to themselves. The reason is that TestContext doesn't add a message to responses: 
{code:java}
@Override
public void send(Address to, Object message) {
  if (to.equals(selfAddress)) {
    messages.add(new Envelope(self(), to, message));
    return;
  }
  responses.computeIfAbsent(to, ignore -> new ArrayList<>()).add(message);
}
{code}
Instead of adding the message to responses the method returns right after message added to messages. 

Here is the example of the assertion that doesn't work in case a function sent a message to itself:
{code:java}
assertThat(harness.invoke(aliceDiseaseDiagnosedEvent()), sentNothing());
{code}
The test won't fail even though the message was really sent.",,sjwiesman,Yarosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 01 16:37:34 UTC 2021,,,,,,,,,,"0|z0nibk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/21 22:31;Yarosh;Please review pull request that fixes this bug https://github.com/apache/flink-statefun/pull/199;;;","01/Mar/21 16:37;sjwiesman;fixed in master: a89d6a066bdea6cc656ec0da99f6b4517ca660b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionContextTest.testCatalogs fail,FLINK-21318,13357439,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,maguowei,maguowei,08/Feb/21 04:08,15/Feb/21 08:41,13/Jul/23 08:07,15/Feb/21 08:41,1.13.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13064&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=e5682198-9e22-5770-69f6-7551182edea8
{code:java}
2021-02-07T22:25:13.3526341Z [ERROR] testDatabases(org.apache.flink.table.client.gateway.local.ExecutionContextTest)  Time elapsed: 0.044 s  <<< ERROR!
2021-02-07T22:25:13.3526885Z org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.
2021-02-07T22:25:13.3527484Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:972)
2021-02-07T22:25:13.3528070Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createExecutionContext(ExecutionContextTest.java:398)
2021-02-07T22:25:13.3528676Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createCatalogExecutionContext(ExecutionContextTest.java:434)
2021-02-07T22:25:13.3529280Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testDatabases(ExecutionContextTest.java:230)
2021-02-07T22:25:13.3529775Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-07T22:25:13.3530232Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-07T22:25:13.3530773Z 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-07T22:25:13.3531246Z 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2021-02-07T22:25:13.3531641Z 	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:68)
2021-02-07T22:25:13.3532223Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:326)
2021-02-07T22:25:13.3532909Z 	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:89)
2021-02-07T22:25:13.3533369Z 	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:97)
2021-02-07T22:25:13.3534119Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(PowerMockJUnit44RunnerDelegateImpl.java:310)
2021-02-07T22:25:13.3534995Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTestInSuper(PowerMockJUnit47RunnerDelegateImpl.java:131)
2021-02-07T22:25:13.3535784Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.access$100(PowerMockJUnit47RunnerDelegateImpl.java:59)
2021-02-07T22:25:13.3536590Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner$TestExecutorStatement.evaluate(PowerMockJUnit47RunnerDelegateImpl.java:147)
2021-02-07T22:25:13.3537395Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.evaluateStatement(PowerMockJUnit47RunnerDelegateImpl.java:107)
2021-02-07T22:25:13.3538183Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTest(PowerMockJUnit47RunnerDelegateImpl.java:82)
2021-02-07T22:25:13.3538998Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(PowerMockJUnit44RunnerDelegateImpl.java:298)
2021-02-07T22:25:13.3539633Z 	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:87)
2021-02-07T22:25:13.3540039Z 	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:50)
2021-02-07T22:25:13.3540592Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:218)
2021-02-07T22:25:13.3541269Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
2021-02-07T22:25:13.3541969Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
2021-02-07T22:25:13.3542523Z 	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
2021-02-07T22:25:13.3543045Z 	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
2021-02-07T22:25:13.3543567Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
2021-02-07T22:25:13.3544312Z 	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
2021-02-07T22:25:13.3544931Z 	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
2021-02-07T22:25:13.3545449Z 	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
2021-02-07T22:25:13.3545854Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-02-07T22:25:13.3546208Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-02-07T22:25:13.3546563Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-07T22:25:13.3546971Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-07T22:25:13.3547378Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-07T22:25:13.3547770Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-07T22:25:13.3548177Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-07T22:25:13.3548571Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-07T22:25:13.3548958Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-02-07T22:25:13.3549445Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-02-07T22:25:13.3549977Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
2021-02-07T22:25:13.3550468Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
2021-02-07T22:25:13.3550967Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-02-07T22:25:13.3551530Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-02-07T22:25:13.3552034Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-02-07T22:25:13.3552761Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-02-07T22:25:13.3553243Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-02-07T22:25:13.3553677Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-02-07T22:25:13.3554474Z Caused by: javax.xml.transform.TransformerFactoryConfigurationError: Provider com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl could not be instantiated: java.lang.reflect.InvocationTargetException
2021-02-07T22:25:13.3555185Z 	at java.xml/javax.xml.transform.FactoryFinder.newInstance(FactoryFinder.java:181)
2021-02-07T22:25:13.3555648Z 	at java.xml/javax.xml.transform.FactoryFinder.find(FactoryFinder.java:257)
2021-02-07T22:25:13.3556097Z 	at java.xml/javax.xml.transform.TransformerFactory.newInstance(TransformerFactory.java:126)
2021-02-07T22:25:13.3556559Z 	at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:2784)
2021-02-07T22:25:13.3557106Z 	at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:2769)
2021-02-07T22:25:13.3557542Z 	at org.apache.hadoop.hive.conf.HiveConf.getConfVarInputStream(HiveConf.java:3628)
2021-02-07T22:25:13.3557987Z 	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:4051)
2021-02-07T22:25:13.3558403Z 	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:4003)
2021-02-07T22:25:13.3558843Z 	at org.apache.flink.table.catalog.hive.HiveTestUtils.createHiveConf(HiveTestUtils.java:122)
2021-02-07T22:25:13.3559430Z 	at org.apache.flink.table.catalog.hive.HiveTestUtils.createHiveCatalog(HiveTestUtils.java:85)
2021-02-07T22:25:13.3560003Z 	at org.apache.flink.table.client.gateway.local.DependencyTest$TestHiveCatalogFactory.createCatalog(DependencyTest.java:277)
2021-02-07T22:25:13.3560578Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.createCatalog(ExecutionContext.java:396)
2021-02-07T22:25:13.3561163Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$5(ExecutionContext.java:684)
2021-02-07T22:25:13.3561646Z 	at java.base/java.util.HashMap.forEach(HashMap.java:1336)
2021-02-07T22:25:13.3562109Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:681)
2021-02-07T22:25:13.3562765Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:265)
2021-02-07T22:25:13.3563324Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:677)
2021-02-07T22:25:13.3563955Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:565)
2021-02-07T22:25:13.3564561Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:187)
2021-02-07T22:25:13.3565101Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:962)
2021-02-07T22:25:13.3565469Z 	... 47 more
2021-02-07T22:25:13.3565711Z Caused by: java.lang.reflect.InvocationTargetException
2021-02-07T22:25:13.3566095Z 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2021-02-07T22:25:13.3566608Z 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2021-02-07T22:25:13.3567190Z 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2021-02-07T22:25:13.3567726Z 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
2021-02-07T22:25:13.3568173Z 	at java.xml/javax.xml.transform.FactoryFinder.newInstance(FactoryFinder.java:169)
2021-02-07T22:25:13.3568565Z 	... 66 more
2021-02-07T22:25:13.3569205Z Caused by: java.lang.IllegalAccessError: class com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl (in unnamed module @0x7e18b9e6) cannot access class jdk.xml.internal.JdkXmlUtils (in module java.xml) because module java.xml does not export jdk.xml.internal to unnamed module @0x7e18b9e6
2021-02-07T22:25:13.3570045Z 	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl.<init>(TransformerFactoryImpl.java:251)
2021-02-07T22:25:13.3570409Z 	... 71 more
 
{code}",,dwysakowicz,jark,lirui,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 15 08:41:57 UTC 2021,,,,,,,,,,"0|z0nh40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/21 14:02;dwysakowicz;[~lirui] Do you mind having a look? It might be some problem with Hive dependencies.;;;","10/Feb/21 14:19;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13177&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=e5682198-9e22-5770-69f6-7551182edea8;;;","11/Feb/21 02:38;lirui;Does this failure happen in the JDK11 test? Hive in general doesn't run with JDK11. We might need to disable these tests in that case.;;;","11/Feb/21 07:06;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13220&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=e5682198-9e22-5770-69f6-7551182edea8;;;","11/Feb/21 07:09;dwysakowicz;Yes, it happens with JDK 11. Could you prepare a pull request to disable it on JAVA 11? You can use {{@Category(FailsOnJava11.class)}} annotation for that.;;;","15/Feb/21 08:18;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13324&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=e5682198-9e22-5770-69f6-7551182edea8;;;","15/Feb/21 08:41;dwysakowicz;Fixed in 9b6f076a66970d3d3ef710f8d5ee66d75d87eba5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase.testStopSavepointWithBoundedInputConcurrently is unstable,FLINK-21312,13357345,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,guoyangze,guoyangze,07/Feb/21 04:43,11/Feb/21 14:58,13/Jul/23 08:07,11/Feb/21 14:58,,,,,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13036&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2,,guoyangze,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 07 04:46:01 UTC 2021,,,,,,,,,,"0|z0ngj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/21 04:46;guoyangze;cc [~roman_khachatryan];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkSecurityManager might avoid fatal system exits,FLINK-21306,13357106,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,05/Feb/21 14:43,28/Aug/21 11:08,13/Jul/23 08:07,30/Mar/21 06:23,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,,"In FLINK-15156, we introduced a feature that allows users to log or completely disable calls to System.exit().
This feature is enabled for certain threads / code sections intended to execute user-code.
However, some user code calls might still lead to fatal errors, which we want to handle by killing the Flink process.
It is likely that this new change (which is disabled by default) can lead to a situation where Flink should exit immediately, but it doesn't (thus leaving the system in an inconsistent state)",,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15156,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 30 06:23:37 UTC 2021,,,,,,,,,,"0|z0nf28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/21 14:53;rmetzger;I propose introducing a {{FlinkSecurityManager.forceExit(int exitCode)}} method, that the Flink framework developers can use to force an exit, irrespective of the configured behavior.;;;","05/Feb/21 15:26;pnowojski;I would like to clarify here. This ticket is about a concern, that some Flink (non-user) `System.exit()` calls could be incorrectly ignored. Example of such case might be all code paths leading to {{FatalExitExceptionHandler}}, for example via {{FutureUtils#assertNoException}}. It is just a safety net, that's used to handle very very unexpected bugs in Flink itself, as normally all kind of errors/exceptions should be handled more gracefully. Which is for example used in {{CheckpointedInputGate#waitForPriorityEvents}}.  However bugs can happen, and if {{FutureUtils#assertNoException}} is triggered, it should never be ignored, regardless of the {{FlinkSecurityManager}} configuration.;;;","30/Mar/21 06:23;rmetzger;Merged to master in https://github.com/apache/flink/commit/4b2139189e2c98018c77d25b919ca1f11dbf4a0d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decouple window aggregate allow lateness with state ttl configuration,FLINK-21301,13357009,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingzhang,jingzhang,jingzhang,05/Feb/21 09:42,21/Nov/22 20:16,13/Jul/23 08:07,10/Jun/21 04:10,,,,,,,,,1.14.0,,,,,,,Table SQL / API,,,,,0,auto-unassigned,pull-request-available,,,,"Currently, state retention time config will also effect state clean behavior of Window Aggregate, which is unexpected for most users.

E.g for the following example,  User would set `MinIdleStateRetentionTime` to 1 Day to clean state in `deduplicate` . However, it will also effects clean behavior of window aggregate. For example, 2021-01-04 data would clean at 2021-01-06 instead of 2021-01-05. 
{code:sql}
SELECT
 DATE_FORMAT(tumble_end(ROWTIME ,interval '1' DAY),'yyyy-MM-dd') as stat_time,
 count(1) first_phone_num
FROM (
 SELECT 
 ROWTIME,
 user_id,
 row_number() over(partition by user_id, pdate order by ROWTIME ) as rn
 FROM source_kafka_biz_shuidi_sdb_crm_call_record 
) cal 
where rn =1
group by tumble(ROWTIME,interval '1' DAY);{code}
It's better to decouple window aggregate allow lateness with `MinIdleStateRetentionTime` .",,eric.xiao,godfreyhe,jark,jingzhang,leonard,libenchao,lzljs3620320,xiangcaohello,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 21 20:03:53 UTC 2022,,,,,,,,,,"0|z0nego:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/21 11:06;jark;We can enable state ttl for window state only when emit strategy is specified. What do you think [~qingru zhang]?;;;","05/Feb/21 14:06;jingzhang;[~jark] Thanks for your reply. Of course, we could set allow lateness to state ttl for window state if `lateFireDelayEnabled` is enabled. However, it's more flexible if we could set allow lateness different with state ttl. For above example, user may want to set `MinIdleStateRetentionTime` to 1 day, but only want set allow lateness to 5 min ~10min instead of 1 day. 

Looking forward to your reply, thanks.;;;","16/Apr/21 22:43;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:47;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","16/May/21 09:48;jingzhang;[~godfreyhe] [~jark]  I could fix the bug that only enable allow lateness only when emit strategy is specified. However I think it's better to decouple allow lateness with state retention configuration because of the following reasons:
 # it is not a intuitive to use state retention configuration to control windowed operator allow lateness 
 # state retention configuration is global configuration for a job which would effects all state operator. If a job contains multiple state operators, e.g (deduplicate/ unbounded aggregate/window operator), user may want to set different values for different operator. For the above mentioned example, 1 day state retention for deduplicate, 5 min allow lateness for window aggregate.
{code:sql}
SELECT
 DATE_FORMAT(tumble_end(ROWTIME ,interval '1' DAY),'yyyy-MM-dd') as stat_time,
 count(1) as cnt, sum(num) as sum_num
FROM (
 SELECT 
 ROWTIME,
 user_id,
 row_number() over(partition by user_id, pdate order by ROWTIME ) as rn
 FROM source_kafka_biz_shuidi_sdb_crm_call_record 
) cal 
where rn =1
group by tumble(ROWTIME,interval '1' DAY);{code}
 

What's your opinion about the issue?;;;","17/May/21 02:59;jark;Hi [~qingru zhang], I thought about this again. Your requirement sounds like a fine-grained state TTL that different operators can have different TTL. Maybe can be addressed by this issue FLINK-17173? ;;;","17/May/21 09:46;jingzhang;Hi [~jark] Thanks for your reply.

I agree that things will be simpler if a fine-grained state TTL that different operators can have different TTL.

However, IMO, allow lateness of window operator has relationship with TTL, but has a little difference. If enable allow lateness, the behavior of state ttl on window operator would be effected. But set state ttl could effect the behavior of allow lateness on window operator? I means allow lateness not only effects state ttl but also trigger behavior and emit behavior. It's better to introduce new configuration or new syntax to handle allow lateness.

What do you think?;;;","17/May/21 11:28;jark;Thanks [~qingru zhang], I'm fine with introducing configurtions e.g. {{table.exec.emit.late-fire.allow-lateness}}. But please still mark it {{@Experimental}} and also keep the state ttl can still take effect when the new config is not set. ;;;","10/Jun/21 04:10;godfreyhe;Fixed in 1.14.0: 145502801c789143074536952f270bea56b30253;;;","21/Jun/21 08:00;lzljs3620320;I'm skeptical about this config option.
 * The trigger of window is complicated, introducing a new config option make more complicated.
 * What is the difference between allow lateness and TTL? The difference between allow lateness and TTL is very subtle. I think that in essence, after late emit is turned on, there is no difference between ordinary AGG and window AGG. They are basically doing the same thing. There is no need to introduce redundant configuration to user.;;;","21/Jun/21 09:20;lzljs3620320;And what's the difference between allow-lateness and late-fire? 

From the name, I think with allow-lateness on, there should be late fire ability. They seem to have the overlap ability.;;;","28/Apr/22 22:42;xiangcaohello;[~lzljs3620320]  -what is the table config for late-fire? I didn't find it in the documentation. [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/config/]-

 

-I also find it useful to have this allow-lateness config separate from state ttl. For example, when I have a join and windowing in the same SQL Script, I only want to set state ttl for join but not allowed lateness for windowing. I don't find another way to do so without table.exec.emit.allow-lateness-

I misunderstood the current solution. [https://www.mail-archive.com/issues@flink.apache.org/msg498605.html] provides a good summarization. ;;;","21/Nov/22 20:03;eric.xiao;Hi there, I had a read of [https://www.mail-archive.com/user@flink.apache.org/msg43316.html] and was wondering if there was any particular reason why the allow-lateness configuration is not enabled for Window TVF aggregations? As I saw that Group Window Aggregation is deprecated [1].

Would it be something hard to implement? We might have bandwidth on our team to work on it.

 

[1] - https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/window-agg/#group-window-aggregation;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python StreamRowBasedOperationITTests fails on azure,FLINK-21292,13356966,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dwysakowicz,dwysakowicz,05/Feb/21 07:54,17/Aug/21 01:58,13/Jul/23 08:07,17/Aug/21 01:58,1.13.0,1.14.0,,,,,,,1.14.0,,,,,,,API / Python,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12959&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=613f538c-bcef-59e6-f9cd-9714bec9fb97

{code}
-- Docs: https://docs.pytest.org/en/stable/warnings.html
============================= slowest 20 durations =============================
8.65s call     pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_map
7.77s call     pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate_list_view
7.74s call     pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate
7.74s call     pyflink/table/tests/test_pandas_udf.py::BlinkStreamPandasUDFITTests::test_all_data_types
7.61s call     pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_slide_group_window_aggregate_function
7.29s call     pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_map
6.88s call     pyflink/table/tests/test_sql.py::JavaSqlTests::test_java_sql_ddl
6.20s call     pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_aggregate
5.97s call     pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_basic_functionality
5.82s call     pyflink/table/tests/test_udtf.py::PyFlinkBatchUserDefinedTableFunctionTests::test_table_function
4.60s call     pyflink/table/tests/test_udf.py::PyFlinkBlinkBatchUserDefinedFunctionTests::test_udf_in_join_condition_2
4.44s call     pyflink/datastream/tests/test_connectors.py::ConnectorTests::test_stream_file_sink
4.38s call     pyflink/dataset/tests/test_execution_environment.py::ExecutionEnvironmentTests::test_execute
4.08s call     pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_function
4.03s call     pyflink/table/tests/test_dependency.py::BlinkStreamDependencyTests::test_set_requirements_with_cached_directory
3.83s call     pyflink/datastream/tests/test_data_stream.py::DataStreamTests::test_primitive_array_type_info
3.83s call     pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_aggregate_with_pandas_udaf
3.81s call     pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_with_aux_group
3.62s call     pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_set_requirements_with_cached_directory
3.58s call     pyflink/table/tests/test_udf.py::PyFlinkBlinkBatchUserDefinedFunctionTests::test_udf_in_join_condition
=========================== short test summary info ============================
FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate
===== 1 failed, 676 passed, 20 skipped, 271 warnings in 414.93s (0:06:54) ======
ERROR: InvocationError for command /__w/1/s/flink-python/.tox/py38-cython/bin/pytest --durations=20 (exited with code 1)

{code}",,dian.fu,dianfu,dwysakowicz,hxbks2ks,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 17 01:58:03 UTC 2021,,,,,,,,,,"0|z0ne74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/21 09:40;hxbks2ks;I will look into this.;;;","16/Apr/21 10:46;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:47;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","30/May/21 11:27;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","07/Jun/21 10:58;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","16/Aug/21 03:21;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22153&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22478;;;","17/Aug/21 01:58;hxbks2ks;Hi [~xtsong] The failed case you provided is essentially a problem that occurred at https://issues.apache.org/jira/browse/FLINK-23765. And I have just merged the  patch to fix it. As for the problems described in the original JIRA, when we fixed other problems before, it should have been fixed by the way, so I temporarily closed this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application mode ignores the pipeline.classpaths configuration,FLINK-21289,13356912,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,PChou,PChou,PChou,05/Feb/21 06:05,15/Dec/21 01:44,13/Jul/23 08:07,10/Nov/21 08:22,1.11.2,1.12.1,,,,,,,1.13.6,1.14.3,1.15.0,,,,,Client / Job Submission,Deployment / Kubernetes,Deployment / YARN,,,0,pull-request-available,,,,,"我尝试将flink作业以application mode方式提交到kubernetes上运行。但程序的依赖包并不完全存在于local:///opt/flink/usrlib/xxxx.jar中。导致找不到类。

在yarn上可以工作，是因为我们用 {color:#ff0000}-C [http://xxxx|http://xxxx/] {color}的方式，让依赖可以被URLClassloader加载。

但我发现，当实验提交到kubernetes时，-C只会在 configmap/flink-conf.yaml 中生成一个pipeline.classpaths 配置条目。我们的main函数可以执行，但是在加载外部依赖类的时候提示找不到类。

通过阅读源码，*我发现运行用户代码的类加载器实际并没有把 pipeline.classpaths 中的条目加入候选URL*，这导致了无法加载类的情况。从源码中，我也发现，通过将依赖包放在usrlib目录下（默认的userClassPath）可以解决问题。但我们的依赖可能是动态的，不合适一次性打到镜像里面。

我提议可以改进这个过程，将pipeline.classpaths也加入到对应的类加载器。这个改动很小，我自己经过测试，可以完美解决问题。

 

 

English translation:

I'm trying to submit flink job to kubernetes cluster with application mode, but throw ClassNotFoundException when some dependency class is not shipped in kind of local:///opt/flink/usrlib/xxxx.jar.

This works on yarn, since we use {color:#ff0000}-C [http://xxxx|http://xxxx/]{color} command line style that let dependency class  can be load by URLClassloader.

But i figure out that not works on kubernetes. When submit to kubernetes cluster, -C is only shipped as item ""pipeline.classpaths"" in configmap/flink-conf.yaml。

After read the source code, *i find out that the Classloader launching the ""main"" entry of user code miss consider adding pipeline.classpaths into candidates URLs*. from source code, i also learn that we can ship the dependency jar in the usrlib dir to solve the problem. But that failed for us, we are not _preferred_ to ship dependencies in image at compile time, since dependencies are known dynamically in runtime

I proposed to improve the process, let the Classloader consider usrlib as well as pipeline.classpaths, this is a quite little change. I test the solution and it works quite well

 

 ","flink: 1.11

kubernetes: 1.15

 ",Echo Lee,PChou,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/21 06:05;PChou;0001-IMP.patch;https://issues.apache.org/jira/secure/attachment/13020051/0001-IMP.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 10 08:22:26 UTC 2021,,,,,,,,,,"0|z0ndv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/21 03:42;wangyang0918;[~PChou] Thanks for reporting this issue. I think it may be also a valid issue for Yarn application deployment. Right?

Do you want to convert the attached patch to a github PR and work on this ticket?;;;","07/Feb/21 04:54;PChou;[~fly_in_gis] yes, i'm going to create a PR together with yarn deployment fix:);;;","15/Oct/21 06:08;wangyang0918;This ticket is not resolved since the PR is never reviewed and merged.;;;","15/Oct/21 06:12;wangyang0918;[~PChou] It is really a pity that your PR is not merged eventually. Do you still want to work on this? I could help with review and merging.;;;","27/Oct/21 03:19;PChou;[~wangyang0918] Since it’s been a long time in the past, I am not sure if this BUG has been fixed. I need to review this PR conflict;;;","27/Oct/21 08:11;wangyang0918;Even though we have a minor refactor of this part, but I am afraid this issue still exists.;;;","02/Nov/21 11:15;trohrmann;I agree with [~wangyang0918]. I think your change is still very valuable [~PChou] and it would be great if you could rebase your PR. Then I am sure that we can quickly merge it.;;;","10/Nov/21 08:22;trohrmann;Fixed via

1.15.0: 40d50f177c8ac63057dea2c755173a0dc138ede5
1.14.1: 18af954152b4c5877f3e3efdd98f82a08703aa3c
1.13.4: ef4a8fc40d5a2d0ba6d564e2edaffa8ea5d23bbb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLClientSchemaRegistryITCase fails to download testcontainers/ryuk:0.3.0,FLINK-21277,13356691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,04/Feb/21 08:20,28/May/21 08:18,13/Jul/23 08:07,04/Feb/21 14:36,1.12.1,1.13.0,,,,,,,1.12.2,1.13.0,,,,,,Tests,,,,,0,pull-request-available,,,,,"Tests using testcontainers fail from time to time downloading required images. Most probably caused by: https://github.com/testcontainers/testcontainers-java/issues/3574

We should upgrade testcontainers to 1.15.1

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12890&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12874&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
Feb 03 19:14:14 java.lang.RuntimeException: Could not build the flink-dist image
Feb 03 19:14:14 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:281)
Feb 03 19:14:14 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.<init>(SQLClientSchemaRegistryITCase.java:88)
Feb 03 19:14:14 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Feb 03 19:14:14 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Feb 03 19:14:14 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Feb 03 19:14:14 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
Feb 03 19:14:14 	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
Feb 03 19:14:14 	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
Feb 03 19:14:14 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 03 19:14:14 	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
Feb 03 19:14:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Feb 03 19:14:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Feb 03 19:14:14 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Feb 03 19:14:14 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Feb 03 19:14:14 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Feb 03 19:14:14 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Feb 03 19:14:14 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Feb 03 19:14:14 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
Feb 03 19:14:14 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
Feb 03 19:14:14 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Feb 03 19:14:14 	at java.lang.Thread.run(Thread.java:748)
Feb 03 19:14:14 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.NotFoundException: Status 404: {""message"":""No such image: testcontainers/ryuk:0.3.0""}
Feb 03 19:14:14 
Feb 03 19:14:14 	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
Feb 03 19:14:14 	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
Feb 03 19:14:14 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)
Feb 03 19:14:14 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.buildBaseImage(FlinkContainer.java:309)
Feb 03 19:14:14 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:268)
Feb 03 19:14:14 	... 20 more
Feb 03 19:14:14 Caused by: com.github.dockerjava.api.exception.NotFoundException: Status 404: {""message"":""No such image: testcontainers/ryuk:0.3.0""}
Feb 03 19:14:14 
Feb 03 19:14:14 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.execute(DefaultInvocationBuilder.java:241)
Feb 03 19:14:14 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.post(DefaultInvocationBuilder.java:125)
Feb 03 19:14:14 	at org.testcontainers.shaded.com.github.dockerjava.core.exec.CreateContainerCmdExec.execute(CreateContainerCmdExec.java:33)
Feb 03 19:14:14 	at org.testcontainers.shaded.com.github.dockerjava.core.exec.CreateContainerCmdExec.execute(CreateContainerCmdExec.java:13)
Feb 03 19:14:14 	at org.testcontainers.shaded.com.github.dockerjava.core.exec.AbstrSyncDockerCmdExec.exec(AbstrSyncDockerCmdExec.java:21)
Feb 03 19:14:14 	at org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd.exec(AbstrDockerCmd.java:35)
Feb 03 19:14:14 	at org.testcontainers.shaded.com.github.dockerjava.core.command.CreateContainerCmdImpl.exec(CreateContainerCmdImpl.java:595)
Feb 03 19:14:14 	at org.testcontainers.utility.ResourceReaper.start(ResourceReaper.java:91)
Feb 03 19:14:14 	at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:203)
Feb 03 19:14:14 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.imageExists(FlinkContainer.java:316)
Feb 03 19:14:14 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.buildBaseImage(FlinkContainer.java:301)
Feb 03 19:14:14 	... 21 more

{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 04 14:36:08 UTC 2021,,,,,,,,,,"0|z0nci0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/21 14:36;dwysakowicz;Fixed in:
* master
** 05aacc4262960340085a533b5515c17055d9cd76
* 1.12.2
** 2ab58f5fcc485f7949b0b94f7e9e1b06bbe68414;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"At per-job mode, during the exit of the JobManager process, if ioExecutor exits at the end, the System.exit() method will not be executed.",FLINK-21274,13356675,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wjc920,wjc920,wjc920,04/Feb/21 06:58,11/Feb/21 18:33,13/Jul/23 08:07,11/Feb/21 18:33,1.10.1,1.11.0,1.12.0,1.9.3,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"h2. =============Latest issue description(2021.02.07)==================

I want to try to describe the issue in a more concise way:

*My issue only appears in per-job mode,*

In JsonResponseHistoryServerArchivist#archiveExecutionGraph, submit the archive task to ioExecutor for execution. At the same time, ClusterEntrypoint#stopClusterServices exits multiple thread pools in parallel (for example, commonRpcService, metricRegistry, MetricRegistryImpl#executor(in metricRegistry.shutdown())). Think about it, assuming that the archiving process takes 10 seconds to execute, then ExecutorUtils.nonBlockingShutdown will wait 10 before exiting. However, through testing, it was found that the JobManager process exited immediately after commonRpcService and metricRegistry exited. At this time, ExecutorUtils.nonBlockingShutdown is still waiting for the end of the archiving process, so the archiving process will not be completely executed.

*There are two specific reproduction methods:*

*Method one:*

Modify the org.apache.flink.runtime.history.FsJobArchivist#archiveJob method to wait 5 seconds before actually writing to HDFS (simulating a slow write speed scenario).
{code:java}
public static Path archiveJob(Path rootPath, JobID jobId, Collection<ArchivedJson> jsonToArchive)
    throws IOException {
    try {
        FileSystem fs = rootPath.getFileSystem();
        Path path = new Path(rootPath, jobId.toString());
        OutputStream out = fs.create(path, FileSystem.WriteMode.NO_OVERWRITE);

        try {
            LOG.info(""===========================Wait 5 seconds.."");
            Thread.sleep(5000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        try (JsonGenerator gen = jacksonFactory.createGenerator(out, JsonEncoding.UTF8)) {
            ...  // Part of the code is omitted here
        } catch (Exception e) {
            fs.delete(path, false);
            throw e;
        }
        LOG.info(""Job {} has been archived at {}."", jobId, path);
        return path;
    } catch (IOException e) {
        LOG.error(""Failed to archive job."", e);
        throw e;
    }
}
{code}
The above modification will cause the archive to fail.

*Method two:*

In ClusterEntrypoint#stopClusterServices, before ExecutorUtils.nonBlockingShutdown is called, submit a task that waits 10 seconds to ioExecutor.
{code:java}
ioExecutor.execute(new Runnable() {
    @Override
    public void run() {
        try {
            LOG.info(""===ioExecutor before sleep"");
            Thread.sleep(10000);
            LOG.info(""===ioExecutor after sleep"");
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
});
terminationFutures.add(ExecutorUtils.nonBlockingShutdown(shutdownTimeout, TimeUnit.MILLISECONDS, ioExecutor));
{code}
According to the above modification, ===ioExecutor before sleep will be printed, but ===ioExecutor after sleep will not be printed.

*The root cause of the above issue is that all user threads (in Akka ActorSystem) have exited during the waiting, and finally the daemon thread (in ioExecutor) cannot be executed completely.*

 

{color:#de350b} *If you already understand my issue, you can skip the following old version of the issue description, and browse the comment area directly*{color}

 

 

 

 

 
h2. ================Older issue description(2021.02.04)================

This is a partial configuration of my Flink History service(flink-conf.yaml), and this is also the configuration of my Flink client.
{code:java}
jobmanager.archive.fs.dir: hdfs://hdfsHACluster/flink/completed-jobs/
historyserver.archive.fs.dir: hdfs://hdfsHACluster/flink/completed-jobs/
{code}
I used {color:#0747a6}flink run -m yarn-cluster /cloud/service/flink/examples/batch/WordCount.jar{color} to submit a WorkCount task to the Yarn cluster. Under normal circumstances, after the task is completed, the flink job execution information will be archived to HDFS, and then the JobManager process will exit. However, when this archiving process takes a long time (maybe the HDFS write speed is slow), the task archive file upload fails.

The specific reproduction method is as follows:

Modify the {color:#0747a6}org.apache.flink.runtime.history.FsJobArchivist#archiveJob{color} method to wait 5 seconds before actually writing to HDFS (simulating a slow write speed scenario).
{code:java}
public static Path archiveJob(Path rootPath, JobID jobId, Collection<ArchivedJson> jsonToArchive) 
    throws IOException {
    try {
        FileSystem fs = rootPath.getFileSystem();
        Path path = new Path(rootPath, jobId.toString());
        OutputStream out = fs.create(path, FileSystem.WriteMode.NO_OVERWRITE);

        try {
            LOG.info(""===========================Wait 5 seconds.."");
            Thread.sleep(5000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        try (JsonGenerator gen = jacksonFactory.createGenerator(out, JsonEncoding.UTF8)) {
            ...  // Part of the code is omitted here
        } catch (Exception e) {
            fs.delete(path, false);
            throw e;
        }
        LOG.info(""Job {} has been archived at {}."", jobId, path);
        return path;
    } catch (IOException e) {
        LOG.error(""Failed to archive job."", e);
        throw e;
    }
}
{code}
After I make the above changes to the code, I cannot find the corresponding task on Flink's HistoryServer(Refer to Figure 1.png and Figure 2.png).

Then I went to Yarn to browse the JobManager log (see attachment application_1612404624605_0010-JobManager.log for log details), and   did not found the following logs in the JobManager log file:
{code:java}
INFO entrypoint.ClusterEntrypoint: Terminating cluster entrypoint process YarnJobClusterEntrypoint with exit code 0.{code}
Usually, if the task exits normally, a similar log will be printed before executing {color:#0747a6}System.exit(returnCode){color}.

If no Exception information is found in the JobManager log, the above situation occurs, indicating that the JobManager is running to a certain point, and there is no user thread in the JobManager process, which causes the program to exit without completing the normal process.

 

 

 

Eventually I found out that multiple services (e.g. ioExecutor, metricRegistry, commonRpcService) were exited asynchronously in {color:#0747a6}org.apache.flink.runtime.entrypoint.ClusterEntrypoint#stopClusterServices{color}, and multiple services would be exited in the shutdown() method of metricRegistry (e.g. executor), these exit actions are executed asynchronously and in parallel. If ioExecutor or executor exits after metricRegistry and commonRpcService , it will cause the above problems.  Why is there no such problem with ioExecutor being exited brefore metricRegistry and commonRpcService? The key difference is that the threads in ioExecutor are daemon threads, while the threads in metricRegistry and commonRpcService are user threads.

 

 

 

I hope to modify the following code to fix this bug. If it is determined that this is a bug (this problem will affect all versions above 1.9), please assign the ticket to me, thank you.

Only need to modify the {color:#0747a6}org.apache.flink.runtime.entrypoint.ClusterEntrypoint#runClusterEntrypoint{color} method:

After fixing：
{code:java}
public static void runClusterEntrypoint(ClusterEntrypoint clusterEntrypoint) {

   final String clusterEntrypointName = clusterEntrypoint.getClass().getSimpleName();
   try {
      clusterEntrypoint.startCluster();
   } catch (ClusterEntrypointException e) {
      LOG.error(String.format(""Could not start cluster entrypoint %s."", clusterEntrypointName), e);
      System.exit(STARTUP_FAILURE_RETURN_CODE);
   }

   int returnCode;
   Throwable throwable = null;
   try {
      returnCode = clusterEntrypoint.getTerminationFuture().get().processExitCode();
   } catch (Throwable e) {
      throwable = e;
      returnCode = RUNTIME_FAILURE_RETURN_CODE;
   }

   LOG.info(""Terminating cluster entrypoint process {} with exit code {}."", clusterEntrypointName, returnCode, throwable);
   System.exit(returnCode);
}{code}
 Before fixing: 
{code:java}
public static void runClusterEntrypoint(ClusterEntrypoint clusterEntrypoint) {

   final String clusterEntrypointName = clusterEntrypoint.getClass().getSimpleName();
   try {
      clusterEntrypoint.startCluster();
   } catch (ClusterEntrypointException e) {
      LOG.error(String.format(""Could not start cluster entrypoint %s."", clusterEntrypointName), e);
      System.exit(STARTUP_FAILURE_RETURN_CODE);
   }

   clusterEntrypoint.getTerminationFuture().whenComplete((applicationStatus, throwable) -> {
      final int returnCode;

      if (throwable != null) {
         returnCode = RUNTIME_FAILURE_RETURN_CODE;
      } else {
         returnCode = applicationStatus.processExitCode();
      }

      LOG.info(""Terminating cluster entrypoint process {} with exit code {}."", clusterEntrypointName, returnCode, throwable);
      System.exit(returnCode);
   });
}
{code}
The purpose of the modification is to ensure that the Main thread exits last.",,trohrmann,wangyang0918,wjc920,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/21 06:57;wjc920;1.png;https://issues.apache.org/jira/secure/attachment/13019963/1.png","04/Feb/21 06:57;wjc920;2.png;https://issues.apache.org/jira/secure/attachment/13019964/2.png","05/Feb/21 08:39;wjc920;Add wait 5 seconds in org.apache.flink.runtime.history.FsJobArchivist#archiveJob.log;https://issues.apache.org/jira/secure/attachment/13020064/Add+wait+5+seconds+in+org.apache.flink.runtime.history.FsJobArchivist%23archiveJob.log","05/Feb/21 08:41;wjc920;Not add wait 5 seconds.log;https://issues.apache.org/jira/secure/attachment/13020066/Not+add+wait+5+seconds.log","04/Feb/21 06:39;wjc920;application_1612404624605_0010-JobManager.log;https://issues.apache.org/jira/secure/attachment/13019965/application_1612404624605_0010-JobManager.log",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 11 18:33:59 UTC 2021,,,,,,,,,,"0|z0nceg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/21 01:58;wjc920;Hi [~fly_in_gis] , could you help me confirm if this is a bug? thank you.;;;","05/Feb/21 06:22;wangyang0918;[~wjc920] I am afraid this ticket is related with FLINK-21008. The root cause of both them are that {{ClusterEntrypoint#shutDownAsync}} is not fully executed. And then this leads to the residual HA related data or archiving failure for completed jobs.

 

However, I cannot agree with your fix. Simply calling the {{getTerminationFuture().get()}} will block the executing of {{runClusterEntrypoint}}. This also could not completely resolve the issue if we receive the SIGTERM very fast. So I prefer the solution posted in FLINK-21008, triggering a {{ClusterEntrypoint.closeAsync()}} if we see a SIGTERM and then wait on the completion. WDYT?;;;","05/Feb/21 08:23;wjc920;I want to try further explanation:

I added some logs that print thread information to {color:#0747a6}org.apache.flink.runtime.entrypoint.ClusterEntrypoint{color} to get the status before the JobManager exits. The code is modified as follows:

// code1
{code:java}
	public static void runClusterEntrypoint(ClusterEntrypoint clusterEntrypoint) {

		final String clusterEntrypointName = clusterEntrypoint.getClass().getSimpleName();
		try {
			clusterEntrypoint.startCluster();
		} catch (ClusterEntrypointException e) {
			LOG.error(String.format(""Could not start cluster entrypoint %s."", clusterEntrypointName), e);
			System.exit(STARTUP_FAILURE_RETURN_CODE);
		}

		// code1-1
		clusterEntrypoint.getTerminationFuture().whenComplete((applicationStatus, throwable) -> {
			final int returnCode;

			LOG.info(""================clusterEntrypoint.getTerminationFuture isTerminal======================================"");
			LOG.info(""===Current thread name is: "" + Thread.currentThread().getName()
				+ "", isDaemon: "" + Thread.currentThread().isDaemon());
			printThreads();

			if (throwable != null) {
				returnCode = RUNTIME_FAILURE_RETURN_CODE;
			} else {
				returnCode = applicationStatus.processExitCode();
			}

			LOG.info(""Terminating cluster entrypoint process {} with exit code {}."", clusterEntrypointName, returnCode, throwable);
			System.exit(returnCode);
		});
	}
{code}
// code2
{code:java}
private void cleanupDirectories() throws IOException {
		LOG.info(""===================Starting cleanupDirectories================================"");
		LOG.info(""cleanupDirectories===Current thread name is: "" + Thread.currentThread().getName()
			+ "", isDaemon: "" + Thread.currentThread().isDaemon());
		printThreads();
		ShutdownHookUtil.removeShutdownHook(shutDownHook, getClass().getSimpleName(), LOG);

		final String webTmpDir = configuration.getString(WebOptions.TMP_DIR);

		FileUtils.deleteDirectory(new File(webTmpDir));
	}
{code}
 

// code3
{code:java}
	protected CompletableFuture<Void> stopClusterServices(boolean cleanupHaData) {
		final long shutdownTimeout = configuration.getLong(ClusterOptions.CLUSTER_SERVICES_SHUTDOWN_TIMEOUT);

		synchronized (lock) {
			Throwable exception = null;

			final Collection<CompletableFuture<Void>> terminationFutures = new ArrayList<>(3);

			if (blobServer != null) {
				try {
					blobServer.close();
				} catch (Throwable t) {
					exception = ExceptionUtils.firstOrSuppressed(t, exception);
				}
			}

			if (haServices != null) {
				try {
					if (cleanupHaData) {
						haServices.closeAndCleanupAllData();
					} else {
						haServices.close();
					}
				} catch (Throwable t) {
					exception = ExceptionUtils.firstOrSuppressed(t, exception);
				}
			}

			if (archivedExecutionGraphStore != null) {
				try {
					archivedExecutionGraphStore.close();
				} catch (Throwable t) {
					exception = ExceptionUtils.firstOrSuppressed(t, exception);
				}
			}

			if (processMetricGroup != null) {
				processMetricGroup.close();
			}

			if (metricRegistry != null) {
				LOG.info(""===metricRegistry is not null"");
				CompletableFuture<Void> futureMetricRegistry = metricRegistry.shutdown();
				terminationFutures.add(futureMetricRegistry);
				futureMetricRegistry.whenComplete((aVoid, throwable) -> {
				    // code3-1
					LOG.info(""========================metricRegistry shutdowns successfully==================================="");
					LOG.info(""metricRegistry===Current thread name is: "" + Thread.currentThread().getName()
						+ "", isDaemon: "" + Thread.currentThread().isDaemon());
					printThreads();
				});
			}

			if (ioExecutor != null) {
				LOG.info(""===ioExecutor is not null"");
				CompletableFuture<Void> futureIoExecutor = ExecutorUtils.nonBlockingShutdown(shutdownTimeout,
					TimeUnit.MILLISECONDS, ioExecutor);
				terminationFutures.add(futureIoExecutor);
				futureIoExecutor.whenComplete((aVoid, throwable) -> {
				    // code3-2
					LOG.info(""==============ioExecutor shutdowns successfully=========================="");
					LOG.info(""ioExecutor===Current thread name is: "" + Thread.currentThread().getName()
						+ "", isDaemon: "" + Thread.currentThread().isDaemon());
					printThreads();
				});
			}

			if (commonRpcService != null) {
				LOG.info(""===commonRpcService is not null"");
				CompletableFuture<Void> futureCommonRpcService = commonRpcService.stopService();
				terminationFutures.add(futureCommonRpcService);
				futureCommonRpcService.whenComplete((aVoid, throwable) -> {
				    // code3-3
					LOG.info(""============================commonRpcService shutdowns successfully====================================="");
					LOG.info(""commonRpcService===Current thread name is: "" + Thread.currentThread().getName()
						+ "", isDaemon: "" + Thread.currentThread().isDaemon());
					printThreads();
				});

			}

			if (exception != null) {
				terminationFutures.add(FutureUtils.completedExceptionally(exception));
			}

			return FutureUtils.completeAll(terminationFutures);
		}
{code}
// code4
{code:java}
	public static void printThreads() {
		ThreadGroup group = Thread.currentThread().getThreadGroup();
		ThreadGroup topGroup = group;
		while (group != null) {
			topGroup = group;
			group = group.getParent();
		}
		int slackSize = topGroup.activeCount() * 2;
		Thread[] slackThreads = new Thread[slackSize];
		int actualSize = topGroup.enumerate(slackThreads);
		Thread[] atualThreads = new Thread[actualSize];
		System.arraycopy(slackThreads, 0, atualThreads, 0, actualSize);
		LOG.info(""===Threads size is "" + atualThreads.length);
		for (Thread thread : atualThreads) {
			LOG.info(""===Thread name : "" + thread.getName()+"", isDaemon: "" + thread.isDaemon());
		}

	}
{code}
 
 The attached log file description:

 

[^Add wait 5 seconds in org.apache.flink.runtime.history.FsJobArchivist#archiveJob.log]This log file corresponds to {color:#0747a6}code6{color}, and a 5-second wait logic is added to the {color:#0747a6}org.apache.flink.runtime.history.FsJobArchivist#archiveJob{color} method

 

[^Not add wait 5 seconds.log]This log file corresponds to {color:#0747a6}code6{color}, delete the logic to wait for 5 seconds in the {color:#0747a6}org.apache.flink.runtime.history.FsJobArchivist#archiveJob{color} method

// code5
{code:java}
public static Path archiveJob(Path rootPath, JobID jobId, Collection<ArchivedJson> jsonToArchive) 
    throws IOException {
    try {
        FileSystem fs = rootPath.getFileSystem();
        Path path = new Path(rootPath, jobId.toString());
        OutputStream out = fs.create(path, FileSystem.WriteMode.NO_OVERWRITE);

        try {
            LOG.info(""===========================Wait 5 seconds.."");
            Thread.sleep(5000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        try (JsonGenerator gen = jacksonFactory.createGenerator(out, JsonEncoding.UTF8)) {
            ...  // Part of the code is omitted here
        } catch (Exception e) {
            fs.delete(path, false);
            throw e;
        }
        LOG.info(""Job {} has been archived at {}."", jobId, path);
        return path;
    } catch (IOException e) {
        LOG.error(""Failed to archive job."", e);
        throw e;
    }
}
{code}
// code6
{code:java}
public static Path archiveJob(Path rootPath, JobID jobId, Collection<ArchivedJson> jsonToArchive) 
    throws IOException {
    try {
        FileSystem fs = rootPath.getFileSystem();
        Path path = new Path(rootPath, jobId.toString());
        OutputStream out = fs.create(path, FileSystem.WriteMode.NO_OVERWRITE);
		
		// Here, will not wait 5 seconds

        try (JsonGenerator gen = jacksonFactory.createGenerator(out, JsonEncoding.UTF8)) {
            ...  // Part of the code is omitted here
        } catch (Exception e) {
            fs.delete(path, false);
            throw e;
        }
        LOG.info(""Job {} has been archived at {}."", jobId, path);
        return path;
    } catch (IOException e) {
        LOG.error(""Failed to archive job."", e);
        throw e;
    }
}
{code}
I think that the thread information printed in {color:#0747a6}code1-1{color} and {color:#0747a6}code2{color} is the thread information before the JobManager process exits.

If add a 5-second wait, {color:#0747a6}code3-2{color} and{color:#0747a6} code1-1{color} will not be executed, which means that ioExecutor fails to exit normally. And {color:#0747a6}code2{color} is executed by the YarnJobClusterEntrypoint shutdown hook. Why does this happen? I think we need to think about it carefully.

If you delete the 5-second wait, we find that the program exits normally, and {color:#0747a6}code1-1{color} is executed by the {color:#0747a6}flink-akka.actor.default-dispatcher-16{color} thread. Corresponding to this, there is another situation. If metricRegistry exits after commonRpcService,{color:#0747a6} code1-1{color} will be executed by {color:#0747a6}flink-metrics-scheduler-1{color}.

JobManager exits multiple thread pools in parallel before JobManager process exiting. Different thread pool exit orders result in different threads executing {color:#0747a6}code1-1{color}. If the thread pool is daemon pool (for example: ioExecutor) and finally exits, {color:#0747a6}code1-1{color} will not be executed. This is the root cause of the problem.

If the {color:#0747a6}org.apache.flink.runtime.util.ExecutorThreadFactory#newThread{color} method corresponding to ioExecutor is modified from {color:#0747a6}code7{color} to {color:#0747a6}code8{color}, after doing so, even if we wait 10 seconds in {color:#0747a6}code5{color}, {color:#0747a6}code1-1{color} will be executed. At this time, the thread executing code1-1 will be {color:#0747a6}ForkJoinPool.commonPool-worker-57{color}

// code7
{code:java}
	@Override
	public Thread newThread(Runnable runnable) {
		Thread t = new Thread(group, runnable, namePrefix + threadNumber.getAndIncrement());
		t.setDaemon(true);

		t.setPriority(threadPriority);

		// optional handler for uncaught exceptions
		if (exceptionHandler != null) {
			t.setUncaughtExceptionHandler(exceptionHandler);
		}

		return t;
	}
{code}
// code8
{code:java}
	@Override
	public Thread newThread(Runnable runnable) {
		Thread t = new Thread(group, runnable, namePrefix + threadNumber.getAndIncrement());
		t.setDaemon(false);

		t.setPriority(threadPriority);

		// optional handler for uncaught exceptions
		if (exceptionHandler != null) {
			t.setUncaughtExceptionHandler(exceptionHandler);
		}

		return t;
	}
{code}
I think {color:#0747a6}code1-1{color} needs to be executed by MainThread, otherwise if the daemon thread pool finally exits, {color:#0747a6}code1-1{color} will not be executed.

[~fly_in_gis] What do you think?;;;","05/Feb/21 09:06;wjc920;[~fly_in_gis]

I think FLINK-21008 is that while the JobManager is exiting, the external device sends a SIGTERM to the JobManager.  May result in {{stopClusterServices}} and {{cleanupDirectories}} not being executed.

But，

According to the scene described in this ticket, I did not send any SIGTERM to the JobManager process, the task in ioExecutor (for example: archiving job info) cannot be completely executed during program exit.

 

From this code:
{code:java}
CompletableFuture<Void> futureIoExecutor = ExecutorUtils.nonBlockingShutdown(shutdownTimeout,
 TimeUnit.MILLISECONDS, ioExecutor);{code}
We can infer that if the task in ioExecutor is not completed temporarily, shutdown will wait for 30 seconds, but the result of my actual test is that shutdown did not wait 30 seconds, and ended immediately.

 

The scenario I gave in this ticket is that the FlinkJob is executed normally and no SIGTERM is received. At this time, if the archiving speed is too slow, the archiving will not be completed.;;;","05/Feb/21 10:59;wjc920;We can also modify {color:#0747a6}org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint#main{color}. Compared with the way I mentioned at the beginning, this way involves the modification of multiple main methods(e.g. {color:#0747a6}org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint#main{color}, {color:#0747a6}org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint{color}#main, and so on...).

org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint#main
{code:java}
public static void main(String[] args) {
   // startup checks and logging
   EnvironmentInformation.logEnvironmentInfo(LOG, YarnJobClusterEntrypoint.class.getSimpleName(), args);
   SignalHandler.register(LOG);
   JvmShutdownSafeguard.installAsShutdownHook(LOG);

   Map<String, String> env = System.getenv();

   final String workingDirectory = env.get(ApplicationConstants.Environment.PWD.key());
   Preconditions.checkArgument(
      workingDirectory != null,
      ""Working directory variable (%s) not set"",
      ApplicationConstants.Environment.PWD.key());

   try {
      YarnEntrypointUtils.logYarnEnvironmentInformation(env, LOG);
   } catch (IOException e) {
      LOG.warn(""Could not log YARN environment information."", e);
   }

   Configuration configuration = YarnEntrypointUtils.loadConfiguration(workingDirectory, env);

   YarnJobClusterEntrypoint yarnJobClusterEntrypoint = new YarnJobClusterEntrypoint(
      configuration);

   ClusterEntrypoint.runClusterEntrypoint(yarnJobClusterEntrypoint);

   int returnCode;
   Throwable throwable = null;
   try {
      returnCode = yarnJobClusterEntrypoint.getTerminationFuture().get().processExitCode();
   } catch (Throwable e) {
      throwable = e;
      returnCode = RUNTIME_FAILURE_RETURN_CODE;
   }

   LOG.info(""Terminating cluster entrypoint process {} with exit code {}."",
      yarnJobClusterEntrypoint.getClass().getSimpleName(),
      returnCode, throwable);
   System.exit(returnCode);
}
{code}
All my modifications are to make the main thread exit last. 

Is there a better way to solve this problem?;;;","05/Feb/21 11:20;wjc920;I investigated the code of Kafka's Broker and Spark service. They also added blocking waiting in the main method. I think this is to ensure that the main thread finally exits. 

In fact, you can use tools to monitor the JobManager process. When the JobManager is running, the main thread no longer exists. This is an abnormal phenomenon.

 

kafka.Kafka#main
{code:java}
   def main(args: Array[String]): Unit = {
    try {
      val serverProps = getPropsFromArgs(args)
      val kafkaServerStartable = KafkaServerStartable.fromProps(serverProps)

      try {
        if (!OperatingSystem.IS_WINDOWS && !Java.isIbmJdk)
          new LoggingSignalHandler().register()
      } catch {
        case e: ReflectiveOperationException =>
          warn(""Failed to register optional signal handler that logs a message when the process is terminated "" +
            s""by a signal. Reason for registration failure is: $e"", e)
      }

      // attach shutdown handler to catch terminating signals as well as normal termination
      Runtime.getRuntime().addShutdownHook(new Thread(""kafka-shutdown-hook"") {
        override def run(): Unit = kafkaServerStartable.shutdown()
      })

      kafkaServerStartable.startup()
      kafkaServerStartable.awaitShutdown()
    }
    catch {
      case e: Throwable =>
        fatal(""Exiting Kafka due to fatal exception"", e)
        Exit.exit(1)
    }
    Exit.exit(0)
  }
{code}
org.apache.spark.deploy.yarn.ApplicationMaster#main
{code:java}
  def main(args: Array[String]): Unit = {
    SignalUtils.registerLogger(log)
    val amArgs = new ApplicationMasterArguments(args)
    master = new ApplicationMaster(amArgs)
    System.exit(master.run())
  }
{code};;;","07/Feb/21 02:44;wjc920;[~fly_in_gis] I updated the issue description and issue title, please review my issue again. Thank you.;;;","07/Feb/21 03:44;wangyang0918;Sorry for late response. I will have a deep dive into this issue today.;;;","07/Feb/21 08:32;wangyang0918;[~wjc920] I think I get your point now. It is a different issue with FLINK-21008. However, both of them could lead to the tasks(e.g. archiving to HDFS, cleaning up HA data) in the io-executor are not completely executed.

 

The root cause of this issue is that all the user threads have terminated in {{ClusterEntrypoint#shutDownAsync}}. After then the io-executors will also terminate since they are daemon threads. As a result, the tasks which have been submitted to io-executors will not be completely executed.

 

cc [~trohrmann], do you have some ideas on why do we not keep the main thread in Flink? ;;;","08/Feb/21 00:36;wjc920;Thank you very much for reviewing this issue carefully, and I agree with you. I hope to fix this bug myself with your assistance. For me, this is a very meaningful thing.;;;","08/Feb/21 04:09;wangyang0918;[~wjc920] IIUC, the solution you are suggesting is to make the main thread blocking. After then it will be a long lived thread, as long as the life of JobManager process.

Currently, I am not aware of any better solutions. Let's wait for [~trohrmann] whether he has some concerns to do this change since he is the author of the {{ClusterEntrypoint}}.;;;","08/Feb/21 10:24;trohrmann;Thanks for reporting and analysing this problem so thoroughly [~wjc920]. I think your findings are correct. The problem is that in some cases we don't have non-daemon thread remaining which causes the JVM to terminate before all tasks have been completed.

I think your solution to change the {{ClusterEntrypoint.runClusterEntrypoint}} to wait on the result of {{clusterEntrypoint.getTerminationFuture().get()}} and do the {{System.exit}} outside of the future callback looks like the best solution to me.;;;","08/Feb/21 10:25;trohrmann;I've assigned you to this ticket [~wjc920]. Happy coding!;;;","11/Feb/21 18:33;trohrmann;Fixed via

1.13.0:

e6585365c931042a94408e0a58b0316b40a270e5
cf451aa73f050b69031366e1dda7bc0a3e0f9f81

1.12.2:

a7f898ab3f6bc887c590abf6e2c6eab9a89d1d12
a7f3b369229328ddc717776fca767ae4428df53a

1.11.4:

b4e3b498ec47deb383b83de87ac00b7707b26314
9946891bb3d6568ae29af6d8c23ccb39bfbfba22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SlotPool.suspend,FLINK-21262,13356541,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,trohrmann,trohrmann,trohrmann,03/Feb/21 17:20,28/May/21 08:17,13/Jul/23 08:07,04/Feb/21 12:30,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Since the completion of FLINK-11719 we no longer need to suspend the {{SlotPool}}. Hence, I suggest to remove this method.",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 04 12:30:09 UTC 2021,,,,,,,,,,"0|z0nbko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/21 12:30;trohrmann;Fixed via f5146af60b218b6f3a7763f0b5a09d9e76f065d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink iceberg table map<string,string> cannot convert to datastream",FLINK-21247,13356394,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,txdong-sz,txdong-sz,03/Feb/21 07:44,08/Oct/21 11:15,13/Jul/23 08:07,26/Apr/21 08:15,1.12.1,,,,,,,,1.13.1,1.14.0,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"Flink Iceberg Table with map<string,string>
!image-2021-02-03-15-38-42-340.png!
 
we want to read the table like this :
 
String querySql = ""SELECT ftime,extinfo,country,province,operator,apn,gw,src_ip_head,info_str,product_id,app_version,sdk_id,sdk_version,hardware_os,qua,upload_ip,client_ip,upload_apn,event_code,event_result,package_size,consume_time,event_value,event_time,upload_time,boundle_id,uin,platform,os_version,channel,brand,model from bfzt3 "";
Table table = tEnv.sqlQuery(querySql);

DataStream<AttaInfo> sinkStream = tEnv.toAppendStream(table, Types.POJO(AttaInfo.class, map));

sinkStream.map(x->1).returns(Types.INT).keyBy(new NullByteKeySelector<Integer>()).reduce((x,y) -> {
 return x+y;
}).print();
 
 
when read  we find a exception
 
2021-02-03 15:37:57
java.lang.ClassCastException: org.apache.iceberg.flink.data.FlinkParquetReaders$ReusableMapData cannot be cast to org.apache.flink.table.data.binary.BinaryMapData
    at org.apache.flink.table.runtime.typeutils.MapDataSerializer.copy(MapDataSerializer.java:107)
    at org.apache.flink.table.runtime.typeutils.MapDataSerializer.copy(MapDataSerializer.java:47)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:166)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:129)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:50)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:69)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:317)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:411)
    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:66)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:241)
 
we find that iceberg map is  ReusableMapData implements MapData 
!image-2021-02-03-15-40-27-055.png!
 
this is the exception 
!image-2021-02-03-15-41-34-426.png!
MapData has two default implements  GenericMapData and BinaryMapData
from iceberg implement is ReusableMapData
 
so i think that code should change to like this 
!image-2021-02-03-15-43-19-919.png!
 ","iceberg master

flink 1.12

 

 ",icshuo,jark,jiabao.sun,lzljs3620320,openinx,txdong-sz,zhangjun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/21 07:38;txdong-sz;image-2021-02-03-15-38-42-340.png;https://issues.apache.org/jira/secure/attachment/13019898/image-2021-02-03-15-38-42-340.png","03/Feb/21 07:40;txdong-sz;image-2021-02-03-15-40-27-055.png;https://issues.apache.org/jira/secure/attachment/13019897/image-2021-02-03-15-40-27-055.png","03/Feb/21 07:41;txdong-sz;image-2021-02-03-15-41-34-426.png;https://issues.apache.org/jira/secure/attachment/13019896/image-2021-02-03-15-41-34-426.png","03/Feb/21 07:43;txdong-sz;image-2021-02-03-15-43-19-919.png;https://issues.apache.org/jira/secure/attachment/13019895/image-2021-02-03-15-43-19-919.png","03/Feb/21 07:52;txdong-sz;image-2021-02-03-15-52-12-493.png;https://issues.apache.org/jira/secure/attachment/13019901/image-2021-02-03-15-52-12-493.png","03/Feb/21 07:53;txdong-sz;image-2021-02-03-15-53-18-244.png;https://issues.apache.org/jira/secure/attachment/13019902/image-2021-02-03-15-53-18-244.png",,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 26 08:15:02 UTC 2021,,,,,,,,,,"0|z0nao0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/21 07:53;txdong-sz; 

i change code to this  and can run normally

print map data can run 

 

sinkStream.addSink(new RichSinkFunction<AttaInfo>() {
 @Override
 public void invoke(AttaInfo value, Context context) throws Exception {


 Map<String, String> map2 = value.getEvent_value();

 map2.entrySet().stream().forEach(x->{

 System.out.println(x.getKey()+"":"" + x.getValue());
 });
 }
});

!image-2021-02-03-15-52-12-493.png!

 

 

!image-2021-02-03-15-53-18-244.png!;;;","03/Feb/21 08:19;jark;[~openinx], not sure what's the potential reason why Iceberg introduced `ReusableMapData`. ;;;","04/Feb/21 02:24;openinx;[~jark],  Assume that there are 5 records in iceberg table ( table schema is:  create table test (row : map<string, String>) ):

{code}
row1: map<{""a"", ""1""}, {""b"", ""2}, {""c"", ""3""}>
row2: map<{""a"", ""1""}>
row3: map<{""c"", ""111""}>
row4: map<{""d"", ""11""}>
row5: map<{""e"", 1""}>
{code}

If we don't use the reuse object,  then for each row we will need to create a new RowData object for each row, which would create so many young objects that produces the GC issues.  But if use the reuse object,  then each children fields inside this object will all be reusable.  In this case,  we will need a ReusableMapData to maintain the key-value pairs for the first row1, then for the second row2,  actually the ReusableMapData has enough space to hold the map of row2, then we don't have to allocate new map -  that will really reduce the heap object allocations.

In our apache iceberg generic parquet reader & writer,  we use this reuse strategy for all the compute engines. Another similar data structure is ReusableArrayData, which extends from ArrayData interfaces.  I think we also need to fix it in this PR. Thanks. ;;;","04/Feb/21 03:14;jark;Thanks for the explanation, I'm fine the the fix way. What do you think [~lzljs3620320]? ;;;","02/Apr/21 10:13;openinx;I think this will need to get merged in flink 1.13.0,  otherwise the newly released flink won't be able to read iceberg map data type,  this's a critical bug.;;;","02/Apr/21 13:43;jark;Thanks [~openinx], I changed the type to BUG, and fix version to 1.13. ;;;","09/Apr/21 08:22;lzljs3620320;Hi [~txdong-sz], Thanks for the contribution. Can you fix the checkstyle and add case in the PR?;;;","12/Apr/21 09:43;icshuo;Hi [~txdong-sz], it seems you are offline for several days, so please excuse me for taking this issue without informing you.;;;","26/Apr/21 08:15;lzljs3620320;Fixed via:

master: 033cdeae0e510a930e9975542ae0a43c4484adeb

release-1.13: 970b2b8b746ba7a29154e0d2f6b282c094bbfc24;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incompatible datetime with MySql JDBC Driver 8.0.23,FLINK-21240,13356193,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,macdoor615,macdoor615,02/Feb/21 12:34,28/May/21 09:00,13/Jul/23 08:07,08/Apr/21 06:48,1.12.1,,,,,,,,1.13.0,,,,,,,Connectors / JDBC,Table SQL / Runtime,,,,0,pull-request-available,,,,,"1. upgrade mysql JDBC Driver from mysql-connector-java-8.0.22.jar to mysql-connector-java-8.0.23.jar 

2. create table on mysql

CREATE TABLE `p_port_packet_loss_5m` ( 
   `id` binary(16) NOT NULL, 
   `coltime` datetime NOT NULL, 
 ...

3. create table in Flink SQL client 
 create table if not exists p_port_packet_loss_5m 
 ( 
   id bytes, 
   coltime timestamp, 
 ...) 
 WITH ( 
   'connector' = 'jdbc', 
   'url' = 'jdbc:mysql://ip:port/mydatabase?rewritebatchedstatements=true',

...)

4. query this table in flink sql client
 select * from p_port_packet_loss_5m;

5. prompt error
{quote}java.lang.ClassCastException: java.time.LocalDateTime cannot be cast to java.sql.Timestamp

    at org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter.lambda$createInternalConverter$ff586f13$9(AbstractJdbcRowConverter.java:170)

    at org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter.lambda$wrapIntoNullableInternalConverter$83a7e581$1(AbstractJdbcRowConverter.java:127)

    at org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter.toInternal(AbstractJdbcRowConverter.java:78)

    at org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat.nextRecord(JdbcRowDataInputFormat.java:265)

    at org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat.nextRecord(JdbcRowDataInputFormat.java:55)

    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:90)

    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)

    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:66)

    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:241)
{quote}
 

6. downgrade to mysql-connector-java-8.0.22.jar , same procedure, no error",,jark,leonard,libenchao,macdoor615,RocMarshal,twalthr,xiaozilong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 06:48:55 UTC 2021,,,,,,,,,,"0|z0n9fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/21 12:39;chesnay;[~macdoor615] It seems something went wrong when you created the ticket.;;;","02/Feb/21 13:04;jark;I think this is a bug in {{AbstractJdbcRowConverter}} where we didn't consider {{LocalDateTime}} instances for TIMESTAMP types. ;;;","08/Apr/21 06:48;jark;Fixed in master: b057eafcb0d5178fd09dce7c9a067038aa6974b1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reintroduce TableColumn.of for backwards compatibility,FLINK-21226,13355881,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,01/Feb/21 09:40,01/Feb/21 14:45,13/Jul/23 08:07,01/Feb/21 14:45,,,,,,,,,1.12.2,1.13.0,,,,,,Table SQL / API,,,,,0,,,,,,FLINK-19341 accidentally dropped the {{TableColumn.of}} method that might be used frequently by downstream projects. We should reintroduce it for 1-2 releases.,,jark,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 01 14:45:06 UTC 2021,,,,,,,,,,"0|z0n7i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/21 14:45;twalthr;Fixed in 1.13.0: a92c264f9afe88bb8a4493d943e23d3dd147964a
Fixed in 1.12.2: 47c96e19ce0f49dd8ca9595b75af2df4031d1b07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OverConvertRule does not consider distinct,FLINK-21225,13355873,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,twalthr,twalthr,01/Feb/21 08:54,15/Feb/21 08:31,13/Jul/23 08:07,10/Feb/21 05:14,,,,,,,,,1.12.2,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"We don't support OVER window distinct aggregates in Table API. Even though this is explicitly documented:

https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/tableApi.html#aggregations

{code}
// Distinct aggregation on over window
Table result = orders
    .window(Over
        .partitionBy($(""a""))
        .orderBy($(""rowtime""))
        .preceding(UNBOUNDED_RANGE)
        .as(""w""))
    .select(
        $(""a""), $(""b"").avg().distinct().over($(""w"")),
        $(""b"").max().over($(""w"")),
        $(""b"").min().over($(""w""))
    );
{code}

The distinct flag is set to false in {{OverConvertRule}}.

See also
http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Unknown-call-expression-avg-amount-when-use-distinct-in-Flink-Thanks-td40905.html",,appleyuchi,jark,libenchao,qingyue,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 15 08:31:38 UTC 2021,,,,,,,,,,"0|z0n7g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/21 09:00;twalthr;You can checkout my branch to reproduce the issue: https://github.com/twalthr/flink/tree/FLINK-21225;;;","01/Feb/21 18:27;qingyue;Hi [~jark] and [~twalthr], I checked the case at [https://github.com/twalthr/flink/tree/FLINK-21225] and found that besides the distinct flag is set to false, {{OverConvertRule}} could not make rex call for children expression like {{distinct(avg/count/sum(field))}}, thus cause {{ExpressionConverter#visit}} throwing exception that ""Unknown call expression: avg(field)"".  And I'd like to fix this issue.;;;","02/Feb/21 03:43;jark;Thanks for the investigation [~qingyue]. I assigned this issue to you. ;;;","04/Feb/21 04:26;appleyuchi;[~twalthr]
My later replies to above link of  mailing list is deleted,
I don't know why.

I tested this problem in  Flink 1.12 Scala2.12
and had carefully checked the dependencies in my pom.xml.;;;","10/Feb/21 03:56;jark;Fixed in 
 - master: 31346e8cdf225bec0ea4745a504c765a1fcc0edf
 - 1.12: 1345c0f9a606a6e5ccffda59bb28a6ccfe054263;;;","12/Feb/21 15:58;twalthr;[~jark] shouldn't this be 1.12.2?;;;","13/Feb/21 14:17;jark;Hi Timo, from my understanding, if there is a version under releasing, we should fix issues into the next one, otherwise, it would be possible to release a un-fixed issue. The release manager should move issues of 1.12.3 to 1.12.2 when creating release candidates. But as there are no 1.12.2 RC for now, I'm fine to update fixed version to 1.12.2. ;;;","15/Feb/21 08:31;twalthr;Yes, you are right. 1.12.3 would also have been fine.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Savepoint (rocks, scale up, rocks timers) end-to-end test",FLINK-21217,13355694,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,maguowei,maguowei,31/Jan/21 02:16,22/Jun/21 13:55,13/Jul/23 08:07,04/Feb/21 17:16,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Checkpointing,,,,,0,test-stability,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12664&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529]

 

Jan 29 15:20:55 [FAIL] 'Resuming Savepoint (rocks, scale up, rocks timers) end-to-end test' failed after 0 minutes and 37 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files",,AHeise,maguowei,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21272,FLINK-21166,,,,,FLINK-19462,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 04 17:16:37 UTC 2021,,,,,,,,,,"0|z0n6co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/21 09:01;arvid;Extracted exceptions

 
{noformat}
2021-01-29 15:20:41,001 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - SlidingWindowCheckMapper -> Sink: SlidingWindowCheckPrintSink (1/2)#0 - asynchronous part of checkpoint 11 could not be completed.
java.util.concurrent.CancellationException: null
	at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2276) ~[?:1.8.0_282]
	at org.apache.flink.runtime.state.StateUtil.discardStateFuture(StateUtil.java:77) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.lambda$cancel$0(OperatorSnapshotFutures.java:173) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.shaded.guava18.com.google.common.io.Closer.close(Closer.java:214) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.cancel(OperatorSnapshotFutures.java:183) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.cleanup(AsyncCheckpointRunnable.java:336) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.close(AsyncCheckpointRunnable.java:305) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.util.IOUtils.closeQuietly(IOUtils.java:275) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancelAsyncCheckpointRunnable(SubtaskCheckpointCoordinatorImpl.java:451) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:340) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$12(StreamTask.java:1070) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$13(StreamTask.java:1083) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:314) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:300) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:188) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:615) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:579) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
2021-01-29 15:20:41,003 WARN  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Could not properly clean up the async checkpoint runnable.
java.lang.IllegalStateException: null
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.util.Preconditions.checkCompletedNormally(Preconditions.java:261) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.FutureUtils.checkStateAndGet(FutureUtils.java:1176) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointMetricsBuilder.build(CheckpointMetricsBuilder.java:133) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.reportAbortedSnapshotStats(AsyncCheckpointRunnable.java:223) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.close(AsyncCheckpointRunnable.java:306) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.util.IOUtils.closeQuietly(IOUtils.java:275) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancelAsyncCheckpointRunnable(SubtaskCheckpointCoordinatorImpl.java:451) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:340) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$12(StreamTask.java:1070) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$13(StreamTask.java:1083) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:314) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:300) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:188) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:615) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:579) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:565) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
2021-01-29 15:20:41,049 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - SlidingWindowOperator (2/2)#0 - asynchronous part of checkpoint 11 could not be completed.
java.util.concurrent.CancellationException: null
	at java.util.concurrent.FutureTask.report(FutureTask.java:121) ~[?:1.8.0_282]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_282]
	at org.apache.flink.runtime.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:621) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:54) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:127) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_282]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
2021-01-29 15:20:41,067 WARN  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Could not properly clean up the async checkpoint runnable.
java.lang.IllegalStateException: null
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.util.Preconditions.checkCompletedNormally(Preconditions.java:261) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.FutureUtils.checkStateAndGet(FutureUtils.java:1176) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointMetricsBuilder.build(CheckpointMetricsBuilder.java:133) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.reportAbortedSnapshotStats(AsyncCheckpointRunnable.java:223) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.close(AsyncCheckpointRunnable.java:306) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.util.IOUtils.closeQuietly(IOUtils.java:275) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancelAsyncCheckpointRunnable(SubtaskCheckpointCoordinatorImpl.java:451) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:340) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$12(StreamTask.java:1070) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$13(StreamTask.java:1083) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:329) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:293) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:188) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:615) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:579) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:565) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
2021-01-29 15:20:41,068 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - SlidingWindowCheckMapper -> Sink: SlidingWindowCheckPrintSink (2/2)#0 - asynchronous part of checkpoint 11 could not be completed.
java.util.concurrent.CancellationException: null
	at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2276) ~[?:1.8.0_282]
	at org.apache.flink.runtime.state.StateUtil.discardStateFuture(StateUtil.java:77) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.lambda$cancel$0(OperatorSnapshotFutures.java:173) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.shaded.guava18.com.google.common.io.Closer.close(Closer.java:214) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.cancel(OperatorSnapshotFutures.java:183) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.cleanup(AsyncCheckpointRunnable.java:336) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.close(AsyncCheckpointRunnable.java:305) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.util.IOUtils.closeQuietly(IOUtils.java:275) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancelAsyncCheckpointRunnable(SubtaskCheckpointCoordinatorImpl.java:451) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:340) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$12(StreamTask.java:1070) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$13(StreamTask.java:1083) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:329) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:293) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:188) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:615) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:579) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]{noformat};;;","04/Feb/21 17:16;pnowojski;merged commit 6a8b030 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamPandasConversionTests Fails,FLINK-21216,13355692,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,maguowei,maguowei,31/Jan/21 01:59,28/May/21 08:15,13/Jul/23 08:07,01/Feb/21 09:17,1.11.4,1.12.2,1.13.0,,,,,,1.11.4,1.12.2,1.13.0,,,,,API / Python,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12699&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3]

 
=================================== FAILURES =================================== 
_______________ StreamPandasConversionTests.test_empty_to_pandas _______________ 
 
self = <pyflink.table.tests.test_pandas_conversion.StreamPandasConversionTests testMethod=test_empty_to_pandas> 
 
 def test_empty_to_pandas(self): 
> table = self.t_env.from_pandas(self.pdf, self.data_type) 
 
pyflink/table/tests/test_pandas_conversion.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyflink/table/table_environment.py:1462: in from_pandas 
 arrow_schema = pa.Schema.from_pandas(pdf, preserve_index=False) 
pyarrow/types.pxi:1315: in pyarrow.lib.Schema.from_pandas 
 ??? 
.tox/py37-cython/lib/python3.7/site-packages/pyarrow/pandas_compat.py:519: in dataframe_to_types 
 type_ = pa.lib._ndarray_to_arrow_type(values, type_) 
pyarrow/array.pxi:53: in pyarrow.lib._ndarray_to_arrow_type 
 ??? 
pyarrow/array.pxi:64: in pyarrow.lib._ndarray_to_type 
 ??? 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 
> ??? 
E pyarrow.lib.ArrowTypeError: Did not pass numpy.dtype object 
 
pyarrow/error.pxi:108: ArrowTypeError 
_________________ StreamPandasConversionTests.test_from_pandas _________________ 
 
self = <pyflink.table.tests.test_pandas_conversion.StreamPandasConversionTests testMethod=test_from_pandas> 
 
 def test_from_pandas(self): 
> table = self.t_env.from_pandas(self.pdf, self.data_type, 5) 
 
pyflink/table/tests/test_pandas_conversion.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",,dian.fu,hxbks2ks,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 01 09:17:29 UTC 2021,,,,,,,,,,"0|z0n6c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/21 02:57;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12701&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3;;;","01/Feb/21 02:07;hxbks2ks;I will take a look asap.;;;","01/Feb/21 03:36;maguowei;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12713&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=f5211ead-5e53-5af8-f827-4dbf08df26bb]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12713&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=613f538c-bcef-59e6-f9cd-9714bec9fb97;;;","01/Feb/21 09:17;dian.fu;Fixed in
- master via f0cb93ab63e2a25db5165e80dcd094aa9b17f9e6
- release-1.12 via c2879d1e43be4a0c0e46fea505d2f80db8c5e137
- release-1.11 via 9c837f137953308bfda22d05e4bfac0fea7ca4c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint was declined because one input stream is finished,FLINK-21215,13355691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,maguowei,maguowei,31/Jan/21 01:45,28/May/21 08:17,13/Jul/23 08:07,01/Feb/21 17:13,1.11.4,1.12.2,1.13.0,,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12691&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9146]
  
[ERROR] Errors: 
[ERROR]   UnalignedCheckpointITCase.execute[parallel pipeline with remote channels, p = 5] » JobExecution  

 ... 4 more 
Caused by: org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold. 
 at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:98) 
 at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:84) 
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1930) 
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1007) 
 at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$declineCheckpoint$9(SchedulerBase.java:1009) 
 at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$processCheckpointCoordinatorMessage$10(SchedulerBase.java:1025) 
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) 
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) 
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) 
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) 
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 
 at java.lang.Thread.run(Thread.java:748) 
 
 
 
 
 

  
  ",,maguowei,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21244,,,,,,FLINK-20675,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 10:43:03 UTC 2021,,,,,,,,,,"0|z0n6c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/21 02:00;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12699&view=logs&j=02c4e775-43bf-5625-d1cc-542b5209e072&t=e5961b24-88d9-5c77-efd3-955422674c25;;;","01/Feb/21 03:36;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12713&view=logs&j=02c4e775-43bf-5625-d1cc-542b5209e072&t=e5961b24-88d9-5c77-efd3-955422674c25;;;","01/Feb/21 08:15;pnowojski;I believe this is caused by

{noformat}
org.apache.flink.util.SerializedThrowable: Asynchronous task checkpoint failed.
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:267) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:174) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_275]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_275]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_275]
Caused by: org.apache.flink.util.SerializedThrowable: Could not materialize checkpoint 17 for operator keyed (4/5)#5.
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:255) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        ... 4 more
Caused by: org.apache.flink.util.SerializedThrowable: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was declined because one input stream is finished
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_275]
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_275]
        at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:66) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:127) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        ... 3 more
Caused by: org.apache.flink.util.SerializedThrowable: Checkpoint was declined because one input stream is finished
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.abortInternal(SingleCheckpointBarrierHandler.java:249) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processEndOfPartition(SingleCheckpointBarrierHandler.java:273) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:186) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:158) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:180) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:406) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:190) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:615) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:579) ~[flink-streaming-java_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:565) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        ... 1 more
{noformat}

Everything starts when task declines checkpoint, because: ""Checkpoint was declined because one input stream is finished"", which is expected. Unfortunately this is detected by AsyncChekpointRunnable and is wrapped into :
{code:java}
                                new CheckpointException(
                                        CheckpointFailureReason.CHECKPOINT_ASYNC_EXCEPTION,
                                        checkpointException));
{code}
This is then passed to the {{CheckpointCoordinator}} and incorrectly bumping the tolerable checkpoint failures counter.;;;","01/Feb/21 17:13;pnowojski;Merged to master as a6eeacdb237^ and a6eeacdb237
Merged to release-1.12 as f393fdfd8e7^ and f393fdfd8e7;;;","03/Feb/21 10:43;pnowojski;merged commit 772e8fb into apache:release-1.11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaProducerITCase.testScaleDownBeforeFirstCheckpoint Failed,FLINK-21214,13355689,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,maguowei,maguowei,31/Jan/21 01:41,15/Dec/21 01:44,13/Jul/23 08:07,29/Nov/21 16:37,1.11.0,1.12.0,1.13.0,1.13.3,1.14.0,,,,1.13.6,1.14.3,1.15.0,,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12687&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5]

 
[ERROR] testScaleDownBeforeFirstCheckpoint(org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase) Time elapsed: 62.857 s <<< ERROR! 
org.apache.kafka.common.errors.TimeoutException: org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId 
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId 
 ",,dwysakowicz,fpaul,gaoyunhaii,kezhuw,lindong,maguowei,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,FLINK-20845,,,,,,,,,,,,,,,,,,,,,FLINK-10737,FLINK-21551,FLINK-16908,FLINK-22342,FLINK-24795,FLINK-25101,FLINK-18634,FLINK-18807,FLINK-24972,FLINK-20983,FLINK-22056,FLINK-18757,FLINK-24744,FLINK-24970,FLINK-20501,FLINK-21706,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 29 16:37:22 UTC 2021,,,,,,,,,,"0|z0n6bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/21 00:56;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12714&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","13/Feb/21 10:30;kezhuw;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13308&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=12861;;;","15/Feb/21 07:35;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13298&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518;;;","19/Feb/21 08:47;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13475&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7;;;","24/Feb/21 07:16;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13667&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7;;;","24/Feb/21 13:40;lindong;Given that this test has the same error message as FlinkKafkaProducerITCase.testRecoverCommittedTransaction described in FLINK-18634 [1], and that both tests uses transaction uses transaction, it is very likely that this test failure is due to bug described in [2].

The solution is to bump up Kafka dependency to 2.5.1.  However, Kafka 2.5.1 has dropped support for Scala 2.11. Flink will also need to drop support for Scala 2.11 in order to bump up the Kafka dependency to 2.5.1. This appears to be a big deal for the Flink community. I will leave to to more experienced Flink developers to drive this effort.

[1] https://issues.apache.org/jira/browse/FLINK-18634
[2] https://issues.apache.org/jira/browse/FLINK-18634?focusedCommentId=17284445&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17284445;;;","04/Mar/21 18:37;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14139&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","11/Mar/21 14:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14443&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","14/Mar/21 12:37;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14569&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=13052;;;","18/Mar/21 03:43;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14896&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=13272;;;","30/Mar/21 07:22;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15701&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=12985;;;","30/Mar/21 07:22;dwysakowicz;[~jqin] Could you take a look?;;;","30/Mar/21 08:18;lindong;[~dwysakowicz] we have investigated this bug previously and the explanation is provided in the comments above. This is due to a known issue in Kafka. The solution is to bump up Kafka dependency to 2.5.1 or later.

However, we could not do it yet because Kafka 2.5.1 has dropped support for Scala 2.11. We will need to first discuss whether we should update Flink to drop support for Scala 2.11. The discussion of this topic is not specific to Kafka. We have not started this discussion yet due to being busy with Flink 1.13 release.

According to the test history, this bug does not happy frequently. Does this bug need the ""critical"" priority?

 ;;;","30/Mar/21 08:21;dwysakowicz;Sorry, [~lindong] yes, I missed the comment :( 

Ok, let me downgrade the priority then.;;;","30/Mar/21 08:24;lindong;Thank you Dawid for keeping track of these bugs!;;;","29/Apr/21 23:47;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/May/21 22:55;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","03/Aug/21 02:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21330&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5&l=6931;;;","18/Aug/21 06:32;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22423&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7393;;;","26/Aug/21 02:49;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22856&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6561;;;","16/Nov/21 07:03;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26570&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6580];;;","16/Nov/21 07:27;arvid;On master, we bumped Kafka to 2.8.1 but we can't backport it to older release channels. I suggest to harden the test on old releases with retry rules.;;;","25/Nov/21 08:17;arvid;Merged into 1.13 as 408ba03902621f69b996a19af11fe9e8e8f13008.;;;","26/Nov/21 15:31;arvid;Merged into 1.14 as 29c288993ce7e24af70063cf7d645f4ff83909d6.

Closing for now as we assume that Kafka 2.8.1 solves the issue on master. Please reopen otherwise.;;;","29/Nov/21 12:53;trohrmann;[~arvid] I am a bit confused. This ticket is closed but https://github.com/apache/flink/pull/17944 has not been merged into the master yet?;;;","29/Nov/21 13:10;arvid;Yes, I wrote the reason for closing O_o. [~fpaul] apparently decided to forward port the fix but I don't know if the ticket number is valid.;;;","29/Nov/21 14:48;fpaul;Although the version bump of the kafka dependencies has probably fixed the Kafka-related problems we should also merge the retries to master/1.15 to stabilize our test setup.;;;","29/Nov/21 16:37;fpaul;Merged to master: faaa7c85d302bbdb448d28a9e6fdfe59d58ecc33;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"e2e test fail with 'As task is already not running, no longer decline checkpoint'",FLINK-21213,13355687,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,maguowei,maguowei,31/Jan/21 01:32,22/Jun/21 13:55,13/Jul/23 08:07,20/Feb/21 06:11,1.13.0,,,,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12686&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=d47e27f5-9721-5d5f-1cf3-62adbf3d115d]

 
Checking for non-empty .out files... 
No non-empty .out files. 
 ",,AHeise,maguowei,pnowojski,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21349,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 20 06:11:03 UTC 2021,,,,,,,,,,"0|z0n6b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/21 13:02;arvid;The sanity check at the end of the e2e fails as the log contains


{noformat}
2021-01-29 23:46:05,857 WARN  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - As task is already not running, no longer decline checkpoint 1.
java.lang.Exception: Could not materialize checkpoint 1 for operator Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1).
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:214) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:149) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_282]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
Caused by: java.util.concurrent.CancellationException
	at java.util.concurrent.FutureTask.report(FutureTask.java:121) ~[?:1.8.0_282]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_282]
	at org.apache.flink.runtime.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:501) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:57) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:112) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	... 3 more {noformat};;;","01/Feb/21 14:28;arvid;This issue is caused by a new warning added in FLINK-20675.
There are a two options:
 * make sure that this warning never pops up, for example by reducing level to DEBUG. We could also completely remove it.
 * add it to the whitelists of allowed exceptions in e2e.

As I'm currently not seeing a benefit for the end-user, I'd go with the first option. 


[~yunta] 

 

Note: I had a bit of trouble of understanding the error message at first. Maybe we can rephrase it to {{Ignore decline of checkpoint 1 as task is not running anymore.}};;;","02/Feb/21 12:10;yunta;[~AHeise] thanks for reminder, I think changing this warning to INFO level should also be okay as Flink-1.8 would [not fail when async checkpoint failure if task not running|https://github.com/apache/flink/blob/0e8e8062bcc159e9ed2a0d4a0a61db4efcb01f2f/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L1156] with code:

{code:java}
	public void handleAsyncException(String message, Throwable exception) {
		if (isRunning) {
			// only fail if the task is still running
			getEnvironment().failExternally(exception);
		}
	}
{code}
;;;","02/Feb/21 12:44;arvid;On INFO, it probably still triggers e2e failure, cf. {{check_logs_for_exceptions}} in common.sh.;;;","20/Feb/21 06:09;yunta;Only degrade to INFO level is not enough, and my PR also remove the exception message.;;;","20/Feb/21 06:11;yunta;merged 
master: 32ed58254303aaec6290a1adb0402279ac5ee7d1
release-1.12: 6ce29325332be84bd15601b45f37fa1b22b1bdff
release-1.11: 9bc2fba0bc404fff36c53a4560c76919705ec383
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyarrow exception when using window with pandas udaf,FLINK-21208,13355494,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,liuyufei,liuyufei,29/Jan/21 17:48,28/May/21 08:58,13/Jul/23 08:07,08/Feb/21 01:58,1.11.0,1.12.0,1.12.1,,,,,,1.11.4,1.12.2,1.13.0,,,,,API / Python,,,,,0,pull-request-available,,,,,"I write a pyflink demo and execute in local environment, the logic is simple:generate records and aggerate in 100s tumle window, using a pandas udaf.
But the job failed after several minutes, I don't think it's a resource problem because the amount of data is small, here is the error trace.

{code:java}
Caused by: org.apache.flink.streaming.runtime.tasks.AsynchronousException: Caught exception while processing timer.
	at org.apache.flink.streaming.runtime.tasks.StreamTask$StreamTaskAsyncExceptionHandler.handleAsyncException(StreamTask.java:1108)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.handleAsyncException(StreamTask.java:1082)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1213)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$17(StreamTask.java:1202)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:302)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
Caused by: TimerException{java.lang.RuntimeException: Failed to close remote bundle}
	... 11 more
Caused by: java.lang.RuntimeException: Failed to close remote bundle
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:371)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:325)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:291)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.checkInvokeFinishBundleByTime(AbstractPythonFunctionOperator.java:285)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$open$0(AbstractPythonFunctionOperator.java:134)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1211)
	... 10 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 3: Traceback (most recent call last):
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 253, in _execute
    response = task()
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 310, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 480, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 515, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 978, in process_bundle
    element.data)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 218, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 330, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 332, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""apache_beam/runners/worker/operations.py"", line 292, in apache_beam.runners.worker.operations.Operation.process
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py"", line 73, in process
    for value in o.value:
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 625, in decode_from_stream
    yield self._decode_one_batch_from_stream(in_stream, in_stream.read_var_int64())
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 636, in _decode_one_batch_from_stream
    return arrow_to_pandas(self._timezone, self._field_types, [next(self._batch_reader)])
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 629, in _load_from_stream
    reader = pa.ipc.open_stream(stream)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyarrow/ipc.py"", line 146, in open_stream
    return RecordBatchStreamReader(source)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyarrow/ipc.py"", line 62, in __init__
    self._open(source)
  File ""pyarrow/ipc.pxi"", line 360, in pyarrow.lib._RecordBatchStreamReader._open
  File ""pyarrow/error.pxi"", line 123, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
OSError: Expected IPC message of type schema but got record batch

	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:57)
	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:458)
	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:547)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:369)
	... 15 more
Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 3: Traceback (most recent call last):
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 253, in _execute
    response = task()
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 310, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 480, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 515, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 978, in process_bundle
    element.data)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 218, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 330, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 332, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""apache_beam/runners/worker/operations.py"", line 292, in apache_beam.runners.worker.operations.Operation.process
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py"", line 73, in process
    for value in o.value:
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 625, in decode_from_stream
    yield self._decode_one_batch_from_stream(in_stream, in_stream.read_var_int64())
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 636, in _decode_one_batch_from_stream
    return arrow_to_pandas(self._timezone, self._field_types, [next(self._batch_reader)])
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 629, in _load_from_stream
    reader = pa.ipc.open_stream(stream)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyarrow/ipc.py"", line 146, in open_stream
    return RecordBatchStreamReader(source)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyarrow/ipc.py"", line 62, in __init__
    self._open(source)
  File ""pyarrow/ipc.pxi"", line 360, in pyarrow.lib._RecordBatchStreamReader._open
  File ""pyarrow/error.pxi"", line 123, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
OSError: Expected IPC message of type schema but got record batch

	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:177)
	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:157)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:309)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:292)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:782)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
{code}

And my test code:

{code:python}
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import *
from pyflink.table.udf import udaf, AggregateFunction
from pyflink.table.window import Tumble


class MyTestAggregateFunction(AggregateFunction):

    def get_value(self, accumulator):
        return accumulator[0]

    def create_accumulator(self):
        return Row(0)

    def accumulate(self, accumulator, *args):
        accumulator[0] = len(args[0])

    def get_result_type(self):
        return DataTypes.BIGINT()


if __name__ == '__main__':
    env = StreamExecutionEnvironment.get_execution_environment()
    f_s_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()
    t_env = StreamTableEnvironment.create(env, None, f_s_settings)

    my_udaf = udaf(MyTestAggregateFunction(), func_type=""pandas"")
    t_env.register_function('my_udaf', my_udaf)
    t_env.sql_update(""""""
    CREATE TABLE `source_table` (
        `header` STRING,
        ts AS PROCTIME()
    ) WITH (
          'connector' = 'datagen',
          'rows-per-second' = '100'
    )
    """""")
    t_env.sql_update(""""""
    CREATE TABLE `sink_table` (
        `content` BIGINT,
        `wstart` TIMESTAMP(3)
    ) WITH (
        'connector' = 'print'
    )
    """""")
    t_env.scan(""source_table"") \
        .window(Tumble.over(""100.second"").on(""ts"").alias(""w"")) \
        .group_by('w') \
        .select(""my_udaf(header), w.start"")\
        .insert_into(""sink_table"")
    t_env.execute(""test_job"")

{code}

",,DaDaShen,dian.fu,hxbks2ks,liuyufei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 08 01:58:15 UTC 2021,,,,,,,,,,"0|z0n54g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/21 15:58;liuyufei;I've made some discoveries, I found beam has a config {{DEFAULT_BUNDLE_PROCESSOR_CACHE_SHUTDOWN_THRESHOLD_S}}=60s in {{sdk_worker.py}}, periodicity shutdown inactive processor, and initialize a new processor need read schema data from the begining of the stream. The stream was generated by {{ArrowSerializer}} in flink operator and only serialize schema data once, afterwards will only write records data.

So, if processor was evict from cache, beam will try to initialize a new processor with the input stream(only records), and thow expected schema but was record batch.

cc [~dian.fu];;;","02/Feb/21 02:52;hxbks2ks;[~liuyufei] Thanks a lot for reporting this issue. As you said, it is indeed because of `DEFAULT_BUNDLE_PROCESSOR_CACHE_SHUTDOWN_THRESHOLD_S` that the reused bundle processor was evicted. We need to reinitialize `ArrowSerializer` every time after finishing the bundle on the java side to let it write the schema information, and the coder on the python side should also be adapted accordingly to avoid the impact of the underlying beam cache. I will fix it as soon as possible.;;;","02/Feb/21 12:27;liuyufei;[~hxbks2ks] I still have a question, I haven't use arrow before and don't know the implementation details. But if we already know schema of data, why should we transfer schema of every bundle?;;;","05/Feb/21 06:55;hxbks2ks; [~liuyufei] The serialization protocol provided by arrow is to serialize the schema info into the header before transmitting data. This is actually a stateful serializer. But for beam, it requires your serializer to be stateless. Both of them are not wrong and have their own considerations, but when used in combination, there will be problems, unless you transmit a schema for each arrow batch.
;;;","08/Feb/21 01:58;hxbks2ks;Merged to master via 78f4b1f361333765dcf9b5cc41cccea77da9024c
Merged to release-1.12 via e7abfa037555095f061d4d7f3e06dc238e837df2
Merged to release-1.11 via 6f7fbb3702af65c4a63f532a1e8b1e25f6a479d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LimitableBulkFormat is invalid when format is orc,FLINK-21195,13355324,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sujun1020,sujun1020,sujun1020,29/Jan/21 03:45,23/Sep/21 17:26,13/Jul/23 08:07,21/Jun/21 06:39,1.12.1,,,,,,,,1.14.0,,,,,,,Connectors / FileSystem,,,,,0,auto-deprioritized-major,pull-request-available,,,,"The orc file will read a stripe data in advance in the createReader() method (see the construction method of RecordReaderImpl in detail), and the parquet file will start to read the block data when the readBatch() method is called, so if all orc files have only one stripe, limitableBulkFormat will be invalid

 

!orc_reader_debug.jpg!

 

 ",,jark,lzljs3620320,sujun1020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/21 04:02;sujun1020;limit_code.jpg;https://issues.apache.org/jira/secure/attachment/13019604/limit_code.jpg","29/Jan/21 03:45;sujun1020;orc_reader_debug.jpg;https://issues.apache.org/jira/secure/attachment/13019601/orc_reader_debug.jpg",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 21 06:39:30 UTC 2021,,,,,,,,,,"0|z0n42o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/21 03:51;sujun1020;[~lzljs3620320]  If confirmed to be a bug, you can assign the issue to me :D;;;","29/Jan/21 04:02;sujun1020;!limit_code.jpg!;;;","22/Apr/21 10:52;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 22:58;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","03/Jun/21 08:47;sujun1020;hi [~jark] [~lzljs3620320],

I found that the master branch still has this problem. After my test, for some SQL containing limit expression, ORC format files are 3 times slower than PARQUET. Does the community have any plan to fix this problem? If so, I can contribute my code;;;","04/Jun/21 02:26;lzljs3620320;[~sujun1020] Assigned to u~;;;","21/Jun/21 06:39;lzljs3620320;Fixed via:

master: d8c19ba2e94648000c1fd463da350a4185aaae60

[~sujun1020] Feel free to re-open and create PR for 1.13/1.12 if you think this should be fixed in 1.13/1.12. (I can't just cherry-pick because there are conflicts);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecordWriterOutput swallows interrupt state when interrupted.,FLINK-21186,13355130,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,pnowojski,arvid,AHeise,28/Jan/21 09:27,31/Dec/21 11:51,13/Jul/23 08:07,31/Dec/21 11:51,1.13.0,,,,,,,,1.15.0,,,,,,,Runtime / Task,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,pull-request-available,,,,,AHeise,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 31 11:51:04 UTC 2021,,,,,,,,,,"0|z0n2vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/21 12:21;pnowojski;How does it swallow it? Do you mean it's wrapping it in a `RuntimeException`? 

If so, I'm not sure if we can do anything, as `InterruptedException` can happen on a backpressure deep in the network stack when collecting a records in the operator chain. Maybe it's better to wrap that kind of exceptions as `RuntimeException` so that the user code is not polluted with them? Also all in all, such exception should bubble up and interrupt/stop the task.;;;","22/Apr/21 10:52;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 22:58;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","20/Dec/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Dec/21 09:03;pnowojski;I still think there is no issue in this code. It's maybe not the prettiest solution, but it works as far as I can tell. Under this ticket we can do a little bit of clean up in `RecordWriterOutput`, to explicitly limit what types of exception (only `IOException`) can be wrapped in `RuntimeException`, and instead of using `RuntimeException` directly, we can thus use `UncheckedIOException`.;;;","29/Dec/21 10:39;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","31/Dec/21 11:51;pnowojski;Merged a small hotfix change as commit ef839ff into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Buffer pool is destroyed error when outputting data over a timer after cancellation.,FLINK-21181,13355112,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,arvid,AHeise,28/Jan/21 08:43,14/Sep/21 09:48,13/Jul/23 08:07,10/May/21 12:29,1.11.0,1.12.3,1.13.0,1.14.0,,,,,1.12.4,1.13.1,1.14.0,,,,,Runtime / Task,,,,,0,pull-request-available,,,,,"A [user reported|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/What-causes-a-buffer-pool-exception-How-can-I-mitigate-it-td40959.html] the issue and provided some taskmanager log with the following relevant lines:
{noformat}
2021-01-26 04:37:43,280 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task forward fill -> (Sink: tag db sink, Sink: back fill db sink, Sink: min max step db sink) (2/2) (8c1f256176fb89f112c27883350a02bc).
2021-01-26 04:37:43,280 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - forward fill -> (Sink: tag db sink, Sink: back fill db sink, Sink: min max step db sink) (2/2) (8c1f256176fb89f112c27883350a02bc) switched from RUNNING to CANCELING.
2021-01-26 04:37:43,280 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code forward fill -> (Sink: tag db sink, Sink: back fill db sink, Sink: min max step db sink) (2/2) (8c1f256176fb89f112c27883350a02bc).
2021-01-26 04:37:43,282 ERROR xxxxxx.pipeline.stream.functions.process.ForwardFillKeyedProcessFunction [] - Error in timer.
java.lang.RuntimeException: Buffer pool is destroyed.
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:110) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:89) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:45) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$BroadcastingOutputCollector.collect(OperatorChain.java:787) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$BroadcastingOutputCollector.collect(OperatorChain.java:740) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at xxxxxx.pipeline.stream.functions.process.ForwardFillKeyedProcessFunction.collect(ForwardFillKeyedProcessFunction.java:452) ~[develop-17e9fd0e.jar:?]
	at xxxxxx.pipeline.stream.functions.process.ForwardFillKeyedProcessFunction.onTimer(ForwardFillKeyedProcessFunction.java:277) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.invokeUserFunction(KeyedProcessOperator.java:94) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.onProcessingTime(KeyedProcessOperator.java:78) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:260) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1181) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$13(StreamTask.java:1172) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:270) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:190) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:558) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [develop-17e9fd0e.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]
Caused by: java.lang.IllegalStateException: Buffer pool is destroyed.
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentFromGlobal(LocalBufferPool.java:339) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegment(LocalBufferPool.java:309) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:290) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:266) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.partition.ResultPartition.getBufferBuilder(ResultPartition.java:213) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.requestNewBufferBuilder(RecordWriter.java:294) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.requestNewBufferBuilder(ChannelSelectorRecordWriter.java:103) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.copyFromSerializerToTargetChannel(RecordWriter.java:149) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:120) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:60) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:107) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	... 24 more
{noformat}
 ",,AHeise,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22424,FLINK-24182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 14 09:48:00 UTC 2021,,,,,,,,,,"0|z0n2rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/21 10:51;roman;I think this is what's happening:
 # Task Thread picks up a mail (or creates a batch of mails), starts execution and pauses
 # Task is cancelled (e.g. by RPC) - method Task.cancelExecution
 ## Task state transitions from RUNNING to CANCELLING
 ## TaskCanceler is created and started
 ## TaskCanceler calls invokable.cancel(), which *schedules* mailboxProcessor shutdown
 ## TaskCanceler releases network resources (Task.closeNetworkResources()), which destroys LocalBufferPools
 # Task Thread continues execution (of a prevously picked mail) and eventually hits destroyed buffer pool

From the comments in the code, I think (2.3) should not return until the task thread actually exits the mailbox loop:
{quote}// Don't do this before cancelling the invokable. Otherwise we
 // will get misleading errors in the logs.
 networkResourcesCloser.run();
{quote}
To fix, it I'd propose TaskCanceller  to wait for mailbox loop to be finished (introducing something like mailboxProcessor.getCompletionFuture().join() ).

WDYT [~AHeise], [~pnowojski]?;;;","22/Apr/21 15:24;pnowojski;I think I've found a related issue. I've modified the code to not re-use the same memory segments, but on recycling always free up the segment. And what I have observed is a similar problem as reported in this ticket, but even more severe:

{noformat}
Caused by: java.lang.RuntimeException: segment has been freed
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:109)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:93)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:50)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase$ReEmitAll.process(UnalignedCheckpointStressITCase.java:477)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase$ReEmitAll.process(UnalignedCheckpointStressITCase.java:468)
	at org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:57)
	at org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:32)
	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.emitWindowContents(WindowOperator.java:577)
	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onProcessingTime(WindowOperator.java:533)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:284)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1395)
	... 11 more
Caused by: java.lang.IllegalStateException: segment has been freed
	at org.apache.flink.core.memory.MemorySegment.put(MemorySegment.java:483)
	at org.apache.flink.core.memory.MemorySegment.put(MemorySegment.java:1398)
	at org.apache.flink.runtime.io.network.buffer.BufferBuilder.append(BufferBuilder.java:100)
	at org.apache.flink.runtime.io.network.buffer.BufferBuilder.appendAndCommit(BufferBuilder.java:82)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:250)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:142)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:104)
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:107)
	... 24 more
{noformat}
That's happening also during cancellation/job failover. It looks to me, that in the case you have analysed, processing time timer tried to request a buffer from destroyed buffer pool.  My case looks identical, but it's failing when trying to write to already `free`'ed up buffer. Without my changes, this code would silently write some data to a buffer that has already been recycled/returned to the pool. If someone else would pick up this buffer, it would easily lead to the data corruption.

As far as I can tell, the exact reason behind my exception is a bit different. The buffer to which timer attempts to write to, has been released from `ResultSubpartition#onConsumedSubpartition`, causing `BufferConsumer` to be closed (which recycles/frees underlying memory segment ), while matching `BufferBuilder` is still being used...;;;","23/Apr/21 07:39;pnowojski;I've created a separate ticket for that: FLINK-22424. The solution might be still the same, or it might be a very different one.;;;","09/May/21 19:37;roman;Merged into master as 420eb6ec0276b208a628b535ca553d6d892a572f.

Merged into 1.13 as ce7c78ca72ce86df0b4a28fbd3233b89da3238e1.

Merged into 1.12 as 4fd134403d535e074edf3b6a82b540e88709f4ec.
 ;;;","14/Sep/21 09:48;pnowojski;I have to basically revert this change and reimplement the fix differently as a result of FLINK-24182. Because on the clean cancellation path we don't want to issue interrupts, we have to close buffer pools to unblock back pressured tasks/threads/mailbox actions. To do so, as part of FLINK-24182 I will re-implement fix for this (FLINK-21181) bug, so that access to closed/destroyed LocalBufferPool will result in {{CancelTaskException}} being thrown.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task failure will not trigger master hook's reset(),FLINK-21178,13355094,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Brian Zhou,Brian Zhou,Brian Zhou,28/Jan/21 06:34,11/Mar/21 09:31,13/Jul/23 08:07,11/Mar/21 09:31,1.11.3,1.12.0,,,,,,,1.12.3,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"In Pravega Flink connector integration with Flink 1.12, we found an issue with our no-checkpoint recovery test case [1].

We expect the recovery will call the ReaderCheckpointHook::reset() function which was the behaviour before 1.12. However FLINK-20222 changes the logic, the reset() call will only be called along with a global recovery. This causes Pravega source data loss when failure happens before the first checkpoint.

[1]  [https://github.com/crazyzhou/flink-connectors/blob/da9f76d04404071471ebd86bf6889b307c9122ff/src/test/java/io/pravega/connectors/flink/FlinkPravegaReaderRGStateITCase.java#L78]",,becket_qin,Brian Zhou,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 09:31:22 UTC 2021,,,,,,,,,,"0|z0n2nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/21 09:31;becket_qin;Merged.
master: a08ba482a0382a02e26e039bccfb81782533f082

release-1.12: 4b1212ddca42a1a49f9d0e25f74c9f7bf0dda297

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Streaming SQL end-to-end test (Old planner)"" e2e test failed",FLINK-21173,13355086,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,maguowei,maguowei,28/Jan/21 05:58,08/Jun/21 10:16,13/Jul/23 08:07,08/Jun/21 10:16,1.12.0,,,,,,,,,,,,,,,Table SQL / Legacy Planner,,,,,0,auto-deprioritized-major,test-stability,,,,"Jan 28 00:03:37 FAIL StreamSQL: Output hash mismatch. Got e7a02d99d5ac2a4ef4792b3b7e6f54bf, expected b29f14ed221a936211202ff65b51ee26. 

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12573&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529",,godfreyhe,joemoe,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 10:16:08 UTC 2021,,,,,,,,,,"0|z0n2ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 10:52;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","20/May/21 10:53;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/21 10:16;joemoe;doesn't exist anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka flink-connector-base dependency should be scope compile,FLINK-21169,13355064,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,28/Jan/21 02:42,28/May/21 08:16,13/Jul/23 08:07,01/Feb/21 01:27,1.12.1,,,,,,,,1.12.2,1.13.0,,,,,,,,,,,0,pull-request-available,,,,,"The dependency is marked provided and therefore missing from an application using the connector. This dependency should be packaged with the application w/o extra declaration by the user.

It appears that was also the intention, based on other usage in flink-connector-files ?",,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-28 02:42:35.0,,,,,,,,,,"0|z0n2gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python dependencies specified via CLI should not override the dependencies specified in configuration,FLINK-21163,13354904,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,27/Jan/21 11:21,29/Jan/21 01:43,13/Jul/23 08:07,29/Jan/21 01:43,,,,,,,,,1.12.2,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,"Currently, the python dependencies specified via CLI will override the dependencies specified in configuration. For python.files and python.archives, it makes more sense to merge them together.",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 01:43:52 UTC 2021,,,,,,,,,,"0|z0n1hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/21 01:43;dian.fu;Fixed in 
- master via 41046087ce917414c9a06c6787192ee94af017f8
- release-1.12 via d4d51eb1631817e3681206af86cb40824c2eff3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLINK SQL IF function semantic incorrect,FLINK-21162,13354890,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhaoWeiNan,hiscat,hiscat,27/Jan/21 10:03,08/Oct/21 11:15,13/Jul/23 08:07,12/Apr/21 07:52,1.12.1,,,,,,,,1.13.0,,,,,,,Table SQL / API,Table SQL / Planner,Table SQL / Runtime,,,0,pull-request-available,,,,,"I have a job using IF() test condition.

when i use ""IF(col = '', 'a', 'b'), WHEN col = '' return a "", it is ok.

when i use ""IF(col IS NULL, 'a', 'b'), WHEN col = NULL return a "", it is ok.

when i use ""IF(col = '' OR col IS NULL, 'a', 'b'),WHEN col = ''return a"", it is ok.

when i use ""IF(col = '' OR col IS NULL, 'a', 'b'),WHEN col = NULL return b"" ,that's amazing.

The semantic seems like inccorrect.

 

my table ddl:

CREATE TABLE print(a STRING, b STRING) WITH ( 'connector' = 'print' );

my dml :

INSERT INTO print
SELECT return_flag,
 IF(return_flag = '' OR return_flag IS NULL, 'N', 'Y') returnFlag
FROM header;

my test data:

3> +I(,N)
5> +I(X,Y)

1> +I(null,Y)
2> +I(,N)
5> +I(null,Y)
3> +I(,N)

 ",,hiscat,jark,jingzhang,leonard,libenchao,RocMarshal,wczhu,xiaozilong,ZhaoWeiNan,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22015,,,,,,,,,,,FLINK-22015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 12:37:32 UTC 2021,,,,,,,,,,"0|z0n1eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/21 10:04;hiscat;I using the COALESCE(return_flag, '') solve the problem, but i think this  is a bug. 

hello [~Leonard Xu], [~jark]

Can someone take a moment to look at this?thanks;;;","27/Jan/21 10:51;jark;Sounds like a bug in code generation of IF function. ;;;","04/Feb/21 05:49;ZhaoWeiNan;[~hiscat] I debug your sql and found that col is null is related to col ='', and there is no code that generates isnull. I think it may be the sqlnode that has been optimized by the parser. In fact, you can get With this result, flink only made the judgment of col ='', so you can get the answer of b;;;","21/Feb/21 08:18;ZhaoWeiNan;[~jark]

[code: testSqlApi(""f14 IN ('This is a test String.', 'String', 'Hello world')"",""false""） in ScalarOperatorsTest.scala|https://github.com/apache/flink/pull/14872/files#diff-1da1e565c0f5f01aec101b57d4f635320432d09785e5234bd384b668ec98d658] file, it should expected false,Should not return null.What do you guys think.;;;","22/Feb/21 02:36;jark;Yes. I think {{null IN (...)}} should return unknown, and unkown is false by default. ;;;","22/Feb/21 02:54;ykt836;I don't think we should change this behavior. I checked all the popular databases and other sql vendors, all of them will return ""NULL"" in query like "" SELECT NULL in ('a', 'b', 'c') "". 

Using false instead of NULL should only occurred in predicates. ;;;","22/Feb/21 03:04;jark;Thanks for the investigation [~ykt836], I also checked {{null in (...)}} is recognized as {{false}} in Flink. Then I think current behavior is correct.;;;","22/Feb/21 03:19;ZhaoWeiNan;[~ykt836] Thanks for the investigation [~ykt836],but when i use ""IF(col = '' OR col IS NULL, 'a', 'b'),WHEN col = NULL return b"" ,the result in uncorrectly.;;;","22/Feb/21 03:37;ZhaoWeiNan;|if (!${castedNeedle.nullTerm}) {
| $resultTerm = $setTerm.contains(${castedNeedle.resultTerm});
| $nullTerm = !$resultTerm && $setTerm.containsNull();
|}else{
| // resultTerm return true, but null in set,we give nullItem = true?
| {color:#FF0000}$resultTerm = $setTerm.containsNull();{color}
{color:#FF0000}| $nullTerm = true;{color}
|}

wo can return resultTerm = true ,help if function make judgments,but make nullTerm = true.;;;","22/Feb/21 08:29;ZhaoWeiNan;[~jark] ,Hello,when i debug,i found that when IS_NULL in OR operate ,if OR change to SEARCH('',null) there was a major mistake,because SEARCH has three return values (true,false,null),but OR Operate only can return (true,false),so wo can't change (IS_NULL OR ...) to SEARCH('',null);;;;","22/Feb/21 10:25;jark;[~ZhaoWeiNan], Yes. I think this is a bug in {{RexSimplify}}. Because this is a class in Calcite and we shouldn't maintain this class for the long-term. Could you create an issue to fix this problem in Calcite community? And mention the Calcite issue in the class javadoc of the copied {{RexSimplify}}.;;;","22/Feb/21 12:37;ZhaoWeiNan;[~jark] ok,i will do it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ValueDeserializerWrapper throws NullPointerException when getProducedType is invoked,FLINK-21160,13354871,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,renqs,renqs,27/Jan/21 09:24,21/May/21 07:27,13/Jul/23 08:07,21/May/21 07:27,1.12.3,,,,,,,,1.13.0,,,,,,,Connectors / Kafka,,,,,0,auto-deprioritized-major,pull-request-available,,,,"The variable {{deserializer}} in class {{ValueDeserializerWrapper}} won't be instantiated until method {{deserialize()}} is invoked in runtime, so in the job compiling stage when invoking {{getProducedType()}}, NPE will be thrown because of referencing the uninstantiated variable {{deserializer}}.

A user reported that the new {{KafkaSource}} fails with a {{NullPointerException}}:

{code}
Exception in thread ""main"" java.lang.NullPointerException
at org.apache.flink.connector.kafka.source.reader.deserializer.ValueDeserializerWrapper.getProducedType(ValueDeserializerWrapper.java:79)
at org.apache.flink.connector.kafka.source.KafkaSource.getProducedType(KafkaSource.java:171)
at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getTypeInfo(StreamExecutionEnvironment.java:2282)
at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.fromSource(StreamExecutionEnvironment.java:1744)
at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.fromSource(StreamExecutionEnvironment.java:1715)
{code}

when setting it up like this:

{code}
val kafkaSource = buildKafkaSource(params)
val datastream = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), ""kafka"")

private fun buildKafkaSource(params: ParameterTool): KafkaSource<String> {
    val builder = KafkaSource.builder<String>()
        .setBootstrapServers(params.get(""bootstrapServers""))
        .setGroupId(params.get(""groupId""))
        .setStartingOffsets(OffsetsInitializer.earliest())
        .setTopics(""topic"")
        .setDeserializer(KafkaRecordDeserializer.valueOnly(StringDeserializer::class.java))

    if (params.getBoolean(""boundedSource"", false)) {
        builder.setBounded(OffsetsInitializer.latest())
    }

    return builder.build()
}
{code}

The problem seems to be that the {{ValueDeserializerWrapper}} does not set the deserializer the deserialize method is called, but {{getProducedType}} is actually called first resulting in the {{NullPointerException}}.

https://lists.apache.org/x/thread.html/r8734f9a18c25fd5996fc2edf9889277c185ee9a0b79280938b1cb792@%3Cuser.flink.apache.org%3E",,lindong,renqs,RocMarshal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21691,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 10:53:02 UTC 2021,,,,,,,,,,"0|z0n1a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 10:52;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","26/Apr/21 02:52;renqs;Fixed by [#14784|https://github.com/apache/flink/pull/14784];;;","20/May/21 10:53;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceEnumerator not sending NoMoreSplitsEvent to unassigned reader,FLINK-21159,13354868,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,27/Jan/21 09:16,28/May/21 08:58,13/Jul/23 08:07,08/Apr/21 22:47,,,,,,,,,1.12.3,1.13.0,,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,,"Kafka split enumerator only sends {{NoMoreSplitsEvent}} when assigning splits to a reader. If a reader is not assigned with any splits, it won't receive `NoMoreSplitsEvent`, and the subtask will stay in RUNNING state forever but not actually working.  ",,Paul Lin,renqs,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 22:47:05 UTC 2021,,,,,,,,,,"0|z0n19k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 17:48;sewen;Fixed in 1.13.0 via
  - dc3d0702087c8681435761eed7f2aa9161b09540;;;","08/Apr/21 22:47;sewen;Fixed in 1.12.3 via
  - 3b9b357cbdd81cdab5f36ed524362c18513d82a9
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong jvm metaspace and overhead size show in  taskmanager metric page,FLINK-21158,13354841,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pengkangjing,pengkangjing,pengkangjing,27/Jan/21 07:48,28/Jan/21 13:12,13/Jul/23 08:07,28/Jan/21 13:12,1.12.1,,,,,,,,1.12.2,1.13.0,,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,"the size of jvm metaspace and jvm overhead  on taskmanager metrics page  not the derived value from flink-conf.yaml  but

always  be the default value.

i used the defalut flink-conf.yaml

default value: 

         JVM_METASPACE  - 256M

         JVM_OVERHEAD_MAX - 1GB

!image-2021-01-27-16-30-51-834.png!

!image-2021-01-27-16-19-16-390.png!

 ",,pengkangjing,RocMarshal,xtsong,,,,,,,,,,,,,,,10800,10800,,0%,10800,10800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/21 08:19;pengkangjing;image-2021-01-27-16-19-16-390.png;https://issues.apache.org/jira/secure/attachment/13019479/image-2021-01-27-16-19-16-390.png","27/Jan/21 08:30;pengkangjing;image-2021-01-27-16-30-51-834.png;https://issues.apache.org/jira/secure/attachment/13019480/image-2021-01-27-16-30-51-834.png","27/Jan/21 08:59;RocMarshal;image-2021-01-27-16-59-28-488.png;https://issues.apache.org/jira/secure/attachment/13019486/image-2021-01-27-16-59-28-488.png","27/Jan/21 09:33;pengkangjing;image-2021-01-27-17-33-25-300.png;https://issues.apache.org/jira/secure/attachment/13019495/image-2021-01-27-17-33-25-300.png","27/Jan/21 09:34;pengkangjing;image-2021-01-27-17-34-32-454.png;https://issues.apache.org/jira/secure/attachment/13019496/image-2021-01-27-17-34-32-454.png","27/Jan/21 10:07;pengkangjing;image-2021-01-27-18-07-16-558.png;https://issues.apache.org/jira/secure/attachment/13019505/image-2021-01-27-18-07-16-558.png","27/Jan/21 10:17;pengkangjing;image-2021-01-27-18-17-01-917.png;https://issues.apache.org/jira/secure/attachment/13019507/image-2021-01-27-18-17-01-917.png","27/Jan/21 10:18;pengkangjing;image-2021-01-27-18-18-18-241.png;https://issues.apache.org/jira/secure/attachment/13019508/image-2021-01-27-18-18-18-241.png","27/Jan/21 10:19;RocMarshal;image-2021-01-27-18-19-31-121.png;https://issues.apache.org/jira/secure/attachment/13019509/image-2021-01-27-18-19-31-121.png","27/Jan/21 10:31;pengkangjing;image-2021-01-27-18-31-07-616.png;https://issues.apache.org/jira/secure/attachment/13019510/image-2021-01-27-18-31-07-616.png",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 28 13:12:40 UTC 2021,,,,,,,,,,"0|z0n13k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/21 08:23;RocMarshal;[~pengkangjing]

Could you describe more information about this scene? Such as pictures, deployment modes, etc.;;;","27/Jan/21 08:36;pengkangjing;i just uploaded two pics   [~RocMarshal].  

could you assigned it to me？;;;","27/Jan/21 09:00;RocMarshal;Hi, [~pengkangjing].


When I set the configuration-items like 'taskmanager.memory.jvm-metaspace.size: 500m
taskmanager.memory.jvm-overhead.max: 800m' in ${FLINK_HOME}/conf/flink-conf.yaml with local standalonesession mode, the page of taskmanager-metric shown as the follows:

!image-2021-01-27-16-59-28-488.png!

And I don't know your deployment mode now, so we need more deployment detail to confirm the cause of it.

 

 ;;;","27/Jan/21 09:34;pengkangjing;[~RocMarshal] you can try to not  set   the value taskmanager.memory.jvm-metaspace.size: 500m and taskmanager.memory.jvm-overhead.max: 800m  in flink-conf.yaml.

 then  you can find  in  taskmanager  log file   like this:

!image-2021-01-27-18-31-07-616.png!

but  on flink ui  is like this:

!image-2021-01-27-17-34-32-454.png!

 ;;;","27/Jan/21 09:40;RocMarshal;cc [~jark];;;","27/Jan/21 09:46;RocMarshal;[~pengkangjing] This looks like jobmanager information？;;;","27/Jan/21 09:49;pengkangjing;[~RocMarshal] [~jark]  could you assigned it to me？ i have been fixed it .;;;","27/Jan/21 09:50;pengkangjing;only taskmanager  [~RocMarshal];;;","27/Jan/21 09:52;chesnay;[~pengkangjing] We haven't even established what the issue is; so what exactly have you ""fixed""?;;;","27/Jan/21 10:00;pengkangjing;sorry , i have some idea to fix it .[~chesnay];;;","27/Jan/21 10:03;chesnay;[~pengkangjing] Please outline your approach that. What are you going to change?;;;","27/Jan/21 10:15;pengkangjing;[~chesnay]  add  the value derived from TaskExecutorProcessUtils.processSpecFromConfig  to  DynamicConfigsStr  

flink/flink-runtime/src/main/java/org/apache/flink/runtime/clusterframework/TaskExecutorProcessUtils  

just like this:

!image-2021-01-27-18-18-18-241.png!  ;;;","27/Jan/21 10:21;RocMarshal;!image-2021-01-27-18-19-31-121.png!

The one you want to post is this picture, right？[~pengkangjing];;;","27/Jan/21 10:25;pengkangjing;[~RocMarshal]  yes , only taskmanager ,  i am sorry for that uoloaded some misleading picture. 

i will replace that misleading pic and edit the description again;;;","27/Jan/21 11:16;chesnay;[~xintongsong]{color:#172b4d} What do you think about the proposed change? (adding \{{JVM_OVERHEAD[_MIN/_MAX]}} to \{{TaskExecutorProcessUtils#generateDynamicConfigsStr}}){color};;;","27/Jan/21 11:33;xtsong;Thanks [~pengkangjing] for reporting this, and thanks [~RocMarshal] and [~chesnay] for looking into this.

I think this is a valid problem. When implementing FLIP-49, we have not set derived jvm metaspace/overhead sizes via dynamic properties, due to the assumption that these values will not be used after the JVM process being launched. This assumption is no longer true since in FLIP-102 we start to display these configuration values on the web ui.

[~pengkangjing], the changes you proposed look good to me. I've assigned you to this ticket. Please go ahead.;;;","27/Jan/21 13:41;pengkangjing;[~xintongsong] Thanks for assigned this work to me，and thanks [~RocMarshal]  and [~chesnay]   for discussing this issuse .  I've done it，the pull request link is [https://github.com/apache/flink/pull/14779]  ;;;","28/Jan/21 13:12;xtsong;Fixed via
* master (1.13): 8ae563a2da0355596c11252dd8ffffefe8fbd564
* release-1.12: fa06f4c8f7507a64be367f07e9299ee781cd0229;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSourceTextLinesITCase.testBoundedTextFileSourceWithTaskManagerFailover does not pass,FLINK-21155,13354732,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,26/Jan/21 18:10,08/Feb/21 10:42,13/Jul/23 08:07,08/Feb/21 10:42,1.12.1,,,,,,,,1.12.2,1.13.0,,,,,,Connectors / FileSystem,,,,,0,pull-request-available,test-stability,,,,"Somehow {{FileSourceTextLinesITCase.testBoundedTextFileSourceWithTaskManagerFailover}} does not pass when executed in IntelliJ. However, when I run the whole test suite {{FileSourceTextLinesITCase}}, then the test passes. This looks like a problem with the test setup.

When run individually, then the test fails with

{code}
java.lang.RuntimeException: Failed to fetch next result

	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testBoundedTextFileSource(FileSourceTextLinesITCase.java:138)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testBoundedTextFileSourceWithTaskManagerFailover(FileSourceTextLinesITCase.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:169)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:118)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	... 30 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:167)
	... 32 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:117)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:114)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:166)
	... 32 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:118)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:80)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:656)
	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:56)
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1483)
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1129)
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1069)
	at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:764)
	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:213)
	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:200)
	at org.apache.flink.runtime.scheduler.SharedSlot.lambda$release$4(SharedSlot.java:272)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683)
	at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010)
	at org.apache.flink.runtime.scheduler.SharedSlot.release(SharedSlot.java:272)
	at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:152)
	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releasePayload(DefaultDeclarativeSlotPool.java:384)
	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlots(DefaultDeclarativeSlotPool.java:360)
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.internalReleaseTaskManager(DeclarativeSlotPoolBridge.java:241)
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.releaseTaskManager(DeclarativeSlotPoolBridge.java:224)
	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:489)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:306)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:159)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:417)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:565)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:187)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	... 12 more
{code}",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 08 10:42:12 UTC 2021,,,,,,,,,,"0|z0n0fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/21 18:16;chesnay;Oh so this issue also occurs in master; so far I only saw it in a dev branch.

As you can see in the stacktrace the restart strategy is set to {{NoRestartBackoffTimeStrategy}}, which shouldn't be the case (the test with TM failover could never pass).
I'm wondering whether this is due to the JM failover test.
Maybe the first JM uses the {{NoRestartBackoffTimeStrategy}}, but when we restart it we use a different restart strategy?;;;","08/Feb/21 10:42;trohrmann;Fixed via 

1.13.0: 920be99cfdbba7b884b41134c9188e0804a1482d
1.12.2: cbcb7c17459b57c614ecc81bef6b574f7611c61d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionFIFOSecuredITCase cannot connect to BlobServer,FLINK-21148,13354616,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,dwysakowicz,dwysakowicz,26/Jan/21 07:53,28/Aug/21 11:12,13/Jul/23 08:07,31/Mar/21 15:27,1.11.3,1.13.0,,,,,,,1.11.4,1.12.3,1.13.0,,,,,Deployment / YARN,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12483&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=ea63c80c-957f-50d1-8f67-3671c14686b9

{code}
java.io.IOException: Could not connect to BlobServer at address 29c91476178c/172.21.0.2:44412
java.io.IOException: Could not connect to BlobServer at address 29c91476178c/172.21.0.2:44412
	at org.apache.flink.runtime.blob.BlobClient.<init>(BlobClient.java:102) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.blob.BlobClient.downloadFromBlobServer(BlobClient.java:137) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:538)
	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


{code}",,dwysakowicz,maguowei,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21598,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/21 10:38;mapohl;flink-21148-testDetachedModeSecureWithPreInstallKeytab-jobmanager.log;https://issues.apache.org/jira/secure/attachment/13023111/flink-21148-testDetachedModeSecureWithPreInstallKeytab-jobmanager.log","29/Mar/21 10:38;mapohl;flink-21148-testDetachedModeSecureWithPreInstallKeytab-taskmanager.log;https://issues.apache.org/jira/secure/attachment/13023112/flink-21148-testDetachedModeSecureWithPreInstallKeytab-taskmanager.log","29/Mar/21 10:38;mapohl;flink-21148.log;https://issues.apache.org/jira/secure/attachment/13023113/flink-21148.log",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 31 15:27:16 UTC 2021,,,,,,,,,,"0|z0mzpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/21 06:43;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12890&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","10/Mar/21 13:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab;;;","18/Mar/21 03:14;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14921&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=6f8201e9-1579-595a-9d2b-7158b26b4c57&l=28913;;;","26/Mar/21 18:48;mapohl;I looked over the issue with [~rmetzger]. The actual reason seems to be that the YARN containers get [killed at the end of the test|https://github.com/apache/flink/blob/5e08e55caede0c81100d7032257133854de1155c/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNSessionFIFOITCase.java#L192]. There's a race condition between stopping the TaskManager and stopping the JobManager. If the JM is stopped first, there is a risk that the TM is trying to access the JM's BLOB server at that moment. It loses the connection and reports the connection problem. The exception ends up in the output of the TaskManager and will trigger the test failure.

The following logs showcase this based on the build reported in the Jira issues' description (application folder: {{./container_1611618440792_0002_01_000001/}}).
{code}
[...]
23:48:07,987 [   Time-limited test] INFO  org.apache.flink.yarn.YarnTestBase                           [] - Found string [switched from state RUNNING to FINISHED] in /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-0_0/application_1611618440792_0002/container_1611618440792_0002_01_000001/jobmanager.log.
23:48:07,987 [   Time-limited test] INFO  org.apache.flink.yarn.YARNSessionFIFOITCase                  [] - Two containers are running. Killing the application
23:48:07,988 [   Time-limited test] INFO  org.apache.hadoop.yarn.client.RMProxy                        [] - Connecting to ResourceManager at 29c91476178c/172.21.0.2:37502
23:48:07,991 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl [] - application_1611618440792_0002 State change from RUNNING to KILLING
23:48:07,991 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl [] - Updating application attempt appattempt_1611618440792_0002_000001 with final state: KILLED
23:48:07,991 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl [] - appattempt_1611618440792_0002_000001 State change from RUNNING to FINAL_SAVING
23:48:07,991 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService [] - Unregistering app attempt : appattempt_1611618440792_0002_000001
23:48:07,992 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl [] - appattempt_1611618440792_0002_000001 State change from FINAL_SAVING to KILLED
23:48:07,992 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl [] - Updating application application_1611618440792_0002 with final state: KILLED
23:48:07,992 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl [] - application_1611618440792_0002 State change from KILLING to FINAL_SAVING
23:48:07,992 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore [] - Storing info for app: application_1611618440792_0002
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl [] - container_1611618440792_0002_01_000001 Container Transitioned from RUNNING to KILLED
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp [] - Completed container: container_1611618440792_0002_01_000001 in state: KILLED event:KILL
23:48:07,992 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl [] - application_1611618440792_0002 State change from FINAL_SAVING to KILLED
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger  [] - USER=hadoop      OPERATION=AM Released Container TARGET=SchedulerApp     RESULT=SUCCESS  APPID=application_1611618440792_0002    CONTAINERID=container_1611618440792_0002_01_000001
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode [] - Released container container_1611618440792_0002_01_000001 of capacity <memory:1024, vCores:1> on host 29c91476178c:36323, which currently has 1 containers, <memory:1024, vCores:1> used and <memory:3072, vCores:665> avail
able, release resources=true
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler [] - Application attempt appattempt_1611618440792_0002_000001 released container container_1611618440792_0002_01_000001 on node: host: 29c91476178c:36323 #containers=1 available=3072 used=1024 with event: KILL
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl [] - container_1611618440792_0002_01_000002 Container Transitioned from RUNNING to KILLED
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp [] - Completed container: container_1611618440792_0002_01_000002 in state: KILLED event:KILL
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger  [] - USER=hadoop      OPERATION=AM Released Container TARGET=SchedulerApp     RESULT=SUCCESS  APPID=application_1611618440792_0002    CONTAINERID=container_1611618440792_0002_01_000002
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode [] - Released container container_1611618440792_0002_01_000002 of capacity <memory:1024, vCores:1> on host 29c91476178c:36323, which currently has 0 containers, <memory:0, vCores:0> used and <memory:4096, vCores:666> availabl
e, release resources=true
23:48:07,992 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler [] - Application attempt appattempt_1611618440792_0002_000001 released container container_1611618440792_0002_01_000002 on node: host: 29c91476178c:36323 #containers=0 available=4096 used=0 with event: KILL
23:48:07,993 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo [] - Application application_1611618440792_0002 requests cleared
23:48:07,993 [     pool-3-thread-4] INFO  org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher [] - Cleaning master appattempt_1611618440792_0002_000001
23:48:07,993 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger  [] - USER=hadoop        OPERATION=Application Finished - Killed TARGET=RMAppManager     RESULT=SUCCESS  APPID=application_1611618440792_0002
23:48:07,993 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary [] - appId=application_1611618440792_0002,name=MyCustomName,user=hadoop,queue=default,state=KILLED,trackingUrl=http://29c91476178c:46794/cluster/app/application_1611618440792_0002,appMasterHost=N/A,startTime=1611618467077,finishTime=16
11618487992,finalStatus=KILLED
23:48:07,996 [Socket Reader #1 for port 36323] INFO  SecurityLogger.org.apache.hadoop.ipc.Server                  [] - Auth successful for appattempt_1611618440792_0002_000001 (auth:SIMPLE)
23:48:07,998 [Socket Reader #1 for port 36323] INFO  SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager [] - Authorization successful for appattempt_1611618440792_0002_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB
23:48:08,000 [IPC Server handler 11 on 36323] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl [] - Stopping container with container Id: container_1611618440792_0002_01_000001
23:48:08,000 [IPC Server handler 11 on 36323] INFO  org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger      [] - USER=hadoop       IP=172.21.0.2   OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1611618440792_0002    CONTAINERID=container_1611618440792_0002_01_000001
23:48:08,000 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container [] - Container container_1611618440792_0002_01_000001 transitioned from RUNNING to KILLING
23:48:08,000 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch [] - Cleaning up container container_1611618440792_0002_01_000001
23:48:08,008 [ContainersLauncher #0] WARN  org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor [] - Exit code from container container_1611618440792_0002_01_000001 is : 143
23:48:08,023 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container [] - Container container_1611618440792_0002_01_000002 transitioned from RUNNING to KILLING
23:48:08,023 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application [] - Application application_1611618440792_0002 transitioned from RUNNING to FINISHING_CONTAINERS_WAIT
23:48:08,023 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container [] - Container container_1611618440792_0002_01_000001 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
23:48:08,023 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch [] - Cleaning up container container_1611618440792_0002_01_000002
23:48:08,029 [ContainersLauncher #1] WARN  org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor [] - Exit code from container container_1611618440792_0002_01_000002 is : 143
23:48:08,043 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container [] - Container container_1611618440792_0002_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
23:48:08,043 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger      [] - USER=hadoop        OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1611618440792_0002    CONTAINERID=container_1611618440792_0002_01_000001
23:48:08,043 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container [] - Container container_1611618440792_0002_01_000001 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
23:48:08,043 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application [] - Removing container_1611618440792_0002_01_000001 from application application_1611618440792_0002
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl [] - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices [] - Got event CONTAINER_STOP for appId application_1611618440792_0002
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger      [] - USER=hadoop        OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1611618440792_0002    CONTAINERID=container_1611618440792_0002_01_000002
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container [] - Container container_1611618440792_0002_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application [] - Removing container_1611618440792_0002_01_000002 from application application_1611618440792_0002
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application [] - Application application_1611618440792_0002 transitioned from FINISHING_CONTAINERS_WAIT to APPLICATION_RESOURCES_CLEANINGUP
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl [] - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices [] - Got event CONTAINER_STOP for appId application_1611618440792_0002
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices [] - Got event APPLICATION_STOP for appId application_1611618440792_0002
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application [] - Application application_1611618440792_0002 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED
23:48:08,044 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler [] - Scheduling Log Deletion for application: application_1611618440792_0002, with delay of 10800 seconds
23:48:08,192 [IPC Server handler 35 on 37502] INFO  org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger  [] - USER=hadoop       IP=172.21.0.2   OPERATION=Kill Application Request      TARGET=ClientRMService  RESULT=SUCCESS  APPID=application_1611618440792_0002
23:48:08,193 [   Time-limited test] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Killed application application_1611618440792_0002
[...]
{code}

The test code identifies the job to have [switched to FINISHED|https://github.com/apache/flink/blob/5e08e55caede0c81100d7032257133854de1155c/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNSessionFIFOITCase.java#L170] at {{23:48:07,987}}. It triggers the killing of the application contains.

{code}
[...]
2021-01-25 23:48:07,329 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom File Source (1/1) (08dfd77d53fd5c13af15596114b0eba2) switched from SCHEDULED to DEPLOYING.
2021-01-25 23:48:07,329 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Custom File Source (1/1) (attempt #0) with attempt id 08dfd77d53fd5c13af15596114b0eba2 to container_1611618440792_0002_01_000002 @ 29c91476178c (dataPort=43007) with allocation id bd438a46d7b4cd9a6b9475fbd569a340
2021-01-25 23:48:07,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Split Reader: Custom File Source -> Flat Map (1/1) (ea3611e4c721d3b510a323c5233f4795) switched from SCHEDULED to DEPLOYING.
2021-01-25 23:48:07,334 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Split Reader: Custom File Source -> Flat Map (1/1) (attempt #0) with attempt id ea3611e4c721d3b510a323c5233f4795 to container_1611618440792_0002_01_000002 @ 29c91476178c (dataPort=43007) with allocation id bd438a46d7b4cd9a6b9475fbd569a340
2021-01-25 23:48:07,335 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (3a21d01cc0bb11d6e02f18a355e19302) switched from SCHEDULED to DEPLOYING.
2021-01-25 23:48:07,335 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Keyed Aggregation -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id 3a21d01cc0bb11d6e02f18a355e19302 to container_1611618440792_0002_01_000002 @ 29c91476178c (dataPort=43007) with allocation id bd438a46d7b4cd9a6b9475fbd569a340
2021-01-25 23:48:07,492 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Split Reader: Custom File Source -> Flat Map (1/1) (ea3611e4c721d3b510a323c5233f4795) switched from DEPLOYING to RUNNING.
2021-01-25 23:48:07,493 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom File Source (1/1) (08dfd77d53fd5c13af15596114b0eba2) switched from DEPLOYING to RUNNING.
2021-01-25 23:48:07,493 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (3a21d01cc0bb11d6e02f18a355e19302) switched from DEPLOYING to RUNNING.
2021-01-25 23:48:07,750 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom File Source (1/1) (08dfd77d53fd5c13af15596114b0eba2) switched from RUNNING to FINISHED.
2021-01-25 23:48:07,788 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Split Reader: Custom File Source -> Flat Map (1/1) (ea3611e4c721d3b510a323c5233f4795) switched from RUNNING to FINISHED.
2021-01-25 23:48:07,802 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (3a21d01cc0bb11d6e02f18a355e19302) switched from RUNNING to FINISHED.
2021-01-25 23:48:07,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Streaming WordCount (5d6119527046c8e3498087511c7bbe6d) switched from state RUNNING to FINISHED.
2021-01-25 23:48:07,805 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Received resource declaration for job 5d6119527046c8e3498087511c7bbe6d: []
2021-01-25 23:48:07,805 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 5d6119527046c8e3498087511c7bbe6d.
2021-01-25 23:48:07,809 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2021-01-25 23:48:07,815 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 5d6119527046c8e3498087511c7bbe6d reached globally terminal state FINISHED.
2021-01-25 23:48:07,829 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job Streaming WordCount(5d6119527046c8e3498087511c7bbe6d).
2021-01-25 23:48:07,832 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [bd438a46d7b4cd9a6b9475fbd569a340].
2021-01-25 23:48:07,836 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Received resource declaration for job 5d6119527046c8e3498087511c7bbe6d: []
2021-01-25 23:48:07,836 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection bbf3844d540bca1b27ead1567a8c1a62: Stopping JobMaster for job Streaming WordCount(5d6119527046c8e3498087511c7bbe6d)..
2021-01-25 23:48:07,836 INFO  org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge [] - Closing slot pool.
2021-01-25 23:48:07,837 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@29c91476178c:38933/user/rpc/jobmanager_2 for job 5d6119527046c8e3498087511c7bbe6d from the resource manager.
2021-01-25 23:48:08,008 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2021-01-25 23:48:08,012 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:38379
[...]
{code}
The JobManager's logs (see above) show that the job finished successfully ({{2021-01-25 23:48:07,80}}). That (or probably even the two log messages before) triggered the killing [in the test code|https://github.com/apache/flink/blob/5e08e55caede0c81100d7032257133854de1155c/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNSessionFIFOITCase.java#L170].
The JobManager received the {{SIGTERM}} at {{23:48:08,008}}. The process got killed.

In the meantime, the TaskManager (see below) tries to access the BLOB server but fails and prints the {{IOException}}. That exception causes the test to fail.
{code}
[...]
2021-01-25 23:47:46,296 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Custom File Source (1/1)#0 (dd105d5cece2a74fcbbcc4ea8d534da8), deploy into slot with allocation id ed45ec1ca967b5cba43f39cf8b316d31.
2021-01-25 23:47:46,302 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom File Source (1/1)#0 (dd105d5cece2a74fcbbcc4ea8d534da8) switched from CREATED to DEPLOYING.
2021-01-25 23:47:46,331 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Custom File Source (1/1)#0 (dd105d5cece2a74fcbbcc4ea8d534da8) [DEPLOYING].
2021-01-25 23:47:46,417 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412
2021-01-25 23:47:46,418 ERROR org.apache.flink.runtime.blob.BlobClient                     [] - Failed to fetch BLOB cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 and store it under /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-localDir-nm-0_0/usercache/hadoop/appcache/application_1611618440792_0001/blobStore-63d93518-0608-430b-b65b-01042e1c8ddb/incoming/temp-00000000 Retrying...
2021-01-25 23:47:46,418 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 (retry 1)
2021-01-25 23:47:46,418 ERROR org.apache.flink.runtime.blob.BlobClient                     [] - Failed to fetch BLOB cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 and store it under /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-localDir-nm-0_0/usercache/hadoop/appcache/application_1611618440792_0001/blobStore-63d93518-0608-430b-b65b-01042e1c8ddb/incoming/temp-00000000 Retrying...
2021-01-25 23:47:46,419 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 (retry 2)
2021-01-25 23:47:46,419 ERROR org.apache.flink.runtime.blob.BlobClient                     [] - Failed to fetch BLOB cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 and store it under /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-localDir-nm-0_0/usercache/hadoop/appcache/application_1611618440792_0001/blobStore-63d93518-0608-430b-b65b-01042e1c8ddb/incoming/temp-00000000 Retrying...
2021-01-25 23:47:46,419 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 (retry 3)
2021-01-25 23:47:46,419 ERROR org.apache.flink.runtime.blob.BlobClient                     [] - Failed to fetch BLOB cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 and store it under /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-localDir-nm-0_0/usercache/hadoop/appcache/application_1611618440792_0001/blobStore-63d93518-0608-430b-b65b-01042e1c8ddb/incoming/temp-00000000 Retrying...
2021-01-25 23:47:46,420 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 (retry 4)
2021-01-25 23:47:46,420 ERROR org.apache.flink.runtime.blob.BlobClient                     [] - Failed to fetch BLOB cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 and store it under /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-localDir-nm-0_0/usercache/hadoop/appcache/application_1611618440792_0001/blobStore-63d93518-0608-430b-b65b-01042e1c8ddb/incoming/temp-00000000 Retrying...
2021-01-25 23:47:46,420 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 (retry 5)
2021-01-25 23:47:46,425 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ed45ec1ca967b5cba43f39cf8b316d31.
2021-01-25 23:47:46,444 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@29c91476178c:39073] has failed, address is now gated for [50] ms. Reason: [Disassociated]
2021-01-25 23:47:46,456 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                 [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2021-01-25 23:47:46,480 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2021-01-25 23:47:46,420 ERROR org.apache.flink.runtime.blob.BlobClient                     [] - Failed to fetch BLOB cb6e32b193e012291c98ab3c336ff7d3/p-aef31baf37b64c9891057b48b089308791bd78e0-8235b727f34796014af8e0c5df700b56 from 29c91476178c/172.21.0.2:44412 and store it under /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-localDir-nm-0_0/usercache/hadoop/appcache/application_1611618440792_0001/blobStore-63d93518-0608-430b-b65b-01042e1c8ddb/incoming/temp-00000000 No retries left.
java.io.IOException: Could not connect to BlobServer at address 29c91476178c/172.21.0.2:44412
        at org.apache.flink.runtime.blob.BlobClient.<init>(BlobClient.java:102) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.blob.BlobClient.downloadFromBlobServer(BlobClient.java:137) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.blob.AbstractBlobCache.getFileInternal(AbstractBlobCache.java:166) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.blob.PermanentBlobCache.getFile(PermanentBlobCache.java:187) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$LibraryCacheEntry.createUserCodeClassLoader(BlobLibraryCacheManager.java:251) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$LibraryCacheEntry.getOrResolveClassLoader(BlobLibraryCacheManager.java:228) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$LibraryCacheEntry.access$1100(BlobLibraryCacheManager.java:199) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$DefaultClassLoaderLease.getOrResolveClassLoader(BlobLibraryCacheManager.java:333) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.createUserCodeClassloader(Task.java:994) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:565) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_275]
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_275]
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_275]
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_275]
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_275]
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_275]
        at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_275]
        at org.apache.flink.runtime.blob.BlobClient.<init>(BlobClient.java:96) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        ... 11 more
2021-01-25 23:47:46,585 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-localDir-nm-0_0/usercache/hadoop/appcache/application_1611618440792_0001/flink-netty-shuffle-84c8d5e6-6022-4af8-be0b-9f7b6a5f5b
1a
[...]
{code}

The only strange thing is that the timetamp is not aligned. The TM receives the {{SIGTERM}} at {{23:47:46,456}}, ~22s earlier than the JobManager and even earlier than the test which is triggering the killing. It looks like there are two test running on the same application nodes. There are also four subfolders {{container_1611618440792_000*_01_000002/}} containg 2 {{taskmanager.log}} and 2 {{jobmanager.log}}. Could it be that two Yarn tests are running in parallel and one killing triggered the killing of the other one? The second {{taskmanager.log}} also received a {{SIGTERM}} but at {{23:48:08,029}} which would match our timestamps of the {{jobmanager.log}} referred to above.;;;","29/Mar/21 11:20;mapohl;I re-iterated over the problem: The actual test failing (in the build referred to in the Jira issues description) is {{YARNSessionFIFOSecuredITCase.testDetachedModeSecureWithPreInstallKeytab}}. The test run takes too long which triggers the timeout of 10 seconds resulting in killing the corresponding YARN containers. The JM gets killed first ({{2021-01-25 23:47:46,001}}). The TM tries to connect to the {{BlobServer}} ({{2021-01-25 23:47:46,417}}) in the meantime resulting into the {{java.io.IOException: Could not connect to BlobServer at address}} after a few retries. The exceptions trigger the test failure in the end. I added the relevant log files to the issue.

My previous findings having two containers (i.e. {{container_1611618440792_0001}} and {{container_1611618440792_0002}}) came due to the fact that {{YARNSessionFIFOSecuredITCase}} runs multiple tests. The next test was already triggered while cleaning up the failed application.

I attached the extracted logs for the [failed build from the Jira issue's description|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12483&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=ea63c80c-957f-50d1-8f67-3671c14686b9] to make the verification of my findings easier.

I made the test fail early when reaching the timeout. Additionally, I increased the timeout to fix the actual problem.;;;","31/Mar/21 15:27;chesnay;master:
ae2b376efebd70b68d63e56003c9c1f15afd2702
a39dfe94ad2a5b6418590e088790fd05dd21fb03
1828c6be560ea1e5c0d3aaf44e90b1bd1435f90f
9439c70a78d0f85779a16eb9ca67e02c04c61880;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesResourceManagerDriver#tryResetPodCreationCoolDown causes fatal error,FLINK-21144,13354602,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,wangyang0918,wangyang0918,26/Jan/21 07:10,28/Jan/21 10:19,13/Jul/23 08:07,28/Jan/21 10:19,1.12.1,,,,,,,,1.12.2,1.13.0,,,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"{{KubernetesResourceManagerDriver#tryResetPodCreationCoolDown}} is calling a not implemented method {{RpcEndpoint.MainThreadExecutor#schedule(Callable<V> callable, long delay, TimeUnit unit)}}. This will cause a fatal error and make JobManager terminate exceptionally.",,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18720,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 28 10:19:22 UTC 2021,,,,,,,,,,"0|z0mzmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/21 07:11;wangyang0918;cc [~xintongsong];;;","26/Jan/21 07:27;wangyang0918;Since {{KubernetesResourceManagerDriver#tryResetPodCreationCoolDown}} has already been replaced with {{ThresholdMeter}} in FLINK-10868. Maybe we need to backport it to 1.12 or add a quick fix just for 1.12 branch.;;;","26/Jan/21 08:56;xtsong;Nice catch!

This escaped our tests because we use a fully implemented executor in the tests.

I think the proper fix might be to implement the missing methods in `MainThreadExecutor`. Per the error message, there're no strong reason against implementing those methods, except for there were not used. And we might also want to do that for the master branch, to avoid similar problems in future.;;;","26/Jan/21 11:37;wangyang0918;Great. +1 to implement these methods to avoid similar issues in the future.;;;","27/Jan/21 06:28;xtsong;After taking a closer look, I found it not trivial to support scheduling periodical executions in `MainThreadExecutor`.

Scheduling periodical executions can be supported through `RpcService#getScheduledExecutor()`. However, exposing the `ScheduledExecutor` to the `RpcEndpoint#MainThreadExecutor` requires invasive changes to the current encapsulation.

For now, I think we may only implement `MainThreadExecutor#schedule(callable, delay, unit)`, which should be enough to fix the `tryResetPodCreationCoolDown` problem.

I've opened a PR. Please take a look. [~fly_in_gis];;;","28/Jan/21 10:19;xtsong;Fixed via
* master (1.13): db27d6da4a9f5436711d1ff7613a3dd70e4be12d
* release-1.12: 79f029c90f8d3109b0d8f6852eae627bc7c25989;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink guava Dependence problem,FLINK-21142,13354577,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,twalthr,YUJIANBO,YUJIANBO,26/Jan/21 05:14,30/Sep/21 14:56,13/Jul/23 08:07,30/Sep/21 14:56,1.12.0,,,,,,,,1.15.0,,,,,,,Connectors / Hadoop Compatibility,Connectors / Hive,,,,0,auto-unassigned,pull-request-available,,,,"We set up a new Hadoop cluster, and we use the flink1.12.0 compiled by the previous release-1.12.0 branch.If I add hive jar to flink/lib/, it will report errors.

*Operating environment：*
     flink1.12.0 
     Hadoop 3.3.0
     hive 3.1.2

*Flink run official demo shell： /tmp/yjb/buildjar/flink1.12.0/bin/flink run -m yarn-cluster /usr/local/flink1.12.0/examples/streaming/WordCount.jar*

If I put one of the jar *flink-sql-connector-hive-3.1.2_2.11-1.12.0.jar* or *hive-exec-3.1.2.jar* in the Lib directory and execute the above shell, an error will be reported  java.lang.NoSuchMethodError : com.google.common . base.Preconditions.checkArgument (ZLjava/lang/String;Ljava/lang/Object;)V. *We can see that it's the dependency conflict of guava.*

*My cluster guava‘s version:*
 /usr/local/hadoop-3.3.0/share/hadoop/yarn/csi/lib/guava-20.0.jar
 /usr/local/hadoop-3.3.0/share/hadoop/common/lib/guava-27.0-jre.jar
 /usr/local/apache-hive-3.1.2-bin/lib/guava-20.0.jar
 /usr/local/apache-hive-3.1.2-bin/lib/jersey-guava-2.25.1.jar
 /usr/local/spark-3.0.1-bin-hadoop3.2/jars/guava-14.0.1.jar

*Can you give me some advice？*
 Thank you！",,jingzhang,lirui,twalthr,YUJIANBO,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 30 14:55:12 UTC 2021,,,,,,,,,,"0|z0mzgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/21 05:19;YUJIANBO;[~jark] [~lirui] Hello！Can you give me some advice?;;;","26/Jan/21 06:28;lirui;Hi [~YUJIANBO], the same issue has been reported in the [mailing list|http://apache-flink.147419.n8.nabble.com/Caused-by-java-lang-NoSuchMethodError-com-google-common-base-Preconditions-checkArgument-ZLjava-langV-td10474.html], and you can check out my reply there.
The root cause is your hadoop version is incompatible with your hive version. Hive-3.1.2 depends on hadoop-3.1.0, so choosing a hadoop version <=3.1.0 would be safer.;;;","26/Jan/21 06:55;YUJIANBO;[~lirui]That's also my question. I didn't notice your letter. Thank you very much. 
I have quesiton, I will try first method to rebuild hive-exe, but my cluster guava' version next. Should I replace all guava dependencies with 27.0-jre's version？

/usr/local/hadoop-3.3.0/share/hadoop/yarn/csi/lib/guava-20.0.jar
/usr/local/hadoop-3.3.0/share/hadoop/common/lib/guava-27.0-jre.jar
/usr/local/apache-hive-3.1.2-bin/lib/guava-20.0.jar
/usr/local/apache-hive-3.1.2-bin/lib/jersey-guava-2.25.1.jar
/usr/local/spark-3.0.1-bin-hadoop3.2/jars/guava-14.0.1.jar

;;;","26/Jan/21 08:12;lirui;[~YUJIANBO] That depends on how the guava jars are used in your job. The principle is to avoid different versions of the same guava class in the class path. So you don't need to replace if it's not added to class path.;;;","27/May/21 13:05;twalthr;I have created a PR for improving this issue. Please let me know what you think there.;;;","10/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","18/Jun/21 22:39;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","30/Sep/21 14:55;twalthr;Fixed in master: 79e4c4cd98d4d4d2033462b931b83e5dacc806b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extract zip file dependencies before adding to PYTHONPATH,FLINK-21140,13354568,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,26/Jan/21 03:13,28/May/21 08:07,13/Jul/23 08:07,27/Jan/21 07:00,,,,,,,,,1.12.2,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,Not all zip files are importable in Python and so we should expand zip file dependencies and add the root directory to PYTHONPATH.,,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 07:00:57 UTC 2021,,,,,,,,,,"0|z0mzew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/21 07:00;dian.fu;Fixed in master via a49ad17f3c2dfdff04bbdba6fcc7015c9f2d6fad and release-1.12 via fa5619c5a0d9589d990dee8a2e35c39ae06d0d35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThresholdMeterTest.testMarkMultipleEvents unstable,FLINK-21139,13354550,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ZhenqiuHuang,oceanfish81,oceanfish81,25/Jan/21 23:47,28/May/21 08:16,13/Jul/23 08:07,02/Feb/21 01:50,,,,,,,,,1.13.0,,,,,,,API / Core,Benchmarks,Build System,Tests,,0,pull-request-available,test-stability,,,,"The test {{ThresholdMeterTest.testMarkMultipleEvents}} seems to be unstable.

The version of Flink is

??$ git log -1??
??commit 6e77cfdff8a358adab4ab770a503197d95a64440 (HEAD -> master, origin/master, origin/HEAD)??
??Author: Roman Khachatryan <khachatryan.roman@gmail.com>??
??Date: Fri Jan 22 14:39:48 2021 +0100??

The test fails with the following exception:

{code}
Expected: a numeric value within <1.0E-6> of <40.0>
     but: <0.0> differed by <39.999999>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.flink.metrics.ThresholdMeterTest.testMarkMultipleEvents(ThresholdMeterTest.java:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}","$ java -version && javac -version
openjdk version ""11.0.8"" 2020-07-14
OpenJDK Runtime Environment AdoptOpenJDK (build 11.0.8+10-202007292333)
Eclipse OpenJ9 VM AdoptOpenJDK (build master-dc7cbe4bd, JRE 11 Linux riscv-64-Bit Compressed References 20200729_78 (JIT disabled, AOT disabled)
OpenJ9 - dc7cbe4bd
OMR - 1c0299f20
JCL - 94b9d6d2c6 based on jdk-11.0.8+10)
javac 11.0.8
$ uname -a
Linux test-gdams-debian10-riscv64-1.adoptopenjdk.net 5.0.0-rc1-56210-g0a657e0d72f0 #1 SMP Fri May 15 18:05:26 EDT 2020 riscv64 GNU/Linux
$ cat /proc/cpuinfo
processor : 0
hart : 1
isa : rv64imafdc
mmu : sv39
uarch : sifive,rocket0",oceanfish81,trohrmann,xtsong,ZhenqiuHuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10868,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 01:50:12 UTC 2021,,,,,,,,,,"0|z0mzaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/21 13:14;trohrmann;Thanks for reporting this issue [~oceanfish81]. The unstable test has been introduced with FLINK-10868. [~xintongsong] and [~hpeter] could you take a look at this problem?;;;","26/Jan/21 13:19;trohrmann;Looking at the code of {{ThresholdMeter}} it looks that we are using {{System.currentTimeMillis}}. I think this is not optimal because {{System.currentTimeMillis}} can be subject to clock jumps. I think it would be better to use {{Clock}} here.;;;","26/Jan/21 18:09;ZhenqiuHuang;[~trohrmann] 
Sure. Please assign it to me to fix it today.;;;","27/Jan/21 02:14;xtsong;Thanks [~oceanfish81] for reporting this and [~trohrmann] for looking into this.

+1 for improve the testability by injecting a {{Clock}}.

Thanks [~ZhenqiuHuang] for offering to fix this. I've assigned you, please go ahead.;;;","31/Jan/21 05:00;ZhenqiuHuang;[~trohrmann] [~xintongsong]
The ManualClock is defined in flink-core. Currently, ThresholdMeter is defined in flink-metrics. I feel it is not wise to make flink-metrics dependent on flink-core module. Any suggestions about this?;;;","31/Jan/21 06:32;xtsong;[~ZhenqiuHuang],
Maybe we can move {{ThreasholdMeter}} to {{org.apache.flink.runtime.metrics}}. There's already {{TimerGauge}}, which also uses {{Clock}}.;;;","31/Jan/21 19:32;ZhenqiuHuang;[~xintongsong]
Make sense. Just created the PR. Please review it when you have time.;;;","02/Feb/21 01:50;xtsong;Fixed via
* master (1.13): b9e576fb845b817d804da3d68471ff8a4723dcf3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KvStateServerHandler is not invoked with user code classloader,FLINK-21138,13354526,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mproch,mproch,mproch,25/Jan/21 20:43,11/Feb/21 18:35,13/Jul/23 08:07,11/Feb/21 17:50,1.11.2,,,,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Queryable State,,,,,0,pull-request-available,,,,,"When using e.g. custom Kryo serializers user code classloader has to be set as context classloader during invocation of methods such as TypeSerializer.duplicat()

KvStateServerHandler does not do this, which leads to exceptions like ClassNotFound etc.",,mproch,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/21 20:16;mproch;TestJob.java;https://issues.apache.org/jira/secure/attachment/13019440/TestJob.java","26/Jan/21 20:21;mproch;stacktrace;https://issues.apache.org/jira/secure/attachment/13019441/stacktrace",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 11 17:50:53 UTC 2021,,,,,,,,,,"0|z0mz5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/21 13:09;trohrmann;Thanks for reporting this issue [~mproch]. Could you give a short example illustrating the problem? I do not fully understand the problem yet. Maybe you have the problematic job already in some repository.;;;","26/Jan/21 20:23;mproch;Hi [~trohrmann], thanks for quick response. 
I attach smallest case I could come up with, and stack trace that is thrown.

I think the problem is relatively straightforward and user classloader just needs to be passed down to KvStateEntry. I think I can prepare a PR, but I'm not quite sure what kind of test would be suitable - as the problem involves custom classloaders etc.

 ;;;","27/Jan/21 17:24;trohrmann;Thanks for posting this example. I am wondering whether the problem isn't that we are using the {{Thread.currentThread().getContextClassLoader()}} [here|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/kryo/KryoSerializer.java#L626] instead of {{original.getClass().getClassLoader()}}. I think this should fix the problem.

Test-wise you could take a look at the {{PojoSerializerUpgradeTest.testPojoSerializerUpgrade}} which creates a new class and instantiates an {{URLClassLoader}} to load it. If you then use the test thread, it should not know about this new class.;;;","28/Jan/21 06:55;mproch;I think using original.getClass().getClassLoader() won't work here - ""original"" is SerializableSerializer wrapper, which is probably loaded by framework classloader. I think with such custom serializers the part loaded by user classloader can hidden via quite a few wrappers/decorators and using contextClassloader is safer, it's done like that also here: [here|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/JavaSerializer.java#L59]

I'll try to come up with PR with test;;;","28/Jan/21 14:37;trohrmann;I think you are right [~mproch]. The problem with context classloader is that it is not very explicit and can easily break (e.g. if a method further downstream sets a different context classloader). The clean solution would probably be to pass in an explicit classloader. But this would be super involved. Hence, I am ok with your proposal. I've assigned you to the ticket.;;;","08/Feb/21 16:59;mproch;I only had time this weekend to look at this: [https://github.com/apache/flink/pull/14902] - here is the change, I added integration test to verify the fix works;;;","11/Feb/21 17:50;trohrmann;Fixed via 

1.13.0:

8d9ffcdb2d05f1d9931a4ed4bb7fd9de6e770551
1e6d1d24ed03eb6c7a5fb1d67fea47cf69552e6c
9c04e64be9eb12d07bc4c97572c658fc6ddca97d
aeeb6171adcd7e305e8c0c64a13ade64170fa799

1.12.2: 

86bdfff24112fcf2793edb82f27ea5a5c248cc5a
98da61b8e6dbd54b09da864e9f7af33eba9e0c40
dad7f57e61d53c076ea9b69a61a71d9ac6edf094
6cc60de8cd5415ff40d3eae6467c756a54506973

1.11.4:

d69d134590f46fea74ac1f7c664435393625533f
5f6a804d11bcdf930a6a44ea48dd7e8f663954a3
95281a7313d107e60a8103db3104a19cda5f9ece
30351d6a3d3f1a9949bf5f01e63293bf4a657424 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-27 Source does not work with synchronous savepoint,FLINK-21133,13354470,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,kezhuw,kezhuw,25/Jan/21 15:25,28/May/21 08:58,13/Jul/23 08:07,19/Apr/21 14:38,1.11.3,1.12.1,,,,,,,1.12.3,1.13.0,,,,,,API / Core,API / DataStream,Runtime / Checkpointing,,,0,pull-request-available,stale-assigned,,,,"I have pushed branch [synchronous-savepoint-conflict-with-bounded-end-input-case|https://github.com/kezhuw/flink/commits/synchronous-savepoint-conflict-with-bounded-end-input-case] in my repository. {{SavepointITCase.testStopSavepointWithFlip27Source}} failed due to timeout.

See also FLINK-21132 and [apache/iceberg#2033|https://github.com/apache/iceberg/issues/2033]..",,aljoscha,becket_qin,gaoyunhaii,kezhuw,mapohl,Ming Li,nkruber,pnowojski,sewen,sjwiesman,trohrmann,ym,,,,,,,,,,,,,,,,FLINK-21469,,,,,,,,,,,,,,,,,,,,FLINK-10740,FLINK-21453,,,,,,,,,,,,,,,FLINK-21323,FLINK-21028,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 19 14:38:41 UTC 2021,,,,,,,,,,"0|z0myt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/21 07:19;mapohl;Findings mentioned in duplicate FLINK-21323:

{quote}
When looking into FLINK-21030 analyzing the stop-with-savepoint behavior we implemented different test jobs covering the old addSource and new fromSource methods for adding sources. The stop-with-savepoint consists of two phase:

Create the savepoint
Stop the source to trigger finalizing the job
The test failing in the second phase using fromSource does not succeed. The reason for this might be that finishTask is not implemented by SourceOperatorStreamTask in contrast to SourceStreamTask which is used when calling addSource in the job definition. Hence, the job termination is never triggered.

We might have missed this due to some naming error of JobMasterStopWithSavepointIT test that is not triggered by Maven due to the wrong suffix used in this case. The IT is failing right now. FLINK-21031 is covering the fix of JobMasterStopWithSavepointIT already.
{quote};;;","09/Feb/21 10:25;trohrmann;fyi [~jqin], [~aljoscha] ;;;","09/Feb/21 14:58;mapohl;The error I described above was caused by not enabling Checkpoints. The test succeeded as expected when Checkpointing was enabled. I'm gonna try to come up with a shorter testcase describing the behavior.;;;","10/Feb/21 05:25;becket_qin;[~mapohl] [~kezhuw], thanks for digging into the issue. The root cause you found sounds correct to me. A quick fix would be implementing {{finishTask()}} in the {{SourceOperatorStreamTask}}. 

That said, the semantic of {{finishTask()}} seems not well defined. It would be good if we can first agree on the behavior before we fix the issue. The current semantic of {{finishTask()}} is following.
 * For non-source tasks, this method is essentially optional and can be regarded as a ""hook"". And in fact it is even not guaranteed to be invoked if I understand correctly - because the checkpoint callback may come after the task has received the {{EndOfInput}} marker from upstream tasks. At that point, the task might have exited.
 * For source tasks, this method must be implemented in order to let the task stop when synchronous savepoint is taken. Supposedly a source task should return an {{EndOfInput}} marker from the source to the downstream so all the tasks will exit. I am not sure if this {{EndOfInput}} marker should be different from an actual {{EndOfInput}} marker as an operator may want to behave differently if the stream has not actually ended. But this is a separate issue.

Assuming we keep the above semantic unchanged for now. For the quick fix, we may need to do the following:
 # Add a {{stop()}} method to the SourceOperator. Once it is called, the SourceOperator#emitNext() method will just return {{END_OF_INPUT}}. 
 # Implement {{finishTask()}} method in the {{SourceOperatorStreamTask}} to stop the {{SourceOperator.}}

CC [~sewen];;;","10/Feb/21 06:30;kezhuw;Hi all, there are actually two cases for FLIP-27 sources not working with stop-with-savepoint:
 # {{SourceOperatorStreamTask}} does not finish itself on {{finishTask}}.
 # {{MultipleInputStreamTask}} does not finish itself if it is the heading task that triggering checkpoint.

Fix to first case is simple. Fix to second case may require extra works, such as moving {{finishTask}} to path {{triggerCheckpointAsync}}, I am unsure of this.

I pushed another [branch with two test cases|https://github.com/kezhuw/flink/commit/3578b67d6d880d7981bc8b3c0d425eb9f57caaf7], {{testStopSavepointWithFlip27SourceChainedOut}} still fail after {{SourceOperatorStreamTask.finishTask}} introduced.

I am wonder whether we could separate the two so the first could be fixed without blocking on the second. The second need extra attention from stop-with-savepoint path.

I could provide a fix for first case if there is no one working on it yet.

[~trohrmann] [~pnowojski] [~jqin], [~aljoscha] ;;;","10/Feb/21 06:58;kezhuw;[~becket_qin] Sorry for not aware of your comment, I am writing test cases. In FLINK-21132, we agreed that {{endInput}} should not be called upon stop-with-savepoint, so first step of your recommended changes is not needed now.;;;","10/Feb/21 13:01;sewen;It looks to me like ""stop with savepoint"" it itself incompletely implemented. The current implementation works by simply ""breaking the legacy source thread out of its loop"". That's why it only works for those specific sources.

I think what stop-with-savepoint should really do is tell the mailbox to take a savepoint and then exit (as if input was empty).
That is the definition of stopping and should cover simultaneously chained and unchained sources, without any other changes.
""Stop()"" is like a graceful ""cancel()"" where the main difference are:
  - not interrupting threads, because don't need to be ""as fast as possible""
  - calling close() on the operators for graceful shutdown and finalization.

If we add a ""stop()"" method to the sources, we are solving this in the wrong place, in my opinion. The sources are designed such that the calling thread (mailbox) can decide when to read and when to stop. The sources only give indications about availability to help the thread make that decision. A ""stop()"" command should go to the thread (mailbox) to tell it to stop its mailbox loop.

That way, there is also a natural distinction between the case where the source reaches the end of the stream (and we needs to emit a MAX_WATERMARK, as implemented in the source operator) and the case where we just stop reading (""stop()""). Otherwise we need weird bookkeeping to understand whether, once the source tells us END_OF_INPUT we should advance the watermark or not.



;;;","10/Feb/21 13:30;sewen;I think that the ""stop()"" method should be on {{StreamTask}} and that way it should work for all setups equally.;;;","10/Feb/21 16:40;kezhuw;{quote}I think what stop-with-savepoint should really do is tell the mailbox to take a savepoint and then exit (as if input was empty).
{quote}
Hi [~sewen], I agree and second this point.

Currently, stop-with-savepoint is implemented through propagation of {{EndOfPartitionEvent}}, it indeed need bookkeeping to avoid {{endInput}} in downstream not only source.

I actually pushed a [preview working|https://github.com/kezhuw/flink/commit/d6c6837cd724913716007265f03a50098bee985e] in discussion of FLINK-21132. It works for all stream tasks but more like an rework but not bug fix. The key change is that it stops tasks independently. The consequence is that {{StreamOperator.close}} will not be called as flushing/sending operation probably will fail.

If {{StreamOperator.close}} is required in api side as guarantee to stop-with-savepoint, I think we finally will resort to another shape of data flow event, say {{StopOfPartitionEvent}}, to avoid bookkeeping.;;;","12/Feb/21 11:49;sewen;I see, the remaining problem is that exiting the mailbox loop in case of the source ending should call {{finishInput()}}, while exiting from {{stop()}} should not.

Digging through the code is that there seems to be a lot of confusion and mixup between cancelling, stopping, reaching end of the stream, reaching the end of the input. For example, the fact that we need to pass a flag {{""isStoppingBySyncSavepoint""}} to the {{""close()""}} method points to a bad abstraction there.

I think it makes sense to revisit this and clean it up. Similar like [~kezhuw] suggested, having an explicit {{EndOfData}} event that would trigger {{finishInput()}} woudl make this cleaner.

In that case
  - stop() would simply exit the mailbox loop, task closes/cleans up, and it send EndOfPartition downstream. The downstream task, once all input channels have seen the EndOfPartition, exits the mailbox in the same way
  - reaching the end of a source, or using ""stop()"" with the additional flag to drain the pipeline, would call ""finishInput()"" and enqueue an EndOfData event before exiting the loop. Downstream tasks call ""finishInput()"" when they saw EndOfData from all their input channels.;;;","16/Feb/21 09:51;sewen;Alternatively, we could also have one EndOfPartition event, with a flag to indicate whether this signals ""end of data"". But we still need to handle the cases where only a subsets of input signal ""end of data"" and some inputs only signal ""end of partition"".;;;","16/Feb/21 14:00;kezhuw;{quote}I think what stop-with-savepoint should really do is tell the mailbox to take a savepoint and then exit (as if input was empty).
{quote}
Based on this, I would say that some of our assumptions and/or current implementations are unnecessary or even harmful for future improvements:
 * not interrupting threads, because don't need to be ""as fast as possible"". I agree with this and I think this could be naming guarantee comparing to ""cancel --savepoint"".
 * calling close() on the operators for graceful shutdown and finalization. I think this actually enforce ""semantics guarantee"" to ""stop-with-savepoint"" and refactor bar for future improvements. After successful savepoint, it is nosense to pretend ""all records have been added to the operators"" and ""flush all remaining buffered data"". Just ""cleanup resources"" and exit is enough. Calling close() also hurt implementation as we have to guarantee ""flush all remaining buffered data"" to be success.
 * Another data flow event(or flag of EndOfPartition). Assumed that ""close"" is required, I think it is viable to replace ""isStoppingBySyncSavepoint"", but also I think it is even more complicated than ""isStoppingBySyncSavepoint"" as it requires supports from more than one stack: network channels(eg. InputGates), stream tasks(eg. StreamTaskNetworkInput, StreamTask).

Given all above, I would say ""stop-with-savepoint"" as an utility method to ""take a savepoint and then exit"" does not deserve any extra semantics guarantees from streaming data flow comparing to ""cancel --savepoint"".

If we could reach above agreement, then I think the refactor is pretty simple, just ""take a savepoint, cleanup resources, then exit"" in single task. No data flow events will involving in. More importantly, we could clearly document that ""close"" will only be ""called after all records have been added to the operators"" to fix api semantics exception.;;;","16/Feb/21 14:03;pnowojski;As I have mentioned in the FLINK-21132, in principle I also think replacing {{isStoppingBySyncSavepoint}} with an event is a cleaner solution on the conceptual level.

One extra complexity is a [closing handshake|https://github.com/apache/flink/pull/14831#issuecomment-774642652] that we are probably going to introduce as a part of FLINK-21086. Also it will mean we will need to support unaligned checkpoint overtaking current {{EndOfPartitionEvent}}. Overtaking means also persisting as part of the checkpoint and later potentially restoring. 

From this perspective I would lean more towards adding a boolean flag to the {{EndOfPartitionEvent}}, as opposed to adding new {{EndOfData}} event. 

However the more I'm thinking about, the less convinced I'm if the added complexity of the {{EndOfData}} (or enriching {{EndOfPartitionEvent}} with a flag) is worth it. It looks to me like we would need to complicate quite a lot of the code in the network stack, to pass this {{EndOfData}} to the {{StreamTask}}.

# As [~sewen] you mentioned, we would need to track how many inputs received {{EndOfPartitionEvent}} and/or {{EndOfData}}.
# {{InputGate#isFinished}} would have to be replaced with something more sophisticated (it's used in quite a bit of places).
# The same applies to the return status of {{StreamTaskNetworkInput#emitNext}} method. Currently it re-uses public facing enum {{InputStatus}}. That would have to be changed, enriched as well and would probably affect other places as well ({{SortingDataInput}}? {{MultiInputSortingDataInput}}?).
# Finally we would reach {{StreamOneInputProcessor#processInput}} where we could ignore this end of input...
# ... but that would still not be enough. We would probably still need to set a flag in the {{OperatorChain}} (basically a copy of the {{isStoppingBySyncSavepoint}}), to pass the distinction between end of input and end of partition to the {{OperatorChain}}... This is because end of the {{OperatorChain}}/{{StreamTask}} inputs and the defacto {{endOfInput}} calls on the operators are quite far apart.
# ... {{OperatorChain}} would need to handle the case, if one of its input has received {{EndOfPartition}} and the other {{EndOfData}}, so we would again need to duplicate the logic from 0.

So we would add a lot of code (0., 1., 2., 3., 4., 6.) and we would end up with the same basic solution (5.)?

Or am I missing something?

CC [~roman_khachatryan];;;","16/Feb/21 14:11;pnowojski;{quote}
Given all above, I would say ""stop-with-savepoint"" as an utility method to ""take a savepoint and then exit"" does not deserve any extra semantics guarantees from streaming data flow comparing to ""cancel --savepoint"".

If we could reach above agreement, then I think the refactor is pretty simple, just ""take a savepoint, cleanup resources, then exit"" in single task. No data flow events will involving in. More importantly, we could clearly document that ""close"" will only be ""called after all records have been added to the operators"" to fix api semantics exception.
{quote}
[~kezhuw] as I tried to mention a couple of time in the FLINK-21132. Simply ""take a savepoint, cleanup resources, then exit"" in a single task, disregarding what the upstream task is doing, can easily lead to a deadlock, if upstream task thread is blocked/fully backpressured. We could probably ignore this problem in network tasks and FLIP-27 sources, by just assuming that if downstream task received an aligned savepoint barrier, upstream task has completely paused production of records, so it's not backpressured. But this assumption doesn't hold with the legacy sources. Maybe it would also cause problems with iterations/cyclic graphs in the future.

And I would be afraid that relaying on such kind of assumption might be fragile. Orderly shutdown from sources to the sinks is from this perspective safer.

However I admit, that such approach has good advantages that you mentioned (not having to flush buffered records, faster closing etc).;;;","16/Feb/21 16:18;kezhuw;{quote}But this assumption doesn't hold with the legacy sources. Maybe it would also cause problems with iterations/cyclic graphs in the future
{quote}
Hi [~pnowojski], I commented about [legacy source|https://issues.apache.org/jira/browse/FLINK-21132?focusedCommentId=17276558&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17276558] in [FLINK-21132|https://issues.apache.org/jira/browse/FLINK-21132?focusedCommentId=17276558&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17276558]. Legacy source always requires some special treatment in implementation. I have not evaluate iterations/cyclic graphs. I have assumption/expectation in implementation that when one task hit ""notifyCheckpointComplete"" in ""stop-with-savepoint"" path, all tasks will hit that point sooner or later, all we have to decide is how to exit(cooperative or independent). Before ""notifyCheckpointComplete"", code-path for all approaches could be same.

I am no trying to cling to one approach or even my approach, I wanted/want to finger out best approach by evaluating/comparing all emerging approaches.
 # {{EndOfPartition}}: It works but not that elegant, and requires extra works from sources(chained or not).
 # {{EndOfData}}: I have raised same concerns as you.
 # Stop task individually: downstream task could be terminated before upstream task. Could cause errors in network stack and potential deadlock.

I will also evaluate FLINK-21086 for further information.

Besides my proposed approach for stop-with-savepoint, I also want to bring another separated question to attentions:

Whether {{StreamOperator.close}} should be called *only* ""after all records have been added to the operators"" ?

Currently, stop-with-savepoint is an exception for {{StreamOperator.close}}. This may be also related to [~pnowojski]'s recently 2pc concerns on FLIP-147 dicussion as {{StreamOperator.close}} is just {{flush-after-all-input-consumed}} from javadoc after FLINK-2647.;;;","16/Feb/21 17:31;sewen;The complexity of adding a new {{EndOfData}} event, versus a flag to the the {{EndOfPartition}} event seem to be pretty similar, because the complexity lies in the book-keeping and how to handle the information.

What I really like is the idea to change the definitions slightly so that we do not have to distinguish these cases at all, and that {{EndOfPartition}} and {{EndOfData}} is always the same, and we need no extra flags.


h3. Stop-with-Savepoint, no draining

This simple variant (the _""suspend case""_) could just become a ""cancel with savepoint"", as [~kezhuw] mentioned.
There is no need to cleanly shut down streams. After the ""{{notifyCheckpointComplete()}}"" call, the action just throws a {{CancelTaskException}} and the pipeline shuts down.

That means no {{EndOfPartition}} would be involved, and ""{{close()}}"" and ""{{finishInput()}}"" would not be called. So we could get rid of all the special cases.

Maybe this needs some JM side work, meaning supporting that the scheduler goes into ""cancelling"" state without actually triggering the ""cancel()"" on the tasks, yet. And that it goes into global recovery if a task fails in that state.
That would also be a clean way to handle FLINK-21030 (issue around ""stop with savepoint"" and regional recovery /cc [~trohrmann])

IIRC, the reason that the stop-with-savepoint implementation currently shuts down ""cleanly"" was so no changes on the JM would be necessary. But in hind-sight, it looks like JM changes are necessary anyways (FLINK-21030) and that with those changes we also get cleaner semantics. Plus, this operation would work with unaligned checkpoints and other future flavors of checkpoints as well.

h3. Stop-with-Savepoint, with pipeline draining

This looks like the same thing as the feature that shuts down a bounded stream with a checkpoint.
The only difference is that mailbox on the sources exits its default action (reading from the sources) due to the ""stop()"" RPC call, rather than because of the ""EndOfInput"" status. Here everything shuts down with {{EndOfPartition}} which is also simultaneously {{EndOfData}}. The {{finishInput()}} and {{close()}} methods actually get called.

To support specifying a savepoint for that shutdown, we would need to be able to shut down the dataflow pipeline with one checkpoint in total. That would be interesting input for the [FLIP-147|https://cwiki.apache.org/confluence/display/FLINK/FLIP-147%3A+Support+Checkpoints+After+Tasks+Finished] discussion.
;;;","18/Feb/21 09:25;becket_qin;It looks that we are aiming to get a long term solution here. My two cents:

At a high level, in Flink any control logic initiated from JM eventually goes to one of the following two types / mechanisms.
 * independent point-to-point control (e.g. all JM to TM RPC)
 ** This is an ""out-of-band"" control flow, pretty much requests / responses between JM and TMs.
 ** In order to make sure the tasks will respond to the control command, the mailbox thread may have to be interrupted from a blocking call.
 * job-graph wide coordinated / orderly control (e.g. checkpoint, EndOfPartition)
 ** This is a control flow combining ""out-of-band"" and ""in-band"" mechanism
 ** JM sends the command to Sources via RPC (i.e. out-of-band)
 ** Sources execute the command, then propagate the command to downstream operators in data stream (in-band)
 ** The downstream operators receives the command from its input data stream (in-band)
 ** JM receives the ack from all the operators and complete the control logic. (out-of-band)
 ** [Optional] JM notifies all the TMs about the execution result (out-of-band) 

When it comes to {{StopWithSavePoint}}, to me the most intuitive semantic is that the tasks naturally stop right after taking the savepoint without any side effects. That means:
 # The operators / tasks stop processing records right after the savepoint is taken.
 # The operators / tasks do not receive {{EndOfPartition}} / {{EndOfInput}} because these events have already been assigned other meanings.
 # {{StreamOperator.close()}} should not be invoked to confuse the operators. Instead, {{Stream}}{{Operator.dispose()}} should be invoked directly.

At this point, I think the second control flow would just work. As long as we adjust the behavior of {{StreamTask}} a little bit.

Implementation wise, would the following work?
 # The JM sends to Sources a ""StopWithSavepoint"" RPC
 # The Sources take a snapshot, sends a checkpoint barrier with a flag indicating stop after checkpointing, then it stops processing data but just blocks on mailbox. This could be done either via a state machine for the task or simply a flag.
 # The downstream tasks align the checkpoint barrier, take their own snapshots, send the barrier to downstream tasks, then also block on mailbox.
 # After the JM finalize the checkpoint, it notifies the tasks of the completion of the checkpoint and the tasks will then exit.
 # When the tasks exit, the operators are not closed, but disposed directly.

For the legacy sources, I agree with [~kezhuw] that special treatment might be inevitable.;;;","19/Feb/21 14:50;pnowojski;[~becket_qin] I think you are missing the *Stop-with-Savepoint, with pipeline draining* case, which [~sewen] mentioned. There, we are emitting {{MAX_WATERMARK}} with {{advanceToEndOfEventTime}} case. In that case we probably should follow the standard clean shutdown procedure. I don't know if we should invoke {{endOfInput}} on the operators in that case or not... I would guess probably yes. If we are flushing window operators, we would also want to flush all buffered records. And if we are never intending to resume this job, {{endOfInput}} makes kind of sense for the downstream operators?

[~sewen], generally speaking I like the idea of changing stop with savepoint (without the drain), to cancel with savepoint. As me and [~roman_khachatryan] mentioned previously, we would like to avoid controlling the flow with exceptions. But that should be as easy to replace throwing `CancelTaskException` with just `StreamTask#cancel` call. 

However I still do not see how one would solve the problem of fully/completely backpressured legacy source task. In this case, source thread can be perpetually blocked while holding {{checkpointLock}}, thus preventing {{notifyCheckpointComplete}} from being ever executed. Maybe we would need to spin using {{StreamTask#runSynchronousSavepointMailboxLoop}} while triggering the checkpoint, and thus also blocking source thread from making any progress after triggering the checkpoint?

Secondly [~sewen] as we discussed offline. We would have to make sure that downstream/upstream task would cancel correctly, without mis-leading error messages, if they receive network connection closed before processing {{notifyCheckpointComplete()}}. I haven't looked how this would affect the code, maybe that's not an issue and it would work as it is, but it has to be verified.
;;;","20/Feb/21 01:56;becket_qin;[~pnowojski] I might get a little confused about what options we eventually want to provide to the users. It would probably help to clarify that first. I am thinking of the following cases:

*Cancel* - Stop the job without any data integrity guarantee. The job will not be able to resume, the output from the job is not guaranteed to be consistent. It is similar to a force quit.

*Stop* - Stop the job elegantly without the intention to resume the job. This is an orderly shutdown without a savepoint. It lets all the records in the pipeline get processed. {{MAX_WATERMARK}} will be emitted to ensure the final result gets written to the output. {{EndOfPartition}} will also be emitted as if all the records have been processed.

*Stop with Savepoint (Suspend / Pause)* - Stop the job elegantly with the intention to resume the job. This is also an orderly shutdown but with a savepoint taken. It lets all the records in the pipeline get processed. No {{MAX_WATERMARK}} or {{EndOfPartition}} will be emitted because the job is supposed to be resumed later.;;;","20/Feb/21 16:52;kezhuw;[~becket_qin] [~pnowojski] [~sewen] Thanks all for recently discussions. It helps me a lot in understanding:P.

[~sewen] I like the idea of clear-distinction between suspend and pipeline-draining cases and ""shut down the dataflow pipeline with one checkpoint in total"". The semantics of `StreamOperator.close` back to no exception again in condition of clean definition of pipeline-draining case.

[~sewen] [~becket_qin] [~pnowojski] I am kind of unsure what is the difference between pipeline-draining case and existing ""terminate"" case ? Where checkpoint should happen in pipeline-draining case ? At trigger/barrier or after end-of-stream-flush ? In case of trigger/barrier(eg. just like existing ""terminate"" case), there is no big code-path difference comparing to suspend case but clean definition. In case of after end-of-stream-flush, it is nearly an real terminal operation, the savepoint may be hard to resume from. In ""after end-of-stream-flush"" case, I think it is tight related to FLIP-147. In either case, currently FLIP-143 sink does not work out with single checkpoint.


{quote}Maybe we would need to spin using StreamTask#runSynchronousSavepointMailboxLoop while triggering the checkpoint, and thus also blocking source thread from making any progress after triggering the checkpoint?
{quote}
[~pnowojski] I think it is a must, otherwise if downstream closed before source, {{SourceContext.collect}} will fail before {{notifyCheckpointComplete}}.


{quote}We would have to make sure that downstream/upstream task would cancel correctly, without mis-leading error messages, if they receive network connection closed before processing {{notifyCheckpointComplete()}}.
{quote}
[~pnowojski]  Currently, {{RecordWriter}} reports error by lazy checking. I think it make sense as network error may caused by caller module intentionally. But I also saw exception swallowed in {{PartitionRequestQueue.handleException}}. Anyway, it deserves more attentions.

Other issue about chained source in {{MultipleInputStreamTask.triggerCheckpointAsync}}: {{advanceToEndOfEventTime}} is ignored.;;;","22/Feb/21 08:13;gaoyunhaii;Hi [~sewen] very thanks for the deep insights!
{quote}To support specifying a savepoint for that shutdown, we would need to be able to shut down the dataflow pipeline with one checkpoint in total. That would be interesting input for the FLIP-147 discussion
{quote}
I tried to think a bit on this issue: for stop with savepoint with pipeline draining, our main target is to get a consistent savepoint for all the tasks. Meanwhile, in considering of the case that the savepoint fails and then we need resume the job, it seems we would still need to first triggering a savepoint, and holding the tasks until the savepoint succeed, then the tasks get to the finish process (e.g., endOfInput & close). In this case we would need a savepoint _before_ the end of input for each operator.

For shut down a bounded stream with a checkpoint, our main target would be for operators interact with external systems, we need to get the pending data committed _after_ processed all the records (e.g., endOfInput), which makes it a bit divergent from stopping with savepoint. On the other side, in this case it would not necessary for all the operators to commit their pending records in one checkpoint (but I totally agree with it would be good to reduce the number of checkpoints required).

Due to this difference, it seems to me that it might be not easy to unify the two processes?

If we only consider whether we could shuts down a bounded stream with a single checkpoint, since we could not broadcast barriers after emitting {{EndOfPartitionEvent}}, we might have to introduce a new {{EndOfInputEvent}} to notify the endOfInput separately, then take a unified checkpoint,  and emit {{EndOfPartitionEvent}} finally. There might be still two problem here, one is whether we want to [support emitting records in notifyCheckpointComplete|https://lists.apache.org/thread.html/rc15b3f7f4ee3b94132f1da8f0b7e988607bccdc2eec49cf156061744%40%3Cdev.flink.apache.org%3E], another is that since we could not guarantee when the sources would finish, we may have to holding the finished tasks for a long time, and the extreme case would be for mixed jobs with both bounded and unbounded sources, in this case we would have to hold the finished tasks forever ? Thus I think even after we introduce {{EndOfInputEvent}} and not emit records in notifyCheckpointComplete, we might still not need to limit the tasks to wait for a single checkpoint. ;;;","22/Feb/21 11:41;trohrmann;Thanks for driving this discussion. I think a lot of things are coming together here because shutdown/closing semantics of operators are tightly coupled to how sources and sinks must work and it should ideally work for bounded/unbounded streams and batch/streaming execution mode similarly.

I think it is a good idea to think about what functionality we want to offer to our users as [~becket_qin] did it before diving into the concrete implementation details:

1. *A user wants to cancel a bounded/unbounded job and does not care about the output correctness*
This is what we currently support with {{Task.cancelExecution}}. Cancel should forcefully terminate the operators and we don't care whether records are properly sent to downstream tasks or written to a sink.

2. *A user wants to suspend a bounded/unbounded job and wants to be able to resume it later*
This is what we currently call **stop-with-savepoint**. Here the user wants to create a savepoint representing the current state of the computation which he can use to resume the job at a later point. The user does not care about whether the job shuts down orderly or not. Hence, we should be able to simply cancel all tasks after taking the savepoint.

3. *A user wants to terminate a bounded/unbounded job and wants all of its buffered data to be flushed to external systems*
This is what we currently call *stop-with-savepoint --drain*. Here the user is not so much interested in the savepoint as in materializing the job's state/result to external systems. The only use case for the created savepoint here I could think of is to get access to Flink's state after the job has terminated. Resuming the job from such a savepoint does not make much sense because the results will be affected by the {{MAX_WATERMARK}} we have sent. Maybe *stop-with-savepoint --drain* is a misnomer.

4. *A bounded job reaches its end and in order to guarantee correctness needs to flush als its buffered data*
This effectively the same as terminating a bounded/unbounded job just that it is not induced by the user but by the sources reaching the end of data.

5. *A user wants to gracefully stop its job w/o creating a savepoint*
The only case I can think of is that a user wants to stop an at-least-once job in such a way that all records up to the point of issuing the command will get processed. Other than that cancel might already be good enough.

Conceptually, 3. and 4. should use the same mechanism because semantically, there is no difference [~gaoyunhaii].

I agree with [~kezhuw] and [~sewen] that 2. should not require the complexity of shutting the job gracefully down. E.g. by allowing the state transition {{RUNNING -> CANCELLED}} on the {{JobMaster}} could allow the {{Tasks}} to simply cancel themselves after receiving {{notifyCheckpointComplete}}. Alternatively, {{Tasks}} could acknowledge the {{notifyCheckpointComplete}} but this adds complexity to the already complex {{CheckpointCoordinator}}.

Whether we need multiple checkpoints to shut down a job in case of 3. and 4. probably depends on whether we allow records to be sent after the final {{notifyCheckpointComplete}} has been executed for a {{Task}} or not. Intuitively, not having to shut down a job with multiple checkpoints seems more desirable for me.

If this were the case and if we could still create a checkpoint after a {{StreamOperator.close}} has been called, then we could simply send a {{EndOfPartitionEnvent}} when an operator reaches the end of input or receives the *terminate* call. Next, the {{StreamTask}} would only have to wait for a checkpoint to succeed before terminating. ;;;","22/Feb/21 16:34;pnowojski;+1 for those use cases/semantics summarised by [~trohrmann]. I agree that 3. and 4. are also effectively the same. Maybe trying to conclude various loose threads that we had here. I see the following, mostly independent, issues:

a) Two phase commit support for 3. and 4. This will be dealt by FLIP-147 (please check the discussion on the dev mailing list)
b) Unfortunately in FLINK-21132 we broke 3. (*stop-with-savepoint --drain*). In this case, `endOfInput()` should be called (CC [~roman_khachatryan]). Otherwise, some operators are not flushing/draining the buffered state (like for example {{AsyncWaitOperator}}, which is doing it only in the {{endOfInput()}} call). Note that before FLINK-21132, 3. was working correctly only if we ignore the issue of committing side effects (two phase commit support). FLINK-21453 will fix this problem.
c) Changing 2., from ""stop with savepoint"" to ""cancel with savepoint"". Previously I thought about it as a refactor/clean up AND optimisation (speed up of the shutdown). However, as we can not used this approach for 3., I think it's just an optimisation that would diverge the code base. For this reason I think it would be better to postpone such optimisation after FLIP-147 is done (if ever).
d) FLIP-27 not supporting stop with savepoint (both 3. and 4.);;;","22/Feb/21 16:56;kezhuw;[~trohrmann] Great summarization!

About user-case#2, we may need to handle failed {{notifyCheckpointComplete}}, otherwise some 2pc context could lost during downtime(eg. kafka transaction timeout). A successful ""stop-with-savepoint"" should commit all side effects to visible and keep user from worry about downtime. This also requires ""shut down the dataflow pipeline with one checkpoint in total"". All these apply to user-case#3 too.

About user-case#3, currently ""stop-with-savepoint --drain"" takes snapshot before end-of-input, so some end-of-input-flushing works are not taken into savepoint. For example, {{AsyncWaitOperator.endInput}} is called after savepoint before FLINK-21132. I think what [~sewen] described in ""pipeline draining"" may deserve a more destructive name, say ""terminate"". To behave as an real terminal operation, the savepoint should be taken after end-of-input-flushing operation.

About user-case#4, I agree it should be no much difference to user-case#3 as long as we don't force to hold finished sources before checkpoint. Before the last checkpoint, there could be only partial or even only one task in running and waiting/expecting a checkpoint to commit side effects. I think this should be part of FLIP-147's goal. If we are targeting one checkpoint/savepoint for all tasks in this cases, it is may not that worth as there could be multiple sources exhausted at different time. But I think it has value that all tasks' states are preserved after last checkpoint/savepoint. If this is the case, I think the last checkpoint/savepoint should be same as user-case#3.
{quote}If this were the case and if we could still create a checkpoint after a StreamOperator.close has been called, then we could simply send a EndOfPartitionEnvent when an operator reaches the end of input or receives the terminate call. Next, the StreamTask would only have to wait for a checkpoint to succeed before terminating.
{quote}
I would say it is attractive!

Currently, {{StreamOperator.close}} was used to both ""flushing buffered data"" and ""cleanup resources"", while FLINK-2647 tried to claim *""Distinguish between ""close"" (flushing buffered data) and ""dispose"" (cleanup resources) in streaming operators""*.

Here is what I have seen:
 # FLINK-16383 ignores {{notifyCheckpointComplete}} after operator closed.
 # I thought {{notifyCheckpointAborted}} should not be invoked after {{StreamOperator.close}} before(FLINK-20389).
 # FLINK-20781 {{SourceOperator}} has to deal with {{notifyCheckpointAborted}} after {{StreamOperator.close}} due to resources cleaned.

There should be more I am not aware of and involved in.
 * Not ""allow records to be sent after the final {{notifyCheckpointComplete}} has been executed""
 * ""we could still create a checkpoint after a StreamOperator.close has been called""

Sadly, both will break something.;;;","24/Feb/21 13:32;pnowojski;[~kezhuw]:
#2, if stop-with-savepoint fails, job can just continue. The problem is in case 3. and 4., where the only viable solution is I think to restart the job from last checkpoint/savepoint. I don't think this is happening right now?

#3, please check FLIP-147 dev mailing list discussion. There is my proposal how to address this problem.

{quote}
If this were the case and if we could still create a checkpoint after a StreamOperator.close has been called, then we could simply send a EndOfPartitionEnvent when an operator reaches the end of input or receives the terminate call. Next, the StreamTask would only have to wait for a checkpoint to succeed before terminating.
{quote}
That would brake the current {{@Public}} API. For example our own {{FlinkKafkaProducer}} would stop working :( And as you mentioned [~trohrmann] it wouldn't solve the problem of shutting the operators one by one if they need to emit something in the {{notifyCheckpointComplete()}}.;;;","24/Feb/21 18:57;trohrmann;For #2, we should fail the stop-with-savepoint operation if the {{notifyCheckpointComplete}} fails and resume the job from the last checkpoint. In this case it would actually be the just created savepoint. Whether we need to restart tasks or not depends on where the failure occurs. #3 and #4 should actually be quite similar in this regard. FLINK-21030 should fix that we restart in case of a failure after the savepoint has been successfully created.

I think it is not important that we keep finished operators running for the final checkpoint. If some operators have finished before the overall job reaches the final checkpoint, then these operators can shut down with a previous checkpoint. We somehow just need to remember that these operators were already completed in case we need to restart the job if a failure happened in {{notifyCheckpointComplete}}.

I am pretty certain that our current API is not sufficient for expressing what we want to express (e.g. not being able to take a checkpoint after we have flushed all records from an operator via {{StreamOperator.close}}). Breaking an API is never good but it should also not hinder us to think about how things should ideally look like and then think about how we could get there. Ideally we don't have to break things but if the current APIs do not allow to express the behaviour we need our operators to have, then at some point we have to take this bullet. In doubt I would prefer this compared to working around the current limitations and never being able to properly define the required operator semantics.

;;;","24/Feb/21 19:08;pnowojski;I agree about that we might need to change the API somehow, but maybe there are less invasive changes. Calling `notifyCheckpointComplete()` after closing will be very invasive/affecting silently a lot of built in and user functions/operators. I would hope there are other options. For example IMO renaming `close()` to `flush()` or `finish()` combined with a contract that `notifyCheckpointComplete()` (and other checkpointing calls) can be called between `finish()` and `dispose()` would be better. It would be just as invasive, but as at least user code would not brake silently. But keep in mind that at the moment on the user functions level we even don't have `dispose()` :/ Let's maybe move this particular discussion to FLIP-147 discussion thread?;;;","25/Feb/21 08:44;trohrmann;Yes, let's move this discussion to the FLIP-147 discussion thread. Let's use this ticket to discuss how to fix the FLIP-27 sources for stop-with-savepoint even if it is just a quick fix for the time being.;;;","23/Mar/21 14:08;kezhuw;[~trohrmann] [~pnowojski] [~becket_qin] [~sewen] Hi all, should we try a fix before 1.13 for possible sole {{SourceOperatorStreamTask}} ? I think it is worth to not block {{SourceOperatorStreamTask}} on more complicated issues. I guess part of stop-with-savepoint is blocked on FLIP-147.

Besides this, is it worth an umbrella issue to track/marshal stop-with-savepoint issues in total ? The discussion in this thread is far beyond FLIP-27 source.;;;","23/Mar/21 14:17;sewen;[~becket_qin] and me talked about this quickly: We agree, to just enable stop-with-savepoint for the FLIP-27 sources, it looks like we need just a simple addition in the {{SourceOperatorStreamTask}}. [~becket_qin] mentioned he wants to look into that.

The bigger FLIP-147 fix will be part of a later release once the design/approach has converged.;;;","23/Mar/21 14:23;kezhuw;[~sewen] [~becket_qin] Good news!;;;","16/Apr/21 10:47;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","19/Apr/21 07:55;trohrmann;Are we going to have a quick fix for this problem in 1.13.0 [~sewen], [~becket_qin]?;;;","19/Apr/21 14:38;becket_qin;Patch merged.

master:  a9cf18b4d25f130e0bd24d51b128bbcf71892b45
release-1.12: 0913824dda91b04612f2f885052635e0ca78f5b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedOneInput.endInput is called when taking synchronous savepoint,FLINK-21132,13354468,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,kezhuw,kezhuw,25/Jan/21 15:20,28/May/21 08:17,13/Jul/23 08:07,03/Feb/21 19:58,1.10.2,1.10.3,1.11.3,1.12.1,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Task,,,,,0,pull-request-available,,,,,"[~elkhand](?) reported on project iceberg that {{BoundedOneInput.endInput}} was [called|https://github.com/apache/iceberg/issues/2033#issuecomment-765864038] when [stopping job with savepoint|https://github.com/apache/iceberg/issues/2033#issuecomment-765557995].

I think it is a bug of Flink and was introduced in FLINK-14230. The [changes|https://github.com/apache/flink/pull/9854/files#diff-0c5fe245445b932fa83fdaf5c4802dbe671b73e8d76f39058c6eaaaffd9639faL577] rely on {{StreamTask.afterInvoke}} and {{OperatorChain.closeOperators}} will only be invoked after *end of input*. But that is not true long before after [FLIP-34: Terminate/Suspend Job with Savepoint|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103090212]. Task could enter state called [*finished*|https://github.com/apache/flink/blob/3a8e06cd16480eacbbf0c10f36b8c79a6f741814/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L467] after synchronous savepoint, that is an expected job suspension and stopping.

[~sunhaibotb] [~pnowojski] [~roman_khachatryan] Could you help confirm this ?

For full context, see [apache/iceberg#2033|https://github.com/apache/iceberg/issues/2033]. I have pushed branch [synchronous-savepoint-conflict-with-bounded-end-input-case|https://github.com/kezhuw/flink/commits/synchronous-savepoint-conflict-with-bounded-end-input-case] in my repository. Test case {{SavepointITCase.testStopSavepointWithBoundedInput}} failed due to {{BoundedOneInput.endInput}} called.

I am also aware of [FLIP-147: Support Checkpoints After Tasks Finished|https://cwiki.apache.org/confluence/x/mw-ZCQ], maybe the three should align on what *finished* means exactly. [~kkl0u] [~chesnay] [~gaoyunhaii]",,aljoscha,alpinegizmo,gaoyunhaii,kezhuw,klion26,knaufk,pnowojski,roman,sjwiesman,thw,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21453,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 24 14:02:21 UTC 2021,,,,,,,,,,"0|z0myso:",9223372036854775807,endInput() is not called anymore (on BoundedOneInput and BoundedMultiInput) when the job is stopping with savepoint.,,,,,,,,,,,,,,,,,,,"25/Jan/21 18:19;pnowojski;Thanks for reporting and analysing the issue. Yes, I think you are right about describing the behaviour. I took a quick look at the code, and the problem seems to be in the {{StreamOperatorWrapper#close}} method. It is called for every clean shutdown of the task, which AFAIK can be either:
* true end of input
* stop with savepoint

And regardless of which of those two triggered the clean shutdown, end of input will be issued to every operator which is not head operator of the chain.

Now depending how we should look at it, there is one of two bugs.

# If we assume stop with savepoint shouldn't trigger end of input, in that case non head operators shouldn't receive end of input.
# If we assume stop with savepoint should trigger end of input, in that case head operator should receive end of input.

[~aljoscha] [~AHeise], what do you think?;;;","25/Jan/21 18:22;pnowojski;If the correct answer for 1., there seems to be a simple quickfix [~kezhuw] - disable operator chaining for the affected operators. If they are head operators, they wouldn't receive end of input at the moment. ;;;","26/Jan/21 09:23;gaoyunhaii;Hi [~kezhuw], very thanks for brought up this issue, I have some thoughts from the sink's perspective: I also think as [~pnowojski] said, there will be two cases, namely the job is indeed bounded and stop with savepoint. 

First of all, I think there would be some problem fro the current implementation that committed in endOfInput(), considering if the job is bounded (namely the first case). The problem is that if the failover happens right after commit() in the endOfInput, then the job will be restarted and fallback to the last checkpoint, which will cause replay of the data committed in endOfInput. [FLIP-147: Support checkpoint after tasks finished|https://cwiki.apache.org/confluence/x/mw-ZCQ] is a precedent step to solve the commit problem of the sinks in the first case. It tries to support checkpoints after some tasks finished (e.g., all the tasks before the sink are finished), and the expected behavior of the sink operator is  

- endOfInput (close all the opening files)
- wait for a new checkpoint
- snapshotState
- notifyCheckpointComplete (here we would do the commit)
- close().

For the second case, namely stop with savepoint, the current process would be
- snapshotState() for the savepoint
- notifyCheckpointComplete() for the savepoint
- possibly process some data
- endOfInput()
- close()

In this case, the sink should still do not need to commit inside endOfInput() because all the state are expected to be committed in the second step, namely notifyCheckpointComplete(). 

The current issue to me might be that we do not solve the first case yet so that we would have to do some workaround, like commit in endOfInput();;;","26/Jan/21 09:57;gaoyunhaii;But for the final picture for sinks, there is indeed an issue that compared with the above two cases, whether a sink operator need to wait for another checkpoint is different in the above two cases, but based on the current implementation the operator would not be able to distinguish them.;;;","27/Jan/21 01:21;kezhuw;[~pnowojski] For option#2, which is the current option, head operator indeed receives end of input currently except for FLIP-27 source(see FLINK-21133). For legacy source head operator, {{StreamSource}} will fire end of input and {{Task}} will write {{EndOfPartitionEvent}} after successful run, this matches my previous description. For no source head operator, after received all {{EndOfPartitionEvent}} for every subpartitions in input, {{StreamTaskNetworkInput}} will report {{InputStatus.END_OF_INPUT}} and {{StreamOneInputProcessor}} will fire end of input, this matches what [apache/iceberg#2033|https://github.com/apache/iceberg/issues/2033] happens in later discussion. So, basically, FLINK-14228 is well implemented, it just has assumption overrode by/conflict with FLIP-34.

Personally, I prefer option#1 since save-with-savepoint is just an utility, it does not contribute to streaming data flow. I think maybe it could be implemented in a way that FLINK-21133 solved without additional work from source side.;;;","27/Jan/21 15:10;aljoscha;It seems to me we just need to make sure that {{endOfInput()}} is not called for the stop-with-savepoint case. That method must only be called when it is *truly* the end of input, that is when the source and all the predecessor operators have shut down. What do you think? With this, the Jira Issue should be easy enough to fix.;;;","27/Jan/21 17:45;kezhuw;I did not find an easy/viable way to distinguish truly end of input from stop-with-savepoint style end of input. They are both {{EndOfPartitionEvent}} in network. Legacy sources could also emit elements between synchronous savepoint and {{EndOfPartitionEvent}}.
{quote}that is when the source and all the predecessor operators have shut down.
{quote}
How this could be viable for, say, a no chained head operator ? AFAIK, no chained operator has only two inputs from outside: rpc call through {{TaskExecutorGateway}} and stream elements and events from predecessor operators through input channel. I saw only {{EndOfPartitionEvent}} related.

I think stop-with-savepoint has enough room to operate on *all tasks* not just *source tasks* without interfering with data flow upon {{StreamTask.notifyCheckpointComplete}}. I could give an simple(probably not ideal) solution, let {{Task}} query {{AbstractInvokable}} to know whether it should *finish* partition writers after successful run. This way FLINK-21133 is solved also.;;;","29/Jan/21 10:37;roman;I think it would be risky to change the behaviour of endOfInput. Existing operators may rely on it being called even in case of stopWithSavepoint.

Instead, I'd propose to add a new interface StopWithSavepointAware \{ void stoppedWithSavepoint(); } and call this method for operators implementing it and endInput otherwise. Also in case of ""true"" end of input the old method would be called.

Then IcebergFilesCommitter should implement it.

 

To distinguish true end-of-input from stop-with-savepoint we can rely purely on checkpoint properties:
 # Sources receive sync-savepoint RPC and only then call operatorChain.close / endInput
 # Downstreams must receive and confirm the checkpoint before getting EOP from source

So in both cases it' enough to remember that a sync-savepoint was received.

Alternative approach is quite invasive as it in addition would change Task, AbstractInvokable, passing and interprting EOP event.

 

I've prepared a prototype here: [https://github.com/rkhachatryan/flink/tree/f21132]. It uses [~kezhuw] IT case (thanks for providing it!).

It doesn't solve FLIP-27 issue but I think FLIP-27 should be addressed separately.

 

WDYT?;;;","29/Jan/21 12:04;kezhuw;If we agree that stop-with-savepoint should not pretending it is an {{END_OF_INPUT}}, I would suggest a new checkpoint type {{END_OF_INPUT_SAVEPOINT}}. This way I think we could mitigate the effect progressive. For existing major version, defaults {{flink stop}} to {{flink stop --end-of-input}}, for future major version, defaults to {{flink stop --no-end-of-input}}. I am kind of against {{StopWithSavepointAware}} for solely solving this unless we have concretes usages which probably will not interfering with {{END_OF_INPUT}} also.;;;","29/Jan/21 14:00;aljoscha;I'll try and expand on what I wrote earlier. The TL;DR is that I still think sending an {{endOfInput()}} to operators on a stop-with-savepoint is a bug.

I differentiate between two different end-of-inputs: 1) _physical_ end-of-input, and 2) _logical/semantical_ end-of-input. The former is signalled when, for example, a network connection is being shut down. The latter happens (should happen) when (bounded) sources have no more data to read and this information propagates through the pipeline. The motivation for introducing {{endOfInput()}} were things like hash-join in the SQL runner where an operator would read from the build side until getting an end-of-input, at which point it would switch over to reading from the probe side. With these use cases in mind sending an {{endOfInput()}} is a bug. The same is true for sinks, which will do some bookkeeping based on knowing that all the input data has been read.

I don't see use cases where operators would be interested in being notified of the physical end-of-input right now. I could be wrong, of course. 😅;;;","29/Jan/21 15:24;pnowojski;I agree with {{endOfInput}} should NOT be called in case of stop with savepoint. IMO adding another interface {{StopWithSavepointAware}} for the sake of backward compatibility would unnecessarily complicate our APIs.

After offline discussion with [~roman_khachatryan], probably the easiest way to do, would be to just suppress {{endOfInput}} calls somewhere in the {{OperatorChain}} or {{StreamOperatorWrapper}} if we know we are waiting for the stop with savepoint. It's not ideal, but adding an extra even to distinguish between {{END_OF_INPUT}} and {{STOP_WITH_SAVEPOINT}} would be quite invasive change.;;;","29/Jan/21 16:09;roman;Thanks for your feedback [~aljoscha] and [~kezhuw] . I've also discussed it offline with [~pnowojski].

We agreed that endInpt should NOT be called (no operators should rely on it, if any do they should be fixed).

(so no StopWithSavepointAware needed)

 

As for the detection (stop-with-savepoint vs other cases), we agree the proposed solution isn't ideal. However, it's much less invasive and we'd like to provide fix ASAP so we lean toward it.

I'm adjusting it to all cases of chaining (these are different code paths).;;;","29/Jan/21 18:14;kezhuw;[~aljoscha] [~pnowojski]  [~roman_khachatryan]  We all actually are on the same page. So, all above approaches are feasible to me as long as there is no api level bug-specific solution.

But I am kind of preferring to not sending {{EndOfPartitionEvent}}, so I would like to present my [local work|https://github.com/apache/flink/commit/d6c6837cd724913716007265f03a50098bee985e] for another approach. Here is a summary for the changes:
 # {{StopTaskException}} to throw from {{AbstractInvokable.invoke}}.
 # Upon receiving savepoint completion, stop task with savepoint. This stop phase is same for mailbox model, so FLINK-21133 does not exist anymore.
 # After run out of mailbox loop, check whether it is stopped by a savepoint. If it is, throws {{StopTaskException}}.
 # If {{StopTaskException}} happens, {{Task.doRun}} will not finish result partition(hence no EndOfPartitionEvent), but still transit task to finished state(I am not sure it is conflict with FLIP-147 or not as I saw ""Reuse Tasks' FINISH status"").
 # Other works: generalize {{SteamTask.finishTask}} to all stream task but not only source tasks; generalize {{StreamTask.cancelTask}} to mailbox model.

The benefits of this approach is that:
 # It does not sending {{EndOfPartitionEvent}} anymore. There will be only true end of input.
 # If finished status caused by stop-with-savepoint is conflict with FLIP-147, then it is easy to change to new status.
 # FLINK-21133 solved with no extra work.

There is one user-notable breaking change: {{SreamOperator.close}} will not be called in case of stop-with-savepoint.

Last, feel free to choice either approach, it is just my personal preference:P, and I don't take any compatibility/invasiveness into account.;;;","01/Feb/21 09:18;roman;Thanks for your proposal [~kezhuw]

I took a look at it and I have some concerns:
 # EoP is needed to stop the job eventually. I expect that with his fix, job will stuck after stop with chaining disabled. Am I missing something?
 # Using exceptions for flow control is error-prone. We had quite some issues with exactly this logic in the past

If I remember correctly (the link doesn't work now), it adds some boolean flags too in addition to chaning error handling. So it doesn't seem to me ""no extra work"".;;;","01/Feb/21 10:19;kezhuw;Hi [~roman_khachatryan], thanks for reviewing, let me detail:
 1. The key change this proposal made is *making old {{StreamTask.finishTask}}(renaming to {{StreamTask.cancelTask}}) works for all tasks but not only source tasks*. So this will works as long as we fire checkpoint-complete on all tasks. Before this change, {{StreamTask.finishTask}} is an nop on no source tasks.
 2. {{StopTaskException}} is just like {{CancelTaskException}}, but got different treatment in {{Task.doRun}}.

3. FLINK-21133 is about FLIP-27 source, which is {{SourceOperatorStreamTask}} but not {{SourceStreamTask}}. {{SourceStreamTask}} get special treatment in old solution also.

I would like list changes in running order:
 1. Up receiving {{notifyCheckpointComplete}}, it stops current task no matter whether it is a source or not. This operator will cause mailbox loop run out.
 2. After stopping current task, it set {{stopWithSavepointId}} for later usage.
 3. After mailbox loop run out, if {{stopWithSavepointId}} is set, throw {{StopTaskException}}.
 4. {{Task.doRun}} will skip result partition finishing(eg. firing EoP) if {{StopTaskException}} is captured.

Another observation, I think current behavior(source EoP) may not work with FLIP-147 which allows no-source heading operator.;;;","01/Feb/21 12:51;roman;Hi [~kezhuw] , thanks for the clarification.

I'm concerned about the order in which tasks would terminate. If we rely on checkpoint completion notifications for that then downstream can terminate before upstream which can fail the job if there are any more data.

I see that StopTaskException is similar to CancelTaskException but adding more logic like CancelTaskException is what I'd like to avoid :) Maybe others won't agree with me though.

 

Also we are discussing an issue of potentially missing endInput if a true end-of-input is encountered after sync-savepoint but before job termination. It looks like the above approach is also subject to it.

 ;;;","01/Feb/21 18:24;kezhuw;Currently, there is no {{runSynchronousSavepointMailboxLoop}} in {{StreamTask.triggerCheckpoint}}, but let's assume it has.

For mailbox tasks, it will not handle any mail until checkpoint confirmation. After checkpoint completion, poison mail will break out mailbox loop. I did not find a hole in between since there is no chance to read/write.

For legacy source, thing get complicated. I would like to list an example body for {{SourceFunction}}:
{code:java}
public void run(SourceContext<T> ctx) {
    while (isRunning && condition) {
        // this synchronized block ensures that state checkpointing,
        // internal state updates and emission of elements are an atomic operation
        synchronized (ctx.getCheckpointLock()) {
            ctx.collect(count);
            count++;
        }
    }
}
{code}
 # If checkpoint comes before {{synchronized}}, after checkpoint completion, {{ctx.collect}} will throw exception, but it is covered by {{isStopped()}} in {{SourceStreamTask}}, and exception is ignored in this case.
 # If checkpoint comes after {{synchronized}} and source function run out, it could finish the thread which cause poison mail. But poison mail actually only break out mailbox loop itself, but not {{MailboxExecutor.yield}} loop. So still wait for checkpoint completion, and from now, it is actually same as mailbox tasks as there is no concurrent thread now. The only hole I saw is in {{StreamSource}}, where executes endInput after {{SourceFunction.run}}. I think we could move it to {{SourceStreamTask.afterInvoke}} if we want to keep this theoretical possibility.

I think some of above apply to eop-detection approach too, hope it could help.;;;","02/Feb/21 12:30;pnowojski;I'm sharing the same concern as [~roman_khachatryan] about controlling the flow using the exception, but that could be easily changed in your version [~kezhuw].

I think the larger difference is that [~roman_khachatryan]'s proposal is doing the clean shutdown, via {{StreamTask#afterInvoke}}, but just set's the flag to not invoke {{endOfInput}}. While [~kezhuw] approach is re-using the cancellation/failure code path of shutting down. Bypassing {{afterInvoke}} seems simpler, as it doesn't require changes in the {{OperatorChain}} and {{StreamOperatorWrapper}} classes. Maybe the appropriate question would be:

Do we want to do the proper shutdown procedure for stopping with savepoint? For example quiescing the timer service, potentially processing some records, finally calling {{close()}} on the operator, flushing the outputs - all of that while we have already decided to successfully stop with savepoint?

{quote}
For mailbox tasks, it will not handle any mail until checkpoint confirmation. After checkpoint completion, poison mail will break out mailbox loop. I did not find a hole in between since there is no chance to read/write.
{quote}
The question is also how to handle situations when stop with savepoint has been cancelled because savepoint failed (for example it was declined as there was a race condition with some source finishing and sending {{END_OF_PARITTION}} event while CheckpointCoordinator was triggering stop with savepoint).;;;","02/Feb/21 14:36;pnowojski;{quote}
I'm concerned about the order in which tasks would terminate. If we rely on checkpoint completion notifications for that then downstream can terminate before upstream which can fail the job if there are any more data.
{quote}
I've discussed this concern offline with [~roman_khachatryan]. Probably when downstream terminates before the upstream would not cause job failure, but in your proposal [~kezhuw] bad things can happen. The root cause of problems is that you are shutting down tasks from {{notifyCheckpointCompleted}} call, while both current solution in the master and [~roman_khachatryan] proposal is relaying on clean shutdown after receiving {{END_OF_PARTITION}} events. So the shutdown happens from the heads/sources to tails/sinks.

Keep in mind that while task is spinning inside {{runSynchronousSavepointMailboxLoop}}, it's not processing inputs BUT it can be producing output via mailbox actions (processing time timers firing {{WindowOperator}}, or just emitting records from the mailbox action like {{AsyncWaitOperator}} or {{ContinuousFileReaderOperator}} are doing). Rarely this can cause a back-pressure to happen and task might be stuck inside {{runSynchronousSavepointMailboxLoop}}.

Now in your proposal [~kezhuw] if downstream task receives {{notifyCheckpointCompleted}} before the upstream, there will be nobody to relieve the upstream task from the backpressure and upstream task would be deadlocked, not able to process {{notifyCheckpointCompleted}}.

It's not ideal what we have at the moment, in the future we might decide to shut down earlier, without waiting for {{END_OF_PARTITION}} to travel through all of the job graph, but that would have to be a separate story, independent of this bug fix.
;;;","02/Feb/21 14:52;kezhuw;You are right [~pnowojski], it depends on whether we want do normal/successful/close/endOfInputStyle shutdown or exceptional/dispose shutdown for stop-with-savepoint. As I noted, it is a breaking change comparing to old behavior. It is kind of an api contract whether {{StreamOperator.close}} should be invoked in not end of input case.

{quote}
The question is also how to handle situations when stop with savepoint has been cancelled because savepoint failed
{quote}

Thanks for mention this, I saw a hole in {{notifyCheckpointAbortAsync}} where there is no {{resetSynchronousSavepointId}}. I will verify it using test case.

{quote}
it was declined as there was a race condition with some source finishing and sending END_OF_PARITTION event while CheckpointCoordinator was triggering stop with savepoint
{quote}

In this case, there will be not savepoint at downstream, right ?

Besides, I only saw END_OF_PARTITION in {{Task.doRun}} after successful {{AbstractInvokable.invoke}}. Is there any other paths ?;;;","02/Feb/21 15:13;kezhuw;{quote}
Keep in mind that while task is spinning inside {{runSynchronousSavepointMailboxLoop}}, it's not processing inputs BUT it can be producing output via mailbox actions (processing time timers firing {{WindowOperator}}, or just emitting records from the mailbox action like {{AsyncWaitOperator}} or {{ContinuousFileReaderOperator}} are doing). 
{quote}

{{StreamTask.{{runSynchronousSavepointMailboxLoop}}}} uses {{TaskMailbox.MAX_PRIORITY}}, I expect that there will be no client firing activities.

 

{quote}
Rarely this can cause a back-pressure to happen and task might be stuck inside {{runSynchronousSavepointMailboxLoop}}.
{quote}

Is this an expected behavior by {{CheckpointType.isSynchronous}} ?

I would consider stop-with-savepoint is a destructive operation, right ? I saw FLINK-21030, the discussion leans global failover if stop-with-savepoint failed.;;;","02/Feb/21 15:20;kezhuw;{quote}It's not ideal what we have at the moment, in the future we might decide to shut down earlier, without waiting for END_OF_PARTITION to travel through all of the job graph, but that would have to be a separate story, independent of this bug fix.
{quote}
[~roman_khachatryan] [~pnowojski] I think what we are currently discuss is far beyond this bug, but another possibility, right ;)?  For this bug fix, I also think [~roman_khachatryan]'s approach is less invasive.;;;","02/Feb/21 15:27;pnowojski;Ehhh, small edit from my side. I think I was wrong in our small part:
{quote}
Keep in mind that while task is spinning inside runSynchronousSavepointMailboxLoop, it's not processing inputs BUT it can be producing output via mailbox actions
{quote}
I think this is not true. But I think because we are not blocking sources from producing records while waiting for stop with savepoint, I think the rest of my post might be still valid. Downstream task would need to relieve source's backpressure?

{quote}
For this bug fix, I also think Roman Khachatryan's approach is less invasive.
{quote}
Regardless of my above mistake, I agree with this. For bug fix that we will be back-porting to other releases, it would be safer to keep the current behaviour.;;;","02/Feb/21 15:53;kezhuw;{quote}
there will be nobody to relieve the upstream task from the backpressure and upstream task would be deadlocked, not able to process notifyCheckpointCompleted.
{quote}
I think data flow and gateway event are go through different network channel.

{quote}
I'm concerned about the order in which tasks would terminate. If we rely on checkpoint completion notifications for that then downstream can terminate before upstream which can fail the job if there are any more data.
{quote}

[~roman_khachatryan] [~pnowojski] I am also kind of worried about this. The stack is kind of big to figure out in days. I am still engaging in Flink. The discussion helps a lot:D. So, thanks!;;;","03/Feb/21 06:23;pnowojski;{quote}
I think data flow and gateway event are go through different network channel.
{quote}
Data flow and events between TaskManagers are using the same channels. RPCs from JobManager to the TaskManagers are using a different channel, but they won't be processed if the Task's thread (mailbox) is blocked. To avoid deadlocks one would have to carefully thought this through.;;;","03/Feb/21 19:58;pnowojski;Merged to master as [1c19ab7dc6f^^^..1c19ab7dc6f]
Merged to release-1.12 as [62081af01e1^^^..62081af01e1]
Merged to release-1.11 as [299820a33c1^^^..299820a33c1];;;","09/Feb/21 06:25;kezhuw;{quote}I saw a hole in notifyCheckpointAbortAsync where there is no resetSynchronousSavepointId. I will verify it using test case. – from me
{quote}
[~roman_khachatryan] [~pnowojski] This does not hold as {{CheckpointCoordinator}} will fail the job({{CheckpointFailureManager.handleSynchronousSavepointFailure}}) after synchronous-savepoint(eg. stop-with-savepoint) failure. But the causal chain is a bit long, so I suggest to {{resetSynchronousSavepointId}} always. [pr#14815|https://github.com/apache/flink/pull/14815/files#diff-0c5fe245445b932fa83fdaf5c4802dbe671b73e8d76f39058c6eaaaffd9639faR1080] already have this, so there is no more concern from my side.
{quote}Data flow and events between TaskManagers are using the same channels. RPCs from JobManager to the TaskManagers are using a different channel, but they won't be processed if the Task's thread (mailbox) is blocked.
{quote}
I probably use wrong words. I use ""data flow"" to represent {{StreamElement}} and {{RuntimeEvent}}(including {{EndOfPartitionEvent}}, {{CheckpointBarrier}}, etc.) and ""gateway event"" to represent {{TaskExecutorGateway.triggerCheckpoint/confirmCheckpoint}} which are rpc methods. I think we are aligned on this.

Checkpoint completion and cancellation are delivered using {{TaskMailbox.MAX_PRIORITY}}, so they will be processed along with other control-mails in {{runSynchronousSavepointMailboxLoop}}. I don't think either approach change this part of code, so if there is deadlock after, it probably exists before.
{quote}To avoid deadlocks one would have to carefully thought this through.
{quote}
[~roman_khachatryan] [~pnowojski] Is it worth another dedicated issue to discuss this no end-of-partition style stop-with-savepoint ? I see values of this approach:
 # It is applicable to mailbox model. FLINK-21133 could be fixed without touch FLIP-27 source code. FLINK-21133 may be relative complicated due to source chaining with multiple input stream. Though, I think this could also be achieved in end-of-partition style by rearranging {{StreamTask.finishTask}} to path from {{StreamTask.triggerCheckpointAsync}}, but this will need more bookkeeping.
 # Clean code ? We don't need end-of-input detection during stop-with-savepoint. It could be tangled/confused in evaluation data-flow during stop-with-savepoint in future, eg. why end-of-input and then stripping of them.;;;","16/Feb/21 13:30;pnowojski;{quote}
To avoid deadlocks one would have to carefully thought this through.
{quote}
By this I meant a solution where downstream task is closing before processing some kind of end of partition event is dangerous and can lead to deadlocks.
{quote}
Is it worth another dedicated issue to discuss this no end-of-partition style stop-with-savepoint ? I see values of this approach
{quote}
I think it would be cleaner in the {{StreamTask}}. Currently {{StreamTask}} is maintaining {{OperatorChain#isStoppingBySyncSavepoint}} based on the currently ongoing savepoint, and is using this to decide whether to mute the {{EndOfPartitionEvent}} or not. If we had a dedicated event similar to the {{EndOfPartitionEvent}}, but one that would convey ""this is not end of input, but we are closing"", the {{StreamTask}} and {{OperatorChain}} would be cleaner. 

However we would add a bit of complexity in the network stack (another event to support). Also currently we are discussing that we might need to introduce [some kind of handshake when closing|https://github.com/apache/flink/pull/14831#issuecomment-774642652]. I'm not sure how a combined solution for both problems would look like. 

*edit: let's move this discussion to the FLINK-21133*;;;","24/Feb/21 14:02;pnowojski;For any watchers of this issue, we made a mistake to always ignore endOfInput calls when stopping with savepoint. When using {{stop-with-savepoint --drain}} the {{endOfInput}} should be called, as the intention is to terminate the job and drain all of the buffered records. Bug and fix: FLINK-21453;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
google-java-format Intellij Plugin 1.7.0.5 causes UnsupportedOperationException in IntelliJ,FLINK-21106,13354139,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,mapohl,mapohl,23/Jan/21 19:12,16/Jan/23 08:23,13/Jul/23 08:07,20/Apr/21 10:37,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"There's a problem with {{google-java-format}} Intellij plugin version {{1.7.0.5}} that causes an {{UnsupportedOperationException}} when creating a new Java class file. Besides the exception, an error dialog pops up and the newly created file is not properly formatted. A simple reformat solves the issue.

This problem is caused by a bug that got fixed in the {{google-java-format}} plugin's codebase in [45fb41a|https://github.com/google/google-java-format/commit/45fb41a7bac3dfe0726601ceb87d1c17bbf494ec].

Unfortunately, this fix got released with the plugin version {{1.8.0.1}} which we cannot upgrade to due to our limitations on sticking to Java 8 for now (see FLINK-20803).",,aljoscha,dian.fu,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20803,,,,,,,,,,"24/Jan/21 14:29;mapohl;google-java-format-1.7-patched.zip;https://issues.apache.org/jira/secure/attachment/13019264/google-java-format-1.7-patched.zip",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 16 08:23:31 UTC 2023,,,,,,,,,,"0|z0mwrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/21 19:16;mapohl;I worked around the issue for now by applying the patch below to the most recent {{google-java-format}} plugin code and build the plugin from source (it essentially reverts [c2c8a51|https://github.com/google/google-java-format/commit/c2c8a517c92fbf3553e53524057cd2ad8ae62962]):
{code:java}
diff --git a/idea_plugin/build.gradle b/idea_plugin/build.gradle
index 6ec1c21..32eb5a0 100644
--- a/idea_plugin/build.gradle
+++ b/idea_plugin/build.gradle
@@ -15,7 +15,7 @@
  */

 plugins {
-  id ""org.jetbrains.intellij"" version ""0.4.22""
+  id ""org.jetbrains.intellij"" version ""0.4.18""
 }

 repositories {
@@ -23,7 +23,7 @@ repositories {
 }

 ext {
-  googleJavaFormatVersion = '1.9'
+  googleJavaFormatVersion = '1.7'
 }

 apply plugin: 'org.jetbrains.intellij'
@@ -37,7 +37,7 @@ intellij {
 patchPluginXml {
   pluginDescription = ""Formats source code using the google-java-format tool. This version of "" +
                       ""the plugin uses version ${googleJavaFormatVersion} of the tool.""
-  version = ""${googleJavaFormatVersion}.0.0""
+  version = ""${googleJavaFormatVersion}-patched""
   sinceBuild = '201'
   untilBuild = ''
 }
{code}

You can find the patched plugin attached to this Jira issue:  [^google-java-format-1.7-patched.zip] ;;;","24/Jan/21 14:48;mapohl;I created issue [#560|https://github.com/google/google-java-format/issues/560] in the {{google-java-format}} plugin repository to request a bugfix release.;;;","25/Jan/21 16:44;aljoscha;Side note: I'm just using the most recent IntelliJ plugin version and it's fine for all of my code so far. For the rare case where more recent versions of the plugin produce formatting that is incompatible with 1.7.5 our CI would catch it and I can run {{mvn spotless:apply}} to fix it. ;;;","25/Jan/21 20:22;mapohl;Yeah, in the end it's only the newline in front of {{.stream()}} that changed between google-java-format 1.7 and 1.9 as far as I know. And you're right in saying that there's an easy workaround for this by calling spotless:apply. But let's see whether we could get a bugfix release for the google-java-format plugin. That would make the workaround obsolete again.;;;","27/Jan/21 10:02;aljoscha;Agreed!;;;","20/Apr/21 10:37;chesnay;master: 5588bc95382fc19643ae3ed93d05dbaaf80bbb6b;;;","16/Jan/23 08:23;mapohl;FYI: version 1.7.0.6 was released in under the [this link|https://plugins.jetbrains.com/plugin/8527-google-java-format/versions/stable/115957]. This information was also made available in the [Flink docs|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/flinkdev/ide_setup/#required-plugins].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UnalignedCheckpointITCase.execute failed with ""IllegalStateException""",FLINK-21104,13354062,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,hxbks2ks,hxbks2ks,23/Jan/21 03:16,22/Jun/21 13:55,13/Jul/23 08:07,01/Feb/21 17:03,1.12.2,1.13.0,,,,,,,1.12.2,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12383&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]
{code:java}
2021-01-22T15:17:34.6711152Z [ERROR] execute[Parallel union, p = 10](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 3.903 s  <<< ERROR!
2021-01-22T15:17:34.6711736Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-01-22T15:17:34.6712204Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-01-22T15:17:34.6712779Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:117)
2021-01-22T15:17:34.6713344Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-01-22T15:17:34.6713816Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-01-22T15:17:34.6714454Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-01-22T15:17:34.6714952Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-01-22T15:17:34.6715472Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:238)
2021-01-22T15:17:34.6716026Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-01-22T15:17:34.6716631Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-01-22T15:17:34.6717128Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-01-22T15:17:34.6717616Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-01-22T15:17:34.6718105Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1046)
2021-01-22T15:17:34.6718596Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2021-01-22T15:17:34.6718973Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-01-22T15:17:34.6719364Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-01-22T15:17:34.6719748Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-01-22T15:17:34.6720155Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-01-22T15:17:34.6720641Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-01-22T15:17:34.6721236Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-01-22T15:17:34.6721706Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-01-22T15:17:34.6722205Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-01-22T15:17:34.6722663Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2021-01-22T15:17:34.6723214Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-01-22T15:17:34.6723723Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-01-22T15:17:34.6724146Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-01-22T15:17:34.6724726Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-01-22T15:17:34.6725198Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-01-22T15:17:34.6725861Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-01-22T15:17:34.6726525Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-01-22T15:17:34.6727278Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-01-22T15:17:34.6727773Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-01-22T15:17:34.6728484Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-01-22T15:17:34.6728969Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-01-22T15:17:34.6729666Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-01-22T15:17:34.6730373Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-01-22T15:17:34.6731022Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-01-22T15:17:34.6731538Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-01-22T15:17:34.6732005Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-01-22T15:17:34.6732658Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
2021-01-22T15:17:34.6733458Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:118)
2021-01-22T15:17:34.6734201Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:80)
2021-01-22T15:17:34.6735053Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)
2021-01-22T15:17:34.6735609Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212)
2021-01-22T15:17:34.6736307Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203)
2021-01-22T15:17:34.6736911Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:655)
2021-01-22T15:17:34.6737449Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:81)
2021-01-22T15:17:34.6737982Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:434)
2021-01-22T15:17:34.6738537Z 	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
2021-01-22T15:17:34.6738959Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-01-22T15:17:34.6739411Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-01-22T15:17:34.6739868Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:306)
2021-01-22T15:17:34.6740384Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
2021-01-22T15:17:34.6741074Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-01-22T15:17:34.6741613Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:159)
2021-01-22T15:17:34.6742141Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-01-22T15:17:34.6742562Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-01-22T15:17:34.6742996Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-01-22T15:17:34.6743424Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-01-22T15:17:34.6743880Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-01-22T15:17:34.6744460Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-01-22T15:17:34.6744893Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-01-22T15:17:34.6745316Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-01-22T15:17:34.6745743Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-01-22T15:17:34.6746231Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-01-22T15:17:34.6746634Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-01-22T15:17:34.6747025Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-01-22T15:17:34.6747539Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-01-22T15:17:34.6748083Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-01-22T15:17:34.6748542Z 	... 4 more
2021-01-22T15:17:34.6749084Z Caused by: java.lang.IllegalStateException: Internal error, #stopPersisting for last checkpoint has not been called.
2021-01-22T15:17:34.6749801Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
2021-01-22T15:17:34.6750591Z 	at org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister.startPersisting(ChannelStatePersister.java:76)
2021-01-22T15:17:34.6751492Z 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.checkpointStarted(LocalInputChannel.java:127)
2021-01-22T15:17:34.6752218Z 	at org.apache.flink.runtime.io.network.partition.consumer.IndexedInputGate.checkpointStarted(IndexedInputGate.java:36)
2021-01-22T15:17:34.6753167Z 	at org.apache.flink.streaming.runtime.io.checkpointing.UnalignedController.preProcessFirstBarrier(UnalignedController.java:72)
2021-01-22T15:17:34.6753967Z 	at org.apache.flink.streaming.runtime.io.checkpointing.AlternatingController.preProcessFirstBarrier(AlternatingController.java:116)
2021-01-22T15:17:34.6754832Z 	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.handleBarrier(SingleCheckpointBarrierHandler.java:170)
2021-01-22T15:17:34.6755708Z 	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:128)
2021-01-22T15:17:34.6756473Z 	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:179)
2021-01-22T15:17:34.6757119Z 	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:156)
2021-01-22T15:17:34.6757726Z 	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:180)
2021-01-22T15:17:34.6758401Z 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2021-01-22T15:17:34.6758943Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:400)
2021-01-22T15:17:34.6759507Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:190)
2021-01-22T15:17:34.6760075Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:609)
2021-01-22T15:17:34.6760567Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573)
2021-01-22T15:17:34.6761032Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763)
2021-01-22T15:17:34.6761601Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:565)
2021-01-22T15:17:34.6761957Z 	at java.lang.Thread.run(Thread.java:748)
{code}",,AHeise,dian.fu,dwysakowicz,hxbks2ks,leonard,maguowei,pnowojski,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21137,,,,,,FLINK-20654,,,,,,,,,,,,,,,,,,,,,FLINK-20824,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 30 10:55:44 UTC 2021,,,,,,,,,,"0|z0mwag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/21 03:19;hxbks2ks;in release-1.12

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12390&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

 ;;;","23/Jan/21 03:22;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12393&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

 ;;;","23/Jan/21 03:23;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12391&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12394&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

 ;;;","23/Jan/21 03:27;hxbks2ks;cc [~AHeise] [~roman_khachatryan] Could you help take a look? The frequency of this failed test is very high.;;;","23/Jan/21 03:40;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12396&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2]

 ;;;","23/Jan/21 05:18;leonard;I think these fail cases are caused by FLINK-20654 and commit should be: 25d1c1b145fe2f5d34a692b3b13b731e9a26f4c0, 
 but I didn't found any PR or passed azure building about the commit.

 

 

 ;;;","23/Jan/21 14:23;arvid;Looking into it.;;;","25/Jan/21 02:23;yunta;Another instances:
https://myasuka.visualstudio.com/flink/_build/results?buildId=233&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a

https://myasuka.visualstudio.com/flink/_build/results?buildId=232&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12427&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","25/Jan/21 02:28;maguowei;Another instances:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12414&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12414&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2]

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12429&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12429&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc;;;","25/Jan/21 07:44;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12396&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12398&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12416&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","25/Jan/21 08:04;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12431&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc;;;","25/Jan/21 13:41;arvid;Merged as 1257d15fb362442dfbb39cf681161beb92b20a88 into master and as 38b51c06264b8964f99538614af9504f8f573e3b into 1.12.;;;","26/Jan/21 07:42;dwysakowicz;It seems it still appears on 1.12:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12485&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0;;;","27/Jan/21 03:55;maguowei;another instances

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12532&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12532&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12534&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc;;;","27/Jan/21 04:50;yunta;Another instance:

https://myasuka.visualstudio.com/flink/_build/results?buildId=239&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87;;;","28/Jan/21 05:53;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12573&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc;;;","29/Jan/21 00:10;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12609&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","29/Jan/21 08:07;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12624&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","30/Jan/21 10:55;pnowojski;fix merged to master as a6b909c3e59..a4ccc6e63ca (this range includes a23744795d9 for FLINK-20654). 
Backport to 1.12 as d4da833ab13..ec065872def;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mention that RocksDB ignores equals/hashCode because it works on binary data,FLINK-21073,13353761,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,rmetzger,rmetzger,21/Jan/21 14:47,28/Aug/21 11:15,13/Jul/23 08:07,20/Apr/21 02:46,,,,,,,,,1.13.0,,,,,,,Documentation,Runtime / State Backends,,,,0,pull-request-available,,,,,"See https://lists.apache.org/thread.html/ra43e2b5d388831290c293b9daf0eee0b0a5d9712543b62c83234a829%40%3Cuser.flink.apache.org%3E

",,nira-david,Paul Lin,rmetzger,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 02:46:37 UTC 2021,,,,,,,,,,"0|z0mufk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 22:43;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","19/Apr/21 11:55;rmetzger;[~flink-jira-bot] I made a pass over the PR and removed the label;;;","20/Apr/21 02:46;yunta;Merged in master:
653811c70895f13f04b8c86fbd9b80d0c0c9a69e

release-1.13:
a771b212f08e00fb2b1d06274962f61016b8d07d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot branches running against flink-docker dev-master branch,FLINK-21071,13353730,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dwysakowicz,dwysakowicz,21/Jan/21 11:21,28/May/21 07:14,13/Jul/23 08:07,25/Jan/21 14:12,1.11.0,1.12.0,,,,,,,1.11.4,1.12.2,1.13.0,,,,,Deployment / Kubernetes,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12324&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2021-01-21T10:43:02.2390215Z Jan 21 10:43:02 error: exec: ""native-k8s"": executable file not found in $PATH
2021-01-21T10:43:02.3705670Z Jan 21 10:43:02 deployment.apps ""flink-native-k8s-session-1"" deleted
2021-01-21T10:43:02.5113136Z Jan 21 10:43:02 clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
2021-01-21T10:43:07.3862359Z Jan 21 10:43:07 pod/flink-native-k8s-session-1-7d7b567485-d4n5w condition met
2021-01-21T10:43:07.3892924Z Jan 21 10:43:07 Stopping minikube ...
2021-01-21T10:43:07.4377503Z Jan 21 10:43:07 * Stopping ""minikube"" in none ...
2021-01-21T10:43:17.9120783Z Jan 21 10:43:17 * Node """" stopped.
2021-01-21T10:43:17.9168982Z Jan 21 10:43:17 [FAIL] Test script contains errors.
2021-01-21T10:43:17.9177181Z Jan 21 10:43:17 Checking for errors...
2021-01-21T10:43:17.9380030Z Jan 21 10:43:17 No errors in log files.
2021-01-21T10:43:17.9385187Z Jan 21 10:43:17 Checking for exceptions...
2021-01-21T10:43:17.9628488Z Jan 21 10:43:17 No exceptions in log files.
2021-01-21T10:43:17.9629652Z Jan 21 10:43:17 Checking for non-empty .out files...
2021-01-21T10:43:17.9650058Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/log/*.out: No such file or directory
2021-01-21T10:43:17.9656038Z Jan 21 10:43:17 No non-empty .out files.
2021-01-21T10:43:17.9656608Z Jan 21 10:43:17 
2021-01-21T10:43:17.9658043Z Jan 21 10:43:17 [FAIL] 'Run kubernetes session test (default input)' failed after 2 minutes and 3 seconds! Test exited with exit code 1
{code}",,dwysakowicz,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 21 17:17:26 UTC 2021,,,,,,,,,,"0|z0mu8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/21 17:17;chesnay;master: 3a8e06cd16480eacbbf0c10f36b8c79a6f741814
1.12: 3df27df039d509d3dc98d29c1c53597d91ceef49
1.11: e9a33bb9d56edd96e36fc39bfd9068ab0926fe54;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Configuration ""parallelism.default"" doesn't take effect for TableEnvironment#explainSql",FLINK-21069,13353720,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,jark,jark,21/Jan/21 10:25,28/May/21 07:13,13/Jul/23 08:07,25/Jan/21 11:53,,,,,,,,,1.12.2,1.13.0,,,,,,Table SQL / API,,,,,0,pull-request-available,starter,,,,"I tried the following test, and the printed node parallelism in json plan is not 5. 

{code:scala}
  @Test
  def testExplainAndExecuteSingleSink(): Unit = {
    val env = TableEnvironmentImpl.create(settings)
    val conf = new Configuration();
    conf.setInteger(""parallelism.default"", 5)
    conf.setInteger(""taskmanager.numberOfTaskSlots"", 1)
    env.getConfig.addConfiguration(conf)
    TestTableSourceSinks.createCsvTemporarySinkTable(
      env, new TableSchema(Array(""first""), Array(STRING)), ""MySink1"")
    TestTableSourceSinks.createPersonCsvTemporaryTable(env, ""MyTable"")
    println(env.explainSql(""insert into MySink1 select first from MyTable"",
      ExplainDetail.JSON_EXECUTION_PLAN))
}
{code}

I think the bug is because TableEnvironemnt#explain will not invoke  {{PlannerBase#translate(modifyOperations: util.List[ModifyOperation])}} where we configure the configuration into underlying {{StreamExecutionEnvironment}}.
",,hackergin,jark,nicholasjiang,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 11:53:31 UTC 2021,,,,,,,,,,"0|z0mu6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/21 10:33;twalthr;[~jark] is this also related to FLINK-21065;;;","21/Jan/21 10:35;jark;[~twalthr] I think it is not related to FLINK-21065. I tried to execute the SQL, and the parallelism in the JobGraph is 5. It just doesn't work for EXPLAIN. ;;;","21/Jan/21 13:20;nicholasjiang;[~jark], I could fix the bug to make configuration ""parallelism.default"" take effect. If no one work for this issue, I would like to work for this issue.;;;","21/Jan/21 15:28;jark;[~nicholasjiang], you can verify this by updating {{TableEnvironmentTest#testStreamTableEnvironmentExecutionExplain}};;;","25/Jan/21 11:53;jark;Fixed in
 - master: 0d3c785cf97e2530bf2b2cae648d34777c040cd0
 - release-1.12: eee299c1395e7cd8505b34b34935893be05acbd1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceEnumerator does not honor consumer properties,FLINK-21059,13353657,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,21/Jan/21 06:28,28/May/21 08:14,13/Jul/23 08:07,27/Jan/21 22:13,1.13.0,,,,,,,,1.12.2,1.13.0,,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,,Enumerator fails when SSL is required because the user provided properties are not used to construct the admin client. Empty properties are created and all provided properties except bootstrap.servers are ignored.,,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 22:13:04 UTC 2021,,,,,,,,,,"0|z0mtsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/21 06:29;thw;{code:java}
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45) ~[kafka-clients-2.4.1.jar:?]
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32) ~[kafka-clients-2.4.1.jar:?]
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89) ~[kafka-clients-2.4.1.jar:?]
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260) ~[kafka-clients-2.4.1.jar:?]
	at org.apache.flink.connector.kafka.source.enumerator.subscriber.TopicListSubscriber.getPartitionChanges(TopicListSubscriber.java:57) ~[flink-connector-kafka_2.12-1.13-20210119.215918-55.jar:1.13-SNAPSHOT]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.discoverAndInitializePartitionSplit(KafkaSourceEnumerator.java:196) ~[flink-connector-kafka_2.12-1.13-20210119.215918-55.jar:1.13-SNAPSHOT]
{code};;;","21/Jan/21 06:31;thw;In this case

security.protocol = PLAINTEXT

was used instead of

security.protocol = SSL

 ;;;","27/Jan/21 22:13;thw;1.12: https://github.com/apache/flink/commit/1822d138016c6efddaf98b73de420ddedb5c6fd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query_configuration.html content out date,FLINK-21049,13353479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hiscat,hiscat,20/Jan/21 08:40,19/Feb/21 03:05,13/Jul/23 08:07,19/Feb/21 03:05,1.12.1,,,,,,,,1.13.0,,,,,,,Documentation,Table SQL / API,,,,0,,,,,,"[https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/query_configuration.html]

code samples is out date.

 

{{tConfig.setIdleStateRetentionTime(Time.hours(12), Time.hours(24))}}

{{}}

Specifies a minimum and a maximum time interval for how long idle state, i.e., state which was not updated, will be retained. State will never be cleared until it was idle for less than the minimum time and will never be kept if it was idle for more than the maximum time.
When new data arrives for previously cleaned-up state, the new data will be handled as if it was the first data. This can result in previous results being overwritten.
Set to 0 (zero) to never clean-up the state.
NOTE: Cleaning up state requires additional bookkeeping which becomes less expensive for larger differences of minTime and maxTime. The difference between minTime and maxTime must be at least 5 minutes.
NOTE: Currently maxTime will be ignored and it will automatically derived from minTime as 1.5 x minTime.

Deprecated
use setIdleStateRetention(Duration) instead.
Params:
minTime – The minimum time interval for which idle state is retained. Set to 0 (zero) to never clean-up the state.
maxTime – The maximum time interval for which idle state is retained. Must be at least 5 minutes greater than minTime. Set to 0 (zero) to never clean-up the state

{{}}",,hiscat,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 06:27:16 UTC 2021,,,,,,,,,,"0|z0msow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/21 03:07;jark;Thanks [~hiscat], are you insterested in updating the docs?;;;","21/Jan/21 08:28;hiscat;[~jark] How to？Can you give me a hint？;;;","21/Jan/21 08:50;jark;[~hiscat] you can update the {{docs/dev/table/streaming/query_configuration.md}} and {{docs/dev/table/streaming/query_configuration.zh.md}} to use the new API methods. ;;;","22/Jan/21 06:27;hiscat;[~jark]，hello jark，how to build the docs on win10。I get an error when i run ./build_docs.sh -p.

it seems a recusive error.

my env:

ruby 2.6

python 2.7

 

进入目录“/e/github/MyLanPangzi/master/flink-master/docs/.rubydeps/ruby/2.6.0/gems/libv8-3.16.14.19/vendor/v8/out”
CXX(target)
/e/github/MyLanPangzi/master/flink-master/docs/.rubydeps/ruby/2.6.0/gems/libv8-3.16.14.19/vendor/v8/out/x64.release/obj.target/preparser_lib/src/allocation.o
../src/allocation.cc:123:3: fatal error: opening dependency file
E:/github/MyLanPangzi/master/flink-master/docs/.rubydeps/ruby/2.6.0/gems/libv8-3.16.14.19/vendor/v8/out/x64.release/.deps/e/github/MyLanPangzi/master/flink-master/docs/.rubydeps/ruby/2.6.0/gems/libv8-3.16.1
4.19/vendor/v8/out/x64.release/obj.target/preparser_lib/src/allocation.o.d.raw:
No such file or directory
 123 | } } // namespace v8::internal
 | ^
compilation terminated.

 

An error occurred while installing libv8 (3.16.14.19), and Bundler cannot
continue.
Make sure that `gem install libv8 -v '3.16.14.19' --source
'https://gems.ruby-china.com/'` succeeds before bundling.

In Gemfile:
 therubyracer was resolved to 0.12.3, which depends on
 libv8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_map_view and test_map_view_iterate test failed,FLINK-21046,13353448,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,hxbks2ks,hxbks2ks,20/Jan/21 06:04,20/Jan/21 06:39,13/Jul/23 08:07,20/Jan/21 06:38,1.13.0,,,,,,,,,,,,,,,API / Python,,,,,0,test-stability,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12257&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3]
{code:java}
2021-01-20T02:25:30.7107189Z E                   py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
2021-01-20T02:25:30.7108168Z E                   : java.lang.RuntimeException: Could not remove element '+I[Hi,Hi2,Hi3,Hi_, 1,1,1,2, Hi:2,Hi2:1,Hi3:1,Hi_:1, 3, hi]', should never happen.
2021-01-20T02:25:30.7108821Z E                   	at org.apache.flink.table.runtime.arrow.ArrowUtils.filterOutRetractRows(ArrowUtils.java:754)
2021-01-20T02:25:30.7109502Z E                   	at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:673)
2021-01-20T02:25:30.7110005Z E                   	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
2021-01-20T02:25:30.7110481Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-01-20T02:25:30.7110983Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2021-01-20T02:25:30.7111503Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2021-01-20T02:25:30.7112281Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2021-01-20T02:25:30.7113163Z E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2021-01-20T02:25:30.7114101Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2021-01-20T02:25:30.7115014Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2021-01-20T02:25:30.7115884Z E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2021-01-20T02:25:30.7116379Z E                   	at java.lang.Thread.run(Thread.java:748)
{code}",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20946,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 20 06:38:06 UTC 2021,,,,,,,,,,"0|z0mshs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/21 06:04;hxbks2ks;I wil fix it asap;;;","20/Jan/21 06:36;dian.fu;Thanks [~hxbks2ks]. It seems caused by FLINK-20946. I have reverted it as a quick fix~;;;","20/Jan/21 06:38;dian.fu;[~hxbks2ks] I'm closing this ticket. Let's fix this problem in FLINK-20946;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SemanticXidGeneratorTest.testXidsUniqueAmongGenerators test failed,FLINK-21044,13353440,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,hxbks2ks,hxbks2ks,20/Jan/21 05:10,22/Jun/21 13:55,13/Jul/23 08:07,21/Jan/21 09:28,1.13.0,,,,,,,,1.13.0,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12245&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20]
{code:java}
2021-01-19T15:21:35.5665063Z [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.1 s <<< FAILURE! - in org.apache.flink.connector.jdbc.xa.SemanticXidGeneratorTest
2021-01-19T15:21:35.5665936Z [ERROR] testXidsUniqueAmongGenerators(org.apache.flink.connector.jdbc.xa.SemanticXidGeneratorTest)  Time elapsed: 0.024 s  <<< FAILURE!
2021-01-19T15:21:35.5666770Z junit.framework.AssertionFailedError: expected:<10000> but was:<9999>
{code}",,AHeise,hxbks2ks,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21043,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 21 09:28:46 UTC 2021,,,,,,,,,,"0|z0msg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/21 05:12;hxbks2ks;cc [~roman_khachatryan] Could you help take a look? Thanks.;;;","20/Jan/21 05:16;roman;Sure, I'll take a look.;;;","21/Jan/21 09:28;arvid;Merged into master as 425baea4af4cdebbcb7e87d479cb04e62de26d33.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Broken links in ""dev/table/legacy_planner.zh.md""",FLINK-21039,13353411,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ZhaoWeiNan,hxbks2ks,hxbks2ks,20/Jan/21 01:58,28/May/21 07:11,13/Jul/23 08:07,20/Jan/21 08:27,1.13.0,,,,,,,,1.13.0,,,,,,,Documentation,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"dev/table/legacy_planner.zh.md contains some English links causing errors.
{code:java}
Liquid Exception: Could not find document 'dev/batch/index.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/legacy_planner.zh.md
Could not find document 'dev/batch/index.md' in tag 'link'.
{code}",,dwysakowicz,hxbks2ks,jark,maguowei,ZhaoWeiNan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 20 08:27:39 UTC 2021,,,,,,,,,,"0|z0ms9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/21 02:58;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12254&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98;;;","20/Jan/21 03:19;ZhaoWeiNan;ok,I will solve this problem as soon as possible.;;;","20/Jan/21 08:27;dwysakowicz;Fixed in c6b37999e5bcdf7e47262e12f1415415563a964c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterStopWithSavepointIT test is not run due to wrong name,FLINK-21031,13353285,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,chesnay,chesnay,19/Jan/21 14:10,28/Aug/21 11:14,13/Jul/23 08:07,12/Apr/21 12:36,1.9.0,,,,,,,,1.13.0,,,,,,,Tests,,,,,0,pull-request-available,,,,,"The {{JobMasterStopWithSavepointIT}} test is not run when testing via maven because the name is not ending on {{ITCase}}.
Coincidentally this test is currently failing, getting stuck when triggering a savepoint.

We should investigate whether this test is required (and if so fix it obviously).

ping [~kkl0u]",,dwysakowicz,kezhuw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11669,,,,,FLINK-21338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 12:36:04 UTC 2021,,,,,,,,,,"0|z0mrhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/21 12:36;dwysakowicz;Reenabled in 30baa74b012ffa5e5e9ca149b18691467fc4911c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken job restart for job with disjoint graph,FLINK-21030,13353254,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,TheoD,TheoD,19/Jan/21 10:55,26/Feb/21 13:50,13/Jul/23 08:07,26/Feb/21 13:49,1.11.2,,,,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Building on top of bugs:

https://issues.apache.org/jira/browse/FLINK-21028

 and https://issues.apache.org/jira/browse/FLINK-21029 : 

I tried to stop a Flink application on YARN via savepoint which didn't succeed due to a possible bug/racecondition in shutdown (Bug 21028). Due to some reason, Flink attempted to restart the pipeline after the failure in shutdown (21029). The bug here:

As I mentioned: My jobgraph is disjoint and the pipelines are fully isolated. Lets say the original error occured in a single task of pipeline1. Flink then restarted the entire pipeline1, but pipeline2 was shutdown successfully and switched the state to FINISHED.

My job thus was in kind of an invalid state after the attempt to stopping: One of two pipelines was running, the other was FINISHED. I guess this is kind of a bug in the restarting behavior that only all connected components of a graph are restarted, but the others aren't...",,kezhuw,klion26,liyu,mapohl,rmetzger,roman,stevenz3wu,TheoD,trohrmann,ym,yunta,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21028,FLINK-21029,,,,,,,,,,,,,,,FLINK-17170,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 13:49:52 UTC 2021,,,,,,,,,,"0|z0mrao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/21 14:29;trohrmann;Thanks for reporting this issue [~TheoD]. I agree that the behaviour you observed is a bug. I think that we should do a global failover where we restart the whole job if the stop-with-savepoint operation fails. This will ensure that all tasks will be reset to the latest checkpoint.

cc [~zhuzh];;;","20/Jan/21 07:21;zhuzh;Agreed to trigger a global failover to bring FINISHED tasks back to RUNNING if stop-with-savepoint fails. 
Maybe right after the stopped checkpoint scheduler is restarted in {{SchedulerBase#stopWithSavepoint()}}.;;;","26/Jan/21 13:47;mapohl;Just to clarify: The expected behavior is then that the command still fails but the job will resume from the most recent checkpoint after this. I would proceed with [~zhuzh]'s proposal which should be straight forward.

I was also looking into where we could test this behavior. Firstly, I identified {{JobMasterStopWithSavepointIT}} as it seems to collect all the stop-with-savepoint related usecases. Unfortunately, this test class was unmaintained for a while due to wrong naming and is failing right now (this is covered by FLINK-21031). Alternatively, I'd propose adding the test in {{SavepointITCase}} to have FLINK-21030 not being depending on FLINK-21031. Do you have any objections against that?;;;","26/Jan/21 15:52;trohrmann;No objections from my side.;;;","01/Feb/21 17:55;mapohl;I paired with [~chesnay] to look into the issue. We came up with a testcase covering the issue. We tried to emulate the problem by causing an Exception when cancelling the Source task (here, I'm not 100% sure whether this is exactly the same usecase [~TheoD] was describing in the issues description). But we were able to reproduce the described behavior. It turns out that the solution is a bit more complex than we thought (please correct me if I'm wrong here): The [current implementation of SchedulerBase.stopWithSavepoint|https://github.com/XComp/flink/blob/60fb0f5c6e4e23759541a531e034f27f496e93b7/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/SchedulerBase.java#L1080] triggers the creation of a new savepoint and returns the completed checkpoint in case of success. The stopping of the execution happens implicitly through the {{CheckpointCoordinator}} which calls {{AbstractInvokable#notifyheckpointCompleteAsync}} on each executing source task. The {{stopWithSavepoint}} method has no information about whether the actual stopping succeeds.

In our case, the stopping failed while [SchedulerBase.stopWithSavepoint|https://github.com/XComp/flink/blob/60fb0f5c6e4e23759541a531e034f27f496e93b7/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/SchedulerBase.java#L1109] waits for the {{ExecutionGraph}} to terminate. The Source task got restarted. No termination of the {{ExecutionGraph}} happens: The {{stopWithSavepoint}} times out, instead.;;;","02/Feb/21 04:09;zhuzh;Thanks for the investigation! [~mapohl]
Does this mean that the future returned by {{checkpointCoordinator.triggerSynchronousSavepoint(...)}} will no be completed exceptionally even if the savepoint fails or timed out? Because otherwise the triggering of failure handling in {{SchedulerBase#stopWithSavepoint(...)}} will not require the terminationFuture of ExecutionGraph to be completed. If so, the restarting of stopped CheckpointScheduler does not work either and I think {{checkpointCoordinator.triggerSynchronousSavepoint(...)}} needs to be fixed.;;;","02/Feb/21 07:43;mapohl;Yes, the notification of tasks happens right at the end of [CheckpointCoordinator.completePendingCheckpoints:1280|https://github.com/XComp/flink/blob/c7ed2e818a4ead70c6081117f771c54a1bbc0fc5/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1280] in {{sendAcknowledgeMessages}} without any failure handling. Hence, the errors while canceling the tasks will be not considered when completing the Future returned by [CheckpointCoordinator. triggerSynchronousSavepoint |https://github.com/XComp/flink/blob/60fb0f5c6e4e23759541a531e034f27f496e93b7/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/SchedulerBase.java#L1080].

My proposal is to change the interface of [TaskManagerGateway.notifyCheckpointComplete|https://github.com/XComp/flink/blob/2c4e0ab921ccfaf003073ee50faeae4d4e4f4c93/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/slots/TaskManagerGateway.java#L96] to return the {{CompletableFuture}} already returned by [TaskExecutorGateway.confirmCheckpoint|https://github.com/XComp/flink/blob/2c4e0ab921ccfaf003073ee50faeae4d4e4f4c93/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutorGateway.java#L152]. This can, then, be forwarded to the {{completePendingCheckpoint}} method and, then, be composed into the overall {{CompletableFuture}} returned by {{CheckpointCoordinator.triggerSynchronousSavepoint}}. Do you see any problems with this approach?;;;","02/Feb/21 07:58;zhuzh;Sounds good to me.
[~yunta] would you also take a look at [~mapohl]'s proposal to see if we missed anything?;;;","02/Feb/21 09:10;yunta;I think returning the complete future to checkpoint coordinator to check whether the synchronous savepoint is finally completed is okay.

The only thing I confuse is that shall we need such high requirement over normal checkpoints. From my understanding, current normal checkpoints could tolerate several executions fail to confirm the complete checkpoint.;;;","02/Feb/21 10:37;mapohl;You're right: My proposal would affect the checkpoint creation as well which, as far as I can tell, shouldn't be like that.;;;","02/Feb/21 11:27;trohrmann;I am not entirely sure whether adjusting the checkpoint protocol wouldn't let the scope explode a bit. I think this is quite a delicate thing for which we definitely would need to involve the people working on the {{CheckpointCoordinator}}. Maybe there is an easier solution to the problem. I'll try to investigate it.;;;","02/Feb/21 16:17;trohrmann;I think the problem can and should be solved in the scope of the {{SchedulerBase}}/{{DefaultScheduler}}. The underlying problem is that by calling {{SchedulerBase.stopWithSavepoint}} the scheduler should go into a ""stop-with-savepoint"" state where it should react differently to the available signals. In fact, the stop with savepoint is a two stage operation which needs different behaviour depending on the stage.

1. Taking a savepoint
2. Stopping the job gracefully (waiting that the notify checkpoint complete messages are sent and the {{Tasks}} shut themselves down)

If a failure occurs in the first stage, then we should fail the stop-with-savepoint operation and recover the job normally. If a failure occurs in the second stage, then it is possible that parts of the {{ExecutionGraph}} have already shut down. Hence, any task failure needs to trigger a global failover where we restart the whole job.

At the moment, the second stage is not treated properly which results into the reported problem.

The easiest solution I can think of is to predicate the stop-with-savepoint operation on the state of the {{Executions}}. Only if all {{Executions}} reach a {{FINISHED}} state, then the job can reach a {{FINISHED}} state. Hence, if one observes an {{Execution}} which reaches another terminal state, then one knows that the savepoint operation has failed. Depending on whether one is in stage 1. or 2. one needs to do nothing or trigger a global job failover.

Alternatively, one could make the state of stop-with-savepoint more explicit by letting the {{SchedulerBase}} store a {{StopWithSavepointOperation}}. This operation would then influence what {{DefaultScheduler.handleTaskFailure}} and {{DefaultScheduler.handleGlobalFailure}} do. If, for example, we are in the 2. stage, then a single task failure should trigger a global failover.

cc [~zhuzh];;;","03/Feb/21 06:51;mapohl;Thanks for the analysis, that makes sense. I went ahead with this approach.;;;","03/Feb/21 07:41;zhuzh;I guess I had misunderstood the protocol of {{CheckpointCoordnator#triggerSynchronousSavepoint(...)}}. Its returned future just indicates whether the savepoint is successfully taken, ignoring the result of {{notifyCheckpointComplete}}.
So I agree that the scheduler should take care of the case that a savepoint is taken but job does not finish in a stop-with-savepoint process.

[~trohrmann]'s solution looks good to me. Yet I think besides triggering a global failover, we also need to restart the {{CheckpointScheduler}}. There is such handling in the end of {{SchedulerBase#stopWithSavepoint}} but I think it does not work in the case that {{notifyCheckpointComplete}} fails when a savepoint is successfully taken.
;;;","03/Feb/21 10:20;trohrmann;Yes, fully agreed. Restarting the {{CheckpointCoordinator}} should happen in both stages if a failure occurs.;;;","03/Feb/21 15:45;mapohl;{quote}1. Taking a savepoint
 2. Stopping the job gracefully (waiting that the notify checkpoint complete messages are sent and the Tasks shut themselves down)
{quote}
We've been implementing two test cases to cover the use-case. Each of them covers a failure in one of the stages [~trohrmann] described above. While doing this, I ran into some timeout issue when the failure happened during phase 1 (savepoint creation). The failed savepoint does not leave the pending state even though the snapshot creation failed. It looks like there's an {{AsyncCheckpointRunnable}} (or something similar) missing that informs the {{CheckpointCoordinator}} about the failed savepoint as part of the exception handling in [SubtaskCheckpointCoordinatorImpl.checkpointState|https://github.com/XComp/flink/blob/f421c6f72b91b7897ff2246bdf4f408e3609f6f0/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java#L305]. In the test, the checkpoint gets cleaned up during cluster shutdown. [~AHeise] Shall I create a Jira issue for that or do I miss something here?;;;","26/Feb/21 13:49;trohrmann;Fixed via

1.13.0: f384b32877979d1118c0478bacae622ac7e0b330
1.12.2: e9af362f0caa16e75e66aa1403c62666e77f98f0
1.11.4: c9124884bbf8f086c00b82e76770bc31f90c60e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming application didn't stop properly ,FLINK-21028,13353251,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,TheoD,TheoD,19/Jan/21 10:45,28/Jul/21 10:42,13/Jul/23 08:07,23/Feb/21 17:47,1.11.2,1.12.2,1.13.0,,,,,,1.11.4,1.12.2,1.13.0,,,,,Runtime / Task,,,,,0,pull-request-available,,,,,"I have a Flink job running on YARN with a disjoint graph, i.e. a single job contains two independent and isolated pipelines.

From time to time, I stop the job with a savepoint like so:
{code:java}
flink stop -p ${SAVEPOINT_BASEDIR}/${FLINK_JOB_NAME}/SAVEPOINTS --yarnapplicationId=${FLINK_YARN_APPID} ${ID}{code}
A few days ago, this job suddenly didn't stop properly as usual but ran into a possible race condition.

On the CLI with stop, I received a simple timeout:
{code:java}
org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""f23290bf5fb0ecd49a4455e4a65f2eb6"".
 at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:495)
 at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:864)
 at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:487)
 at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:931)
 at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
 at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)
Caused by: java.util.concurrent.TimeoutException
 at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
 at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:493)
 ... 9 more{code}
 

The root of the problem however is that on a taskmanager, I received an exception in shutdown, which lead to restarting (a part) of the pipeline and put it back to running state, thus the console command for stopping timed out (as the job was (partially) back in running state). the exception which looks like a race condition for me in the logs is:
{code:java}
2021-01-12T06:15:15.827877+01:00 WARN org.apache.flink.runtime.taskmanager.Task Source: rawdata_source1 -> validation_source1 -> enrich_source1 -> map_json_source1 -> Sink: write_to_kafka_source1) (3/18) (bc68320cf69dd877782417a3298499d6) switched from RUNNING to FAILED.
java.util.concurrent.ExecutionException: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
 at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:161)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:130)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:80)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:302)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:576)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:544)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
 at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
 at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.emitWatermark(OperatorChain.java:642)
 at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:41)
 at org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator$WatermarkEmitter.emitWatermark(TimestampsAndWatermarksOperator.java:165)
 at org.apache.flink.streaming.runtime.operators.util.AssignerWithPeriodicWatermarksAdapter.onPeriodicEmit(AssignerWithPeriodicWatermarksAdapter.java:54)
 at org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.close(TimestampsAndWatermarksOperator.java:125)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$closeOperator$5(StreamOperatorWrapper.java:205)
 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:203)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:177)
 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
 at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:90)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:155)
 ... 13 more
Caused by: java.lang.RuntimeException
 at org.apache.flink.streaming.runtime.io.RecordWriterOutput.emitWatermark(RecordWriterOutput.java:123)
 at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:41)
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:570)
 at org.apache.flink.streaming.api.operators.ProcessOperator.processWatermark(ProcessOperator.java:72)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.emitWatermark(OperatorChain.java:638)
 ... 25 more
Caused by: java.lang.IllegalStateException
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:179)
 at org.apache.flink.runtime.io.network.buffer.BufferBuilder.append(BufferBuilder.java:83)
 at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.copyToBufferBuilder(SpanningRecordSerializer.java:90)
 at org.apache.flink.runtime.io.network.api.writer.RecordWriter.copyFromSerializerToTargetChannel(RecordWriter.java:136)
 at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.broadcastEmit(ChannelSelectorRecordWriter.java:80)
 at org.apache.flink.streaming.runtime.io.RecordWriterOutput.emitWatermark(RecordWriterOutput.java:121)
 ... 29 more{code}
I already raised a question regarding this bug on the user mailing list with the conclusion to just open a ticket here. Original on user mailing list: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Bugs-in-Streaming-job-stopping-Weird-graceful-stop-restart-for-disjoint-job-graph-td40610.html

edit:
In Flink 1.12.x this bug can probably lead to corrupted data stream and all kinds of deserialisation errors on the downstream task.
Also the same bug can leave any code (regardless if it's Flink's network stack, user code, or some 3rd party library) that sleeps interruptible in an invalid state.",,kezhuw,pnowojski,roman,TheoD,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21447,FLINK-22928,,,,,,,FLINK-21515,,,FLINK-21029,FLINK-21133,,,,,,,,,,,,,,,FLINK-21030,FLINK-23527,FLINK-23528,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 17:47:40 UTC 2021,,,,,,,,,,"0|z0mra0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/21 14:37;trohrmann;Thanks for creating this issue [~TheoD]. How does this ticket relate to FLINK-21029 and FLINK-21030? Can this ticket be closed or do you want a different aspect fixed with this issue?;;;","19/Jan/21 15:55;TheoD;Hi Till, thanks for your feedback.

Originally on the mailing list, I split it up into three different issue in order to focus on different anspects as for me, those issues occured at the same time but still are somehow different.

In this issue here, I assume definitely a bug in Flinks shutdown behavior, something like a race condition. Some Buffer was already closed even though it was still assumed to be opened. (No hardware failure or anything else, just flink internal)

In FLINK-21029, I raise a more general question: If _anything unexpected_ occurs during a requested job shutdown: Should we force shutdown or try to restart? So this is not directly related to the bug here but general behavior.

In FLINK-21030, I see a follow up error on top of this bug: If a failing job starts to recover, it seems to don't recover the entire pipeline but only the connected components.

 

I wasn't sure whether to open a single ticket for all of them or just creating one, but seeing your comment on FLINK-21029, I think it was a good decision to split them up: We can discuss there how Flink should handle any failure in shutdown whereas this bug here stays ""clean"" and focused on why under this special circumstances there was this race condition.;;;","19/Jan/21 16:15;trohrmann;Alright, thanks for the clarification [~TheoD]. I think it makes sense to investigate whether the Task failure was a coincidence or caused by the stop-with-savepoint command. What would definitely help is if you could provide us with the debug logs of the run where Flink failed.

cc [~AHeise].;;;","19/Jan/21 16:29;TheoD;Sadly, I don't have any DEBUG logs here as I can't reproduce the issue. This job runs since ~ 2 years and is stopped and restarted afterwards once a week and this is the first time that this happened.

There isn't anything meaningful in the INFO-Logs: I can see on all taskmanagers the logs that all checkpoints succeeded up to the time when I called ""stop"". Afterwards, there are only messages from various tasks that they switched their state from RUNNING to FINISHED. Only this one task prints this exception which is already printed above and switched its state to FAILING. 

It might be helpful to note that the job runs on 18 task managers, each having one slot. The cluster has 10 or 15 nodes, so definitely less than 18. Some machines thus run multiple task managers (Shouldn't be a problem, but might be helpful?). 

Note also the feature of the ""distinct graph"" here: We have a parallelism of 18 (18 Task Managers, 1 Slot), and we have one particual subgraph with parallelism 18 and some other subgraphs with lower parallelism. In total, there are like 200 tasks and some task managers run multiple subgraphs in parallel, even though they only have one slot... Maybe that is part of the issue here?;;;","08/Feb/21 13:20;trohrmann;Stephan suggested that this might be race condition between shutting down and the old sources still doing something.

cc [~pnowojski];;;","09/Feb/21 16:47;pnowojski;[~TheoD], could you post full logs, including those from Job Manager and at least from those two Task Managers that:
{quote}
two tasks had something that looks like a racecondition
{quote}
? Frankly I stared at the code and the provided stack trace for a better part of last couple of hours and the only potential explanation that I could come up with, was if someone had interrupted the task's thread (1) while it was waiting for the buffer and this interrupted exception would later be swallowed by someone else (2). The problem might be that {{RecordWriter#finishBufferBuilder}} leaves the finished buffer without setting it to {{null}}, which is later detected as an {{IllegalStateException}}. If that's the case, this problem was accidentally fixed in FLINK-19297.

(1) If that was Flink cancelling those two tasks, there must be some reason behind this cancellation attempt. Some different third task, would have to have failed due to a completely different reason. Alternatively, are you sure that neither you, nor anyone else has cancelled the job while it was being stopped? Or maybe something completely different, some watchdog script issued SIGINT to the Flink process? 

The second part (2), might be actually something somehow expected. We are first interrupting the source function thread, and later the task thread. During this period maybe {{StreamTask}} managed to start closing itself and it hit this illegal state problem?

TL;DR I'm not sure, but my only theory is a combination of something else interrupting a task plus a minor bug fixed accidentally in FLINK-19297.;;;","10/Feb/21 00:08;TheoD;Hi [~pnowojski] , there really isn't anything else in the logs. I only see/saw succeeded checkpoints all the time and than the printed error on two taskmanagers following all respectively a lot to restart while others were stopped.

But as you post your questions: Can this be related to the disjoint job graph setup? I once realized that if (on YARN) I start my job and have both disjoint pipelines with parallelism 1 setup and have yarn configurted with 1 slot per taskmanager, I still have only one task manager running so that both graphs run on the same taskmanager. => Might it be that the stop tried to stop both ""graphs"" and those graphs caused kind of a race against each other? (I have no idea about flink internals...);;;","10/Feb/21 02:53;kezhuw;[~trohrmann] [~pnowojski] Thanks for renew, I used to think it could caused by FLINK-20888, but failed to find a concrete path:(.

[~pnowojski] Could it be {{sourceThread.interrupt()}} in {{SourceStreamTask.cancelTask}} ? {{SourceStreamTask}} itself is concurrent with user function.

After FLINK-19297, symptom could be different. A null but not full buffer with later {{EndOfPartitionEvent}} could introduce a corrupted record. This could be case of [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/flink-kryo-exception-td41228.html].

Besides above, I found that {{BufferWritingResultPartition.requestNewBufferBuilderFromPool}} turns {{InterruptedException}} to {{IOException}} with no cause. This could be problematic. I saw {{SourceStreamTask}} depends on {{InterruptedException}} in cancelling.

Actually, I am kind of confused about the pipeline. [~TheoD] I saw the chained task was named as {{Source: rawdata_source1 -> validation_source1 -> enrich_source1 -> map_json_source1 -> Sink: write_to_kafka_source1)}}. If this is a source to sink chained task, how this could happen ? There is no output at all.;;;","10/Feb/21 08:13;pnowojski;{quote}
Can this be related to the disjoint job graph setup? 
{quote}
[~TheoD] I don't think so. At least I do not see how, but as I don't understand how is it happening, I might be wrong.

{quote}
After FLINK-19297, symptom could be different. A null but not full buffer with later EndOfPartitionEvent could introduce a corrupted record.
{quote}
[~kezhuw] you might be right. Keep also in mind, that by looking at the stack trace here, this ""later"", doesn't necessarily have to be {{EndOfPartitionEvent}}, but ""later"" could be records that are being emitted during the closing procedure:

{noformat}
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:161)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:130)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:80)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:302)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:576)
{noformat}
{quote}
Actually, I am kind of confused about the pipeline. Theo Diefenthal I saw the chained task was named as Source: rawdata_source1 -> validation_source1 -> enrich_source1 -> map_json_source1 -> Sink: write_to_kafka_source1). If this is a source to sink chained task, how this could happen ? There is no output at all.
{quote}
Good question. I second that (maybe a side output?). Also it's suspicious that on the stack trace we have 6 operator wrappers, while the task name suggests there should be 5.

[~TheoD], please share the logs with us, we might spot something that you missed, or at the very least we would understand the context better. Could you also share the JobGraph with us?

{quote}
Piotr Nowojski Could it be sourceThread.interrupt() in SourceStreamTask.cancelTask ? SourceStreamTask itself is concurrent with user function.
{quote}
It could be, that's what I'm suspecting, but something would have to trigger {{SourceStreamTask}} cancellation in the first place, or something completely not related to Flink, would have to send SIGINT to the Flink process.
;;;","10/Feb/21 08:29;kezhuw;{quote}but something would have to trigger SourceStreamTask cancellation in the first place
{quote}
[~pnowojski] I saw path {{StreamTask.notifyCheckpointComplete}} ==> {{SourceStreamTask.finishTask}} ==> {{SourceStreamTask.cancelTask}} ==> {{SourceStreamTask.sourceThread.interrupt}}. It is part of stop-with-savepoint procedure.;;;","10/Feb/21 14:39;pnowojski;Yes, thanks [~kezhuw] good catch. I think you are right. That explains the problem. For clean shutdown we should never interrupt threads. While:

StreamTask.notifyCheckpointComplete ==> SourceStreamTask.finishTask ==> SourceStreamTask.cancelTask ==> SourceStreamTask.sourceThread.interrupt

is doing just that leaving the network stack in the inconsistent state, and later we are entering the ""clean shutdown procedure"", which attempts to write more records to the network stack. Here (in 1.11.2) it display itself with

{noformat}
Caused by: java.lang.IllegalStateException
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:179)
 at org.apache.flink.runtime.io.network.buffer.BufferBuilder.append(BufferBuilder.java:83)
{noformat}

While in Flink 1.12.x all kinds of stream corruptions exceptions can happen. I'm not saying those are the only potential outcomes. If interrupted exception is thrown/caught from the user code/3rd party library it can also be left in an invalid state, causing whole bunch of different problems.;;;","11/Feb/21 09:43;trohrmann;Shall we make this ticket a blocker for the 1.13.0 release? It sounds like we want to quickly fix it.;;;","23/Feb/21 11:49;pnowojski;merged commit 19ceee0 into apache:master;;;","23/Feb/21 17:47;pnowojski;merged commit 463d9e3 into apache:release-1.11
merged commit 54beaa3 into apache:release-1.12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align column list specification with Hive in INSERT statement,FLINK-21026,13353232,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,docete,docete,docete,19/Jan/21 08:57,28/May/21 08:06,13/Jul/23 08:07,26/Jan/21 07:20,,,,,,,,,1.13.0,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,"HIVE-9481 allows column list specification in INSERT statement. The syntax is:
{code:java}
INSERT INTO TABLE table_name 
[PARTITION (partcol1[=val1], partcol2[=val2] ...)] 
[(column list)] 
select_statement FROM from_statement
{code}
In the MeanWhile, flink introduces PARTITION syntax that the PARTITION clause appears after the COLUMN LIST clause. It looks weird and luckily we don't support COLUMN LIST clause now.  I think it'a good chance to align this with Hive now.

 
  
  ",,jark,leonard,lzljs3620320,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 07:20:13 UTC 2021,,,,,,,,,,"0|z0mr5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/21 07:20;lzljs3620320;master (release-1.13): 35247bb07ccba43ac537a914b82d84da17aca8fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic properties get exposed to job's main method if user parameters are passed,FLINK-21024,13353222,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,mapohl,mapohl,19/Jan/21 08:42,28/May/21 07:12,13/Jul/23 08:07,21/Jan/21 10:53,1.12.1,,,,,,,,1.12.2,1.13.0,,,,,,Runtime / Configuration,,,,,0,pull-request-available,starter,,,,"A bug was identified in the [user ML|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-Application-cluster-standalone-job-some-JVM-Options-added-to-Program-Arguments-td40719.html] by Alexey exposing dynamic properties into the job user code.

I was able to reproduce this issue by slightly adapting the WordCount example ({{org.apache.flink.streaming.examples.wordcount.WordCount2}} in attached  [^WordCount.jar] ).

Initiating a standalone job without using the {{--input}} parameter will result in printing an empty array:
{code}
./bin/standalone-job.sh start --job-classname org.apache.flink.streaming.examples.wordcount.WordCount2
{code}
The corresponding {{*.out}} file looks like this:
{code}
[]
Executing WordCount2 example with default input data set.
Use --input to specify file input.
Printing result to stdout. Use --output to specify output path.
{code}

In contrast, initiating the standalone job using the {{--input}} parameter will expose the dynamic properties:
{code}
./bin/standalone-job.sh start --job-classname org.apache.flink.streaming.examples.wordcount.WordCount2 --input /opt/flink/config/flink-conf.yaml
{code}
Resulting in the following output:
{code}
[--input, /opt/flink/config/flink-conf.yaml, -D, jobmanager.memory.off-heap.size=134217728b, -D, jobmanager.memory.jvm-overhead.min=201326592b, -D, jobmanager.memory.jvm-metaspace.size=268435456b, -D, jobmanager.memory.heap.size=1073741824b, -D, jobmanager.memory.jvm-overhead.max=201326592b]
Printing result to stdout. Use --output to specify output path.
{code}

Interestingly, this cannot be reproduced on a local standalone session cluster.",,mapohl,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/21 08:44;mapohl;WordCount.jar;https://issues.apache.org/jira/secure/attachment/13018992/WordCount.jar",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 21 10:53:04 UTC 2021,,,,,,,,,,"0|z0mr3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/21 09:36;xtsong;I think the problem is that, `standalone-job.sh` failed to make sure all flink options come before the job arguments.

{{ClusterEntrypoint}} reads flink options from the beginning of the JVM process argument list. It stops at the first unrecognized option, and treat the rest as the job arguments.

This is not reproduced on a standalone session cluster because job arguments are provided at submitting the job rather than at starting the cluster.

I can try to provide a quick fix.;;;","19/Jan/21 10:38;mapohl;Interesting finding. Thanks for looking into that, [~xintongsong]. [bin/mesos-jobmanager.sh|https://github.com/apache/flink/blob/0a51d85255b9c7480eb5e939d88e9ccc5e98af69/flink-dist/src/main/flink-bin/mesos-bin/mesos-jobmanager.sh#L32] needs to be adapted accordingly.;;;","21/Jan/21 10:53;xtsong;Fixed via
* master (1.13): 1678b607ec16d58c595b40758192ed1d555b37d8
* release-1.12: b95cf42ccc78bc22394537370dfabd778cff2f03;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner does not ingest timestamp into StreamRecord,FLINK-21013,13353114,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,twalthr,twalthr,18/Jan/21 16:23,28/May/21 09:02,13/Jul/23 08:07,09/Feb/21 10:38,1.12.0,,,,,,,,1.11.4,1.12.2,1.13.0,,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,,,"Currently, the rowtime attribute is not put into the StreamRecord when leaving the Table API to DataStream API. The legacy planner supports this, but the timestamp is null when using the Blink planner.

{code}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
            EnvironmentSettings settings =
                    EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();
            StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);

        DataStream<Order> orderA =
                env.fromCollection(
                        Arrays.asList(
                                new Order(1L, ""beer"", 3),
                                new Order(1L, ""diaper"", 4),
                                new Order(3L, ""rubber"", 2)));

        DataStream<Order> orderB =
                orderA.assignTimestampsAndWatermarks(
                        new AssignerWithPunctuatedWatermarks<Order>() {
                            @Nullable
                            @Override
                            public Watermark checkAndGetNextWatermark(
                                    Order lastElement, long extractedTimestamp) {
                                return new Watermark(extractedTimestamp);
                            }

                            @Override
                            public long extractTimestamp(Order element, long recordTimestamp) {
                                return element.user;
                            }
                        });

        Table tableA = tEnv.fromDataStream(orderB, $(""user"").rowtime(), $(""product""), $(""amount""));

        // union the two tables
        Table result = tEnv.sqlQuery(""SELECT * FROM "" + tableA);

        tEnv.toAppendStream(result, Row.class)
                .process(
                        new ProcessFunction<Row, Row>() {
                            @Override
                            public void processElement(Row value, Context ctx, Collector<Row> out)
                                    throws Exception {
                                System.out.println(""TIMESTAMP"" + ctx.timestamp());
                            }
                        })
                .print();

        env.execute();
{code}

",,begginghard,gaoyunhaii,godfreyhe,jark,leonard,libenchao,twalthr,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 09 10:38:23 UTC 2021,,,,,,,,,,"0|z0mqfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/21 16:27;twalthr;[~jark] is this due to the recent temporal refactoring or existing since the Blink merge?;;;","19/Jan/21 03:26;jark;I think this is a missing feature since Blink merge. 
See the difference between blink-planner[1] and old-planner [2].

[1] https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLegacySink.scala#L133
[2] https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/DataStreamConversions.scala#L108;;;","26/Jan/21 08:31;twalthr;[~ykt836] How should we continue with this? FLIP-136 will definitely fix it for 1.13 but we should also backport a fix to 1.12 and 1.11. A user complained on the user mailing list that it is currently not possible to define a time-based operations in DataStream API after a call to Table API.;;;","26/Jan/21 08:44;ykt836;Yes, I also think this bug deserved to be back ported. [~Leonard Xu] would you like to fix this before FLIP 162?;;;","26/Jan/21 08:55;leonard;I'd like to fix and back port to 1.11/1.12, please assign this ticket to me.;;;","26/Jan/21 09:02;twalthr;Thanks [~Leonard Xu]. I assigned it to you.;;;","08/Feb/21 05:15;ym;Hey [~Leonard Xu] and [~jark], what is the status of this ticket? 

I saw the PRs are ready to review, how far away these PRs to be merged?

I am asking because this ticket is marked as a blocker for Flink 1.12.2 release, wondering is it still the case?;;;","08/Feb/21 07:47;twalthr;[~ym] I will review it today. We should fix this.;;;","09/Feb/21 10:38;twalthr;Fixed in 1.13.0: e955ec1d1913a2c244a1e3835304c97fc97b3ea1
Fixed in 1.12.2: 20030c255849cc1f0a01045fdaa8a16d883726b6
Fixed in 1.11.4: 9984b5869b2f35b1fb8ce75e09ad73574d9f91ec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroFileFormatFactory uses non-deserializable lambda function,FLINK-21012,13353084,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,airblader,airblader,airblader,18/Jan/21 14:04,28/May/21 09:05,13/Jul/23 08:07,14/Apr/21 02:23,1.12.0,1.12.1,,,,,,,1.12.3,1.13.0,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"In AvroFileFormatFactory#RowDataAvroWriterFactory a lambda function is used to create the factory. This can causes
{code:java}
Caused by: java.lang.IllegalArgumentException: Invalid lambda deserialization{code}
There's other similar issues like FLINK-20147, FLINK-18857 and FLINK-18006 and the solution so far seems to have been to replace the lambda with an anonymous class instead.",,airblader,dwysakowicz,jark,leonard,lzljs3620320,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 02:23:07 UTC 2021,,,,,,,,,,"0|z0mq8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/21 14:45;twalthr;CC [~lzljs3620320];;;","12/Apr/21 06:35;dwysakowicz;[~jark] [~lzljs3620320]  Do you plan to include it in 1.13?;;;","12/Apr/21 07:52;airblader;I can take this over, would be great to put this into 1.13.;;;","12/Apr/21 07:57;dwysakowicz;Thanks [~airblader] !;;;","13/Apr/21 06:41;lzljs3620320;Thanks [~airblader], I'll take a look~;;;","13/Apr/21 11:27;lzljs3620320;[~airblader] Can you create a pull request for release-1.12?;;;","13/Apr/21 11:55;airblader;[~lzljs3620320] Sure, I've opened it. It's simply a cherry-pick onto release-1.12, I assume that's all I need to do, right?;;;","14/Apr/21 02:21;lzljs3620320;[~airblader] Yes~ Then we can make sure the test passes and we can merge it.;;;","14/Apr/21 02:23;lzljs3620320;master (1.13): 7cb5d1c1977a0ac65e0954ea0abf49461a4c0e6c

1.12: 0073a4beb7ecc117dbd16960bfd3aa17e948dd79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not disable certain options in Elasticsearch 7 connector,FLINK-21009,13353048,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,18/Jan/21 10:26,28/May/21 07:11,13/Jul/23 08:07,19/Jan/21 16:01,1.11.3,1.12.1,,,,,,,1.11.4,1.12.2,1.13.0,,,,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,,"The underlying elasticsearch client disables certain options with a value of -1. Those options are:
* bulk flush max actions
* bulk flush max size
* bulk flush interval

Because of custom checks in our builder for ES 7 -1 is not a valid value for these options. We should fix those preconditions to make them accept -1.",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 19 16:01:53 UTC 2021,,,,,,,,,,"0|z0mq14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/21 16:01;dwysakowicz;Fixed in:
* master
** 9f24c486c87238aeee0083f57ca4e1b132c22bfb
* 1.12.2
** e7995106793c1e75ab41ac1c242370d9f7eb9569
* 1.11.4
** 11c2adb86a3e4cff5617a8ec66e4300d92edd2eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Residual HA related Kubernetes ConfigMaps and ZooKeeper nodes when cluster entrypoint received SIGTERM in shutdown,FLINK-21008,13353039,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,wangyang0918,wangyang0918,18/Jan/21 09:49,20/Apr/22 10:21,13/Jul/23 08:07,09/Apr/21 10:15,1.11.3,1.12.1,1.13.0,,,,,,1.11.4,1.12.3,1.13.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Recently, in our internal use case for native K8s integration with K8s HA enabled, we found that the leader related ConfigMaps could be residual in some corner situations.

After some investigations, I think it is possibly caused by the inappropriate shutdown process.

In {{ClusterEntrypoint#shutDownAsync}}, we first call the {{closeClusterComponent}}, which also includes deregistering the Flink application from cluster management(e.g. Yarn, K8s). Then we call the {{stopClusterServices}} and {{cleanupDirectories}}. Imagine that the cluster management do the deregister very fast, the JobManager process receives SIGNAL 15 before or is being executing the {{stopClusterServices}} and {{cleanupDirectories}}. The jvm process will directly exit then. So the two methods may not be executed.",,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26772,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 10:15:18 UTC 2021,,,,,,,,,,"0|z0mpz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/21 09:58;trohrmann;Is the problem that deregistering the application from K8s will trigger K8s to send a SIGTERM to the JobManager process? I guess then this behaves a bit differently from Yarn and needs to be changed. Is there a way to let the process properly terminate but still deleting the K8s resource (e.g. deployment)? Maybe we need to register a shutdown hook which waits on the {{ClusterEntrypoint}} to have completed its shutdown.;;;","19/Jan/21 10:19;wangyang0918;You are right. Deregistering the application from K8s(aka delete the JobManager deployment) will let the kubelet send a SIGTERM to JobManager process. But the Yarn has the same behavior. The reason why we do not come into this issue when deploying Flink application on Yarn is that the SIGTERM is sent a little late. Because Yarn ResourceManager tells NodeManager to kill(SIGTERM and followed by a SIGKILL) the JobManager via heartbeat, which is 3 seconds by default. However, on Kubernetes, kubelet is informed via watcher, which is no delay.

Assume that the cluster entrypoint costs more than 3 seconds for the internal clean up( {{stopClusterServices}} and {{cleanupDirectories}}), we will run into the same situation on Yarn deployment.;;;","19/Jan/21 10:41;trohrmann;I see, then an alternative solution would be to signal the external system to shut down after the whole Flink clean up has been done. The problem here is that the communication logic with the external client is encapsulated in the {{ResourceManager}} which at this point is already shut down.;;;","19/Jan/21 10:56;wangyang0918;Actually, maybe we do not need to respond to the SIGTERM in Yarn/K8s deployment since the cluster entrypoint will call the {{System.exit()}} eventually.;;;","19/Jan/21 12:16;trohrmann;Is it possible to ignore the SIGTERM signal in the JVM?;;;","19/Jan/21 15:50;wangyang0918;We could implement an empty {{SignalHandler}}. And then register it in the {{KubernetesApplicationClusterEntrypoint}} and {{KubernetesSessionClusterEntrypoint}}. For Yarn related entrypoint, we could also have this done.;;;","19/Jan/21 16:42;trohrmann;I think I would be in favour of triggering a {{ClusterEntrypoint.closeAsync()}} if we see a SIGTERM and then wait on the completion. That way, we also cover the case properly when someone sends us SIGTERM and the system wasn't shutting down. What do you think?;;;","20/Jan/21 03:25;wangyang0918;Triggering {{ClusterEntrypoint.closeAsync()}} for the SIGTERM is a better solution. Since ignoring the SIGTERM could cause other issues. For example, deleting a pod gracefully is impossible now.;;;","20/Jan/21 08:57;trohrmann;Do you wanna try to take a stab at the problem [~fly_in_gis]?;;;","20/Jan/21 10:41;wangyang0918;Yes. I will try to get this done by letting {{SignalHandler}} could trigger the {{ClusterEntrypoint.closeAsync()}} for SIGTERM.;;;","22/Jan/21 15:39;wangyang0918;After more consideration, I think it might be wrong to do the {{ClusterEntrypoint.closeAsync()}} when retrieved the SIGTERM. Imagine that the JobManager entrypoint is running normally, we send a SIGTERM and then the HA related data will be cleaned up. It is not right.

Only when we are calling the {{shutDownAsync}} with {{cleanupHaData = true}}, but SIGTERM received before {{haServices.closeAndCleanupAllData()}} is executed. In such situation, we will have residual HA related ConfigMaps and ZooKeeper Nodes.

So maybe using a shutdown hook to do the {{haServices.closeAndCleanupAllData()}} could solve the problem. And whether {{ClusterEntrypoint#shutDownAsync}} is fully executed or not does not matter.;;;","22/Jan/21 16:32;trohrmann;Wouldn't it work if {{ClusterEntrypoint.closeAsync()}} calls 

{code}
shutDownAsync(
                        ApplicationStatus.UNKNOWN,
                        ""Cluster entrypoint has been closed externally."",
                        false)
{code}

meaning that a SIGTERM won't clean up the HA data. Now if the cluster entrypoint wants to shut down (e.g. because the job of the per-job cluster has finished), it will call {{shutDownAsync(.., .., true)}} which will clean up the HA data.;;;","25/Jan/21 08:46;wangyang0918;This residual issue could be resolved if {{ClusterEntrypoint.closeAsync()}} is executed with {{cleanupHaData = false}}. I also think it is the right behavior. ""closed externally"" should not clean up the HA related data.

Moreover, {{ClusterEntrypoint.shutDownAsync}} will be guarded by {{lock}} to guarantee that it is fully executed when received SIGTERM.;;;","23/Mar/21 13:52;trohrmann;If we don't manage to complete this ticket for {{1.13.0}} then it will be bumped to the next release.;;;","26/Mar/21 13:58;trohrmann;[~fly_in_gis] do you think that we can fix this problem for the {{1.13.0}} release?;;;","27/Mar/21 15:57;wangyang0918;Thanks for the reminding. I will try to provide a PR before next Monday.;;;","09/Apr/21 10:15;trohrmann;Fixed via

master: e2943006aec1febcfd1c33eef1b1f44367019a98
1.12.3: f541f9da2ccb7c07bc349fe32c8c8ded75cac4aa
1.11.4: 35babaf865550f0bfdcca0c5823559cccdb5140d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HBaseTablePlanTest tests failed in haoop 3.1.3 with ""java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V""",FLINK-21006,13352990,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,hxbks2ks,hxbks2ks,18/Jan/21 07:35,28/May/21 08:14,13/Jul/23 08:07,27/Jan/21 15:22,1.13.0,,,,,,,,1.13.0,,,,,,,Connectors / HBase,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12159&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51]
{code:java}
2021-01-15T22:48:58.1843544Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
2021-01-15T22:48:58.1844358Z 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
2021-01-15T22:48:58.1845035Z 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
2021-01-15T22:48:58.1845805Z 	at org.apache.flink.connector.hbase.options.HBaseOptions.getHBaseConfiguration(HBaseOptions.java:157)
2021-01-15T22:48:58.1846960Z 	at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:73)
2021-01-15T22:48:58.1848020Z 	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:119)
2021-01-15T22:48:58.1848574Z 	... 49 more
{code}
The exception seems that the different version of guava caused. 

 ",,dwysakowicz,hxbks2ks,jark,leonard,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20812,,,,,,,,,,,,,,,,,,,,,FLINK-19445,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 15:22:17 UTC 2021,,,,,,,,,,"0|z0mpo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/21 02:33;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12199&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51]

 ;;;","19/Jan/21 06:29;jark;Fixed in master: c1b700ba0ba3e2361b82434441946eb26d25afea;;;","22/Jan/21 03:07;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12349&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51]

Maybe it hasn't been fixed yet;;;","22/Jan/21 04:09;jark;I think the problem is the guava conflicts instead of we are not using the correct {{Configuration#set}} method. ;;;","22/Jan/21 04:48;jark;The set() method of Hadoop Configuration calls Predication.checkArgument, see: https://github.com/apache/hadoop/blob/branch-3.1.3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java#L1357

Hadoop upgrades guave to 27.0-jre in 3.x, see: https://issues.apache.org/jira/browse/HADOOP-16213

Hive also encountered this problem, and fix this by using guava 27.0-jre, see: https://issues.apache.org/jira/browse/HIVE-22915

I think the problem maybe we have a guava conflicts in the testing environment. ;;;","22/Jan/21 06:09;jark;It maybe related to this issue FLINK-19971 which changes the dependencies of hbase connector a lot. 

Could you also help to check this ? [~rmetzger] [~mgergely];;;","25/Jan/21 07:48;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12396&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","25/Jan/21 10:37;rmetzger;I agree with [~jark] that the issue seems to have started appearing with FLINK-19971. Let's see if [~mgergely] has time to pick it up, otherwise, I'll take a look. ;;;","26/Jan/21 07:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12483&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","27/Jan/21 07:22;rmetzger;Okay, I'm taking a look at this now ... ;;;","27/Jan/21 15:22;rmetzger;Resolved in https://github.com/apache/flink/commit/dac3e72c6777678af8d2dbbbd61aaff1e385981d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-raw-1.12.jar does not exist,FLINK-20998,13352643,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,svend,svend,svend,16/Jan/21 11:02,28/May/21 07:12,13/Jul/23 08:07,22/Jan/21 05:51,1.12.0,,,,,,,,1.12.2,1.13.0,,,,,,Documentation,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"[Flink Raw format documentation|https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/raw.html#dependencies] currently states that the {{flink-raw-x.yz.jar}} is required in order to use this format.

As I understand, such jar does not exist though:
 * [broken link to mavenrepository|https://mvnrepository.com/artifact/org.apache.flink/flink-raw]
 * [the list of format in the flink repo on github does not contain raw|https://github.com/apache/flink/tree/master/flink-formats]

The raw format seems to currently be present [in the blink table planner|https://github.com/apache/flink/tree/master/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw]

I guess either documentation should be updated, or a specific jar should to be created? I'm happy to help with either.

 ",,jark,svend,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 21 05:37:36 UTC 2021,,,,,,,,,,"0|z0mnjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/21 14:06;jark;Yes. This is a documentation mistake. The raw format is an built-in format distriubted with blink-table-planner.;;;","21/Jan/21 05:37;jark;Fixed in
 - master: b607ef2ca9eedb4a82656587a052cda11f7088e2
 - release-1.12: 2a9c0b00a2b15d0363b41f66d9f595d9402f3b9a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnTestBaseTest fails due to NPE,FLINK-20997,13352637,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Paul Lin,Paul Lin,Paul Lin,16/Jan/21 09:59,28/May/21 07:10,13/Jul/23 08:07,18/Jan/21 12:22,1.11.3,1.12.0,,,,,,,1.13.0,,,,,,,Deployment / YARN,Tests,,,,0,pull-request-available,,,,,"YarnTestBase depends on classpaths generated by Maven dependency plugin in `package` phase, but YarnTestBaseTest is a unit test that executed in `test` phase (which is before `package`),   so it's unable to find `yarn.classpath` and causes NPE.",,Paul Lin,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/21 09:54;Paul Lin;截屏2021-01-16 17.51.54.png;https://issues.apache.org/jira/secure/attachment/13018869/%E6%88%AA%E5%B1%8F2021-01-16+17.51.54.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 18 12:22:28 UTC 2021,,,,,,,,,,"0|z0mni0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/21 12:22;rmetzger;Resolved in master in https://github.com/apache/flink/commit/5395c5fdb14fa1cd2d55f5d870dd0f0439671805;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint cleanup can kill JobMaster,FLINK-20992,13352479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,trohrmann,trohrmann,15/Jan/21 13:07,08/Sep/21 07:33,13/Jul/23 08:07,20/Jan/21 09:40,1.12.0,,,,,,,,1.12.2,1.13.0,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"A user reported that cancelling a job can lead to an uncaught exception which kills the {{JobMaster}}. The problem seems to be that the {{CheckpointsCleaner}} might trigger {{CheckpointCoordinator}} actions after the job has reached a terminal state and, thus, is shut down. Apparently, we do not properly manage the lifecycles of {{CheckpointCoordinator}} and checkpoint post clean up actions.

The uncaught exception is 

{code}
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@41554407 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5d0ec6f7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 25977] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063
 at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830
 at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326
 at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533
 at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622
 at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668
 at org.apache.flink.runtime.concurrent.ScheduledExecutorServiceAdapter.execute(ScheduledExecutorServiceAdapter.java:62
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.scheduleTriggerRequest(CheckpointCoordinator.java:1152
 at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.lambda$cleanCheckpoint$0(CheckpointsCleaner.java:58
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624
 at java.lang.Thread.run(Thread.java:748 undefined)
{code}

cc [~roman_khachatryan].

https://lists.apache.org/thread.html/r75901008d88163560aabb8ab6fc458cd9d4f19076e03ae85e00f9a3a%40%3Cuser.flink.apache.org%3E",,caozhen1937,dwysakowicz,Paul Lin,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20993,FLINK-23874,,,,,FLINK-17073,,,,,FLINK-21053,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 20 10:16:29 UTC 2021,,,,,,,,,,"0|z0mmiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/21 11:39;roman;I've published a simple PR to address the issue directly.

 

However, this is not the first time we hit this RejectedExecutionException problem (e.g. FLINK-18290).

I think the reason is that the executors used by coordinator aren't aware of it's lifecycle.

So I propose to:
 # Create executors inside CheckpointCoordinator (both io & timer thread pools)
 # Check isShutdown() in their error handlers (if yes and it's RejectedExecutionException then just log; otherwise delegate to FatalExitExceptionHandler)
 # (this will allow to remove such RejectedExecutionException checks from coordinator code)

 

Additionally, I found that during the shutting down we don't wait for checkpoint cleanup to complete (or any other tasks submitted to executors):
{code:java}
checkpointCoordinatorTimer.shutdownNow() // in ExecutionGraph
scheduledExecutorService.shutdownNow(); // in JobManagerSharedServices
{code}
So only currently executing actions will complete, but not any queued.

I think we SHOULD complete cleanup on shutdown and propose the following:
 # Replace shutdownNow with shutdown to allow cleanup to finish
 # Add awaitTermination (with timeout)
 # At least log the result of shutdownNow (list of runnables)

 

 WDYT [~trohrmann]?

I'd create separate tickets for the latter two issues.;;;","18/Jan/21 17:49;trohrmann;Thanks for creating this PR [~roman_khachatryan].

I agree that there is a lifecycle problem of the {{CheckpointCoordinator}} and its services. However, I am not sure whether we should harden the exception handling logic or rather make the {{CheckpointCoordinator}} no longer enqueuing something into the {{Executor}} after it is shut down. To me it makes more sense that the {{CheckpointCoordinator}} which knows about its state is responsible for making sure that we only execute a {{Runnable}} if we are still running.

I think you are also right that we currently don't wait for the checkpoint cleanup to complete. I think the component responsible for the clean up tasks should make sure that they are completed before shutting down or hand them over to a new owner who is responsible for them. Hence, if the {{CheckpointCoordinator}} is responsible, then the {{CheckpointCoordinator.shutdown}} method should make sure that all checkpoints are cleaned up. Alternatively, {{CheckpointCoordinator.shutdown}} could return a {{CompletableFuture}} which is completed once everything is cleaned up. Consequently, I wouldn't make it the responsibility of the exeuctors to make sure that all checkpoints are properly cleaned up by waiting on the completion of all enqueued tasks.

Both things should be fixed imo.;;;","18/Jan/21 19:17;roman;{quote}To me it makes more sense that the CheckpointCoordinator which knows about its state is responsible for making sure that we only execute a Runnable if we are still running.
{quote}
I think the current issue can be solved this way by adding a check *and synchronized* in scheduleTriggerRequest;
 But for cases like FLINK-18290 it's not possible as scheduling a callback is done by j.u.c. (e.g. handleAsync).

 

I've created FLINK-21015 to implement waiting for cleanup completion (and added your comment there).
 ;;;","19/Jan/21 09:25;trohrmann;I don't think that FLINK-18290 is impossible to solve differently [~roman_khachatryan]. For example, one could use a {{handle}} call in which one first checks whether the {{CheckepointCoordinator}} is still running before enqueuing a new runnable into the {{timer}} {{Executor}}.;;;","19/Jan/21 10:59;roman;{quote}one could use a handle call in which one first checks whether the CheckepointCoordinator is still running
{quote}
But checking *inside* handle(Async) means checking inside an already *submitted* action - i.e. too late:
{code:java}
coordinatorCheckpointsComplete.handleAsync(x -> {
    if (executor.isShutdown()) { // too late
        // ...
    }
} , timer);
{code}
 Or am I missing something?;;;","19/Jan/21 12:13;trohrmann;If you call {{handleAsync}}, then this is the case. However, if you use

{code}
coordinatorCheckpointsComplete.handle(x -> {
    if (executor.isShutdown()) { // too late
        timer.execute(() -> foobar());
    }
});
{code}

then you are running in the thread which completed the {{coordinatorCheckpointsComplete}}. That's what I meant with using {{handle}}.

Of course, this has to happen atomically.;;;","19/Jan/21 15:01;roman;I see what you're saying. But the current code mostly uses async versions. For example, in CheckpointCoordinator.startTriggeringCheckpoint there are 4 submissions of future callbacks using timer.;;;","19/Jan/21 16:36;trohrmann;Yes, I am also not saying that we should change it. I just wanted to document that there would also be a different solution approach.;;;","20/Jan/21 09:40;trohrmann;Fixed via 

1.13.0: 2ccbaf35f8171c93ac3fffce1e3fa7607f1f7b78
1.12.2: 7cac1ab5646bb64c9e3bb00d0abdefc1c30ad4bd;;;","20/Jan/21 09:57;roman;Thanks for clarifying and reviewing/merging!

I've created FLINK-21053 where I described both approaches to prevent further such cases.;;;","20/Jan/21 10:16;dwysakowicz;I'm afraid this change broke the 1.12 branch: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12274&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Functions in ExplodeFunctionUtil should handle null data to avoid NPE,FLINK-20989,13352426,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,15/Jan/21 07:51,28/May/21 07:09,13/Jul/23 08:07,17/Jan/21 15:20,1.11.3,,,,,,,,1.11.4,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The following test case will encounter NPE:

{code:scala}
    val t = tEnv.fromValues(
      DataTypes.ROW(
        DataTypes.FIELD(""a"", DataTypes.INT()),
        DataTypes.FIELD(""b"", DataTypes.ARRAY(DataTypes.STRING()))
      ),
      row(1, Array(""aa"", ""bb"", ""cc"")),
      row(2, null),
      row(3, Array(""dd""))
    )
    tEnv.registerTable(""T"", t)

    tEnv.executeSql(""SELECT a, s FROM T, UNNEST(T.b) as A (s)"").print()
{code}

Exception is 

{code:java}
Caused by: java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192)
	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.flink.table.planner.plan.utils.ObjectExplodeTableFunc.eval(ExplodeFunctionUtil.scala:34)
{code}

The reason is functions in ExplodeFunctionUtil do not handle null data. Since 1.12, the bug is fixed, see https://issues.apache.org/jira/browse/FLINK-18528
",,godfreyhe,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 17 15:20:34 UTC 2021,,,,,,,,,,"0|z0mm74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/21 15:20;godfreyhe;Fixed in 1.11.4: 0c090cdd48d2b2362b805fe40a45cdfced5f2738;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
USE DATABASE & USE CATALOG fails with quoted identifiers containing characters to be escaped in Flink SQL client,FLINK-20977,13352233,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,zhangjun,zhangjun,14/Jan/21 11:01,16/Mar/21 12:00,13/Jul/23 08:07,16/Mar/21 12:00,1.12.0,,,,,,,,1.12.3,,,,,,,Table SQL / Client,,,,,1,pull-request-available,,,,,"I have a database which name is mod, when I use `use mod` to switch to the db,the system throw an exception, I surround it with backticks ,it is still not well",,fsk119,jark,leonard,morsapaes,sunzheng,xiaozilong,zhangjun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21572,FLINK-21494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 12:00:32 UTC 2021,,,,,,,,,,"0|z0ml08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/21 11:02;zhangjun;it is ok in flink 1.11;;;","14/Jan/21 12:09;jark;Could you share the exception?;;;","15/Jan/21 02:43;zhangjun; 

hi, [~jark] the exception is like this, I tracked the code and found that it seems to be a calcite parsing error
{code:java}
Flink SQL> create database `mod`;
[INFO] Database has been created.


Flink SQL> show databases;
db
default
iceberg_db
mod


Flink SQL> use `mod`;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.sql.parser.impl.ParseException: Incorrect syntax near the keyword 'USE' at line 1, column 1.
Was expecting one of:
    ""ABS"" ...
    ""ALTER"" ...
    ""ARRAY"" ...
    ""AVG"" ...
    ""CALL"" ...
    ""CARDINALITY"" ...
    ""CASE"" ...
{code}
 

 ;;;","22/Jan/21 01:19;zhangjun;This bug only appears when using sqlclient ，it is no problem when I use DataStream api;;;","22/Jan/21 04:24;jark;Yes. I think the bug is in {{org.apache.flink.table.client.cli.CliClient#callUseDatabase}} where it doesn't quotes identifiers. ;;;","22/Jan/21 04:29;xiaozilong;Hi [~jark], Do we need to do a hotfix for this? I want to fix it if yes.;;;","22/Jan/21 05:01;sunzheng;[~jark] I submitted a PR ,Can you help me review? Thanks.;;;","22/Jan/21 05:50;jark;[~sunzheng], we should reach consensus about how to fix and get assigned before submitting pull request. 
It seems that you have a different fix way for this problem. Could you explain how to fix it?;;;","22/Jan/21 06:32;sunzheng;[~jark] I created a `mod` table and executed it successfully with the ""desc`mod`"" command.
So I compared ""use `mod`"" and ""desc `mod` two commands, and tracked the code execution flow of the two commands and found Is in the 
org.apache.flink.table.client.cli.SqlCommandParser#parseByRegexMatching 
 cmd.hasRegexPattern() method to judge regular expression
org.apache.flink.table.client.cli.SqlCommandParser.SqlCommand#USE
org.apache.flink.table.client.cli.SqlCommandParser.SqlCommand#DESC;;;","22/Jan/21 06:53;zhangjun;I found that   USE CATALOG `mod`  has the same problem;;;","15/Mar/21 15:46;morsapaes;+1, ran into a similar issue trying to use the Pulsar catalog (Flink 1.12.0):
{code:java}
...

[INFO] Catalog has been created.

Flink SQL> USE CATALOG pulsar;

Flink SQL> SHOW DATABASES;
public/default
public/functions
sample/standalone/ns1

Flink SQL> USE `public/default`;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.sql.parser.impl.ParseException: Encountered ""/"" at line 1, column 11.
Was expecting one of:
    <EOF>
    ""."" ...{code}
Before finding the ticket, I tried a couple of escape sequences to see if there was a workaround, with no success.

 ;;;","16/Mar/21 02:24;jark;This has been fixed in master when working on FLINK-21614. 

I will create a fix PR for 1.12. ;;;","16/Mar/21 12:00;jark;Fixed in release-1.12: 336c65ad99a1f35454ecca4ff77c10ba0b31cba8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSourceITCase.testPartitionFilter fails on AZP,FLINK-20975,13352227,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,trohrmann,trohrmann,14/Jan/21 10:41,25/Jan/23 08:53,13/Jul/23 08:07,25/Jan/23 08:53,1.13.0,,,,,,,,1.13.7,1.14.0,,,,,,Connectors / Hive,,,,,0,auto-deprioritized-critical,auto-unassigned,pull-request-available,stale-assigned,test-stability,"The test {{HiveTableSourceITCase.testPartitionFilter}} fails on AZP with the following exception:

{code}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.flink.connectors.hive.HiveTableSourceITCase.testPartitionFilter(HiveTableSourceITCase.java:278)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,lirui,mapohl,rmetzger,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30784,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 25 08:53:33 UTC 2023,,,,,,,,,,"0|z0mkyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/21 07:35;rmetzger;I reopened this issue, because it reappeared in the 1.13 branch https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18294&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354;;;","08/Jun/21 22:42;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","16/Jun/21 22:39;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","24/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","02/Jul/21 22:40;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","12/Jul/21 02:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20256&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=23014;;;","12/Jul/21 02:09;xtsong;[~lirui], could you please help take a look at this?;;;","12/Jul/21 03:50;lirui;Sure I'll look into this.;;;","12/Jul/21 12:59;lirui;I tried the test locally dozens of times but didn't reproduce it. According to test logs, the test failed because of:
1. Direct SQL failed due to:
{noformat}
Caused by: org.apache.derby.iapi.error.StandardException: Invalid character string format for type DECIMAL.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.iapi.types.DataType.invalidFormat(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.iapi.types.DataType.setValue(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.exe.ac1d3a42f2x017ax8d9bxd637x0000338c7ac889.e4(Unknown Source) ~[?:?]
	at org.apache.derby.impl.services.reflect.DirectCall.invoke(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.sql.execute.NestedLoopJoinResultSet.getNextRowCore(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.getNextRow(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedResultSet.movePosition(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedResultSet.next(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.datanucleus.store.rdbms.query.ForwardQueryResult.initialise(ForwardQueryResult.java:99) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:676) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:368) ~[datanucleus-api-jdo-4.2.4.jar:?]
	... 121 more
{noformat}
Since the partition column type is INT and should be safe to cast to decimal, this might be a derby bug.
2. Then hive tried to fallback to JDO method to pushdown the filter. But failed again because by default ""Filtering is supported only on partition keys of type string"".
3. Finally the pushdown is aborted and we fallback to listing all partitions in metastore, and thus the test failure.

I guess we can enable filter pushdown on integral partition column types so that we'll have a 2nd chance in step #2;;;","19/Jul/21 13:45;lirui;Pushed a fix to:
* master: a846a4856549a0992f2172fec9d1c3abe2d46215
* release-1.13: a6b823498aa77e3de5ff34e191e3a2e3d42f3620
* release-1.12: da62d0dcd12e915507b783a94a177907d441c761

Let's keep this open for a while.;;;","04/Sep/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","25/Jan/23 08:53;mapohl;I'm closing this issue because it hasn't appeared for a while. We experienced a failure of the same test but with a different stacktrace. I created FLINK-30784 as a new issue covering that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink throws NullPointerException for tables created from DataStream with no assigned timestamps and watermarks,FLINK-20961,13351987,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Yuval.Itzchakov,Yuval.Itzchakov,Yuval.Itzchakov,13/Jan/21 11:12,28/May/21 07:13,13/Jul/23 08:07,26/Jan/21 04:11,1.12.0,,,,,,,,1.12.2,1.13.0,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,," 

Given the following program:
{code:java}
//import org.apache.flink.api.common.eventtime.{ SerializableTimestampAssigner, WatermarkStrategy }
import org.apache.flink.streaming.api.functions.source.SourceFunction
import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _}
import org.apache.flink.streaming.api.watermark.Watermark
import org.apache.flink.table.annotation.{DataTypeHint, FunctionHint}
import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment
import org.apache.flink.table.api.{$, AnyWithOperations}
import org.apache.flink.table.functions.{AggregateFunction, ScalarFunction}
import java.time.Instant

object BugRepro {
  def text: String =
    s""""""
       |{
       |  ""s"": ""hello"",
       |  ""i"": ${Random.nextInt()}
       |}
       |"""""".stripMargin  
  def main(args: Array[String]): Unit = {
    val flink =
      StreamExecutionEnvironment.createLocalEnvironment()
    val tableEnv = StreamTableEnvironment.create(flink)
    val dataStream = flink
      .addSource {
        new SourceFunction[(Long, String)] {
          var isRunning = true          
          override def run(ctx: SourceFunction.SourceContext[(Long, String)]): Unit =
            while (isRunning) {
              val x = (Instant.now().toEpochMilli, text)
              ctx.collect(x)
              ctx.emitWatermark(new Watermark(x._1))
              Thread.sleep(300)
            }          
            override def cancel(): Unit =
              isRunning = false
        }
      }
//      .assignTimestampsAndWatermarks(
//        WatermarkStrategy
//          .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(30))
//          .withTimestampAssigner {
//            new SerializableTimestampAssigner[(Long, String)] {
//              override def extractTimestamp(element: (Long, String), recordTimestamp: Long): Long =
//                element._1
//            }
//          }
//      )
//
    tableEnv.createTemporaryView(""testview"", dataStream, $(""event_time"").rowtime(), $(""json_text""))
    val res = tableEnv.sqlQuery(""""""
                                  |SELECT json_text
                                  |FROM testview
                                  |"""""".stripMargin)    
    val sink = tableEnv.executeSql(
      """"""
        |CREATE TABLE SINK (
        |  json_text STRING
        |)
        |WITH (
        |  'connector' = 'print'
        |)
        |"""""".stripMargin
    )    res.executeInsert(""SINK"").await()
    ()
  }
    res.executeInsert(""SINK"").await()

{code}
 

Flink will throw a NullPointerException at runtime:
{code:java}
Caused by: java.lang.NullPointerExceptionCaused by: java.lang.NullPointerException at SourceConversion$3.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:305) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:394) at ai.hunters.pipeline.BugRepro$$anon$1.run(BugRepro.scala:78) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:215)
{code}
This is due to the fact that the DataStream did not assign a timestamp to the underlying source. This is the generated code:
{code:java}
public class SourceConversion$3 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator
          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {        private final Object[] references;
        private transient org.apache.flink.table.data.util.DataFormatConverters.CaseClassConverter converter$0;
        org.apache.flink.table.data.GenericRowData out = new org.apache.flink.table.data.GenericRowData(2);
        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);        public SourceConversion$3(
            Object[] references,
            org.apache.flink.streaming.runtime.tasks.StreamTask task,
            org.apache.flink.streaming.api.graph.StreamConfig config,
            org.apache.flink.streaming.api.operators.Output output,
            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
          this.references = references;
          converter$0 = (((org.apache.flink.table.data.util.DataFormatConverters.CaseClassConverter) references[0]));
          this.setup(task, config, output);
          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
              .setProcessingTimeService(processingTimeService);
          }
        }        @Override
        public void open() throws Exception {
          super.open();
          
        }        @Override
        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) (org.apache.flink.table.data.RowData) converter$0.toInternal((scala.Tuple2) element.getValue());
          
          org.apache.flink.table.data.TimestampData result$1;
          boolean isNull$1;
          org.apache.flink.table.data.binary.BinaryStringData field$2;
          boolean isNull$2;
          isNull$2 = in1.isNullAt(1);
          field$2 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$2) {
            field$2 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1));
          }
          
          ctx.element = element;
          
          
          
          result$1 = org.apache.flink.table.data.TimestampData.fromEpochMillis(ctx.timestamp());
          if (result$1 == null) {
            throw new RuntimeException(""Rowtime timestamp is null. Please make sure that a "" +
              ""proper TimestampAssigner is defined and the stream environment uses the EventTime "" +
              ""time characteristic."");
          }
          isNull$1 = false;
          if (isNull$1) {
            out.setField(0, null);
          } else {
            out.setField(0, result$1);
          }
                    
          
          
          if (isNull$2) {
            out.setField(1, null);
          } else {
            out.setField(1, field$2);
          }
                    
                  
          output.collect(outElement.replace(out));
          ctx.element = null;
          
        }                @Override
        public void close() throws Exception {
           super.close();
          
        }        
      }
{code}
The important line is here:
{code:java}
result$1 = org.apache.flink.table.data.TimestampData.fromEpochMillis(ctx.timestamp()); 

if (result$1 == null) { throw new RuntimeException(""Rowtime timestamp is null. Please make sure that a "" + ""proper TimestampAssigner is defined and the stream environment uses the EventTime "" + ""time characteristic."");
{code}
`ctx.timestamp` returns null in case no timestamp assigner was created, and `TimestampData.fromEpochMillis` expects a primitive `long`, so a deference fails. The actual check should be:
{code:java}
if (!ctx.hasTimestamp) {
  throw new RuntimeException(""Rowtime timestamp is null. Please make sure that a "" + ""proper TimestampAssigner is defined and the stream environment uses the EventTime "" + ""time characteristic."");
}

result$1 = TimestampData.fromEpochMillis(ctx.timestamp());{code}
 ",,jark,Yuval.Itzchakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 14:04:38 UTC 2021,,,,,,,,,,"0|z0mjhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/21 11:45;jark;Good catch!  Would you like to contribute a fix?;;;","13/Jan/21 12:36;Yuval.Itzchakov;Working on it :);;;","25/Jan/21 14:04;jark;Fixed in 
 - master: e78eca47d2e30b246ddba13faa1f50acdd53f33f
 - releaes-1.12: e26c0a84c8d7ff879dc8303c6a35236d59781552;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskTest.testProcessWithUnAvailableOutput unstable,FLINK-20957,13351957,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,trohrmann,trohrmann,13/Jan/21 09:35,09/Feb/21 08:31,13/Jul/23 08:07,09/Feb/21 08:31,1.13.0,,,,,,,,1.13.0,,,,,,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,,"The {{StreamTaskTest.testProcessWithUnAvailableOutput}} fails on AZP. 

{code}
[ERROR] testProcessWithUnAvailableOutput(org.apache.flink.streaming.runtime.tasks.StreamTaskTest)  Time elapsed: 0.145 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: a value equal to or greater than <42L>
     but: <41L> was less than <42L>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskTest.testProcessWithUnAvailableOutput(StreamTaskTest.java:1333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{code}

cc [~pnowojski]",,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 09 08:31:24 UTC 2021,,,,,,,,,,"0|z0mjaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/21 08:31;pnowojski;merged commit 10f4ed0 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Idle source doesn't work when pushing watermark into the source,FLINK-20947,13351911,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,weijiaxu,weijiaxu,13/Jan/21 02:59,28/May/21 08:07,13/Jul/23 08:07,26/Jan/21 11:48,1.12.0,,,,,,,,1.12.2,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"I use table sql to create stream with kafka source, and write data from Kafka into a Hive partitioned table.

The following sql to create kafka table:
{code:java}
// code placeholder
tableEnv.executeSql(
    ""CREATE TABLE stream_tmp.kafka_tmp (`messageId` STRING, `message_type` STRING,`payload` STRING,`timestamp` BIGINT, "" +
            ""  procTime AS PROCTIME(),"" +
            ""  eventTime AS TO_TIMESTAMP(FROM_UNIXTIME(`timestamp` / 1000,'yyyy-MM-dd HH:mm:ss')),"" +
            ""  WATERMARK FOR eventTime AS eventTime - INTERVAL '15' SECOND )"" +
            ""  WITH ('connector' = 'kafka',"" +
            "" 'topic' = 'XXX-topic',"" +
            "" 'properties.bootstrap.servers'='kafka-server:9092',"" +
            "" 'properties.group.id' = 'XXX-group_id',"" +
            "" 'scan.startup.mode' = 'latest-offset',"" +
            "" 'format' = 'json',"" +
            "" 'json.fail-on-missing-field' = 'false',"" +
            "" 'json.ignore-parse-errors' = 'true' )""
            );{code}
  

And, the following sql to create Hive table:
{code:java}
// code placeholder
tableEnv.executeSql(
     ""CREATE TABLE hive_tmp.kafka_hive_tmp (`message_id` STRING,`message_type` STRING,`payload` STRING,`event_ts` BIGINT ) "" +
             "" PARTITIONED BY (ts_date STRING,ts_hour STRING, ts_minute STRING)"" +
             "" STORED AS PARQUET TBLPROPERTIES ("" +
             "" 'sink.partition-commit.trigger' = 'partition-time',"" +
             "" 'sink.partition-commit.delay' = '1 min',"" +
             "" 'sink.partition-commit.policy.kind'='metastore,success-file',"" +
             "" 'sink.partition-commit.success-file.name'='_SUCCESS',"" +
             "" 'partition.time-extractor.timestamp-pattern' = '$ts_date $ts_hour:$ts_minute:00')"");
{code}
 

For the kafka topic used above,  which has multi partitions,  with some of the partitions there's data coming in, while other partitions with no data coming.

I noticed that there's config ""_table.exec.source.idle-timeout_"" can handle the situation for the ""idle"" source partition, but event though set this config, it still cannot trigger the Hive partition commit (that means the ""_SUCCESS"" file will not be created for the partition).

After do the analysis for this issue, find the root cause is that the watermark for the ""idle"" partition will not advance, which cause the Hive partition cannot be committed.

 

 ",,fsk119,hackergin,jark,leonard,txhsj,weijiaxu,yanchenyun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 08:21:14 UTC 2021,,,,,,,,,,"0|z0mj0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/21 04:40;jark;cc [~fsk119] could you help to check whether the idle timeout works for the push downed watermark?

[~weijiaxu] could you share the explain result of your query?;;;","13/Jan/21 05:15;weijiaxu;I think the root cause is the watermark for the ""idle"" partition will not advance, the reason why this happens, is because when set the ""_table.exec.source.idle-timeout_"", and the idle timeout value is valid, the watermark strategy that enriched with idleness detection should be updated, but actually not (after checking from the source code)

source code class:  _PushWatermarkIntoTableSourceScanRuleBase.java_

 ;;;","13/Jan/21 06:19;jark;Good catch [~weijiaxu]! I think you are right. 

[~fsk119] will help to fix this. We should think about how to add a good test for this. It is not test covered when supporting the watermark pushdown. ;;;","13/Jan/21 09:51;weijiaxu;My pleasure, [~jark] 

I think fix this issue is easy, what's more important, as you said, need setup the test for certain scenarios.

If I can be involved in the discussion for how to setup the test and learn more about this ?

 ;;;","13/Jan/21 12:03;jark;1. I find that we don't have watermark pushdown with idle timeout **plan test**. We need add one into {{PushWatermarkIntoTableSourceScanRuleTest}}.
2. We can add an IT case in {{KafkaTableITCase}}, 
   - prepare 2 partitions, each with several messages. The last event time in the first partition is {{2021-01-13 20:00:00}}, the last event time in the second partition is {{2021-01-13 19:30:00}}. 
   - window query on the source with tumbling 1 hour
   - the query should have output with window [19:00, 20:00) which means the watermark has reached 20:00. ;;;","17/Jan/21 06:49;fsk119;Sorry for the late response. I think the tests works like the rule test in `PushPartitionIntoTableSourceScanRule`. I will fix it soon.;;;","22/Jan/21 15:42;jark;Fixed in 
 - master: 90e680c9c579b2ffa1ca93ad7dfbaa5502dd8701
 - release-1.12: 13aa323aaa6e3920765a5a7a01ad8e25f9ae59ad;;;","22/Jan/21 15:43;jark;Hi [~weijiaxu], we have fixed this problem in master branch. Could you help to check whether this resolve your problem if you have time?;;;","25/Jan/21 08:21;weijiaxu;Hi [~jark] & [~fsk119], just applied the latest fix and it worked as what we expected. Thanks for your effort. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Launching in application mode requesting a ClusterIP rest service type results in an Exception,FLINK-20944,13351868,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,jasonbrome,jasonbrome,12/Jan/21 21:33,23/Aug/21 13:54,13/Jul/23 08:07,21/Jan/21 09:33,1.12.0,,,,,,,,1.12.2,1.13.0,,,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"Run a Flink job in Kubernetes in application mode, specifying kubernetes.rest-service.exposed.type=ClusterIP, results in the job being started, however the call to ./bin/flink throws an UnknownHostException Exception on the client.

Command line:

{{./bin/flink run-application --target kubernetes-application -Dkubernetes.cluster-id=myjob-qa -Dkubernetes.container.image=_SOME_REDACTED_PATH/somrepo/someimage_ -Dkubernetes.service-account=flink-service-account -Dkubernetes.namespace=myjob-qa -Dkubernetes.rest-service.exposed.type=ClusterIP local:///opt/flink}}
{{/usrlib/my-job.jar}}

Output:

2021-01-12 20:29:19,047 INFO org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration blob.server.port will be set to 6124
2021-01-12 20:29:19,048 INFO org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration taskmanager.rpc.port will be set to 6122
2021-01-12 20:29:20,369 ERROR org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient [] - A Kubernetes exception occurred.
java.net.UnknownHostException: myjob-qa-rest.myjob-qa: Name or service not known
 at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method) ~[?:1.8.0_275]
 at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929) ~[?:1.8.0_275]
 at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324) ~[?:1.8.0_275]
 at java.net.InetAddress.getAllByName0(InetAddress.java:1277) ~[?:1.8.0_275]
 at java.net.InetAddress.getAllByName(InetAddress.java:1193) ~[?:1.8.0_275]
 at java.net.InetAddress.getAllByName(InetAddress.java:1127) ~[?:1.8.0_275]
 at java.net.InetAddress.getByName(InetAddress.java:1077) ~[?:1.8.0_275]
 at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.getWebMonitorAddress(HighAvailabilityServicesUtils.java:193) ~[flink-dist_2.12-1.12.0.jar:1.12.0]
 at org.apache.flink.kubernetes.KubernetesClusterDescriptor.lambda$createClusterClientProvider$0(KubernetesClusterDescriptor.java:114) ~[flink-dist_2.12-1.12.0.jar:1.12.0]
 at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployApplicationCluster(KubernetesClusterDescriptor.java:185) ~[flink-dist_2.12-1.12.0.jar:1.12.0]
 at org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:64) ~[flink-dist_2.12-1.12.0.jar:1.12.0]
 at org.apache.flink.client.cli.CliFrontend.runApplication(CliFrontend.java:207) ~[flink-dist_2.12-1.12.0.jar:1.12.0]
 at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:974) ~[flink-dist_2.12-1.12.0.jar:1.12.0]
 at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1047) ~[flink-dist_2.12-1.12.0.jar:1.12.0]
 at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) [flink-dist_2.12-1.12.0.jar:1.12.0]
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1047) [flink-dist_2.12-1.12.0.jar:1.12.0]

------------------------------------------------------------
 The program finished with the following exception:

java.lang.RuntimeException: org.apache.flink.client.deployment.ClusterRetrieveException: Could not create the RestClusterClient.
 at org.apache.flink.kubernetes.KubernetesClusterDescriptor.lambda$createClusterClientProvider$0(KubernetesClusterDescriptor.java:118)
 at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployApplicationCluster(KubernetesClusterDescriptor.java:185)
 at org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:64)
 at org.apache.flink.client.cli.CliFrontend.runApplication(CliFrontend.java:207)
 at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:974)
 at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1047)
 at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1047)
Caused by: org.apache.flink.client.deployment.ClusterRetrieveException: Could not create the RestClusterClient.
 ... 8 more
Caused by: java.net.UnknownHostException: myjob-qa-rest.myjob-qa: Name or service not known
 at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
 at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
 at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
 at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
 at java.net.InetAddress.getAllByName(InetAddress.java:1193)
 at java.net.InetAddress.getAllByName(InetAddress.java:1127)
 at java.net.InetAddress.getByName(InetAddress.java:1077)
 at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.getWebMonitorAddress(HighAvailabilityServicesUtils.java:193)
 at org.apache.flink.kubernetes.KubernetesClusterDescriptor.lambda$createClusterClientProvider$0(KubernetesClusterDescriptor.java:114)
 ... 7 more

 ","Kubernetes 1.17

Flink 1.12

Running ./bin/flink from an Ubuntu 18.04 host",alexsell,jasonbrome,trohrmann,wangyang0918,zhisheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 23 13:54:04 UTC 2021,,,,,,,,,,"0|z0mir4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/21 03:08;wangyang0918;If the {{kubernetes.rest-service.exposed.type}} is {{ClusterIP}}, then the namespaced service will be used to access the rest endpoint. However, {{myjob-qa-rest.myjob-qa}} could only be resolved in the K8s cluster. That is the reason why you have an UnknownHostException.

If you set the exposed type to {{NodePort}}, I think it could work now. Another work around is that you could run the submission command in the K8s cluster(e.g. start a pod and tunnel in).

 

Moreover, maybe we could improve the behavior since in the application mode we do not need to actually retrieve the rest endpoint.;;;","13/Jan/21 09:25;trohrmann;Yes, maybe we shouldn't fail if {{HighAvailabilityServicesUtils.getWebMonitorAddress}} cannot retrieve the address. If it fails to resolve the address, then we could simply return the unresolved address, for example.;;;","13/Jan/21 09:49;wangyang0918;If the {{kubernetes.rest-service.exposed.type}} is {{ClusterIP}}, then we do not need to resolve the service name. Passing the {{NO_ADDRESS_RESOLUTION}} for {{HighAvailabilityServicesUtils.getWebMonitorAddress}} in such situation makes sense.;;;","15/Jan/21 08:53;rmetzger;[~fly_in_gis] are you taking care of this issue?;;;","15/Jan/21 09:41;wangyang0918;Yeah. I will attach a PR soon.;;;","19/Jan/21 07:29;wangyang0918;[~rmetzger] I have attached a PR for this issue. Could you please assign this ticket to me?;;;","21/Jan/21 09:33;trohrmann;Fixed via

1.13.0: 65da60ba4b5038aedbe0edb10d6459a5265cf055
1.12.2: 8bfddc606e39e656d144ad3d60c70627a0010fc7;;;","23/Aug/21 09:32;alexsell;Hello, 

as I saw in these commits this resolves the issue in the case of a native kubernetes cluster (correct me if I am wrong), I am just now facing the issue for a standalone kubernetes cluster which uses this [Descriptor|https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/StandaloneClusterDescriptor.java].

My question would be, can I somehow use the Kubernetes descriptor for a standalone cluster in kubernetes? Or wouldn't make sense to apply the same solution to the standalone Descriptor?

Thanks;;;","23/Aug/21 13:54;trohrmann;Hi [~alexsell], the property of the standalone cluster is that it does not know about where it runs. Hence, it is difficult for Flink to retrieve any specific information about its runtime when deployed that way. So I fear that this change does not make a lot of sense for the {{StandaloneClusterDescriptor}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Digest of FLOAT literals throws UnsupportedOperationException,FLINK-20942,13351797,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,danny0405,twalthr,twalthr,12/Jan/21 15:08,28/May/21 08:16,13/Jul/23 08:07,02/Feb/21 10:17,,,,,,,,,1.12.2,1.13.0,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The recent refactoring of Calcite's digests might have caused a regression for FLOAT literals. {{org.apache.calcite.rex.RexLiteral#appendAsJava}} throws a UnsupportedOperationException for the following query:

{code}
def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val source = env.fromElements(
      (1.0f, 11.0f, 12.0f),
      (2.0f, 21.0f, 22.0f),
      (3.0f, 31.0f, 32.0f),
      (4.0f, 41.0f, 42.0f),
      (5.0f, 51.0f, 52.0f)
    )
    val settings = EnvironmentSettings.newInstance()
        .inStreamingMode()
        .useBlinkPlanner()
        .build()
    val tEnv = StreamTableEnvironment.create(env, settings)
    tEnv.createTemporaryView(""myTable"", source, $(""id""), $(""f1""), $(""f2""))
    val query =
      """"""
        |select * from myTable where id in (1.0, 2.0, 3.0)
        |"""""".stripMargin
    tEnv.executeSql(query).print()
}
{code}

Stack trace:
{code}
Exception in thread ""main"" java.lang.UnsupportedOperationException: class org.apache.calcite.sql.type.SqlTypeName: FLOAT
	at org.apache.calcite.util.Util.needToImplement(Util.java:1075)
	at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:703)
	at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408)
	at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276)
	at org.apache.calcite.rex.RexLiteral.<init>(RexLiteral.java:223)
	at org.apache.calcite.rex.RexLiteral.toLiteral(RexLiteral.java:737)
	at org.apache.calcite.rex.RexLiteral.lambda$printSarg$4(RexLiteral.java:710)
	at org.apache.calcite.util.RangeSets$Printer.singleton(RangeSets.java:397)
	at org.apache.calcite.util.RangeSets.forEach(RangeSets.java:237)
	at org.apache.calcite.util.Sarg.lambda$printTo$0(Sarg.java:110)
	at org.apache.calcite.linq4j.Ord.forEach(Ord.java:157)
	at org.apache.calcite.util.Sarg.printTo(Sarg.java:106)
	at org.apache.calcite.rex.RexLiteral.printSarg(RexLiteral.java:709)
	at org.apache.calcite.rex.RexLiteral.lambda$appendAsJava$1(RexLiteral.java:652)
	at org.apache.calcite.util.Util.asStringBuilder(Util.java:2502)
	at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:651)
	at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408)
	at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276)
	at org.apache.calcite.rex.RexLiteral.<init>(RexLiteral.java:223)
	at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:971)
	at org.apache.calcite.rex.RexBuilder.makeSearchArgumentLiteral(RexBuilder.java:1066)
	at org.apache.calcite.rex.RexSimplify$SargCollector.fix(RexSimplify.java:2786)
	at org.apache.calcite.rex.RexSimplify.lambda$simplifyOrs$6(RexSimplify.java:1843)
	at java.util.ArrayList.forEach(ArrayList.java:1257)
	at org.apache.calcite.rex.RexSimplify.simplifyOrs(RexSimplify.java:1843)
	at org.apache.calcite.rex.RexSimplify.simplifyOr(RexSimplify.java:1817)
	at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:313)
	at org.apache.calcite.rex.RexSimplify.simplifyUnknownAs(RexSimplify.java:282)
	at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:257)
	at org.apache.flink.table.planner.plan.utils.FlinkRexUtil$.simplify(FlinkRexUtil.scala:213)
	at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.simplify(SimplifyFilterConditionRule.scala:63)
	at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.onMatch(SimplifyFilterConditionRule.scala:46)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.immutable.Range.foreach(Range.scala:160)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1329)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:707)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1107)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:666)
	at org.apache.flink.table.examples.scala.basics.WordCountTable$.main(WordCountTable.scala:59)
{code}",,danny0405,jark,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 10:17:59 UTC 2021,,,,,,,,,,"0|z0mibc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/21 15:10;twalthr;[~danny0405] could you forward this issue to the Calcite community?;;;","12/Jan/21 15:12;twalthr;Since this limits the usage of a commonly used data type, I would suggest to fix this in Flink in one of the next minor releases.;;;","22/Jan/21 08:54;danny0405;Thanks [~twalthr] for reporting this issue, i would take a look and fix it soon ~;;;","22/Jan/21 09:42;danny0405;Hmm, the root cause is that the new introduced `Sarg` tries to make literals recursively, for this example, it tries to create `RexLiteral`s with `FLOAT` as type name which is not possible from the normally SQL context, so, this expects to be a bug, let me see how to fix it quickly in Flink side.;;;","27/Jan/21 08:05;danny0405;I have fired a fix in CALCITE-4479 but i have no idea how to fix quickly in Flink side, should we copy the {{RexLiteral}} then ? The class is huge ..;;;","27/Jan/21 14:10;twalthr;Thanks for investigating this [~danny0405]. Once the fix has been merged in Calcite, I'm fine with copying the class to the Flink side. We have done that with a lot of classes in the past, including bigger classes such as SqlValidator. We should create a subtask for removing it in the next Calcite upgrade.;;;","29/Jan/21 07:39;danny0405;Thanks [~twalthr], i have fired a fix via https://github.com/apache/flink/pull/14801;;;","02/Feb/21 10:17;twalthr;Fixed in 1.13.0: 017bb12310dcf91e5439d5147a3a4f22ad40ba75
Fixed in 1.12.2: 3624a08082bf4d977f078274d8c97f5c55681ff5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fails when DROP a table with illegal watermark definition,FLINK-20937,13351690,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhuShang,MarcelLeon,MarcelLeon,12/Jan/21 09:07,28/May/21 07:09,13/Jul/23 08:07,15/Jan/21 14:40,1.12.0,,,,,,,,1.13.0,,,,,,,Table SQL / Client,,,,,0,pull-request-available,starter,,,,"In sql-client:

I create a kafka table with ""watermark for proctime()"" syntax.

Then i try execute a query or drop it, it report fails.

I know point watermark with process_time is meaningless, but while i drop it doesn't work , it will affect use.",,jark,MarcelLeon,romainr,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/21 09:10;MarcelLeon;drop table fail.png;https://issues.apache.org/jira/secure/attachment/13018613/drop+table+fail.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 15 14:40:46 UTC 2021,,,,,,,,,,"0|z0mhnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/21 10:40;ZhuShang;[~jark] i want to take it.;;;","15/Jan/21 14:40;jark;Fixed in master: de19a27fc58688712c86079333e3af3a6ccaa235;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Config Python Operator Use Managed Memory In Python DataStream,FLINK-20933,13351642,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,12/Jan/21 04:40,28/May/21 07:10,13/Jul/23 08:07,18/Jan/21 05:09,1.12.0,1.13.0,,,,,,,1.12.2,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,"Now the way to set `Python DataStream Operator` to use managed memory is to set a hook in the `execute` method of `Python StreamExecutionEnvironment` to traverse the `StreamGraph` and set the `Python Operator` to use managed memory.
But when the user’s job uses `from_data_stream` to convert the `DataStream` to a `Table`, the `TableEnvironment.execute` method is used at the end rather than `StreamExecutionEnvironment.execute`, so the `Python DataStream` related operators will not have `Managed Memory` set.",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 18 05:16:17 UTC 2021,,,,,,,,,,"0|z0mhcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/21 05:09;dian.fu;Fixed in master via e1d98e2d806493ff921b2984ad2a1eb200835c04;;;","18/Jan/21 05:16;dian.fu;Fixed in release-1.12 via 78799ac2f4b12b4ce789be6ad39ef829e361e61f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Date/Time/Timestamp in Python DataStream,FLINK-20921,13351376,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,11/Jan/21 06:50,28/May/21 07:07,13/Jul/23 08:07,14/Jan/21 07:19,1.12.0,1.13.0,,,,,,,1.12.2,1.13.0,,,,,,API / Python,,,,,0,pull-request-available,,,,,Currently the Date/Time/Timestamp type doesn't works in Python DataStream.,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 07:19:32 UTC 2021,,,,,,,,,,"0|z0mfq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/21 02:21;dian.fu;Fixed in master via bc4207fd3a437803e36166fb0cadf34e7d6fff47;;;","14/Jan/21 07:19;dian.fu;Fixed in release-1.12 via a7deac4d769d3f5cee65c5be4375ea4aa40766ad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManagerCustomLogHandlerTest.testGetJobManagerCustomLogsExistingButForbiddenFileWithObfuscatedPath can be deleted,FLINK-20916,13351349,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,gn_nate,gn_nate,11/Jan/21 02:15,28/May/21 07:07,13/Jul/23 08:07,13/Jan/21 15:05,,,,,,,,,1.13.0,,,,,,,Runtime / REST,,,,,0,pull-request-available,,,,," 

The [testGetJobManagerCustomLogsExistingButForbiddenFileWithObfuscatedPath|https://github.com/apache/flink/blob/b561010b0ee741543c3953306037f00d7a9f0801/flink-runtime/src/test/java/org/apache/flink/runtime/rest/handler/cluster/JobManagerCustomLogHandlerTest.java#L149] test for CVE-2020-17519 Path Traversal has a typo that causes it to inaccurately test for the vuln. 

It uses for format string ""..%%252%s"" when it should be ""..%%252f%s"".",,gn_nate,mapohl,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 15:05:21 UTC 2021,,,,,,,,,,"0|z0mfk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/21 13:47;mapohl;Thanks for bringing this up, [~gn_nate]. You're right - the test does not follow the specification report by CVE-2020-17519. By looking into that issue, I also realize that the test itself does not test the functionality it should test as the {{JobManagerCustomLogHandler}} does not decode the URL encoding at all. The URL parsing and decoding happens already earlier in the process (see [RouteHandler|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/router/RouterHandler.java#L86]) and does not need to be tested individually in this test class. Hence, this testcase is obsolete: The solution is to delete it. I updated the issue's title accordingly.;;;","13/Jan/21 15:05;rmetzger;Thanks for catching and fixing this.

Merged to master in https://github.com/apache/flink/commit/37dc5b9da90d770c60adcfdd40bc9504e79b4814.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve new HiveConf(jobConf, HiveConf.class)",FLINK-20913,13351291,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dixingxing@yeah.net,dixingxing@yeah.net,dixingxing@yeah.net,10/Jan/21 09:48,28/May/21 07:10,13/Jul/23 08:07,21/Jan/21 03:05,1.12.0,1.12.1,1.12.2,,,,,,1.11.4,1.12.2,1.13.0,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"When we query hive tables We got an Exception in  org.apache.flink.connectors.hive.util.HivePartitionUtils#getAllPartitions

Exception:

 
{code:java}
org.apache.thrift.transport.TTransportException
{code}
 

SQL: 
{code:java}
select * from dxx1 limit 1;
{code}
 

After debug we found that new HiveConf will override the configurations in jobConf，in my case `hive.metastore.sasl.enabled` was reset to `false`, which is unexpected.
{code:java}
// org.apache.flink.connectors.hive.util.HivePartitionUtils
new HiveConf(jobConf, HiveConf.class){code}
 

*I think we should add an HiveConfUtils to create HiveConf, which would be like this:*

 
{code:java}
HiveConf hiveConf = new HiveConf(jobConf, HiveConf.class);
hiveConf.addResource(jobConf);{code}
Above code can fix the error, i will make a PR if this improvement is acceptable.

 

Here is the detail error stack:
{code:java}
2021-01-10 17:27:11,995 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.2021-01-10 17:27:11,995 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL query. at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryInternal(LocalExecutor.java:527) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:365) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:634) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:324) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_202] at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:216) [flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:141) [flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) [flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.SqlClient.main(SqlClient.java:196) [flink-sql-client_2.11-1.12.0.jar:1.12.0]Caused by: org.apache.flink.connectors.hive.FlinkHiveException: Failed to collect all partitions from hive metaStore at org.apache.flink.connectors.hive.util.HivePartitionUtils.getAllPartitions(HivePartitionUtils.java:142) ~[flink-connector-hive_2.11-1.12.0.jar:1.12.0] at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:133) ~[flink-connector-hive_2.11-1.12.0.jar:1.12.0] at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:119) ~[flink-connector-hive_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalTableSourceScan.createSourceTransformation(CommonPhysicalTableSourceScan.scala:88) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.scala:94) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.scala:44) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.translateToPlan(BatchExecTableSourceScan.scala:44) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlanInternal(BatchExecLimit.scala:105) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlanInternal(BatchExecLimit.scala:47) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlan(BatchExecLimit.scala:47) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.translateToPlanInternal(BatchExecExchange.scala:141) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.translateToPlanInternal(BatchExecExchange.scala:52) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.translateToPlan(BatchExecExchange.scala:52) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlanInternal(BatchExecLimit.scala:105) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlanInternal(BatchExecLimit.scala:47) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlan(BatchExecLimit.scala:47) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLegacySink.translateToTransformation(BatchExecLegacySink.scala:129) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLegacySink.translateToPlanInternal(BatchExecLegacySink.scala:95) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLegacySink.translateToPlanInternal(BatchExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLegacySink.translateToPlan(BatchExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:86) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:85) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:85) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:167) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1267) ~[flink-table_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1259) ~[flink-table_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.getPipeline(StreamTableEnvironmentImpl.java:327) ~[flink-table_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$createPipeline$1(ExecutionContext.java:286) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:257) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.ExecutionContext.createPipeline(ExecutionContext.java:283) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryInternal(LocalExecutor.java:521) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] ... 8 moreCaused by: org.apache.thrift.transport.TTransportException at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[hive-exec-2.0.1.jar:2.0.1] at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[hive-exec-2.0.1.jar:2.0.1] at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[hive-exec-2.0.1.jar:2.0.1] at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[hive-exec-2.0.1.jar:2.0.1] at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[hive-exec-2.0.1.jar:2.0.1] at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[hive-exec-2.0.1.jar:2.0.1] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:1282) ~[hive-exec-2.0.1.jar:2.0.1] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:1268) ~[hive-exec-2.0.1.jar:2.0.1] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1272) ~[hive-exec-2.0.1.jar:2.0.1] at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_202] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_202] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:152) ~[hive-exec-2.0.1.jar:2.0.1] at com.sun.proxy.$Proxy32.getTable(Unknown Source) ~[?:?] at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_202] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_202] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2095) ~[hive-exec-2.0.1.jar:2.0.1] at com.sun.proxy.$Proxy32.getTable(Unknown Source) ~[?:?] at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.getTable(HiveMetastoreClientWrapper.java:117) ~[flink-connector-hive_2.11-1.12.0.jar:1.12.0] at org.apache.flink.connectors.hive.util.HivePartitionUtils.getAllPartitions(HivePartitionUtils.java:114) ~[flink-connector-hive_2.11-1.12.0.jar:1.12.0] at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:133) ~[flink-connector-hive_2.11-1.12.0.jar:1.12.0] at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:119) ~[flink-connector-hive_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalTableSourceScan.createSourceTransformation(CommonPhysicalTableSourceScan.scala:88) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.scala:94) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.scala:44) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.translateToPlan(BatchExecTableSourceScan.scala:44) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlanInternal(BatchExecLimit.scala:105) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlanInternal(BatchExecLimit.scala:47) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlan(BatchExecLimit.scala:47) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.translateToPlanInternal(BatchExecExchange.scala:141) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.translateToPlanInternal(BatchExecExchange.scala:52) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.translateToPlan(BatchExecExchange.scala:52) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlanInternal(BatchExecLimit.scala:105) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlanInternal(BatchExecLimit.scala:47) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.translateToPlan(BatchExecLimit.scala:47) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLegacySink.translateToTransformation(BatchExecLegacySink.scala:129) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLegacySink.translateToPlanInternal(BatchExecLegacySink.scala:95) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLegacySink.translateToPlanInternal(BatchExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLegacySink.translateToPlan(BatchExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:86) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:85) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:85) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:167) ~[flink-table-blink_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1267) ~[flink-table_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1259) ~[flink-table_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.getPipeline(StreamTableEnvironmentImpl.java:327) ~[flink-table_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$createPipeline$1(ExecutionContext.java:286) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:257) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.ExecutionContext.createPipeline(ExecutionContext.java:283) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryInternal(LocalExecutor.java:521) ~[flink-sql-client_2.11-1.12.0.jar:1.12.0] ... 8 more
{code}
 ","Hive 2.0.1

Flink 1.12.0

Query with SQL client",dixingxing@yeah.net,jark,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 20 09:29:20 UTC 2021,,,,,,,,,,"0|z0mf74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/21 13:23;lirui;Hi [~dixingxing@yeah.net] thanks for reporting the issue. Could you explain more about how could {{HiveConf}} override properties in the {{jobConf}}? It seems strange to me because if all the properties are reset to their default values, then the client wouldn't even know the URI of the metastore server.;;;","13/Jan/21 02:20;dixingxing@yeah.net;Hi [~lirui], i looked into the code, i found the configurations will not be reset which default value is 'null' or empty string, since the default value of '_hive.metastore.uris_' is empty string, so it not been reset. 

[More details|https://docs.google.com/document/d/1i0us6Czk28He844MAj4s0r6-wDaQNiUp-BEhye92E2U/edit?usp=sharing]

I will submit a PR to fix it.;;;","13/Jan/21 04:10;lirui;[~dixingxing@yeah.net] Thanks for the explanations. According to hive [docs|https://github.com/apache/hive/blob/rel/release-2.3.4/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L417], it seems hive constructor assumes the {{jobConf}} only contains hadoop properties. So the proposed fix looks good to me.;;;","19/Jan/21 07:00;jark;Fixed in
 - master: ccc306f398f1b74681f12667de855b940dc61498
 - release-1.12: fb93fdbe4eaa8149ef4f0e61a94d9d9a32fc5306
 - release-1.11: 0767fd8d709adbc01328084f9b7099fce6d26846;;;","19/Jan/21 07:00;lirui;Hi [~dixingxing@yeah.net] could you please also submit PRs for 1.11 and 1.12?;;;","19/Jan/21 15:26;dixingxing@yeah.net;[~lirui] , I would like to, and i'm working on it.;;;","20/Jan/21 09:29;dixingxing@yeah.net;Thanks [~lirui], I've sumitted PRs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code of BatchExpand & LocalNoGroupingAggregateWithoutKeys grows beyond 64 KB ,FLINK-20898,13351064,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,shared_ptr,shared_ptr,08/Jan/21 10:50,22/Jul/21 07:22,13/Jul/23 08:07,22/Jul/21 07:22,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,auto-deprioritized-major,,,,,"When we write a complex batch aggregation SQL, the generated code can easily exceed the 64KB size limitation for BatchExpand and LocalNoGroupingAggregateWithoutKeys operator. Especially for the analyze table scenario. 

For a simple sql of
{code:java}
analyze table tpc_ds.call_center compute statistics for all columns{code}
the underlying sql to execute will be:
{code:java}
SELECT CAST(COUNT(1) AS BIGINT),
    CAST(COUNT(DISTINCT `cc_call_center_sk`) AS BIGINT),
    CAST(
        (COUNT(1) - COUNT(`cc_call_center_sk`)) AS BIGINT
    ),
    CAST(8.0 AS DOUBLE),
    CAST(8.0 AS INTEGER),
    CAST(MAX(`cc_call_center_sk`) AS BIGINT),
    CAST(MIN(`cc_call_center_sk`) AS BIGINT),
    CAST(COUNT(DISTINCT `cc_call_center_id`) AS BIGINT),
    CAST(
        (COUNT(1) - COUNT(`cc_call_center_id`)) AS BIGINT
    ),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_call_center_id`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_call_center_id`)) AS INTEGER),
    CAST(MAX(`cc_call_center_id`) AS VARCHAR),
    CAST(MIN(`cc_call_center_id`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_rec_start_date`) AS BIGINT),
    CAST(
        (COUNT(1) - COUNT(`cc_rec_start_date`)) AS BIGINT
    ),
    CAST(12.0 AS DOUBLE),
    CAST(12.0 AS INTEGER),
    CAST(MAX(`cc_rec_start_date`) AS DATE),
    CAST(MIN(`cc_rec_start_date`) AS DATE),
    CAST(COUNT(DISTINCT `cc_rec_end_date`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_rec_end_date`)) AS BIGINT),
    CAST(12.0 AS DOUBLE),
    CAST(12.0 AS INTEGER),
    CAST(MAX(`cc_rec_end_date`) AS DATE),
    CAST(MIN(`cc_rec_end_date`) AS DATE),
    CAST(COUNT(DISTINCT `cc_closed_date_sk`) AS BIGINT),
    CAST(
        (COUNT(1) - COUNT(`cc_closed_date_sk`)) AS BIGINT
    ),
    CAST(8.0 AS DOUBLE),
    CAST(8.0 AS INTEGER),
    CAST(MAX(`cc_closed_date_sk`) AS BIGINT),
    CAST(MIN(`cc_closed_date_sk`) AS BIGINT),
    CAST(COUNT(DISTINCT `cc_open_date_sk`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_open_date_sk`)) AS BIGINT),
    CAST(8.0 AS DOUBLE),
    CAST(8.0 AS INTEGER),
    CAST(MAX(`cc_open_date_sk`) AS BIGINT),
    CAST(MIN(`cc_open_date_sk`) AS BIGINT),
    CAST(COUNT(DISTINCT `cc_name`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_name`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_name`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_name`)) AS INTEGER),
    CAST(MAX(`cc_name`) AS VARCHAR),
    CAST(MIN(`cc_name`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_class`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_class`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_class`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_class`)) AS INTEGER),
    CAST(MAX(`cc_class`) AS VARCHAR),
    CAST(MIN(`cc_class`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_employees`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_employees`)) AS BIGINT),
    CAST(4.0 AS DOUBLE),
    CAST(4.0 AS INTEGER),
    CAST(MAX(`cc_employees`) AS INTEGER),
    CAST(MIN(`cc_employees`) AS INTEGER),
    CAST(COUNT(DISTINCT `cc_sq_ft`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_sq_ft`)) AS BIGINT),
    CAST(4.0 AS DOUBLE),
    CAST(4.0 AS INTEGER),
    CAST(MAX(`cc_sq_ft`) AS INTEGER),
    CAST(MIN(`cc_sq_ft`) AS INTEGER),
    CAST(COUNT(DISTINCT `cc_hours`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_hours`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_hours`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_hours`)) AS INTEGER),
    CAST(MAX(`cc_hours`) AS VARCHAR),
    CAST(MIN(`cc_hours`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_manager`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_manager`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_manager`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_manager`)) AS INTEGER),
    CAST(MAX(`cc_manager`) AS VARCHAR),
    CAST(MIN(`cc_manager`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_mkt_id`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_mkt_id`)) AS BIGINT),
    CAST(4.0 AS DOUBLE),
    CAST(4.0 AS INTEGER),
    CAST(MAX(`cc_mkt_id`) AS INTEGER),
    CAST(MIN(`cc_mkt_id`) AS INTEGER),
    CAST(COUNT(DISTINCT `cc_mkt_class`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_mkt_class`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_mkt_class`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_mkt_class`)) AS INTEGER),
    CAST(MAX(`cc_mkt_class`) AS VARCHAR),
    CAST(MIN(`cc_mkt_class`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_mkt_desc`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_mkt_desc`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_mkt_desc`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_mkt_desc`)) AS INTEGER),
    CAST(MAX(`cc_mkt_desc`) AS VARCHAR),
    CAST(MIN(`cc_mkt_desc`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_market_manager`) AS BIGINT),
    CAST(
        (COUNT(1) - COUNT(`cc_market_manager`)) AS BIGINT
    ),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_market_manager`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_market_manager`)) AS INTEGER),
    CAST(MAX(`cc_market_manager`) AS VARCHAR),
    CAST(MIN(`cc_market_manager`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_division`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_division`)) AS BIGINT),
    CAST(4.0 AS DOUBLE),
    CAST(4.0 AS INTEGER),
    CAST(MAX(`cc_division`) AS INTEGER),
    CAST(MIN(`cc_division`) AS INTEGER),
    CAST(COUNT(DISTINCT `cc_division_name`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_division_name`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_division_name`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_division_name`)) AS INTEGER),
    CAST(MAX(`cc_division_name`) AS VARCHAR),
    CAST(MIN(`cc_division_name`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_company`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_company`)) AS BIGINT),
    CAST(4.0 AS DOUBLE),
    CAST(4.0 AS INTEGER),
    CAST(MAX(`cc_company`) AS INTEGER),
    CAST(MIN(`cc_company`) AS INTEGER),
    CAST(COUNT(DISTINCT `cc_company_name`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_company_name`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_company_name`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_company_name`)) AS INTEGER),
    CAST(MAX(`cc_company_name`) AS VARCHAR),
    CAST(MIN(`cc_company_name`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_street_number`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_street_number`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_street_number`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_street_number`)) AS INTEGER),
    CAST(MAX(`cc_street_number`) AS VARCHAR),
    CAST(MIN(`cc_street_number`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_street_name`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_street_name`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_street_name`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_street_name`)) AS INTEGER),
    CAST(MAX(`cc_street_name`) AS VARCHAR),
    CAST(MIN(`cc_street_name`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_street_type`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_street_type`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_street_type`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_street_type`)) AS INTEGER),
    CAST(MAX(`cc_street_type`) AS VARCHAR),
    CAST(MIN(`cc_street_type`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_suite_number`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_suite_number`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_suite_number`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_suite_number`)) AS INTEGER),
    CAST(MAX(`cc_suite_number`) AS VARCHAR),
    CAST(MIN(`cc_suite_number`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_city`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_city`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_city`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_city`)) AS INTEGER),
    CAST(MAX(`cc_city`) AS VARCHAR),
    CAST(MIN(`cc_city`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_county`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_county`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_county`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_county`)) AS INTEGER),
    CAST(MAX(`cc_county`) AS VARCHAR),
    CAST(MIN(`cc_county`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_state`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_state`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_state`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_state`)) AS INTEGER),
    CAST(MAX(`cc_state`) AS VARCHAR),
    CAST(MIN(`cc_state`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_zip`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_zip`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_zip`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_zip`)) AS INTEGER),
    CAST(MAX(`cc_zip`) AS VARCHAR),
    CAST(MIN(`cc_zip`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_country`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_country`)) AS BIGINT),
    CAST(
        AVG(CAST(CHAR_LENGTH(`cc_country`) AS DOUBLE)) AS DOUBLE
    ),
    CAST(MAX(CHAR_LENGTH(`cc_country`)) AS INTEGER),
    CAST(MAX(`cc_country`) AS VARCHAR),
    CAST(MIN(`cc_country`) AS VARCHAR),
    CAST(COUNT(DISTINCT `cc_gmt_offset`) AS BIGINT),
    CAST((COUNT(1) - COUNT(`cc_gmt_offset`)) AS BIGINT),
    CAST(8.0 AS DOUBLE),
    CAST(8.0 AS INTEGER),
    CAST(MAX(`cc_gmt_offset`) AS DOUBLE),
    CAST(MIN(`cc_gmt_offset`) AS DOUBLE),
    CAST(COUNT(DISTINCT `cc_tax_percentage`) AS BIGINT),
    CAST(
        (COUNT(1) - COUNT(`cc_tax_percentage`)) AS BIGINT
    ),
    CAST(8.0 AS DOUBLE),
    CAST(8.0 AS INTEGER),
    CAST(MAX(`cc_tax_percentage`) AS DOUBLE),
    CAST(MIN(`cc_tax_percentage`) AS DOUBLE)
FROM `bytedance_hive`.`tpc_ds`.`call_center`
{code}
 For this sql, we will get the following root cause exception for compiling code in TM side.
{code:java}
Caused by: org.codehaus.janino.InternalCompilerException: Code of method ""processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V"" of class ""BatchExpand$36757"" grows beyond 64 KB
{code}
and
{code:java}
Caused by: org.codehaus.janino.InternalCompilerException: Code of method ""processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V"" of class ""LocalNoGroupingAggregateWithoutKeys$34429"" grows beyond 64 KB
{code}
We need split the generated code for BatchExpand and LocalNoGroupingAggregateWithoutKeys for complex sql. Just like the ExprCodeGenerator and AggsHandlerCodeGenerator.

 ",,fifth,godfreyhe,jark,jingzhang,libenchao,lirui,lzljs3620320,shared_ptr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23007,,,,,,,,,,"11/Jan/21 09:54;fifth;exception.log;https://issues.apache.org/jira/secure/attachment/13018498/exception.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 22 07:22:26 UTC 2021,,,,,,,,,,"0|z0mdsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/21 09:54;fifth;With version 1.11.2&1.11.3&1.12.0,I have a similar problem. When I run a complex SQL just like :

 
{code:java}
select a,b,c,
sum(some_col),
....（A lot of aggregation operations）
from some_table
group by a,b,c
{code}
 

in batchMode with blink planner ，it will throw exception like：[^exception.log]
{code:java}
Caused by: org.codehaus.janino.InternalCompilerException: Code of method ""processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V"" of class ""HashAggregateWithKeys$3329"" grows beyond 64 KB{code}
However, it performs normally under streamingMode;;;","11/Jan/21 10:19;shared_ptr;Hi [~fifth],

Streaming mode is using org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator for agg codeGen, which has opened the code split here. And Batch mode is using org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper at present, which has no code split yet. 

I think these methods in batch mode can be fixed together. ;;;","25/Jan/21 09:30;shared_ptr;Hi [~jark],[~lirui], what do you think about for this issue? ;;;","22/Apr/21 10:55;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 10:17;godfreyhe;cc [~lzljs3620320];;;","20/May/21 10:53;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Jul/21 07:22;lzljs3620320;This should be resolved in FLINK-23007, you can check latest master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnLocalAndRemoteChannel test failed with ""TestTimedOutException""",FLINK-20893,13350997,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hxbks2ks,hxbks2ks,08/Jan/21 02:43,12/Feb/21 13:31,13/Jul/23 08:07,12/Feb/21 13:31,1.11.0,,,,,,,,1.12.0,,,,,,,Runtime / Checkpointing,,,,,0,test-stability,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11768&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=981eced9-6683-5752-3201-62faf56c149b]
{code:java}
2021-01-07T22:19:05.1814074Z [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 365.616 s <<< FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointITCase
2021-01-07T22:19:05.1815029Z [ERROR] shouldPerformUnalignedCheckpointOnLocalAndRemoteChannel(org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 300.01 s  <<< ERROR!
2021-01-07T22:19:05.1819455Z org.junit.runners.model.TestTimedOutException: test timed out after 300 seconds
2021-01-07T22:19:05.1820123Z 	at java.base@11.0.9.1/jdk.internal.misc.Unsafe.park(Native Method)
2021-01-07T22:19:05.1820721Z 	at java.base@11.0.9.1/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
2021-01-07T22:19:05.1821427Z 	at java.base@11.0.9.1/java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1796)
2021-01-07T22:19:05.1822153Z 	at java.base@11.0.9.1/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3128)
2021-01-07T22:19:05.1823030Z 	at java.base@11.0.9.1/java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1823)
2021-01-07T22:19:05.1825723Z 	at java.base@11.0.9.1/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1998)
2021-01-07T22:19:05.1827155Z 	at app//org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1671)
2021-01-07T22:19:05.1827937Z 	at app//org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:73)
2021-01-07T22:19:05.1828519Z 	at app//org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1651)
2021-01-07T22:19:05.1829199Z 	at app//org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1634)
2021-01-07T22:19:05.1829745Z 	at app//org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:161)
2021-01-07T22:19:05.1835881Z 	at app//org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnLocalAndRemoteChannel(UnalignedCheckpointITCase.java:147)
2021-01-07T22:19:05.1836851Z 	at java.base@11.0.9.1/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-01-07T22:19:05.1837534Z 	at java.base@11.0.9.1/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-01-07T22:19:05.1838309Z 	at java.base@11.0.9.1/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-01-07T22:19:05.1839275Z 	at java.base@11.0.9.1/java.lang.reflect.Method.invoke(Method.java:566)
2021-01-07T22:19:05.1839715Z 	at app//org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-01-07T22:19:05.1840157Z 	at app//org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-01-07T22:19:05.1840617Z 	at app//org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-01-07T22:19:05.1841060Z 	at app//org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-01-07T22:19:05.1841444Z 	at app//org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
2021-01-07T22:19:05.1841825Z 	at app//org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-01-07T22:19:05.1842274Z 	at app//org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-01-07T22:19:05.1842980Z 	at app//org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-01-07T22:19:05.1843453Z 	at java.base@11.0.9.1/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2021-01-07T22:19:05.1843881Z 	at java.base@11.0.9.1/java.lang.Thread.run(Thread.java:834)
{code}",,hxbks2ks,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 12 13:31:38 UTC 2021,,,,,,,,,,"0|z0mde0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/21 13:31;pnowojski;We have rewritten the test in release-1.12 branch to use less resources, and most likely won't be back porting the fix to the release-1.11 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SystemResourcesCounterTest  may caused endless loop when some error occured,FLINK-20891,13350884,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,zlzhang0122,zlzhang0122,07/Jan/21 13:05,28/Aug/21 12:18,13/Jul/23 08:07,18/Jun/21 20:54,1.11.1,1.12.0,,,,,,,1.14.0,,,,,,,Runtime / Metrics,Tests,,,,0,auto-deprioritized-major,pull-request-available,,,,"When the function run() in  class SystemResourcesCounter  come across Throwable exception, the loop in the function run() of the SystemResourcesCounter class will break, the SystemResourcesCounter thread will finish and the loop in the class SystemResourcesCounterTest will continue because of the condition is still true, the test will in a endless loop.",,zlzhang0122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 18 20:54:54 UTC 2021,,,,,,,,,,"0|z0mcow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/21 13:24;zlzhang0122;[INFO] Running org.apache.flink.runtime.metrics.utils.SystemResourcesCounterTest
Exception in thread ""SystemResourcesCounter probing thread"" java.lang.UnsatisfiedLinkError: Failed to create temporary file for /com/sun/jna/linux-x86-64/libjnidispatch.so library: Permission denied
at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:866)
at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:826)
at com.sun.jna.Native.(Native.java:140)
at oshi.jna.platform.linux.Libc.(Libc.java:38)
at oshi.hardware.platform.linux.LinuxCentralProcessor.getSystemLoadAverage(LinuxCentralProcessor.java:218)
at org.apache.flink.runtime.metrics.util.SystemResourcesCounter.calculateCPUUsage(SystemResourcesCounter.java:210)
at org.apache.flink.runtime.metrics.util.SystemResourcesCounter.run(SystemResourcesCounter.java:103);;;","22/Apr/21 10:55;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:00;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","18/Jun/21 20:54;chesnay;master: 246f0db2b7410bcd534c39e1a5b93a0cba6bffc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContinuousFileReaderOperator should not close the output on close(),FLINK-20888,13350877,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,arvid,gaoyunhaii,gaoyunhaii,07/Jan/21 12:13,23/Sep/21 17:54,13/Jul/23 08:07,01/Jul/21 09:22,1.12.0,,,,,,,,1.12.5,1.13.2,1.14.0,,,,,Connectors / FileSystem,,,,,0,auto-deprioritized-major,pull-request-available,,,,"Currently ContinuousFileReaderOperator would close the output on close(), if it is chained with more operators, it would also close the following operators before their endOfInput() is called (by default, we would call op1.endInput(), op1.close(), op2.endInput(), op2.close()... in order). This might cause some problems like in 

[https://lists.apache.org/thread.html/r50a94aaea4fe25f3927a4274ea8272e6b76ecec8f3fe48d2566689bd%40%3Cuser.flink.apache.org%3E]",,AHeise,aljoscha,gaoyunhaii,jingzhang,jinyius,kezhuw,leonard,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 01 10:27:30 UTC 2021,,,,,,,,,,"0|z0mcnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/21 12:37;arvid;Thanks for the investigation, we would take it on soonish.;;;","07/Jan/21 12:57;aljoscha;Nice debugging! I think that in general operators are never supposed to call {{close()}} on their outputs.;;;","22/Apr/21 10:55;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:00;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","29/Jun/21 18:43;arvid;Merged into master as dfeae1b652bf268aadce6df1e98c377e98e56655.;;;","30/Jun/21 07:50;arvid;Merged into 1.12 as 83a26d519261f3d7607a2e25ce77e5f2c10b11f2 and into 1.13 as 8636a4e08b9eaaef838ac3b2a4ae83afadd5a202.;;;","01/Jul/21 09:04;lzljs3620320;Hi [~arvid] Has this JIRA been completed? Maybe we should correct Fix version?;;;","01/Jul/21 09:22;arvid;Sorry yes. I have updated the status.;;;","01/Jul/21 10:27;lzljs3620320;Thanks~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-deterministic functions return different values even if it is referred with the same column name,FLINK-20887,13350871,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,TsReaper,TsReaper,07/Jan/21 11:58,25/Jun/23 06:00,13/Jul/23 08:07,25/Jun/23 05:59,1.13.0,,,,,,,,1.18.0,,,,,,,Table SQL / Planner,,,,,0,auto-deprioritized-major,auto-unassigned,pull-request-available,,,"Add the following test case to {{CalcITCase.scala}}

{code:scala}
@Test
def testRand(): Unit = {
  checkResult(
    s""""""
       |SELECT b - a FROM (
       |  SELECT r + 5 AS a, r + 7 AS b FROM (
       |    SELECT RAND() AS r FROM SmallTable3
       |  ) t1
       |) t2
       |"""""".stripMargin,
    Seq(row(2), row(2), row(2))
  )
}
{code}

Failure messages are
{code}
Results
 == Correct Result - 3 ==   == Actual Result - 3 ==
!2                          1.051329250417921
!2                          1.3649146677814379
!2                          1.787784536771345
        
Plan:
== Abstract Syntax Tree ==
LogicalProject(EXPR$0=[-($1, $0)])
+- LogicalProject(a=[+($0, 5)], b=[+($0, 7)])
   +- LogicalProject(r=[RAND()])
      +- LogicalTableScan(table=[[default_catalog, default_database, SmallTable3]])

== Optimized Logical Plan ==
Calc(select=[-(+(RAND(), 7), +(RAND(), 5)) AS EXPR$0])
+- BoundedStreamScan(table=[[default_catalog, default_database, SmallTable3]], fields=[a, b, c])
{code}

It seems that the projections are merged incorrectly. However if you run the following test case in {{FlinkCalcMergeRuleTest.scala}}

{code:scala}
@Test
def testCalcMergeWithRandomUdf(): Unit = {
  val sqlQuery = ""SELECT ts + a, ts + b FROM "" +
    ""(SELECT a, b, random_udf(a) AS ts FROM MyTable WHERE a = b) t""
  util.verifyRelPlan(sqlQuery)
}
{code}

The result is
{code:xml}
<Root>
  <TestCase name=""testCalcMergeWithRandomUdf"">
    <Resource name=""sql"">
      <![CDATA[SELECT ts + a, ts + b FROM (SELECT a, b, random_udf(a) AS ts FROM MyTable WHERE a = b) t]]>
    </Resource>
    <Resource name=""ast"">
      <![CDATA[
LogicalProject(EXPR$0=[+(random_udf($0), $0)], EXPR$1=[+(random_udf($0), $1)])
+- LogicalFilter(condition=[=($0, $1)])
   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
]]>
    </Resource>
    <Resource name=""optimized rel plan"">
      <![CDATA[
FlinkLogicalCalc(select=[+(random_udf(a), a) AS EXPR$0, +(random_udf(a), b) AS EXPR$1], where=[=(a, b)])
+- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
]]>
    </Resource>
  </TestCase>
</Root>
{code}

It seems that the plan is incorrect from the AST. So this seems to be a bug in Calcite?",,elkhand,godfreyhe,jark,kezhuw,leonard,libenchao,lincoln.86xy,qingyue,TsReaper,waywtdcc,yinxiaoxixi,zhangbinzaifendou,zhongqishang,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 25 05:59:26 UTC 2023,,,,,,,,,,"0|z0mcm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/21 11:59;TsReaper;[~Leonard Xu] please take a look. It might be that {{FlinkCalcMergeRule}} does not work correctly with non-deterministic functions.;;;","10/Jan/21 15:53;leonard;Hi, [~TsReaper] 
 I think it's not related to the  FlinkCalcMergeRule because the plan will not be matched by the rule and it always fails no matter the  FlinkCalcMergeRule is added or not. 
And I think the fail reason is that  the RAND() function materialized when it is used rather than when it is declared.;;;","16/Apr/21 10:48;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:48;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","29/Apr/21 10:22;godfreyhe;The {{deterministic}} value for {{RANK}} is true, while the {{isDynamicFunction}} value is true. I think FlinkCalcMergeRule should also consider dynamic function. WDYT ? [~Leonard Xu][~TsReaper];;;","30/May/21 11:27;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","07/Jun/21 10:58;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","24/Jun/21 10:15;yinxiaoxixi;I meet this bug too,please fix this bug as soon as possible;;;","25/Jun/23 05:59;lincoln.86xy;Fixed in master: 65217c35e7c8f19acb1c7761d2a513ec9c0d316d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deserialization exception when using 'canal-json.table.include' to filter out the binlog of the specified table,FLINK-20885,13350858,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,jark,jark,07/Jan/21 10:20,28/May/21 07:08,13/Jul/23 08:07,14/Jan/21 14:05,,,,,,,,,1.13.0,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"I found a bug in the canal code. 'canal-json.table.include' does not filter out the binlog of the specified table correctly, which will cause an error in the parsing section. For example, if I want to read the binlog of canal-json.table.include = 'a' table, there is a source field of int in table a, but at this time if table b also has a source field of string, An error will be reported.


 !screenshot-1.png! ",,fsk119,jark,lmagics,nicholasjiang,wangfeiair2324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/21 06:52;wangfeiair2324;image-2021-01-08-14-52-53-443.png;https://issues.apache.org/jira/secure/attachment/13018352/image-2021-01-08-14-52-53-443.png","08/Jan/21 06:53;wangfeiair2324;image-2021-01-08-14-53-40-476.png;https://issues.apache.org/jira/secure/attachment/13018353/image-2021-01-08-14-53-40-476.png","13/Jan/21 07:02;wangfeiair2324;image-2021-01-13-15-02-33-266.png;https://issues.apache.org/jira/secure/attachment/13018661/image-2021-01-13-15-02-33-266.png","07/Jan/21 10:20;jark;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13018295/screenshot-1.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 14:05:29 UTC 2021,,,,,,,,,,"0|z0mcj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/21 10:21;jark;This is reported in the comments of FLINK-20385.;;;","08/Jan/21 02:33;wangfeiair2324;[~jark] When is this expected to be fixed ;;;","08/Jan/21 04:17;jark;[~wangfeiair2324] What Flink version are you using? ;;;","08/Jan/21 04:17;jark;Hi [~nicholasjiang] are you interested in this? ;;;","08/Jan/21 04:35;wangfeiair2324;[~jark] I'm using Flink Master based compilation, probably 1.13.;;;","08/Jan/21 06:52;wangfeiair2324;I have modified the code locally to fix this bug, and if you do, it is recommended that canal-json.database.include and canal-json.table.include support regular matching, and contain relationships rather than equal relationships. Filter isDdl=true for all binlogs that have an alter or create error.

 

我在本地修改了代码 来解决这个bug，如果你们来解决的话，建议canal-json.database.include 和 canal-json.table.include 应该支持正则匹配，应该是包含关系 而不是等于关系。并且如果有alter 或者create 等binlog也会报错，建议过滤掉isDdl=true的binlog。;;;","08/Jan/21 06:53;wangfeiair2324;!image-2021-01-08-14-52-53-443.png!;;;","08/Jan/21 06:53;wangfeiair2324;!image-2021-01-08-14-53-40-476.png!;;;","08/Jan/21 06:55;wangfeiair2324;This is just a temporary solution for my production environment, looking forward to the community's update, Thanks.;;;","12/Jan/21 13:00;nicholasjiang;[~jark], I would like to work for this issue. Please assign to me for fix.;;;","13/Jan/21 02:58;nicholasjiang;[~wangfeiair2324], could you please provide me with the binlog record above mentioned?;;;","13/Jan/21 03:08;wangfeiair2324;[~nicholasjiang]

CREATE TABLE `test_2` (
 `id` int(11) NOT NULL,
 `username` int(11) DEFAULT NULL COMMENT '用户名',
 `password` varchar(255) DEFAULT NULL COMMENT '密码',
 `AddTime` datetime NOT NULL COMMENT '创建时间',
 PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

{""data"":[\{""id"":""0"",""username"":""22"",""password"":""111111"",""AddTime"":""0000-00-00 00:00:00""}],""database"":""plink"",""es"":1610507187000,""id"":90481,""isDdl"":false,""mysqlType"":\{""id"":""int(11)"",""username"":""int(11)"",""password"":""varchar(255)"",""AddTime"":""datetime""},""old"":[\{""password"":null}],""pkNames"":[""id""],""sql"":"""",""sqlType"":\{""id"":4,""username"":4,""password"":12,""AddTime"":93},""table"":""test_2"",""ts"":1610507187259,""type"":""UPDATE""}


CREATE TABLE `test_1` (
 `id` int(11) NOT NULL,
 `username` varchar(255) DEFAULT NULL COMMENT '用户名',
 `password` varchar(255) DEFAULT NULL COMMENT '密码',
 `AddTime` datetime NOT NULL COMMENT '创建时间',
 PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

{""data"":[\{""id"":""0"",""username"":""zhangsna"",""password"":""1111111"",""AddTime"":""0000-00-00 00:00:00""}],""database"":""plink"",""es"":1610507126000,""id"":90469,""isDdl"":false,""mysqlType"":\{""id"":""int(11)"",""username"":""varchar(255)"",""password"":""varchar(255)"",""AddTime"":""datetime""},""old"":[\{""username"":null,""password"":null}],""pkNames"":[""id""],""sql"":"""",""sqlType"":\{""id"":4,""username"":12,""password"":12,""AddTime"":93},""table"":""test_1"",""ts"":1610507127009,""type"":""UPDATE""}

 ;;;","13/Jan/21 03:11;wangfeiair2324;table test_2.userName (int); table test_1.userName (varchar);
 You create a ddl with canal-json.table.include=test_2 .

Then add the binlog of test_1, as in the binlog format above

{""data"":[\{""id"":""0"",""username"":""zhangsna"",""password"":""11111111"",""AddTime"":""0000-00-00 00:00:00""}],""database"":""plink"",""es"":1610507775000,""id"":90557,""isDdl"":false,""mysqlType"":\{""id"":""int(11)"",""username"":""varchar(255)"",""password"":""varchar(255)"",""AddTime"":""datetime""},""old"":[\{""password"":""1111111""}],""pkNames"":[""id""],""sql"":"""",""sqlType"":\{""id"":4,""username"":12,""password"":12,""AddTime"":93},""table"":""test_1"",""ts"":1610507775184,""type"":""UPDATE""}

Then you get an error

 ;;;","13/Jan/21 06:39;nicholasjiang;[~wangfeiair2324], the ""data"" field in binlog record you provided above has some problem to parse. Could you check the correct of the binlog record? And canal-json.database.include and canal-json.table.include only match one database and table.;;;","13/Jan/21 06:46;wangfeiair2324;|{""data"":[\{""id"":""0"",""username"":""zhangsna"",""password"":""passwd"",""AddTime"":""2020-12-25 14:23:07""}],""database"":""plink"",""es"":1610520384000,""id"":92151,""isDdl"":false,""mysqlType"":\{""id"":""int(11)"",""username"":""varchar(255)"",""password"":""varchar(255)"",""AddTime"":""datetime""},""old"":[\{""password"":""11111111""}],""pkNames"":[""id""],""sql"":"""",""sqlType"":\{""id"":4,""username"":12,""password"":12,""AddTime"":93},""table"":""test_1"",""ts"":1610520384579,""type"":""UPDATE""}|;;;","13/Jan/21 06:48;wangfeiair2324;You can use the JSON above。 

 

and canal-json.database.include and canal-json.table.include，

It cannot be modified to support regular or wildcard characters？

 ;;;","13/Jan/21 06:54;nicholasjiang;[~wangfeiair2324], I have already used https://www.bejson.com to check your JSON above provided. But unfortunately, this format of JSON above checks fails.;;;","13/Jan/21 07:01;wangfeiair2324;Remove \, the JIRA automatically adds \

 

!image-2021-01-13-15-02-33-266.png!

 ;;;","14/Jan/21 14:05;jark;Fixed in master: 9d674edd478706cc8ef7233d42385e948bae60e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[CVE-2020-17518] Directory traversal attack: remote file writing through the REST API,FLINK-20875,13350785,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,ana4,ana4,07/Jan/21 02:55,14/Jan/21 14:46,13/Jul/23 08:07,14/Jan/21 12:12,1.9.0,,,,,,,,1.10.3,1.11.3,1.12.0,,,,,Runtime / REST,,,,,0,,,,,,"Flink 1.5.1 introduced a REST handler that allows you to write an uploaded file to an arbitrary location on the local file system, through a maliciously modified HTTP HEADER. The files can be written to any location accessible by Flink 1.5.1.",,ana4,mapohl,rmetzger,xiaozilong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 14:45:49 UTC 2021,,,,,,,,,,"0|z0mc2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/21 11:45;mapohl;Hi [~ana4],
thanks for bringing this up. Unfortunately, we're actually not supporting Flink 1.10 anymore. You are free to create a backport of fix [a5264a6f|https://github.com/apache/flink/commit/a5264a6f41524afe8ceadf1d8ddc8c80f323ebc4] yourself if you're not able to upgrade to one of the supported versions {{1.11.3+}} or {{1.12.0+}}. 

I'm gonna close this issue for now as we would only create a dedicated {{1.10.3}} release containing the backport if there's a bigger desire by the Flink community.;;;","08/Jan/21 02:08;ana4;Thanks. It's important to create a dedicated 1.10.3 release.;;;","12/Jan/21 12:21;mapohl;[~ana4] I started a discussion in the [dev mailing list|http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Releasing-Apache-Flink-1-10-3-td47784.html] on creating a release {{1.10.3}}. Feel free to reply to that thread to express your needs.;;;","14/Jan/21 12:12;rmetzger;It looks like we are going to do a 1.10.3 release.

Note that the issue is already fixed in the ""release-1.10"" branch of Flink (in commit e87607367e13c6cae8d19032e260fd5aa526aeba), no need to manually backport it.

I'm closing this ticket, as this has been resolved.;;;","14/Jan/21 13:40;mapohl;True, I missed that. Thanks for checking it, [~rmetzger];;;","14/Jan/21 14:45;chesnay;master: a5264a6f41524afe8ceadf1d8ddc8c80f323ebc4
1.11: 275cc3bed101b8acf924033d7ce3fdeb921a24ae
1.10: e87607367e13c6cae8d19032e260fd5aa526aeba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calculating numBuckets exceeds the maximum value of int and got a negative number,FLINK-20855,13350554,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hejiefang,hejiefang,hejiefang,06/Jan/21 01:28,28/May/21 09:09,13/Jul/23 08:07,17/Apr/21 01:45,1.11.1,1.12.0,,,,,,,1.13.0,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,stale-assigned,,,,"When i run the TPCDS of 500G，i get a exception
{code:java}
Caused by: java.lang.IllegalArgumentException
        at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:123)
        at org.apache.flink.table.runtime.hashtable.LongHashPartition.setNewBuckets(LongHashPartition.java:223)
        at org.apache.flink.table.runtime.hashtable.LongHashPartition.<init>(LongHashPartition.java:176)
        at org.apache.flink.table.runtime.hashtable.LongHybridHashTable.buildTableFromSpilledPartition(LongHybridHashTable.java:432)
        at org.apache.flink.table.runtime.hashtable.LongHybridHashTable.prepareNextPartition(LongHybridHashTable.java:354)
        at org.apache.flink.table.runtime.hashtable.LongHybridHashTable.nextMatching(LongHybridHashTable.java:145)
        at LongHashJoinOperator$40166.endInput2$(Unknown Source)
        at LongHashJoinOperator$40166.endInput(Unknown Source)
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:101)
        at org.apache.flink.streaming.runtime.tasks.OperatorChain.endHeadOperatorInput(OperatorChain.java:278)
        at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.checkFinished(StreamTwoInputProcessor.java:211)
        at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:183)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:349)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:191)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:564)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:534)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
        at java.lang.Thread.run(Thread.java:748)
{code}
The reason is： when calculate the numBuckets in LongHashPartition，the result exceeds the maximum value of int and got a negative number
{code:java}
LongHashPartition(
      LongHybridHashTable longTable,
      int partitionNum,
      BinaryRowDataSerializer buildSideSerializer,
      int bucketNumSegs,
      int recursionLevel,
      List<MemorySegment> buffers,
      int lastSegmentLimit) {
   this(longTable, buildSideSerializer, listToArray(buffers));
   this.partitionNum = partitionNum;
   this.recursionLevel = recursionLevel;

   int numBuckets = MathUtils.roundDownToPowerOf2(bucketNumSegs * segmentSize / 16);
   MemorySegment[] buckets = new MemorySegment[bucketNumSegs];
   for (int i = 0; i < bucketNumSegs; i++) {
      buckets[i] = longTable.nextSegment();
   }
   setNewBuckets(buckets, numBuckets);
   this.finalBufferLimit = lastSegmentLimit;
}
{code}
A way to avoid the exception is to adjust the calculation order

change
{code:java}
int numBuckets = MathUtils.roundDownToPowerOf2(bucketNumSegs * segmentSize / 16);
{code}
to
{code:java}
int numBuckets = MathUtils.roundDownToPowerOf2(segmentSize / 16 * bucketNumSegs);
{code}",,fsk119,hejiefang,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 17 01:45:49 UTC 2021,,,,,,,,,,"0|z0mank:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 02:48;jark;cc [~lzljs3620320]
;;;","16/Apr/21 10:48;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","17/Apr/21 01:45;ykt836;Thanks [~hejiefang] for the contribution.

Merged: fd8e34c03b663aff96a625ed751b66244da8793e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka consumer ID is not specified correctly in new KafkaSource,FLINK-20848,13349066,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,renqs,renqs,05/Jan/21 07:02,28/May/21 07:06,13/Jul/23 08:07,13/Jan/21 02:51,,,,,,,,,1.12.2,1.13.0,,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,,"In the constructor of {{KafkaPartitionSplitReader}}, {{subtaskId}} should be assigned before invoking {{createConsumerClientId}} method because this method uses subtask ID as Kafka consumer ID's suffix",,becket_qin,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 02:51:55 UTC 2021,,,,,,,,,,"0|z0m1ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/21 02:51;becket_qin;Merged.
Master: 9313b01a73d5d05fab2fc8be8fac21a3ede039d6
release-1.12: e203e24cf775e4d5cdd0469b9db3cf2894b2f9c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix compile error due to duplicated generated files,FLINK-20841,13348902,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,04/Jan/21 09:32,28/May/21 07:00,13/Jul/23 08:07,04/Jan/21 14:44,1.11.4,1.12.1,1.13.0,,,,,,1.11.4,1.12.1,1.13.0,,,,,Build System,,,,,0,pull-request-available,,,,,FLINK-20790 and FLINK-20805 caused a compile error due to duplicated generated Java files. The previous location for the file generation was not removed properly as the folder appeared in Flink's {{.gitignore}}.,,mapohl,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20790,FLINK-20805,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 04 12:18:40 UTC 2021,,,,,,,,,,"0|z0m0a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/21 10:35;pnowojski;Example failure:

{code:java}
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 11.057 s
[INFO] Finished at: 2021-01-04T08:56:41+01:00
[INFO] Final Memory: 137M/1207M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-parquet_2.11: Compilation failure: Compilation failure: 
[ERROR] flink/flink-formats/flink-parquet/target/generated-test-sources/org/apache/flink/formats/parquet/generated/NestedRecord.java:[16,8] duplicate class: org.apache.flink.formats.parquet.generated.NestedRecord
[ERROR] flink/flink-formats/flink-parquet/target/generated-test-sources/org/apache/flink/formats/parquet/generated/SimpleRecord.java:[16,8] duplicate class: org.apache.flink.formats.parquet.generated.SimpleRecord
[ERROR] flink/flink-formats/flink-parquet/target/generated-test-sources/org/apache/flink/formats/parquet/generated/MapItem.java:[16,8] duplicate class: org.apache.flink.formats.parquet.generated.MapItem
[ERROR] flink/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java:[15,8] duplicate class: org.apache.flink.formats.parquet.generated.Bar
[ERROR] flink/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java:[15,8] duplicate class: org.apache.flink.formats.parquet.generated.ArrayItem
[ERROR] /Users/pnowojski/flink/flink-formats/flink-parquet/target/generated-test-sources/org/apache/flink/formats/parquet/generated/Address.java:[16,8] duplicate class: org.apache.flink.formats.parquet.generated.Address
{code}
;;;","04/Jan/21 12:18;chesnay;master:
a1f90d79bde03d9f7c54cb37742e3e418ec1e0e6
86a9af30d2a4792b8d632174a7e91d052d6aadca
1.12:
f9d166862b34d247aff8981c36b74ac3d97f1cc4
1.11:
055c733e1b21561e8410a962c1e3ddfdfa3bbf9a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Projection pushdown doesn't work in temporal(lookup) join ,FLINK-20840,13348891,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,04/Jan/21 08:39,01/Sep/22 02:09,13/Jul/23 08:07,19/Jan/21 02:28,1.12.0,,,,,,,,1.13.0,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"{code:java}
sql 1: 
|SELECT T.*, D.id
|FROM MyTable AS T
|JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
|ON T.a = D.id

optmized plan:
Calc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id])
+- LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], async=[false], lookup=[id=a], select=[a, b, c, proctime, rowtime, id, name, age])
   +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])

sql 2:
|SELECT T.a, D.id
|FROM MyTable AS T
|JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
|ON T.a = D.id

optmized plan:
LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], async=[false], lookup=[id=a], select=[a, id])
+- Calc(select=[a])
   +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])


{code}",,fsk119,godfreyhe,hailong wang,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18778,FLINK-29138,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 19 02:28:10 UTC 2021,,,,,,,,,,"0|z0m07s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/21 08:45;fsk119;It's duplicate of the FLINK-18778;;;","04/Jan/21 13:51;leonard;[~fsk119] 
 The rule `FlinkProjectJoinTransposeRule` was added in logical optimize phase which uses volcano optimizer, the optimized plan is uncertain for example above case.;;;","19/Jan/21 02:28;godfreyhe;Fixed in 1.13.0: b9945db0b902f620b4957c6a14ca78171fc5207d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some mistakes in state.md and state_zh.md ,FLINK-20839,13348887,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wym_maozi,wym_maozi,wym_maozi,04/Jan/21 08:26,28/May/21 07:00,13/Jul/23 08:07,04/Jan/21 12:34,1.12.0,,,,,,,,1.13.0,,,,,,,Documentation,,,,,0,pull-request-available,,,,,"When creating a ListStateDescriptor object, the generic type information is inconsistent with the specified state type information:
The generic type of ListStateDescriptor is: *Tuple<String,Integer>*, but the specified type information is: *Tuple<Long，Long>.* Details as attached",,wym_maozi,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/21 08:14;wym_maozi;Snipaste_2021-01-04_16-13-05.png;https://issues.apache.org/jira/secure/attachment/13018105/Snipaste_2021-01-04_16-13-05.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 04 12:34:21 UTC 2021,,,,,,,,,,"0|z0m06w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/21 08:28;wym_maozi;I will fix the mistake, and push a pr,please assign to me, thank you ;;;","04/Jan/21 09:53;yunta;I think you're right and already assign to you. Please go ahead.;;;","04/Jan/21 12:34;chesnay;master: 9da6b76ddf064172e134d5c18e1812e0580e888a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deliver bootstrap resouces ourselves for website and documentation,FLINK-20832,13348850,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,hiscat,hiscat,04/Jan/21 03:24,28/May/21 07:03,13/Jul/23 08:07,13/Jan/21 04:05,1.12.0,,,,,,,,1.11.4,1.12.2,1.13.0,,,,,API / Core,Documentation,,,,0,pull-request-available,,,,,"The typesetting of the official website is abnormal. The typesetting seen by users in China is abnormal.Because user can't load https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js CDN.

 !screenshot-1.png! ",,hiscat,jark,rmetzger,sxb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/21 06:52;jark;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13018351/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 08 14:32:37 UTC 2021,,,,,,,,,,"0|z0lzyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/21 06:56;jark;The website and documentation have been broken for a week. Lots of users from China are unable to read the docs. The reason might be maxcdn.bootstrapcdn.com is blocked (not sure when it will be recovered). 

I have an offline dicussion with [~vthinkxie], he suggested to replace the resources with https://www.jsdelivr.com/ which is stable and fast in China and global. 
For example, we can replace https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js  with https://cdn.jsdelivr.net/npm/bootstrap@3.3.4/dist/css/bootstrap.min.css

What do you think [~rmetzger] ?;;;","08/Jan/21 07:00;rmetzger;Oh damn. Let's fix this asap.

How about delivering the required files ourselves, instead of using a CDN?;;;","08/Jan/21 07:20;jark;Good idea! We also deliver fontawesome ourselves. I will prepare a pull request soon. ;;;","08/Jan/21 14:32;jark;Fixed in 
 - master: af36844d27fd1bd08755bb4ca3bff12563a68923
 - release-1.12: 6df7e4fd71223c225c1297e26c46c8cd4b034bbd
 - release-1.11: 47fc9b2800c5c5d635295576a01e725aa6ec1dcf
 - flink-web: be97ebb0ecbbc09c51b91aefb802b30aad19aa57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some aggregate and flat_aggregate tests failed in py35,FLINK-20828,13348664,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,01/Jan/21 12:49,28/May/21 06:59,13/Jul/23 08:07,02/Jan/21 12:11,1.13.0,,,,,,,,1.13.0,,,,,,,API / Python,,,,,0,pull-request-available,test-stability,,,,"We need to fix some aggregate/flat_aggregate tests due to FLINK-20769 , which cased the difference of result oder.

These tests include:

pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_aggregate
pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate_list_view
pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_list_view
pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_map_view_iterate",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 02 12:11:17 UTC 2021,,,,,,,,,,"0|z0lytc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/21 12:11;dian.fu;Fixed in master via bab7632e0c145780eef08e2bf42d4af3ef8ad8b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
