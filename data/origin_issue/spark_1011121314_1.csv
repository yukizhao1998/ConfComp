Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Outward issue link (Required),Inward issue link (Supercedes),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Exception in resultHandler could crash DAGScheduler and shutdown SparkContext ,SPARK-2322,12724369,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,rxin,rxin,29/Jun/14 22:39,17/Jul/14 01:08,14/Jul/23 06:24,17/Jul/14 01:08,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Spark Core,,,,,0,,,,,,,,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2323,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402552,,,2014-06-29 22:39:27.0,,,,,,,,,,"0|i1xadz:",402619,,,,,,,,,,,,,1.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of tasks on executors become negative after executor failures,SPARK-2319,12724326,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,rxin,rxin,29/Jun/14 05:26,20/Aug/15 22:17,14/Jul/23 06:25,27/Jun/15 05:36,1.0.0,,,,,,,,1.4.0,,,,,Web UI,,,,,1,,,,,,See attached screenshot.,,andrewor14,dparekh,KaiXinXIaoLei,meiyoula,rxin,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2015,SPARK-2698,,SPARK-10141,,,,,,,,"02/Jul/15 12:07;KaiXinXIaoLei;active tasks.png;https://issues.apache.org/jira/secure/attachment/12743297/active+tasks.png","29/Jun/14 05:28;rxin;num active tasks become negative (-16).jpg;https://issues.apache.org/jira/secure/attachment/12653043/num+active+tasks+become+negative+%28-16%29.jpg",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402511,,,Thu Jul 02 17:22:11 UTC 2015,,,,,,,,,,"0|i1xa4f:",402578,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/15 05:36;srowen;Given the many fixes related to negative executors from 1.0 to 1.4, I imagine this is not a problem now.;;;","02/Jul/15 12:02;KaiXinXIaoLei;using the lastest version (1.4),I also met the same problem.;;;","02/Jul/15 12:07;KaiXinXIaoLei;Using 1.4, num active tasks become negative, and ""Complete Tasks"" is more bigger than ""Total Tasks"";;;","02/Jul/15 17:22;andrewor14;[~KaiXinXIaoLei] the negative active tasks is fixed in SPARK-8560;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageStatusListener should avoid O(blocks) operations,SPARK-2316,12724303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,pwendell,pwendell,28/Jun/14 18:54,08/Jan/15 11:53,14/Jul/23 06:25,02/Aug/14 19:15,1.0.0,,,,,,,,1.1.0,,,,,Spark Core,Web UI,,,,0,,,,,,"In the case where jobs are frequently causing dropped blocks the storage status listener can bottleneck. This is slow for a few reasons, one being that we use Scala collection operations, the other being that we operations that are O(number of blocks). I think using a few indices here could make this much faster.

{code}
 at java.lang.Integer.valueOf(Integer.java:642)
        at scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:70)
        at org.apache.spark.storage.StorageUtils$$anonfun$9.apply(StorageUtils.scala:82)
        at scala.collection.TraversableLike$$anonfun$groupBy$1.apply(TraversableLike.scala:328)
        at scala.collection.TraversableLike$$anonfun$groupBy$1.apply(TraversableLike.scala:327)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.TraversableLike$class.groupBy(TraversableLike.scala:327)
        at scala.collection.AbstractTraversable.groupBy(Traversable.scala:105)
        at org.apache.spark.storage.StorageUtils$.rddInfoFromStorageStatus(StorageUtils.scala:82)
        at org.apache.spark.ui.storage.StorageListener.updateRDDInfo(StorageTab.scala:56)
        at org.apache.spark.ui.storage.StorageListener.onTaskEnd(StorageTab.scala:67)
        - locked <0x00000000a27ebe30> (a org.apache.spark.ui.storage.StorageListener)
{code}",,andrewor,apachespark,llai,pwendell,pwolfe,shivaram,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2228,SPARK-3882,,,,,,,,SPARK-2228,SPARK-2675,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402488,,,Thu Jan 08 11:53:53 UTC 2015,,,,,,,,,,"0|i1x9zb:",402555,,,,,,,,,,,,,1.1.0,,,,,,,,,,"17/Jul/14 18:06;shivaram;I'd just like to add that in cases where we have many thousands of blocks, this stack trace occupies one core constantly on the Master and is probably one of the reasons why the WebUI stops functioning after a certain point. ;;;","25/Jul/14 18:03;shivaram;On a related note, can we have flags to turn off some of the UI listeners ? If the StorageTab is going to be too expensive to update, it'll be good to have a way to turn it off and just have the JobProgress show up in the UI;;;","31/Jul/14 03:41;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/1679;;;","02/Aug/14 19:15;pwendell;This was fixed via:
https://github.com/apache/spark/pull/1679;;;","08/Jan/15 11:53;pwolfe;Any workaround ideas for users who can't yet upgrade (stuck on version 1.0.0)? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RDD actions are only overridden in Scala, not java or python",SPARK-2314,12724268,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,staple,marmbrus,marmbrus,28/Jun/14 02:05,16/Sep/14 18:47,14/Jul/23 06:25,16/Sep/14 18:47,1.0.0,1.0.1,,,,,,,1.2.0,,,,,SQL,,,,,1,starter,,,,,"For example, collect and take().  We should keep these two in sync, or move this code to schemaRDD like if possible.",,apachespark,marmbrus,staple,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402453,,,Fri Jul 25 20:01:10 UTC 2014,,,,,,,,,,"0|i1x9rj:",402520,,,,,,,,,,,,,1.1.0,,,,,,,,,,"15/Jul/14 17:42;staple;I added a PR for the java api:
https://github.com/apache/spark/pull/1421

I didn't see a straightforward way to implement using SchemaRDDLike, so I implemented in JavaSchemaRDD.;;;","16/Jul/14 06:08;staple;Hi, sorry if I didn't indicate this clearly enough - my patch was just for the java api and I haven't handled the python side, which I believe requires separate treatment.  Would it make sense to reopen this ticket until the python api is handled as well?  I can implement that too if you want.;;;","16/Jul/14 17:01;marmbrus;Yeah I think we might need to do this in python too as at least take is implemented manually in rdd.py;;;","25/Jul/14 19:59;staple;Hi, I added a PR that I handles overriding collect and take in Python, and count in Java:

https://github.com/apache/spark/pull/1592;;;","25/Jul/14 20:01;apachespark;User 'staple' has created a pull request for this issue:
https://github.com/apache/spark/pull/1592;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark should accept port via a command line argument rather than STDIN,SPARK-2313,12724261,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,pwendell,pwendell,28/Jun/14 01:14,16/Feb/15 23:38,14/Jul/23 06:25,16/Feb/15 23:38,,,,,,,,,1.3.0,,,,,PySpark,,,,,0,,,,,,Relying on stdin is a brittle mechanism and has broken several times in the past. From what I can tell this is used only to bootstrap worker.py one time. It would be strictly simpler to just pass it is a command line.,,apachespark,davies,farrellee,joshrosen,lvsoft,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3140,,,SPARK-5810,SPARK-2111,,,,,SPARK-1670,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402446,,,Mon Feb 16 23:38:21 UTC 2015,,,,,,,,,,"0|i1x9pz:",402513,,,,,,,,,,,,,1.1.0,,,,,,,,,,"28/Jun/14 12:25;farrellee;components involved -
 0. pyspark - python program that initiates a py4j setup when constructing the SparkContext (calls launch_gateway form java_gateway.py)
 1. launch_gateway - invokes ""o.a.s.d.SparkSubmit pyspark-shell"" via spark-class via spark-submit, which invokes py4j.GatewayServer
 2. py4j.GatewayServer - py4j specific code that listens on a port and prints it to stdout (see GatewayServer.java#L610)
 3. launch_gateway - reads the port from stdin and constructs the client side of the py4j channel

comments -
 a. by allowing the child to pick an ephemeral port there's a guarantee of success (except for the case of no available ports)
 b. having the parent pick a port and pass it to the child introduces a risk that when the child tries to use the port it will no longer be available. thus, not strictly simpler to keep the same guarantees that currently exist.
 c. printing the port to stdout from the child (py4j gatewayserver) is the intended method for discovery, see https://github.com/bartdag/py4j/blob/master/py4j-java/src/py4j/GatewayServer.java#L610
 d. any data on stdout from spark-submit, spark-class or o.a.s.d.SparkSubmit can interfere with the py4j setup

because of (d), i consider this fragile - good meaning, unrelated changes are likely to break it.

i'll take a look at this;;;","16/Jul/14 13:15;farrellee;as this stands, having another communication mechanism for py4j that can be controlled by the parent is the proper solution. using something like a domain socket may also assist in the return path from py4j (tmp file).

fyi, a recent change pushed all existing output to stderr in the spark-class/spark-submit path

i'm not actively working on this;;;","24/Nov/14 07:58;apachespark;User 'lvsoft' has created a pull request for this issue:
https://github.com/apache/spark/pull/3424;;;","24/Nov/14 21:50;davies;[~farrellee] Thew new approach could be:

1) bind to random socket in python, 
2) pass the port into JVM, connect to it
3) Java Gateway binds to random port
4) pass the port back via socket (created in 1)
5) read the port from socket (created in 1), close it

The logic will similar as current, the cost is create a temporary socket.;;;","12/Feb/15 14:12;farrellee;that'd work, also requires a py4j change;;;","14/Feb/15 04:57;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4603;;;","16/Feb/15 23:38;joshrosen;Issue resolved by pull request 4603
[https://github.com/apache/spark/pull/4603];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Actors do not handle unknown messages in their receive methods,SPARK-2312,12724247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,isaias.barroso,kkasravi,kkasravi,27/Jun/14 23:31,15/Apr/15 21:42,14/Jul/23 06:25,02/Sep/14 08:08,1.0.0,,,,,,,,1.4.0,,,,,Spark Core,,,,,0,starter,,,,,"Per akka documentation - an actor should provide a pattern match for all messages including _ otherwise akka.actor.UnhandledMessage will be propagated. 
Noted actors:
MapOutputTrackerMasterActor, ClientActor, Master, Worker...
Should minimally do a 
logWarning(s""Received unexpected actor system event: $_"") so message info is logged in correct actor.",,apachespark,isaias.barroso,kkasravi,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,SPARK-4172,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402432,,,Thu Oct 30 23:22:05 UTC 2014,,,,,,,,,,"0|i1x9lr:",402494,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/14 01:54;isaias.barroso;Hi Kam,

I can fix that issue.
;;;","20/Aug/14 00:33;isaias.barroso;Created a Pull Request https://github.com/apache/spark/pull/2048;;;","20/Aug/14 00:36;apachespark;User 'isaias' has created a pull request for this issue:
https://github.com/apache/spark/pull/2048;;;","20/Aug/14 08:36;apachespark;User 'isaias' has created a pull request for this issue:
https://github.com/apache/spark/pull/2055;;;","30/Oct/14 23:22;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3027;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkUI Storage page cached statuses incorrect,SPARK-2307,12724184,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,27/Jun/14 18:11,05/Nov/14 10:45,14/Jul/23 06:25,27/Jun/14 22:23,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Spark Core,Web UI,,,,0,,,,,,"See attached: the executor has 512MB, but somehow it has cached (279 + 27 + 279 + 27) = 612MB? (The correct answer is 279MB).",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/14 18:11;andrewor;Screen Shot 2014-06-27 at 11.09.54 AM.png;https://issues.apache.org/jira/secure/attachment/12652869/Screen+Shot+2014-06-27+at+11.09.54+AM.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402369,,,Fri Jul 04 05:49:07 UTC 2014,,,,,,,,,,"0|i1x97r:",402431,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/14 22:23;pwendell;Issue resolved by pull request 1249
[https://github.com/apache/spark/pull/1249];;;","04/Jul/14 05:49;pwendell;There was a follow up patch:
https://github.com/apache/spark/pull/1255;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedPriorityQueue is private and not registered with Kryo,SPARK-2306,12724156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankitBhardwaj12,darabos,darabos,27/Jun/14 16:27,23/Jan/17 05:35,14/Jul/23 06:25,05/Jul/14 05:07,,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"Because BoundedPriorityQueue is private and not registered with Kryo, RDD.top cannot be used when using Kryo (the recommended configuration).

Curiously BoundedPriorityQueue is registered by GraphKryoRegistrator. But that's the wrong registrator. (Is there one for Spark Core?)",,ankitBhardwaj12,apachespark,darabos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402341,,,Mon Jan 23 05:35:02 UTC 2017,,,,,,,,,,"0|i1x927:",402404,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/14 05:15;ankitBhardwaj12;Please assign this issue to me .I am working on this.;;;","02/Jul/14 10:50;darabos;You're the best, Ankit! Thanks!;;;","04/Jul/14 06:44;ankitBhardwaj12;Created a pull request for it :https://github.com/apache/spark/pull/1299;;;","08/Jul/14 12:29;darabos;Thanks! Until 1.1 is out, here's a workaround:

    kryo.register(Class.forName(""org.apache.spark.util.BoundedPriorityQueue""))
;;;","23/Jan/17 05:35;apachespark;User 'bhardwajank' has created a pull request for this issue:
https://github.com/apache/spark/pull/1299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark shell hides stderr output,SPARK-2300,12724005,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor,26/Jun/14 23:49,05/Nov/14 10:45,14/Jul/23 06:25,26/Jun/14 23:50,1.0.0,,,,,,,,1.1.0,,,,,PySpark,,,,,0,,,,,,"All the important Spark logging information is piped to stderr, which is masked in the pyspark shell.",,andrewor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402190,,,Thu Jun 26 23:50:22 UTC 2014,,,,,,,,,,"0|i1x85b:",402255,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/14 23:50;andrewor;https://github.com/apache/spark/pull/1178;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskSchedulerImpl and TaskSetManager do not properly prioritize which tasks get assigned to an executor,SPARK-2294,12723927,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,kayousterhout,kayousterhout,26/Jun/14 17:43,06/Aug/14 06:03,14/Jul/23 06:25,06/Aug/14 06:03,1.0.0,1.0.1,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"If an executor E is free, a task may be speculatively assigned to E when there are other tasks in the job that have not been launched (at all) yet.  Similarly, a task without any locality preferences may be assigned to E when there was another NODE_LOCAL task that could have been scheduled. 

This happens because TaskSchedulerImpl calls TaskSetManager.resourceOffer (which in turn calls TaskSetManager.findTask) with increasing locality levels, beginning with PROCESS_LOCAL, followed by NODE_LOCAL, and so on until the highest currently allowed level.  Now, supposed NODE_LOCAL is the highest currently allowed locality level.  The first time findTask is called, it will be called with max level PROCESS_LOCAL; if it cannot find any PROCESS_LOCAL tasks, it will try to schedule tasks with no locality preferences or speculative tasks.  As a result, speculative tasks or tasks with no preferences may be scheduled instead of NODE_LOCAL tasks.

cc [~matei]",,codingcat,kayousterhout,mridulm80,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402112,,,Mon Jul 07 04:31:13 UTC 2014,,,,,,,,,,"0|i1x7of:",402178,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/14 01:58;mridulm80;I agree; We should bump no locality pref and speculative tasks to NODE_LOCAL level after NODE_LOCAL tasks have been scheduled (if available), and not check for them at PROCESS_LOCAL max locality. So they get scheduled before RACK_LOCAL but after NODE_LOCAL.
This is an artifact of the design when there was no PROCESS_LOCAL and NODE_LOCAL was the best schedule possible (without explicitly having these level : we had node and any).;;;","07/Jul/14 04:31;codingcat;PR: https://github.com/apache/spark/pull/1313;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove use of spark.worker.instances,SPARK-2289,12723868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,tgraves,tgraves,26/Jun/14 13:22,26/Jun/14 13:23,14/Jul/23 06:25,26/Jun/14 13:23,1.0.0,,,,,,,,1.0.1,1.1.0,,,,YARN,,,,,0,,,,,,"spark.worker.instances was added as part of this commit: 1617816

My understanding is that SPARK_WORKER_INSTANCES is supported for backwards compatibility,
but spark.worker.instances is never used (SparkSubmit.scala sets spark.executor.instances) so should
not have been added.
",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402053,,,Thu Jun 26 13:22:58 UTC 2014,,,,,,,,,,"0|i1x7bb:",402119,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/14 13:22;tgraves;https://github.com/apache/spark/pull/1214;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed tasks reported as success if the failure reason is not ExceptionFailure,SPARK-2284,12723789,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,26/Jun/14 01:06,26/Jun/14 20:46,14/Jul/23 06:25,26/Jun/14 05:35,,,,,,,,,1.0.1,1.1.0,,,,,,,,,0,,,,,,"We have many other failure reasons, such as TaskResultLost, TaskKilled, FetchFailed etc. In the web ui, we count non-ExceptionFailure failures as successful tasks, which is highly misleading. ",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2015,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401974,,,2014-06-26 01:06:20.0,,,,,,,,,,"0|i1x6tr:",402040,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PruningSuite fails if run right after HiveCompatibilitySuite,SPARK-2283,12723777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,25/Jun/14 23:35,26/Jun/14 20:44,14/Jul/23 06:25,26/Jun/14 20:44,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{{HiveCompatibilitySuite}} caches test tables in memory.  As the  {{srcpart}} test table is loaded by default, it's cached by {{HiveCompatibilitySuite}}. Later, it is used by the first test case in {{PruningTest}} to test column pruning. But currently {{InMemoryColumnarTableScan}} doesn't support column pruning.

The solution is to reset the test environment before running {{PruningSuite}}.",,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401962,,,Thu Jun 26 20:44:15 UTC 2014,,,,,,,,,,"0|i1x6rj:",402030,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/14 20:44;marmbrus;https://github.com/apache/spark/pull/1221;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark crashes if too many tasks complete quickly,SPARK-2282,12723773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,25/Jun/14 23:04,31/Jul/14 22:37,14/Jul/23 06:25,04/Jul/14 06:03,0.9.1,1.0.0,1.0.1,,,,,,0.9.2,1.0.0,1.0.1,1.1.0,,PySpark,,,,,0,,,,,,"Upon every task completion, PythonAccumulatorParam constructs a new socket to the Accumulator server running inside the pyspark daemon. This can cause a buildup of used ephemeral ports from sockets in the TIME_WAIT termination stage, which will cause the SparkContext to crash if too many tasks complete too quickly. We ran into this bug with 17k tasks completing in 15 seconds.

This bug can be fixed outside of Spark by ensuring these properties are set (on a linux server);
echo ""1"" > /proc/sys/net/ipv4/tcp_tw_reuse
echo ""1"" > /proc/sys/net/ipv4/tcp_tw_recycle

or by adding the SO_REUSEADDR option to the Socket creation within Spark.",,apachespark,carlilek,freeman-lab,ilikerps,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401958,,,Thu Jul 31 22:36:59 UTC 2014,,,,,,,,,,"0|i1x6qn:",402026,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/14 00:13;carlilek;We may be running into this issue on our cluster. Any input on whether this property needs to be set on all nodes or on only the master? I ask because we dynamically spin up spark clusters on a larger general purpose compute cluster, so I'm hesitant to start changing sysctls willy nilly unless I absolutely have to. 

Alternately, is that SO_REUSEADDR merely a setting one can chnage in one of the conf files, or is that within the software written for spark? (I'm coming at this from a sysadmin point of view, so the former would be much easier!)

Odd thing is that we're seeing it on 1.0.1, in which it is supposed to be fixed...
Thanks, 
Ken;;;","16/Jul/14 06:26;ilikerps;This should actually only be necessary on the master. Use of the SO_REUSEADDR property (equivalently, sysctl tcp_tw_reuse) means that the number of used sockets will increase to the maximum number of ephemeral ports, but then should remain constant. It's possible that if another process tries to allocate an ephemeral port during this time, it will fail.

While tcp_tw_reuse is generally considered ""safe"", setting *tcp_tw_recycle* can lead to unexpected packet arrival from closed streams (though it's very unlikely), but is a more guaranteed solution. This should cause the connections to be recycled immediately after the TCP teardown, and thus no buildup of sockets should occur.

Please let me know if setting either of these parameters helps on the driver machine. You can also verify that this problem is occurring by doing a {{netstat -lpn}} during execution, iirc, which should display an inordinate number of open connections on the Spark Driver process and on a Python daemon one.;;;","17/Jul/14 15:06;carlilek;So we've just given this a try with a 32 node cluster. Without the two sysctl commands, it obviously failed, using this code in pyspark: 

{code}
data = sc.parallelize(range(0,30000000), 2000).map(lambda x: range(0,300))
data.cache()
data.count()
for i in range(0,20): data.count()
{code}

Unfortunately, with the two sysctls implemented on all nodes in the cluster, it also failed. Here's the java errors we see: 
{code:java}
14/07/17 10:55:37 ERROR DAGSchedulerActorSupervisor: eventProcesserActor failed; shutting down SparkContext
java.net.NoRouteToHostException: Cannot assign requested address
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:579)
        at java.net.Socket.connect(Socket.java:528)
        at java.net.Socket.<init>(Socket.java:425)
        at java.net.Socket.<init>(Socket.java:208)
        at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:404)
        at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:387)
        at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:72)
        at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:280)
        at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:278)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at org.apache.spark.Accumulators$.add(Accumulators.scala:278)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:820)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1226)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/spark-current/python/pyspark/rdd.py"", line 708, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File ""/usr/local/spark-current/python/pyspark/rdd.py"", line 699, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File ""/usr/local/spark-current/python/pyspark/rdd.py"", line 619, in reduce
    vals = self.mapPartitions(func).collect()
  File ""/usr/local/spark-current/python/pyspark/rdd.py"", line 583, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File ""/usr/local/spark-current/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"", line 537, in __call__
  File ""/usr/local/spark-current/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o158.collect.
: org.apache.spark.SparkException: Job 14 cancelled as part of cancellation of all jobs
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
        at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1009)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:499)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)
        at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
        at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:499)
        at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1170)
        at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1166)
        at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295)
        at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:253)
        at akka.actor.ActorCell.handleFailure(ActorCell.scala:338)
        at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:423)
        at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)
        at akka.dispatch.Mailbox.run(Mailbox.scala:218)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

>>> 14/07/17 10:55:38 ERROR OneForOneStrategy: Cannot assign requested address
java.net.NoRouteToHostException: Cannot assign requested address
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:579)
        at java.net.Socket.connect(Socket.java:528)
        at java.net.Socket.<init>(Socket.java:425)
        at java.net.Socket.<init>(Socket.java:208)
        at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:404)
        at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:387)
        at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:72)
        at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:280)
        at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:278)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at org.apache.spark.Accumulators$.add(Accumulators.scala:278)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:820)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1226)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

I did not see an inordinate number of connections from netstat; here's a sample output at around 10 iterations. 

{code}
Every 2.0s: netstat -lpn                                                                                                             Thu Jul 17 11:06:09 2014

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name
tcp        0      0 10.38.103.37:7077           0.0.0.0:*                   LISTEN      17945/java
tcp        0      0 0.0.0.0:1191                0.0.0.0:*                   LISTEN      5870/mmfsd
tcp        0      0 0.0.0.0:4040                0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 10.38.103.37:43721          0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 0.0.0.0:111                 0.0.0.0:*                   LISTEN      3385/rpcbind
tcp        0      0 0.0.0.0:8080                0.0.0.0:*                   LISTEN      17945/java
tcp        0      0 0.0.0.0:42833               0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 0.0.0.0:37429               0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      3827/sshd
tcp        0      0 127.0.0.1:25                0.0.0.0:*                   LISTEN      5937/master
tcp        0      0 127.0.0.1:55227             0.0.0.0:*                   LISTEN      19907/python
tcp        0      0 127.0.0.1:37723             0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 0.0.0.0:6463                0.0.0.0:*                   LISTEN      4688/sge_execd
tcp        0      0 0.0.0.0:35072               0.0.0.0:*                   LISTEN      -
tcp        0      0 0.0.0.0:47457               0.0.0.0:*                   LISTEN      3495/rpc.statd
tcp        0      0 0.0.0.0:5666                0.0.0.0:*                   LISTEN      18741/nrpe
tcp        0      0 0.0.0.0:48451               0.0.0.0:*                   LISTEN      19916/java
udp        0      0 0.0.0.0:41547               0.0.0.0:*                               -
udp        0      0 0.0.0.0:56397               0.0.0.0:*                               14452/rsyslogd
udp        0      0 0.0.0.0:51920               0.0.0.0:*                               3495/rpc.statd
udp        0      0 0.0.0.0:111                 0.0.0.0:*                               3385/rpcbind
udp        0      0 0.0.0.0:1012                0.0.0.0:*                               3385/rpcbind
udp        0      0 0.0.0.0:631                 0.0.0.0:*                               3324/portreserve
udp        0      0 10.38.103.37:123            0.0.0.0:*                               5700/ntpd
udp        0      0 10.36.103.37:123            0.0.0.0:*                               5700/ntpd
udp        0      0 127.0.0.1:123               0.0.0.0:*                               5700/ntpd
udp        0      0 0.0.0.0:123                 0.0.0.0:*                               5700/ntpd
udp        0      0 0.0.0.0:703                 0.0.0.0:*                               3495/rpc.statd
Active UNIX domain sockets (only servers)
Proto RefCnt Flags       Type       State         I-Node PID/Program name    Path
unix  2      [ ACC ]     STREAM     LISTENING     14434  3671/sssd_nss       /var/lib/sss/pipes/nss
unix  2      [ ACC ]     STREAM     LISTENING     14442  3672/sssd_pam       /var/lib/sss/pipes/pam
unix  2      [ ACC ]     STREAM     LISTENING     21858  5937/master         public/cleanup
unix  2      [ ACC ]     STREAM     LISTENING     21865  5937/master         private/tlsmgr
unix  2      [ ACC ]     STREAM     LISTENING     21869  5937/master         private/rewrite
unix  2      [ ACC ]     STREAM     LISTENING     21873  5937/master         private/bounce
unix  2      [ ACC ]     STREAM     LISTENING     21877  5937/master         private/defer
unix  2      [ ACC ]     STREAM     LISTENING     21881  5937/master         private/trace
unix  2      [ ACC ]     STREAM     LISTENING     21885  5937/master         private/verify
unix  2      [ ACC ]     STREAM     LISTENING     21889  5937/master         public/flush
unix  2      [ ACC ]     STREAM     LISTENING     21893  5937/master         private/proxymap
unix  2      [ ACC ]     STREAM     LISTENING     21897  5937/master         private/proxywrite
unix  2      [ ACC ]     STREAM     LISTENING     21901  5937/master         private/smtp
unix  2      [ ACC ]     STREAM     LISTENING     21905  5937/master         private/relay
unix  2      [ ACC ]     STREAM     LISTENING     21909  5937/master         public/showq
unix  2      [ ACC ]     STREAM     LISTENING     21913  5937/master         private/error
unix  2      [ ACC ]     STREAM     LISTENING     21917  5937/master         private/retry
unix  2      [ ACC ]     STREAM     LISTENING     21921  5937/master         private/discard
unix  2      [ ACC ]     STREAM     LISTENING     21925  5937/master         private/local
unix  2      [ ACC ]     STREAM     LISTENING     21929  5937/master         private/virtual
unix  2      [ ACC ]     STREAM     LISTENING     21933  5937/master         private/lmtp
unix  2      [ ACC ]     STREAM     LISTENING     21937  5937/master         private/anvil
unix  2      [ ACC ]     STREAM     LISTENING     21941  5937/master         private/scache
unix  2      [ ACC ]     STREAM     LISTENING     23356  5870/mmfsd          /var/mmfs/mmpmon/mmpmonSocket
unix  2      [ ACC ]     STREAM     LISTENING     8544   1/init              @/com/ubuntu/upstart
unix  2      [ ACC ]     STREAM     LISTENING     14408  3669/sssd           /var/lib/sss/pipes/private/sbus-monitor
unix  2      [ ACC ]     STREAM     LISTENING     13457  3385/rpcbind        /var/run/rpcbind.sock
unix  2      [ ACC ]     STREAM     LISTENING     14967  3813/mcelog         /var/run/mcelog-client
unix  2      [ ACC ]     STREAM     LISTENING     13603  3477/dbus-daemon    /var/run/dbus/system_bus_socket
unix  2      [ ACC ]     STREAM     LISTENING     14444  3672/sssd_pam       /var/lib/sss/pipes/private/pam
unix  2      [ ACC ]     STREAM     LISTENING     14017  3608/hald           @/var/run/hald/dbus-bOXq61fPG8
unix  2      [ ACC ]     STREAM     LISTENING     21315  5740/uuidd          /var/run/uuidd/request
unix  2      [ ACC ]     STREAM     LISTENING     14012  3608/hald           @/var/run/hald/dbus-cI10ZZX1oL
unix  2      [ ACC ]     STREAM     LISTENING     14417  3670/sssd_be        /var/lib/sss/pipes/private/sbus-dp_default.3670
{code}
The cluster is composed of 32 identical Dell R620s with 2x SandyBridge 8 core Xeons (16 cores total/server), 128GB RAM, and 10Gb ethernet. Our latency is ~0.1ms between all servers in the cluster. 1 node runs as the master, with the other 31 as slaves. 

Please let me know if you need more information or whether this looks like a different bug. ;;;","17/Jul/14 15:22;carlilek;A little more info: 
Nodes are running Scientific Linux 6.3 (Linux 2.6.32-279.el6.x86_64 #1 SMP Thu Jun 21 07:08:44 CDT 2012 x86_64 x86_64 x86_64 GNU/Linux)
Spark is run against Python 2.7.6, Java 1.7.0.25, and Scala 2.10.3. 

spark-env.sh
{code}
#!/usr/bin/env bash
ulimit -n 65535
export SCALA_HOME=/usr/local/scala-2.10.3
export SPARK_WORKER_DIR=/scratch/spark/work
export JAVA_HOME=/usr/local/jdk1.7.0_25
export SPARK_LOG_DIR=~/.spark/logs/$JOB_ID/
export SPARK_EXECUTOR_MEMORY=100g
export SPARK_DRIVER_MEMORY=100g
export SPARK_WORKER_MEMORY=100g
export SPARK_LOCAL_DIRS=/scratch/spark/tmp
export PYSPARK_PYTHON=/usr/local/python-2.7.6/bin/python
export SPARK_SLAVES=/scratch/spark/tmp/slaves
{code}

spark-defaults.conf:
{code}
spark.akka.timeout=300 
spark.storage.blockManagerHeartBeatMs=30000 
spark.akka.retry.wait=30 
spark.akka.frameSize=10000
{code};;;","17/Jul/14 16:53;ilikerps;This problem does look identical. I think I gave you the wrong netstat command, as ""-l"" only show listening sockets. Try with ""-a"" instead to see all open connections to confirm this, but the rest of your symptoms align perfectly.

I did a little Googling around for your specific kernel version, and it turns out [someone else|http://lists.openwall.net/netdev/2011/07/13/39] has had success with tcp_tw_recycle on 2.6.32. Could you try to make absolutely sure that the sysctl is taking effect? Perhaps you can try adding ""net.ipv4.tcp_tw_recycle = 1"" to /etc/sysctl.conf and then running a ""sysctl -p"" before restarting pyspark.;;;","17/Jul/14 18:14;carlilek;So I've tried a few different things at this point, and I see the behavior regardless of how I have the sysctls set. Using a {code}watch -n1 ""netstat -anp | grep TIME_WAIT | wc -l""{code} command, I can see the number of ephemeral ports used climb up and up and up (to somewhere north of 29000), and then it crashes out, and the number of TIME_WAITs gradually decreases. If I use a 5 node cluster, I can see it climbing, but it decreases while it is running as well, and the 20 iterations manage to complete. The maximum I have seen with a 5 node is ~19000 TIME_WAITs. 

I see the exact same behavior with the sysctls turned off, so I have to assume that the code change has worked around the issue. However, our cluster has very, very fast communication between nodes, so we still run up against the core issue (that of Spark using A TON of ephemeral ports for pyspark) with larger worker counts (ie, greater than, say, 10). ;;;","17/Jul/14 18:27;ilikerps;This problem is kinda silly because we're accumulating these updates from a single thread in the DAGScheduler, so we should only really have one socket open at a time, but it's very short lived. We could just reuse the connection with a relatively minor refactor of accumulators.py and PythonAccumulatorParam.;;;","17/Jul/14 21:05;carlilek;Awesome. I was afraid we were trying to chase down something else here. Glad to hear that it's a known issue and that you've got a good idea how to fix it. Thanks for the quick response!

--Ken;;;","20/Jul/14 23:45;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/1503;;;","20/Jul/14 23:48;ilikerps;Hey Ken,

I created [PR 1503|https://github.com/apache/spark/pull/1503] to implement the solution I mentioned. It would be great if you could try testing out this patch on your cluster.

While testing, I noticed that the ephemeral ports were still growing with number of tasks due to how we launch new tasks on the PySpark daemon. However, this should only affect workers, and the rate of buildup should be divided by the number of workers. In other words, it should only ever be a problem on a very small cluster.;;;","21/Jul/14 01:12;carlilek;Hi Aaron, 

I have pulled the spark-master repo and implemented the pull request. In testing with the script above, I was able to iterate 20 and 40 times successfully on a 30 node cluster. This is looking good. I'll leave it to the scientist who uses the cluster to do some more testing, but I think at first blush, this is a great solution. Thanks for the quick work!

-Ken;;;","22/Jul/14 03:18;freeman-lab;Hi all, I'm ""the scientist"", a couple updates from more real world testing, looking very promising!

- Set-up: 60 node cluster, an analysis with iterative updates (essentially a sequence of two map-reduce steps on each iteration), data cached and counted before starting iterations
- 250 GB data set, 4000 tasks / stage, ~6 seconds for each stage to complete. Before the patch I reliably hit the error after about 5 iterations, with the patch 20+ complete.
- 2.3 TB data set, 26000 tasks / stage, ~27 seconds for each stage to complete. Before the patch more than one iteration always failed, with the patch 20+ complete.

So it's looking really good. I can also try the other extreme (very small cluster) to see if that issue manifests. Aaron, big thanks for helping with this, it's a big deal for our workflows, so really terrific to get to the bottom of it!

-- Jeremy;;;","22/Jul/14 15:10;carlilek;Hi Aaron, 

Another question for you. Would it work for me to just drop the two changed files into our install of Spark 1.0.1 release copy, or is that likely to cause issues? 

Thanks, 
Ken;;;","22/Jul/14 17:08;ilikerps;Great to hear! These files haven't been changed since the 1.0.1 release besides this patch, so it should be fine to just drop them in. (A generally safer option would be to do a git merge, though, against Spark's refs/pull/1503/head branch.);;;","22/Jul/14 22:31;pwendell;[~carlilek] I'd actually recommend just pulling Spark from the branch-1.0 maintaince branch. We usually recommend users do this since we only add stability fixes on those branches.;;;","23/Jul/14 00:38;ilikerps;[~pwendell] That would in general be the right solution, but this particular change hasn't been merged yet (referring to the second PR on this bug, which is a more complete fix).;;;","23/Jul/14 00:43;pwendell;Ah my b. I was confused.;;;","23/Jul/14 12:51;carlilek;Well, something didn't work quite right.. our copy of 1.0.1 is the prebuilt copy for Hadoop 1/CDH3. So I did a git init in that directory, then did a git pull https://github.com/apache/spark/ +refs/pull/1503/head

Well, that didn't work... 

I don't expect you to solve my git noob problems, so I'll work with someone here to figure it out. ;;;","23/Jul/14 14:22;carlilek;Merging just the two files also did not work. I received a bunch of these errors during the test: 
{code}Exception happened during processing of request from ('127.0.0.1', 33116)
Traceback (most recent call last):
  File ""/usr/local/python-2.7.6/lib/python2.7/SocketServer.py"", line 295, in _handle_request_noblock
    self.process_request(request, client_address)
  File ""/usr/local/python-2.7.6/lib/python2.7/SocketServer.py"", line 321, in process_request
    self.finish_request(request, client_address)
  File ""/usr/local/python-2.7.6/lib/python2.7/SocketServer.py"", line 334, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File ""/usr/local/python-2.7.6/lib/python2.7/SocketServer.py"", line 649, in __init__
    self.handle()
  File ""/usr/local/spark-current/python/pyspark/accumulators.py"", line 224, in handle
    num_updates = read_int(self.rfile)
  File ""/usr/local/spark-current/python/pyspark/serializers.py"", line 337, in read_int
    raise EOFError
EOFError
{code}
And then it errored out with the usual java thing. ;;;","31/Jul/14 22:36;joshrosen;Merged the improved fix from https://github.com/apache/spark/pull/1503 into 1.1.;;;",,,,,,,,,,,,,
Kryo cannot serialize results returned by asJavaIterable (and thus groupBy/cogroup are broken in Java APIs when Kryo is used),SPARK-2270,12723544,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,rxin,rxin,25/Jun/14 02:18,25/Jun/14 19:43,14/Jul/23 06:25,25/Jun/14 19:43,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Spark Core,,,,,0,,,,,,"The combination of Kryo serializer & Java API could lead to the following exception in groupBy/groupByKey/cogroup:
{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Exception while deserializing and fetching task: java.lang.UnsupportedOperationException
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
scala.Option.foreach(Option.scala:236)
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
akka.actor.ActorCell.invoke(ActorCell.scala:456)
akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
akka.dispatch.Mailbox.run(Mailbox.scala:219)
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)3:45 PM
{code}

or
{code}
14/06/24 16:38:09 ERROR TaskResultGetter: Exception while getting task result
java.lang.UnsupportedOperationException
at java.util.AbstractCollection.add(AbstractCollection.java:260)
at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109)
at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
at carbonite.serializer$mk_collection_reader$fn__50.invoke(serializer.clj:57)
at clojure.lang.Var.invoke(Var.java:383)
at carbonite.ClojureVecSerializer.read(ClojureVecSerializer.java:17)
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:338)
at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:293)
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:144)
at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
at org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:480)
at org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:316)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1213)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:744)
{code}


Thanks [~sorenmacbeth] for reporting this.
",,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401729,,,Wed Jun 25 19:43:55 UTC 2014,,,,,,,,,,"0|i1x5bz:",401798,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/14 19:43;pwendell;Issue resolved by pull request 1206
[https://github.com/apache/spark/pull/1206];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up and add unit tests for resourceOffers in MesosSchedulerBackend,SPARK-2269,12723535,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengt,pwendell,pwendell,25/Jun/14 01:48,11/Nov/14 22:29,14/Jul/23 06:25,11/Nov/14 22:29,1.1.0,,,,,,,,1.2.0,,,,,Mesos,,,,,0,,,,,,"This function could be simplified a bit. We could re-write it without offerableIndices or creating the mesosTasks array as large as the offer list. There is a lot of logic around making sure you get the correct index into mesosTasks and offers, really we should just build mesosTasks directly from the offers we get back. To associate the tasks we are launching with the offers we can just create a hashMap from the slaveId to the original offer.

The basic logic of the function is that you take the mesos offers, convert them to spark offers, then convert the results back.

One reason I think it might be designed as it is now is to deal with the case where Mesos gives multiple offers for a single slave. I checked directly with the Mesos team and they said this won't ever happen, you'll get at most one offer per mesos slave within a set of offers.",,pwendell,tnachen,tstclair,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2204,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401720,,,Fri Jul 18 21:10:31 UTC 2014,,,,,,,,,,"0|i1x59z:",401789,,,,,,,,,,,,,1.2.0,,,,,,,,,,"18/Jul/14 21:10;tnachen;Created a PR for this: https://github.com/apache/spark/pull/1487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log exception when TaskResultGetter fails to fetch/deserialize task result,SPARK-2267,12723523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,25/Jun/14 00:16,26/Jun/14 20:46,14/Jul/23 06:25,25/Jun/14 07:20,1.0.0,,,,,,,,1.0.1,,,,,,,,,,0,,,,,,"In 1.0.0, it is pretty confusing to get an error message that just says: ""Exception while deserializing and fetching task ..."" without a stack trace. 



This has been fixed in master by [~matei] https://github.com/apache/spark/commit/a01f3401e32ca4324884d13c9fad53c6c87bb5f0
",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2015,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401708,,,2014-06-25 00:16:50.0,,,,,,,,,,"0|i1x57r:",401779,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Log page on Worker UI displays ""Some(app-id)""",SPARK-2266,12723519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,andrewor14,andrewor,25/Jun/14 00:09,05/Nov/14 10:43,14/Jul/23 06:25,25/Jun/14 21:31,1.0.0,,,,,,,,1.0.1,1.1.0,,,,,,,,,0,,,,,,Oops.,,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/14 00:09;andrewor;Screen Shot 2014-06-24 at 5.07.54 PM.png;https://issues.apache.org/jira/secure/attachment/12652317/Screen+Shot+2014-06-24+at+5.07.54+PM.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401704,,,Wed Jun 25 21:31:02 UTC 2014,,,,,,,,,,"0|i1x56v:",401775,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/14 21:31;pwendell;Resolved by:
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9aa603296c285e1acf4bde64583f203008ba3e91;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CachedTableSuite SQL Tests are Failing,SPARK-2264,12723493,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,pwendell,pwendell,24/Jun/14 22:08,25/Jun/14 02:05,14/Jul/23 06:25,25/Jun/14 02:05,,,,,,,,,1.1.0,,,,,SQL,,,,,0,,,,,,"{code}
[info] CachedTableSuite:
[info] - read from cached table and uncache *** FAILED ***
[info]   java.lang.RuntimeException: Table Not Found: testData
[info]   at scala.sys.package$.error(package.scala:27)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:64)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:64)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog.lookupRelation(Catalog.scala:64)
[info]   at org.apache.spark.sql.SQLContext.table(SQLContext.scala:185)
[info]   at org.apache.spark.sql.CachedTableSuite$$anonfun$1.apply$mcV$sp(CachedTableSuite.scala:43)
[info]   at org.apache.spark.sql.CachedTableSuite$$anonfun$1.apply(CachedTableSuite.scala:27)
[info]   at org.apache.spark.sql.CachedTableSuite$$anonfun$1.apply(CachedTableSuite.scala:27)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
[info]   ...
[info] - correct error on uncache of non-cached table *** FAILED ***
[info]   Expected exception java.lang.IllegalArgumentException to be thrown, but java.lang.RuntimeException was thrown. (CachedTableSuite.scala:55)
[info] - SELECT Star Cached Table *** FAILED ***
[info]   java.lang.RuntimeException: Table Not Found: testData
[info]   at scala.sys.package$.error(package.scala:27)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:64)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:64)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog.lookupRelation(Catalog.scala:64)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$1.applyOrElse(Analyzer.scala:67)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$1.applyOrElse(Analyzer.scala:65)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:183)
[info]   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
[info]   ...
[info] - Self-join cached *** FAILED ***
[info]   java.lang.RuntimeException: Table Not Found: testData
[info]   at scala.sys.package$.error(package.scala:27)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:64)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:64)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog.lookupRelation(Catalog.scala:64)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$1.applyOrElse(Analyzer.scala:67)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$1.applyOrElse(Analyzer.scala:65)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:183)
[info]   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
[info]   ...
[info] - 'CACHE TABLE' and 'UNCACHE TABLE' SQL statement *** FAILED ***
[info]   java.lang.RuntimeException: Table Not Found: testData
[info]   at scala.sys.package$.error(package.scala:27)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:64)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:64)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.catalyst.analysis.SimpleCatalog.lookupRelation(Catalog.scala:64)
[info]   at org.apache.spark.sql.SQLContext.cacheTable(SQLContext.scala:189)
[info]   at org.apache.spark.sql.execution.CacheCommand.sideEffectResult$lzycompute(commands.scala:110)
[info]   at org.apache.spark.sql.execution.CacheCommand.sideEffectResult(commands.scala:108)
[info]   at org.apache.spark.sql.execution.CacheCommand.execute(commands.scala:118)
[info]   at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:322)
[info]   ...
{code}",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401678,,,Wed Jun 25 02:05:29 UTC 2014,,,,,,,,,,"0|i1x51b:",401750,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/14 02:05;pwendell;Fixed by:
https://github.com/apache/spark/pull/1201;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Can't insert Map<K, V> values to Hive tables",SPARK-2263,12723491,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,24/Jun/14 22:05,25/Jun/14 07:16,14/Jul/23 06:25,25/Jun/14 07:16,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"Scala {{Map\[K, V\]}} values are not converted to their Java correspondence:

{code}
scala> loadTestTable(""src"")

scala> hql(""create table m(value map<int, string>)"")
res1: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[0] at RDD at SchemaRDD.scala:100
== Query Plan ==
<Native command: executed by Hive>

scala> hql(""insert overwrite table m select map(key, value) from src"")
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0:0 failed 1 times, most recent failure: Exception failure in TID 0 on host localhost: java.lang.ClassCastException: scala.collection.immutable.Map$Map1 cannot be cast to java.util.Map
        org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getMap(StandardMapObjectInspector.java:82)
        org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:515)
        org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)
        org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$2$$anonfun$apply$1.apply(InsertIntoHiveTable.scala:200)
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$2$$anonfun$apply$1.apply(InsertIntoHiveTable.scala:192)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:149)
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$1.apply(InsertIntoHiveTable.scala:158)
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$1.apply(InsertIntoHiveTable.scala:158)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:112)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1040)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1024)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1022)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1022)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:640)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:640)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:640)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1214)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


scala> 
{code}",,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401676,,,Wed Jun 25 01:36:27 UTC 2014,,,,,,,,,,"0|i1x50v:",401748,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/14 01:36;lian cheng;PR: https://github.com/apache/spark/pull/1205;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark submit standalone-cluster mode is broken,SPARK-2260,12723423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor,24/Jun/14 18:12,27/Jun/16 22:08,14/Jul/23 06:25,30/Jul/14 06:52,1.0.1,,,,,,,,1.1.0,,,,,Spark Core,,,,,2,,,,,,"Well, it is technically not officially supported... but we should still fix it.

In particular, important configs such as spark.master and the application jar are not propagated to the worker nodes properly, due to obvious missing pieces in the code.",,andrewor,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3560,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401610,,,Wed Jul 30 06:52:39 UTC 2014,,,,,,,,,,"0|i1x4mn:",401683,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 02:40;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/1538;;;","30/Jul/14 06:52;pwendell;Issue resolved by pull request 1538
[https://github.com/apache/spark/pull/1538];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark submit documentation for --deploy-mode is highly misleading,SPARK-2259,12723405,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor,24/Jun/14 17:15,05/Nov/14 10:45,14/Jul/23 06:25,27/Jun/14 23:12,1.0.1,,,,,,,,1.0.1,1.1.0,,,,Documentation,,,,,0,,,,,,"There are a few issues:

1. Client mode does not necessarily mean the driver program must be launched outside of the cluster.

2. For standalone clusters, only client mode is currently supported. This was the case supported even before 1.0.

Currently, the docs tell the user to use cluster deploy mode when deploying ""your driver program within the cluster,"" which is true also for standalone-client mode. In short, the docs encourage the user to use standalone-cluster, an unsupported mode.",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401592,,,Fri Jun 27 23:12:04 UTC 2014,,,,,,,,,,"0|i1x4in:",401665,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/14 22:53;andrewor;https://github.com/apache/spark/pull/1200;;;","27/Jun/14 23:12;pwendell;Issue resolved by pull request 1200
[https://github.com/apache/spark/pull/1200];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker UI displays zombie executors,SPARK-2258,12723399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,24/Jun/14 16:24,22/Feb/23 23:14,14/Jul/23 06:25,26/Jun/14 00:33,1.0.0,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,See attached.,,andrewor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/14 16:24;andrewor;Screen Shot 2014-06-24 at 9.23.18 AM.png;https://issues.apache.org/jira/secure/attachment/12652221/Screen+Shot+2014-06-24+at+9.23.18+AM.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401586,,,Thu Jun 26 00:33:51 UTC 2014,,,,,,,,,,"0|i1x4hb:",401659,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/14 00:33;andrewor;https://github.com/apache/spark/pull/1213;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The algorithm of ALS in mlib lacks a parameter ,SPARK-2257,12723339,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,bing,bing,24/Jun/14 12:01,24/Jun/14 12:17,14/Jul/23 06:25,24/Jun/14 12:17,1.0.0,,,,,,,,1.0.1,1.1.0,,,,MLlib,,,,,0,patch,,,,,"When I test ALS algorithm using netflix data, I find I cannot get the acurate results declared by the paper. The best  MSE value is 0.9066300038109709(RMSE 0.952), which is worse than the paper's result. If I increase the number of features or the number of iterations, I will get a worse result. After I studing the paper and source code, I find a bug in the updateBlock function of ALS.

orgin code is:
    while (i < rank) {
        // ---
       fullXtX.data(i * rank + i) += lambda

        i += 1
      }

The code doesn't consider the number of products that one user rates. So this code should be modified:
    while (i < rank) {
 
        //ratingsNum(index) equals the number of products that a user rates
        fullXtX.data(i * rank + i) += lambda * ratingsNum(index)
        i += 1
      } 

After I modify code, the MSE value has been decreased, this is one test result
conditions:
val numIterations =20
val features = 30
val model = ALS.train(trainRatings,features, numIterations, 0.06)

result of modified version:
MSE: Double = 0.8472313396478773
RMSE: 0.92045


results of version of 1.0
MSE: Double = 1.2680743123043832
RMSE: 1.1261

In order to add the vector ratingsNum, I want to change the InLinkBlock structure as follows:
private[recommendation] case class InLinkBlock(elementIds: Array[Int], ratingsNum:Array[Int], ratingsForBlock: Array[Array[(Array[Int], Array[Double])]])
So I could calculte the vector ratingsNum in the function of makeInLinkBlock. This is the code I add in the makeInLinkBlock:

........... 
//added 
  val ratingsNum = new Array[Int](numUsers)
   ratings.map(r => ratingsNum(userIdToPos(r.user)) += 1)
//end of added
  InLinkBlock(userIds, ratingsNum, ratingsForBlock)
........


Is this solution reasonable??",spark 1.0,bing,mengxr,,,,,,,,,,,,,,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401526,,,Tue Jun 24 12:16:56 UTC 2014,,,,,,,,,,"0|i1x43z:",401599,,,,,,,,,,,,,1.0.0,,,,,,,,,,"24/Jun/14 12:13;srowen;I don't think this is a bug, in the sense that it is just a different formulation of ALS. It's in the ALS-WR paper, but not the more well-known Hu/Koren/Volinsky paper. 

This is ""weighted regularization"" and it does help in some cases. In fact, it's already implemented in MLlib, although went in just after 1.0.0:

https://github.com/apache/spark/commit/a6e0afdcf0174425e8a6ff20b2bc2e3a7a374f19#diff-2b593e0b4bd6eddab37f04968baa826c

I think this is therefore already implemented.;;;","24/Jun/14 12:16;bing;I find it and I will close this jira;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark: <RDD>.take doesn't work ... sometimes ...,SPARK-2256,12723304,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,angel2014,angel2014,24/Jun/14 07:51,03/Oct/14 13:29,14/Jul/23 06:25,03/Oct/14 13:29,1.0.0,,,,,,,,1.1.0,,,,,PySpark,,,,,0,pyspark,RDD,take,windows,,"If I try to ""take"" some lines from a file, sometimes it doesn't work

Code: 

myfile = sc.textFile(""A_ko"")
print myfile.take(10)

Stacktrace:

14/06/24 09:29:27 INFO DAGScheduler: Failed to run take at mytest.py:19
Traceback (most recent call last):
  File ""mytest.py"", line 19, in <module>
    print myfile.take(10)
  File ""spark-1.0.0-bin-hadoop2\python\pyspark\rdd.py"", line 868, in take
    iterator = mapped._jrdd.collectPartitions(partitionsToTake)[0].iterator()
  File ""spark-1.0.0-bin-hadoop2\python\lib\py4j-0.8.1-src.zip\py4j\java_gateway.py"", line 537, in __call__
  File ""spark-1.0.0-bin-hadoop2\python\lib\py4j-0.8.1-src.zip\py4j\protocol.py"", line 300, in get_return_value

Test data:

<START TEST DATA>
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
<END TEST DATA>

Note: If I remove only one ""A"" character, it works OK!",local file/remote HDFS,angel2014,farrellee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/14 16:01;angel2014;A_test.zip;https://issues.apache.org/jira/secure/attachment/12656282/A_test.zip",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401491,,,Fri Oct 03 07:33:12 UTC 2014,,,,,,,,,,"0|i1x3w7:",401564,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/14 15:33;farrellee;[~angel2014] i've tried this using a local file and line lengths from 1 to 64K (by powers of 2) and have not been able to reproduce this. how frequently does this fail? are you still seeing this issue on the tip of master?;;;","17/Jul/14 16:01;angel2014;I've tried with different files and sizes ... but I can't figure out the
reason why it doesn't work ...

If I try with the files downloaded from
https://github.com/richardbishop/PerformanceTestData ... everything works
OK.





;;;","17/Jul/14 16:20;farrellee;are you using a local master, mesos, yarn?

for me -

{code}
./dist/bin/pyspark
...
[repeat this a bunch, w/ while True]
sc.textFile(""A_ko"").take(10)
sc.textFile(""A_ko"").take(50)
...
{code}

and i cannot reproduce;;;","17/Jul/14 17:53;angel2014;I've tried using local and master spark in standalone mode.

;;;","17/Jul/14 18:02;farrellee;maybe there's an issue in the platform?

i'm on -
{code}
$ head -n1 /etc/issue
Fedora release 20 (Heisenbug)
$ python --version 
Python 2.7.5
$ java -version
openjdk version ""1.8.0_05""
OpenJDK Runtime Environment (build 1.8.0_05-b13)
OpenJDK 64-Bit Server VM (build 25.5-b02, mixed mode)
{code};;;","18/Jul/14 11:14;angel2014;It seems so!

Launching from my local IDE ... it doesn't work:

Windows 7 64 bits
Eclipse IDE
Python 2.7.6 |Anaconda 2.0.0 (64-bit)
java version ""1.7.0_55""
Java(TM) SE Runtime Environment (build 1.7.0_55-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)

Launching from the cluster (pyspark console) ... it works perfectly:

Ubuntu 12.04.4 LTS
Python 2.7.3
java version ""1.6.0_31""
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)

So ... it seems like a problem with my f**** Windows environment.

Thanks.





;;;","18/Jul/14 14:19;farrellee;ok. i don't have a windows machine to try. good luck!;;;","03/Oct/14 07:33;angel2014;It seems the problem has been solved in Spark 1.1.0 !!! 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
mathjax doesn't work in HTTPS (math formulas not rendered),SPARK-2252,12723268,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,24/Jun/14 03:45,24/Jun/14 06:19,14/Jul/23 06:25,24/Jun/14 06:19,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Documentation,,,,,0,,,,,,,,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401455,,,2014-06-24 03:45:42.0,,,,,,,,,,"0|i1x3o7:",401528,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLLib Naive Bayes Example SparkException: Can only zip RDDs with same number of elements in each partition,SPARK-2251,12723262,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,xiejuncs,xiejuncs,24/Jun/14 02:46,11/Oct/15 18:22,14/Jul/23 06:25,27/Jun/14 04:47,1.0.0,,,,,,,,1.0.1,1.1.0,,,,MLlib,,,,,0,Naive-Bayes,,,,,"I follow the exact code from Naive Bayes Example (http://spark.apache.org/docs/latest/mllib-naive-bayes.html) of MLLib.

When I executed the final command: 
val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()

It complains ""Can only zip RDDs with same number of elements in each partition"".

I got the following exception:
{code}
14/06/23 19:39:23 INFO SparkContext: Starting job: count at <console>:31
14/06/23 19:39:23 INFO DAGScheduler: Got job 3 (count at <console>:31) with 2 output partitions (allowLocal=false)
14/06/23 19:39:23 INFO DAGScheduler: Final stage: Stage 4(count at <console>:31)
14/06/23 19:39:23 INFO DAGScheduler: Parents of final stage: List()
14/06/23 19:39:23 INFO DAGScheduler: Missing parents: List()
14/06/23 19:39:23 INFO DAGScheduler: Submitting Stage 4 (FilteredRDD[14] at filter at <console>:31), which has no missing parents
14/06/23 19:39:23 INFO DAGScheduler: Submitting 2 missing tasks from Stage 4 (FilteredRDD[14] at filter at <console>:31)
14/06/23 19:39:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
14/06/23 19:39:23 INFO TaskSetManager: Starting task 4.0:0 as TID 8 on executor localhost: localhost (PROCESS_LOCAL)
14/06/23 19:39:23 INFO TaskSetManager: Serialized task 4.0:0 as 3410 bytes in 0 ms
14/06/23 19:39:23 INFO TaskSetManager: Starting task 4.0:1 as TID 9 on executor localhost: localhost (PROCESS_LOCAL)
14/06/23 19:39:23 INFO TaskSetManager: Serialized task 4.0:1 as 3410 bytes in 1 ms
14/06/23 19:39:23 INFO Executor: Running task ID 8
14/06/23 19:39:23 INFO Executor: Running task ID 9
14/06/23 19:39:23 INFO BlockManager: Found block broadcast_0 locally
14/06/23 19:39:23 INFO BlockManager: Found block broadcast_0 locally
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:0+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:24+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:0+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:24+24
14/06/23 19:39:23 ERROR Executor: Exception in task ID 9
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
14/06/23 19:39:23 ERROR Executor: Exception in task ID 8
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
14/06/23 19:39:23 WARN TaskSetManager: Lost TID 8 (task 4.0:0)
14/06/23 19:39:23 WARN TaskSetManager: Loss was due to org.apache.spark.SparkException
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
14/06/23 19:39:23 ERROR TaskSetManager: Task 4.0:0 failed 1 times; aborting job
14/06/23 19:39:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
14/06/23 19:39:23 INFO DAGScheduler: Failed to run count at <console>:31
14/06/23 19:39:23 INFO TaskSetManager: Loss was due to org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition [duplicate 1]
14/06/23 19:39:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
14/06/23 19:39:23 INFO TaskSchedulerImpl: Cancelling stage 4
org.apache.spark.SparkException: Job aborted due to stage failure: Task 4.0:0 failed 1 times, most recent failure: Exception failure in TID 8 on host localhost: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
        org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
        scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
        org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
        org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
        org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:724)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1038)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1022)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1020)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1020)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:638)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:638)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:638)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1212)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}","OS: Fedora Linux
Spark Version: 1.0.0. Git clone from the Spark Repository",apachespark,mengxr,pwendell,xiejuncs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401449,,,Sun Oct 11 18:22:02 UTC 2015,,,,,,,,,,"0|i1x3mv:",401522,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/14 21:43;srowen;For what it's worth, I can reproduce this. In the sample, the ""test"" RDD has 2 partitions, containing 2 and 1 examples. The ""prediction"" RDD has 2 partitions, containing 1 and 2 examples respectively. So they aren't matched up, even though one is a 1-1 map() of the other. 

That seems like it shouldn't happen? maybe someone more knowledgeable can say whether that itself should occur. ""test"" is a PartitionwiseSampledRDD and ""prediction"" is a MappedRDD of course. 

If it is allowed to happen, then the example should be fixed, and I could easily supply a patch. It can be done without having to zip up RDDs to begin with.;;;","25/Jun/14 05:27;xiejuncs;Hi, Sean. Thanks very much for your insight. I am new to Spark. So if you can easily supply a patch. Please. Really appreciate it.

I am digging it according to your suggestion to see what is going on. At the same time, familiar myself with Spark. ;;;","25/Jun/14 08:07;srowen;Well the change to the examples is pretty straightforward. Instead of separately computing ""predictions"", you just:

{code}
val predictionAndLabel = test.map(x => (model.predict(x.features), x.label))
{code}

... and similarly for other languages, and other examples. In fact it seems more straightforward.

But I am wondering if this is actually a bug in PartitionwiseSampledRDD. [~mengxr] is this a bit of code you wrote or are familiar with?;;;","26/Jun/14 06:47;mengxr;[~xiejuncs] Are you running the example on the latest master or v1.0.0? I tested it on v1.0.0 and it worked well. But it did fail in the latest master. `RDD.zip` was modified after v1.0.0. So could you confirm the version you are running? ;;;","26/Jun/14 09:05;mengxr;Found a bug introduced by me in random sampler. PR: https://github.com/apache/spark/pull/1229;;;","26/Jun/14 20:33;pwendell;This is fixed in 1.0.1 via:
https://github.com/apache/spark/pull/1234/files;;;","27/Jun/14 02:49;xiejuncs;I use the following command:

git log

The first entry is 
commit 601032f5bfe2dcdc240bfcc553f401e6facbf5ec
Author: Zongheng Yang <zongheng.y@gmail.com>
Date:   Tue Jun 10 21:59:01 2014 -0700

How to find out the current version of my branch? My current branch is in master, and I add Apache/Spark as the remote upstream.

Jun;;;","27/Jun/14 04:47;pwendell;Issue resolved by pull request 1229
[https://github.com/apache/spark/pull/1229];;;","11/Oct/15 18:22;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1229;;;",,,,,,,,,,,,,,,,,,,,,,,,
spark.default.parallelism does not apply in local mode,SPARK-2248,12723257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gq,matei,matei,24/Jun/14 01:34,25/Jun/14 02:45,14/Jul/23 06:25,25/Jun/14 02:45,,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,Starter,,,,,"LocalBackend.defaultParallelism ignores the ""spark.default.parallelism"" property, unlike the other SchedulerBackends. We should make it take this in for consistency.",,gq,matei,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401444,,,Wed Jun 25 02:45:19 UTC 2014,,,,,,,,,,"0|i1x3lr:",401517,,,,,,,,,,,,,1.1.0,,,,,,,,,,"24/Jun/14 07:55;gq;PR: https://github.com/apache/spark/pull/1194;;;","25/Jun/14 02:45;pwendell;Issue resolved by pull request 1194
[https://github.com/apache/spark/pull/1194];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running sc.parallelize(..).count() hangs pyspark,SPARK-2242,12723054,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,mengxr,mengxr,23/Jun/14 03:30,05/Nov/14 10:45,14/Jul/23 06:25,25/Jun/14 17:48,1.1.0,,,,,,,,1.1.0,,,,,PySpark,,,,,0,,,,,,"Running the following code hangs pyspark in a shell:

{code}
sc.parallelize(range(100), 100).count()
{code}

It happens in the master branch, but not branch-1.0. And it seems that it only happens in a pyspark shell. [~andrewor14] helped confirm this bug.",,andrewor,mengxr,rxin,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2244,,SPARK-2244,,,SPARK-1850,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401241,,,Wed Jun 25 17:48:18 UTC 2014,,,,,,,,,,"0|i1x2dr:",401319,,,,,,,,,,,,,1.1.0,,,,,,,,,,"25/Jun/14 17:48;mengxr;Fixed in https://github.com/apache/spark/pull/1178;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EC2 script should handle quoted arguments correctly,SPARK-2241,12723053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,orikremer,pwendell,pwendell,23/Jun/14 03:22,23/Jul/14 07:10,14/Jul/23 06:25,23/Jun/14 03:24,0.9.1,1.0.0,,,,,,,0.9.2,1.0.1,1.1.0,,,EC2,,,,,0,,,,,,We should pass quoted arguments correctly to the underlying ec2 script in spark-ec2,,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401240,,,Mon Jun 23 03:24:40 UTC 2014,,,,,,,,,,"0|i1x2dj:",401318,,,,,,,,,,,,,0.9.2,1.0.1,1.1.0,,,,,,,,"23/Jun/14 03:24;pwendell;Issue resolved by pull request 1169
[https://github.com/apache/spark/pull/1169];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SizeBasedRollingPolicy throw an java.lang.IllegalArgumentException,SPARK-2229,12722994,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gq,gq,gq,22/Jun/14 04:33,23/Jun/14 01:25,14/Jul/23 06:25,23/Jun/14 01:25,,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"[RollingPolicy.scala#L112|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala#L112]  error in JDK 6
{code}
import java.text.SimpleDateFormat
import java.util.Calendar
  val formatter = new SimpleDateFormat(""--YYYY-MM-dd--HH-mm-ss--SSSS"")
{code}
=>
{code}
val formatter = new SimpleDateFormat(""--YYYY-MM-dd--HH-mm-ss--SSSS"")
java.lang.IllegalArgumentException: Illegal pattern character 'Y'
	at java.text.SimpleDateFormat.compile(SimpleDateFormat.java:768)
	at java.text.SimpleDateFormat.initialize(SimpleDateFormat.java:575)
	at java.text.SimpleDateFormat.<init>(SimpleDateFormat.java:500)
	at java.text.SimpleDateFormat.<init>(SimpleDateFormat.java:475)
	at .<init>(<console>:9)

{code}",JDK6 Mac OS x,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401181,,,2014-06-22 04:33:21.0,,,,,,,,,,"0|i1x20n:",401260,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix remaining Hive Commands,SPARK-2220,12722738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,20/Jun/14 08:59,31/Oct/14 18:35,14/Jul/23 06:25,31/Oct/14 18:35,,,,,,,,,1.2.0,,,,,SQL,,,,,0,,,,,,"None of the following have an execution plan:
{code}
private[hive] case class ShellCommand(cmd: String) extends Command
private[hive] case class SourceCommand(filePath: String) extends Command
private[hive] case class AddFile(filePath: String) extends Command
{code}

dfs is being fixed in a related PR.",,apachespark,lian cheng,marmbrus,tianyi,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2227,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400925,,,Fri Oct 31 18:35:10 UTC 2014,,,,,,,,,,"0|i1x0hj:",401010,,,,,,,,,,,,,1.2.0,,,,,,,,,,"31/Oct/14 14:23;lian cheng;It turned out that {{ShellCommand}} and {{SourceCommand}} are wrongly interpreted in Spark SQL previously. These two classes correspond to the {{\!}} and {{SOURCE}} syntaxes respectively in Spark SQL. However, back in Hive, {{\!}} is interpreted in different ways by Hive CLI and Beeline, and {{SOURCE}} is only supported by Hive CLI.

For {{\!}}, in Hive CLI, {{\!}} starts a shell command (e.g. {{\!ls;}} and {{\!cat foo;}}), while in Beeline {{\!}} starts a Beeline command (e.g. {{\!connect jdbc:hive://localhost:10000}} and {{\!run script.sql}}). And the {{SOURCE file}} command in Hive CLI is equivalent to the {{\!run file}} command in Beeline.

In a word, functionalities of these two commands should not be implemented in {{sql/core}} and/or {{sql/hive}}, but are already implemented as part of Spark SQL CLI and Hive Beeline.;;;","31/Oct/14 14:46;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3038;;;","31/Oct/14 18:35;marmbrus;Issue resolved by pull request 3038
[https://github.com/apache/spark/pull/3038];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AddJar doesn't work,SPARK-2219,12722736,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,20/Jun/14 08:55,14/Feb/15 20:54,14/Jul/23 06:25,05/Sep/14 01:48,,,,,,,,,1.2.0,,,,,SQL,,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5818,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400923,,,Wed Sep 03 01:05:34 UTC 2014,,,,,,,,,,"0|i1x0h3:",401008,,,,,,,,,,,,,1.1.0,,,,,,,,,,"03/Sep/14 01:05;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2242;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rename Equals to EqualTo in Spark SQL expressions,SPARK-2218,12722706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,20/Jun/14 05:43,20/Jun/14 07:35,14/Jul/23 06:25,20/Jun/14 07:35,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,The class name Equals is very error prone because there exists scala.Equals. I just wasted a bunch of time debugging the optimizer because of this.,,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1800,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400897,,,Fri Jun 20 05:58:33 UTC 2014,,,,,,,,,,"0|i1x0bb:",400982,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/14 05:58;rxin;Michael has a PR here https://github.com/apache/spark/pull/734

It is not fully ready yet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cast to boolean on boolean value gets turned into NOT((boolean_condition) = 0),SPARK-2210,12722694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,20/Jun/14 04:52,20/Jun/14 07:01,14/Jul/23 06:25,20/Jun/14 07:01,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{code}
explain select cast(cast(key=0 as boolean) as boolean) aaa from src
{code}

should be

{code}
[Physical execution plan:]
[Project [(key#10:0 = 0) AS aaa#7]]
[ HiveTableScan [key#10], (MetastoreRelation default, src, None), None]
{code}

However, it is currently
{code}
[Physical execution plan:]
[Project [NOT((key#10=0) = 0) AS aaa#7]]
[ HiveTableScan [key#10], (MetastoreRelation default, src, None), None]
{code}",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400885,,,2014-06-20 04:52:20.0,,,,,,,,,,"0|i1x08n:",400970,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cast shouldn't do null check twice,SPARK-2209,12722686,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,20/Jun/14 02:44,20/Jun/14 07:01,14/Jul/23 06:25,20/Jun/14 07:01,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"Cast does two null checks, one in eval and another one in the function returned by nullOrCast. It's best to get rid of the one in nullOrCast (since eval will be the more common code path).",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400877,,,Fri Jun 20 02:45:57 UTC 2014,,,,,,,,,,"0|i1x06v:",400962,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/14 02:45;rxin;https://github.com/apache/spark/pull/1143;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary exchange operators in a join on multiple tables with the same join key.,SPARK-2205,12722593,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,yhuai,yhuai,19/Jun/14 21:12,03/Aug/15 03:44,14/Jul/23 06:25,03/Aug/15 03:44,,,,,,,,,1.5.0,,,,,SQL,,,,,1,,,,,,"{code}
hql(""select * from src x join src y on (x.key=y.key) join src z on (y.key=z.key)"")

SchemaRDD[1] at RDD at SchemaRDD.scala:100
== Query Plan ==
Project [key#4:0,value#5:1,key#6:2,value#7:3,key#8:4,value#9:5]
 HashJoin [key#6], [key#8], BuildRight
  Exchange (HashPartitioning [key#6], 200)
   HashJoin [key#4], [key#6], BuildRight
    Exchange (HashPartitioning [key#4], 200)
     HiveTableScan [key#4,value#5], (MetastoreRelation default, src, Some(x)), None
    Exchange (HashPartitioning [key#6], 200)
     HiveTableScan [key#6,value#7], (MetastoreRelation default, src, Some(y)), None
  Exchange (HashPartitioning [key#8], 200)
   HiveTableScan [key#8,value#9], (MetastoreRelation default, src, Some(z)), None
{code}

However, this is fine...
{code}
hql(""select * from src x join src y on (x.key=y.key) join src z on (x.key=z.key)"")

res5: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[5] at RDD at SchemaRDD.scala:100
== Query Plan ==
Project [key#26:0,value#27:1,key#28:2,value#29:3,key#30:4,value#31:5]
 HashJoin [key#26], [key#30], BuildRight
  HashJoin [key#26], [key#28], BuildRight
   Exchange (HashPartitioning [key#26], 200)
    HiveTableScan [key#26,value#27], (MetastoreRelation default, src, Some(x)), None
   Exchange (HashPartitioning [key#28], 200)
    HiveTableScan [key#28,value#29], (MetastoreRelation default, src, Some(y)), None
  Exchange (HashPartitioning [key#30], 200)
   HiveTableScan [key#30,value#31], (MetastoreRelation default, src, Some(z)), None
{code}",,apachespark,chenghao,dsdinter,jeanlyn,joshrosen,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6380,,,SPARK-2215,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400784,,,Mon Aug 03 03:44:40 UTC 2015,,,,,,,,,,"0|i1wznj:",400875,,,,,,,,,,,,,1.5.0,,,,,,,,,,"19/Jun/14 21:21;yhuai;The cause of this bug is that in HashJoin, outputPartitioning returns the output partitioning of its left child. ;;;","11/Nov/14 20:49;yhuai;Just a note to myself. It will be good to also look at if outputPartitioning in other physical operators are properly set. For example, the outputPartitioning in LeftSemiJoinHash is using the default UnknownPartitioning.;;;","06/Jun/15 10:58;apachespark;User 'jeanlyn' has created a pull request for this issue:
https://github.com/apache/spark/pull/6682;;;","27/Jul/15 05:19;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/7685;;;","30/Jul/15 02:55;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7773;;;","03/Aug/15 03:44;yhuai;Issue resolved by pull request 7773
[https://github.com/apache/spark/pull/7773];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler for Mesos in fine-grained mode launches tasks on wrong executors,SPARK-2204,12722586,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srainville,srainville,srainville,19/Jun/14 20:53,13/Aug/14 09:42,14/Jul/23 06:25,25/Jun/14 20:21,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Mesos,,,,,0,,,,,,"MesosSchedulerBackend.resourceOffers(SchedulerDriver, List[Offer]) is assuming that TaskSchedulerImpl.resourceOffers(Seq[WorkerOffer]) is returning task lists in the same order as the offers it was passed, but in the current implementation TaskSchedulerImpl.resourceOffers shuffles the offers to avoid assigning the tasks always to the same executors. The result is that the tasks are launched on the wrong executors.",,pwendell,xuzhongxing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2269,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400777,,,Wed Jun 25 20:21:56 UTC 2014,,,,,,,,,,"0|i1wzm7:",400869,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"19/Jun/14 21:17;srainville;Created PR: https://github.com/apache/spark/pull/1140;;;","25/Jun/14 20:21;pwendell;Issue resolved by pull request 1140
[https://github.com/apache/spark/pull/1140];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark does not infer default numPartitions in same way as Spark,SPARK-2203,12722572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,19/Jun/14 19:43,20/Jun/14 07:07,14/Jul/23 06:25,20/Jun/14 07:07,1.0.0,,,,,,,,1.1.0,,,,,PySpark,,,,,0,,,,,,"For shuffle-based operators, such as rdd.groupBy() or rdd.sortByKey(), PySpark will always assume that the default parallelism to use for the reduce side is ctx.defaultParallelism, which is a constant typically determined by the number of cores in cluster.

In contrast, Spark's Partitioner#defaultPartitioner will use the same number of reduce partitions as map partitions unless the defaultParallelism config is explicitly set. This tends to be a better default in order to avoid OOMs, and should also be the behavior of PySpark.",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400763,,,2014-06-19 19:43:21.0,,,,,,,,,,"0|i1wzj3:",400855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark invoke DecisionTree by Java,SPARK-2197,12722462,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,onepiece,onepiece,19/Jun/14 10:16,03/Aug/14 17:38,14/Jul/23 06:25,03/Aug/14 17:38,,,,,,,,,1.1.0,,,,,MLlib,,,,,0,,,,,,"Strategy strategy = new Strategy(Algo.Classification(), new Impurity() {
			@Override
			public double calculate(double arg0, double arg1, double arg2) {
				return Gini.calculate(arg0, arg1, arg2);
			}

			@Override
			public double calculate(double arg0, double arg1) {
				return Gini.calculate(arg0, arg1);
			}
		}, 5, 100, QuantileStrategy.Sort(), null, 256);
		DecisionTree decisionTree = new DecisionTree(strategy);
		final DecisionTreeModel decisionTreeModel = decisionTree.train(labeledPoints.rdd());

i try to run it on spark, but find an error on the console:
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.regression.LabeledPoint;
	at org.apache.spark.mllib.tree.DecisionTree$.findSplitsBins(DecisionTree.scala:990)
	at org.apache.spark.mllib.tree.DecisionTree.train(DecisionTree.scala:56)
	at org.project.modules.spark.java.SparkDecisionTree.main(SparkDecisionTree.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

i view source code, find  
val numFeatures = input.take(1)(0).features.size
this is a problem.",,apachespark,josephkb,joshrosen,mengxr,onepiece,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2478,,,,SPARK-2737,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400653,,,Sun Aug 03 17:38:24 UTC 2014,,,,,,,,,,"0|i1wyv3:",400747,,,,,,,,,,,,,1.1.0,,,,,,,,,,"30/Jul/14 00:17;josephkb;This error is at least partly caused by issues with collect() inside DecisionTree: https://issues.apache.org/jira/browse/SPARK-2737;;;","02/Aug/14 20:51;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/1740;;;","03/Aug/14 17:38;mengxr;Issue resolved by pull request 1740
[https://github.com/apache/spark/pull/1740];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix nullability of CaseWhen.,SPARK-2196,12722457,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,19/Jun/14 09:52,20/Jun/14 07:13,14/Jul/23 06:25,20/Jun/14 07:13,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,{{CaseWhen}} should use {{branches.length}} to check if {{elseValue}} is provided or not.,,rxin,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400648,,,Thu Jun 19 09:57:36 UTC 2014,,,,,,,,,,"0|i1wytz:",400742,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/14 09:57;ueshin;PRed: https://github.com/apache/spark/pull/1133;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet extraMetadata can contain key information,SPARK-2195,12722449,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,marmbrus,marmbrus,19/Jun/14 09:02,21/Jun/14 18:28,14/Jul/23 06:25,21/Jun/14 18:28,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{code}
14/06/19 01:52:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: file:/Users/pat/Projects/spark-summit-training-2014/usb/data/wiki-parquet/part-r-1.parquet start: 0 length: 24971040 hosts: [localhost] blocks: 1 requestedSchema: same as file fileSchema: message root {

  optional int32 id;

  optional binary title;

  optional int64 modified;

  optional binary text;

  optional binary username;

}

 extraMetadata: {org.apache.spark.sql.parquet.row.metadata=StructType(List(StructField(id,IntegerType,true), StructField(title,StringType,true), StructField(modified,LongType,true), StructField(text,StringType,true), StructField(username,StringType,true))), path=************************ MY AWS KEYS!!! ************************} readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata=StructType(List(StructField(id,IntegerType,true), StructField(title,StringType,true), StructField(modified,LongType,true), StructField(text,StringType,true), StructField(username,StringType,true))), path=**************************** MY AWS KEYS!!!! ***********************************}}
{code}",,marmbrus,schumach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400640,,,Sat Jun 21 18:27:52 UTC 2014,,,,,,,,,,"0|i1wys7:",400734,,,,,,,,,,,,,1.1.0,,,,,,,,,,"20/Jun/14 10:30;schumach;Since commit
https://github.com/apache/spark/commit/f479cf3743e416ee08e62806e1b34aff5998ac22
the path is no longer stored in the extraMetadata. So I guess this issue can be closed?;;;","21/Jun/14 18:27;marmbrus;Yeah, thanks for taking care of this so quickly!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Examples Data Not in Binary Distribution,SPARK-2192,12722429,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,cheffpj,cheffpj,19/Jun/14 07:28,01/Dec/14 08:34,14/Jul/23 06:25,01/Dec/14 08:32,1.0.0,,,,,,,,1.2.0,,,,,Build,MLlib,,,,0,,,,,,The data used by examples is not packaged up with the binary distribution. The data subdirectory of spark should make it's way in to the distribution somewhere so the examples can use it.,,apachespark,cheffpj,hsaputra,mengxr,pat.mcdonough@databricks.com,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400620,,,Mon Dec 01 08:32:45 UTC 2014,,,,,,,,,,"0|i1wynr:",400714,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/14 07:31;cheffpj;[~pacoid] - thanks for pointing this out. I guess we'll have to fall back to using the data from src;;;","20/Jun/14 01:06;pwendell;It might be good to have all the example data in src/main/resources.;;;","25/Jun/14 01:35;hsaputra;I think several examples already have the data in the main/resources. Do you have list of which ones missing?;;;","25/Jun/14 18:06;pat.mcdonough@databricks.com;Based on a very quick and not thorough search, the only mention I found of
those files came in the docs (bagel-programming-guide.md -->
pagerank_data.txt). But you'll also note that SparkKMeans and SparkPageRank
seem to work with those files.


On Wed, Jun 25, 2014 at 10:52 AM, Henry Saputra (JIRA) <jira@apache.org>

;;;","25/Nov/14 11:48;srowen;Data files are now consolidated under ""data/"", and they are not in the binary distribution. It would be easy to add them, and seems like a reasonable thing to do. However, I'm not clear all of those data files can be distributed; MovieLens data for example isn't supposed to be AFAIK. In fact, I'm not clear it should be in the Spark repo even.

Any support for me adding this to the distro, but removing examples based on things like Movielens that shouldn't be redistributed?;;;","25/Nov/14 19:10;cheffpj;[~srowen] - I fully support that and agree that Movielens needs to be removed (unless the Spark project was granted permission to re-host it, which is very possible, but should probably be noted in a readme or license file).;;;","26/Nov/14 13:07;srowen;Oops, on further inspection I see that the file is not Movielens data, but merely in the same format. The comments do say this in MovieLensALS.scala. I'll cook up a PR to add the example data to the distro.;;;","26/Nov/14 13:34;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3480;;;","26/Nov/14 22:04;cheffpj;Thanks [~srowen]. And yes, Xiangrui confirmed he just generated the data.;;;","01/Dec/14 08:32;mengxr;Issue resolved by pull request 3480
[https://github.com/apache/spark/pull/3480];;;",,,,,,,,,,,,,,,,,,,,,,,
Double execution with CREATE TABLE AS SELECT,SPARK-2191,12722426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,19/Jun/14 06:27,19/Jun/14 21:14,14/Jul/23 06:25,19/Jun/14 21:14,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"Reproduction:
{code}
scala> hql(""CREATE TABLE foo AS select unix_timestamp() from src limit 1"").collect()
res5: Array[org.apache.spark.sql.Row] = Array()

scala> hql(""SELECT * FROM foo"").collect()
res6: Array[org.apache.spark.sql.Row] = Array([1403159129], [1403159130])
{code}",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400617,,,2014-06-19 06:27:08.0,,,,,,,,,,"0|i1wyn3:",400711,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specialized ColumnType for Timestamp,SPARK-2190,12722423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,marmbrus,marmbrus,19/Jun/14 06:14,21/Jul/14 07:48,14/Jul/23 06:25,21/Jul/14 07:48,1.0.0,,,,,,,,1.1.0,,,,,SQL,,,,,0,,,,,,I'm going to call this a bug since currently its like 300X slower than it needs to  be.,,lian cheng,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1513,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400614,,,Wed Jul 16 12:36:06 UTC 2014,,,,,,,,,,"0|i1wymf:",400708,,,,,,,,,,,,,1.1.0,,,,,,,,,,"16/Jul/14 12:36;lian cheng;PR https://github.com/apache/spark/pull/1440;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Method for removing temp tables created by registerAsTable,SPARK-2189,12722422,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,marmbrus,marmbrus,19/Jun/14 06:08,03/Nov/14 00:00,14/Jul/23 06:25,03/Nov/14 00:00,1.0.0,,,,,,,,1.2.0,,,,,SQL,,,,,0,,,,,,,,apachespark,gvramana,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3212,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400613,,,Mon Nov 03 00:00:59 UTC 2014,,,,,,,,,,"0|i1wym7:",400707,,,,,,,,,,,,,1.2.0,,,,,,,,,,"25/Aug/14 05:43;gvramana;Please assign this to me.;;;","25/Aug/14 17:45;marmbrus;Thanks for offering to work on this.  Can you briefly describe what you plan to do here?  I think there are some subtle interface questions at the moment due to the way we handle cached tables vs temporary tables.  Specifically, what happens when you cache a table and then call unregisterTempTable(""cachedTableName"").;;;","26/Aug/14 14:22;gvramana;unregisterTempTable(""cachedTableName"") this api should uncache the registered tables with InMemoryRelation.
I could not get any useful use case where cache still is required after unregisterTempTable.

If there is any valid usecase, then api can be modified to give more control to user, 
unregisterTempTable(""cachedTableName"", unCacheTables=true)  . this api by default should uncache the registered tables with InMemoryRelation.
However user can pass unCacheTables=false to change the behaviour.
Please comment.;;;","26/Aug/14 18:49;marmbrus;I think the complication here is there is no way to distinguish between a temporary table and a cached table.  They both live in the same hash map.  I think resolving this PR might need to wait until we fix caching to not use the temporary table mechanism.

The JIRA for that is here: https://issues.apache.org/jira/browse/SPARK-3212;;;","31/Oct/14 17:10;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3039;;;","03/Nov/14 00:00;marmbrus;Issue resolved by pull request 3039
[https://github.com/apache/spark/pull/3039];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Explain command should not run the optimizer twice,SPARK-2187,12722393,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,19/Jun/14 01:20,19/Jun/14 05:44,14/Jul/23 06:25,19/Jun/14 05:44,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,,,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400584,,,2014-06-19 01:20:31.0,,,,,,,,,,"0|i1wyfr:",400678,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler should print warning when tasks are larger than the recommended size,SPARK-2185,12722379,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,kayousterhout,kayousterhout,19/Jun/14 00:32,09/Jul/17 20:02,14/Jul/23 06:25,01/Jul/14 08:57,1.0.0,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"This commit https://github.com/apache/spark/commit/57579934f0454f258615c10e69ac2adafc5b9835 changed the scheduler to print a warning for tasks larger than the recommended size, but I did a bad git merge of some scheduling code shortly after that commit was added that wiped out the line of code that sets TaskInfo.serializedSize, meaning the warning added is never emitted.",,kayousterhout,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21349,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400570,,,2014-06-19 00:32:17.0,,,,,,,,,,"0|i1wycn:",400664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AddExchange isn't idempotent,SPARK-2184,12722373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,18/Jun/14 23:31,19/Jun/14 00:53,14/Jul/23 06:25,19/Jun/14 00:53,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400564,,,2014-06-18 23:31:01.0,,,,,,,,,,"0|i1wybb:",400658,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scalastyle rule blocking unicode operators,SPARK-2182,12722362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,aash,aash,18/Jun/14 22:24,07/Feb/20 17:23,14/Jul/23 06:25,16/Sep/14 16:22,,,,,,,,,,,,,,Build,,,,,0,,,,,,"Some IDEs don't support Scala's [unicode operators|http://www.scala-lang.org/old/node/4723] so we should consider adding a scalastyle rule to block them for wider compatibility among contributors.

See this PR for a place we reverted a unicode operator: https://github.com/apache/spark/pull/1119

",,aash,apachespark,dorx,prashant,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1338,,,,,,,,,,,,,"18/Jun/14 22:31;dorx;Screen Shot 2014-06-18 at 3.28.44 PM.png;https://issues.apache.org/jira/secure/attachment/12651281/Screen+Shot+2014-06-18+at+3.28.44+PM.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400553,,,Tue Sep 16 16:22:21 UTC 2014,,,,,,,,,,"0|i1wy8v:",400647,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/14 22:26;rxin;It was Eclipse. ;;;","18/Jun/14 22:31;dorx;How I spotted it in Eclipse;;;","09/Sep/14 17:16;pwendell;[~prashant] as our resident expert on scalastyle... is this possible?;;;","11/Sep/14 10:39;prashant;We will have to come up with a regex that allows all character on keyboard and blocks everything else.

Something like :
{noformat}
[a-zA-Z0-9\t\n ./<>?;:""'`!@#$%^&*()[]{}_+=|\\-]
{noformat};;;","11/Sep/14 10:45;prashant;Or there is even better 

Regex for all ascii character. I hope this should suffice ?

{noformat}

scala> ""A "".matches(""""""\p{ASCII}+"""""")
res8: Boolean = true

scala> ""λ"".matches(""""""\p{ASCII}+"""""")
res9: Boolean = false

{noformat};;;","11/Sep/14 11:51;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/2358;;;","12/Sep/14 11:56;prashant;Found this SO link useful, http://stackoverflow.com/questions/23224219/does-the-scala-compiler-work-with-utf-8-encoded-source-files. ;;;","16/Sep/14 16:22;pwendell;Fixed by Prashant in:
https://github.com/apache/spark/pull/2358;;;",,,,,,,,,,,,,,,,,,,,,,,,,
The keys for sorting the columns of Executor page in SparkUI are incorrect,SPARK-2181,12722273,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,coderxiang,coderxiang,18/Jun/14 17:26,08/Jul/14 03:09,14/Jul/23 06:25,27/Jun/14 05:01,,,,,,,,,1.0.2,1.1.0,,,,Spark Core,,,,,0,,,,,,"Under the Executor page of SparkUI, each column is sorted alphabetically (after clicking). However, it should be sorted by the value, not the string.",,coderxiang,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400464,,,Tue Jul 08 03:09:05 UTC 2014,,,,,,,,,,"0|i1wxq7:",400563,,,,,,,,,,,,,1.0.1,,,,,,,,,,"19/Jun/14 15:21;gq;PR: https://github.com/apache/spark/pull/1135;;;","08/Jul/14 03:09;coderxiang;Hi, I've merged the latest version but the column of Memory Usage inside the RDD storage Info tab is not correct. It is still sorted by the string not the value.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic HAVING clauses support for HiveQL,SPARK-2180,12722258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,willbenton,willbenton,willbenton,18/Jun/14 16:35,11/Dec/14 20:57,14/Jul/23 06:25,11/Dec/14 20:57,1.0.0,,,,,,,,1.1.0,,,,,SQL,,,,,0,,,,,,The HiveQL implementation doesn't support HAVING clauses for aggregations.  This prevents some of the TPCDS benchmarks from running.,,rxin,willbenton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2226,SPARK-2225,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400449,,,Thu Dec 11 20:57:57 UTC 2014,,,,,,,,,,"0|i1wxn3:",400548,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/14 16:36;willbenton;(I'm working on a fix and will submit a PR soon.);;;","19/Jun/14 05:50;rxin;This would be great to have, [~willbenton].;;;","19/Jun/14 19:09;willbenton;PR is here:  https://github.com/apache/spark/pull/1136;;;","20/Jun/14 20:53;rxin;[~willbenton] I added two related issues. Let me know if you would like to work on them. Thanks!
;;;","11/Dec/14 20:57;srowen;Looks like this was merged for 1.1.0: https://github.com/apache/spark/commit/171ebb3a824a577d69443ec68a3543b27914cf6d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe table result contains only one column,SPARK-2177,12721859,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,rxin,rxin,18/Jun/14 07:36,20/Jun/14 06:42,14/Jul/23 06:25,20/Jun/14 06:42,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{code}
scala> hql(""describe src"").collect().foreach(println)

[key                 	string              	None                ]
[value               	string              	None                ]
{code}

The result should contain 3 columns instead of one. This screws up JDBC or even the downstream consumer of the Scala/Java/Python APIs.",,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400055,,,Thu Jun 19 20:15:31 UTC 2014,,,,,,,,,,"0|i1wv7r:",400154,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"18/Jun/14 19:12;yhuai;I think we need a better way to handle the result of a native command. The current PR is just a workaround.

Also, if we upgrade Hive to 0.13, we need to check the results of context.sessionState.isHiveServerQuery() to determine how to split the result. This method is introduced by https://issues.apache.org/jira/browse/HIVE-4545.;;;","18/Jun/14 19:15;yhuai;We may want to ask Hive to use JsonMetaDataFormatter for the output of a DDL statement (`set hive.ddl.output.format=json`) in future.;;;","19/Jun/14 20:14;yhuai;Generally Hive generates results of DDL statements as plain text (unless we use ""set hive.ddl.output.format=json""). It is not quite easy to parse those plain strings and I think it is not a good idea to understand how Hive works for every describe commands and write our code to generate the exactly same output. With changes made in this PR,  Spark SQL can support a subset of describe commands which are commonly used. This subset is defined by 
{code}
DESCRIBE [EXTENDED] [db_name.]table_name
{code}
All other cases are still treated as native commands.;;;","19/Jun/14 20:15;yhuai;We should also put what cases we support in the release note. But, where is that field?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extra unnecessary exchange operator in the result of an explain command,SPARK-2176,12721856,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,rxin,rxin,18/Jun/14 07:30,18/Jun/14 18:28,14/Jul/23 06:25,18/Jun/14 18:28,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{code}
hql(""explain select * from src group by key"").collect().foreach(println)

[ExplainCommand [plan#27:0]]
[ Aggregate false, [key#25], [key#25,value#26]]
[  Exchange (HashPartitioning [key#25:0], 200)]
[   Exchange (HashPartitioning [key#25:0], 200)]
[    Aggregate true, [key#25], [key#25]]
[     HiveTableScan [key#25,value#26], (MetastoreRelation default, src, None), None]
{code}

There are two exchange operators.

However, if we do not use explain...
{code}
hql(""select * from src group by key"")

res4: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[8] at RDD at SchemaRDD.scala:100
== Query Plan ==
Aggregate false, [key#8], [key#8,value#9]
 Exchange (HashPartitioning [key#8:0], 200)
  Aggregate true, [key#8], [key#8]
   HiveTableScan [key#8,value#9], (MetastoreRelation default, src, None), None
{code}
The plan is fine.",,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400052,,,Wed Jun 18 16:58:16 UTC 2014,,,,,,,,,,"0|i1wv73:",400151,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"18/Jun/14 16:11;yhuai;Seems it is a bug related to Explain command.
When I use ...
{code}
hql(""select * from src group by key"")
{code}
The plan is ...
{code}
res4: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[8] at RDD at SchemaRDD.scala:100
== Query Plan ==
Aggregate false, [key#8], [key#8,value#9]
 Exchange (HashPartitioning [key#8:0], 200)
  Aggregate true, [key#8], [key#8]
   HiveTableScan [key#8,value#9], (MetastoreRelation default, src, None), None
{code};;;","18/Jun/14 16:58;yhuai;OK. Let me explain the cause of this bug.

When we create a execution.ExplainCommand, we use the executedPlan as the child of this ExplainCommand. But, this executedPlan is prepared for execution again when we generate the executedPlan for the ExplainCommand. Basically, prepareForExecution is called twice on a physical plan. Because after prepareForExecution we have already bound those references (in BoundReferences), AddExchange cannot figure out we are using the same partitioning (we use AttributeReferences to create an ExchangeOperator and then those references will be changed to BoundReferences after prepareForExecution is called). So, an extra ExchangeOperator is inserted.

I think in CommandStrategy, we should just use the sparkPlan (sparkPlan is the input of prepareForExecution) to initialize the ExplainCommand instead of using executedPlan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark cannot import mllib modules in YARN-client mode,SPARK-2172,12721809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,frol,frol,17/Jun/14 22:54,25/Aug/14 06:05,14/Jul/23 06:25,26/Jun/14 05:04,1.0.0,1.1.0,,,,,,,1.0.1,1.1.0,,,,MLlib,PySpark,Spark Core,YARN,,1,mllib,python,,,,"Here is the simple reproduce code:

{noformat}
$ HADOOP_CONF_DIR=/etc/hadoop/conf MASTER=yarn-client ./bin/pyspark
{noformat}

{code:title=issue.py|borderStyle=solid}
>>> from pyspark.mllib.regression import LabeledPoint

>>> sc.parallelize([1,2,3]).map(lambda x: LabeledPoint(1, [2])).count()
{code}

Note: The same issue occurs with .collect() instead of .count()

{code:title=TraceBack|borderStyle=solid}
Py4JJavaError: An error occurred while calling o110.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8.0:0 failed 4 times, most recent failure: Exception failure in TID 52 on host ares: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/mnt/storage/bigisle/yarn/1/yarn/local/usercache/blb/filecache/18/spark-assembly-1.0.0-hadoop2.2.0.jar/pyspark/worker.py"", line 73, in main
    command = pickleSer._read_with_length(infile)
  File ""/mnt/storage/bigisle/yarn/1/yarn/local/usercache/blb/filecache/18/spark-assembly-1.0.0-hadoop2.2.0.jar/pyspark/serializers.py"", line 146, in _read_with_length
    return self.loads(obj)
ImportError: No module named mllib.regression

        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:115)
        org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:145)
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:78)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

However, this code works as expected:

{code:title=noissue.py|borderStyle=solid}
>>> from pyspark.mllib.regression import LabeledPoint

>>> sc.parallelize([1,2,3]).map(lambda x: LabeledPoint(1, [2])).first()
>>> sc.parallelize([1,2,3]).map(lambda x: LabeledPoint(1, [2])).take(3)
{code}","Ubuntu 14.04
Java 7
Python 2.7
CDH 5.0.2 (Hadoop 2.3.0): HDFS, YARN
Spark 1.0.0 and git master",frol,joao,mengxr,piotrszul,shubhamc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400005,,,Mon Aug 25 06:05:12 UTC 2014,,,,,,,,,,"0|i1wuxj:",400107,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/14 02:34;frol;I've tried to run the code in standalone and local modes. There is no such error, but I want to exercise YARN.
I've also tried to run similar code in spark-shell (Scala) and it does well:

{code}
scala> import org.apache.spark.mllib.regression.LabeledPoint
scala> import org.apache.spark.mllib.linalg.{Vector, Vectors}
scala> val array: Array[Double] = Array(1, 2)
scala> val vector: Vector = Vectors.dense(array)
scala> sc.parallelize(1 to 3).map(x => LabeledPoint(x, vector)).collect()
res2: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array(LabeledPoint(1.0, [1.0,2.0]), LabeledPoint(2.0, [1.0,2.0]), LabeledPoint(3.0, [1.0,2.0]))
{code};;;","25/Jun/14 23:56;piotrszul;I got the same problem while runing the kmeans.py from examples, i.e.:
{noformat}
$ spark-submit --master yarn-client examples/src/main/python/mllib/kmeans.py kmeans_data.txt 3
Spark assembly has been built with Hive, including Datanucleus jars on classpath
--args is deprecated. Use --arg instead.
14/06/26 09:52:59 WARN TaskSetManager: Lost TID 0 (task 0.0:0)
14/06/26 09:52:59 WARN TaskSetManager: Loss was due to org.apache.spark.api.python.PythonException
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/data/02/yarn/local/usercache/szu004/filecache/45/spark-assembly-1.0.0-hadoop2.2.0.jar/pyspark/worker.py"", line 73, in main
    command = pickleSer._read_with_length(infile)
  File ""/data/02/yarn/local/usercache/szu004/filecache/45/spark-assembly-1.0.0-hadoop2.2.0.jar/pyspark/serializers.py"", line 146, in _read_with_length
    return self.loads(obj)
ImportError: No module named mllib._common
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:115)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:78)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
{noformat};;;","26/Jun/14 00:01;piotrszul;I believe the problem is that the spark-assembly-1.0.0-xxx.jar does not include the pyspark/mllib package 
(only the pyspark python code is included).
It works in the local mode becasue then $SPARK_HOME/python is on PYTHONPATH and it has acces to both pyspark and pyspark/mllib modules.

To fix the assembly should include  pyspark/mllib 
;;;","26/Jun/14 05:04;mengxr;Fixed in https://github.com/apache/spark/pull/1223 by [~piotrszul] .;;;","25/Aug/14 06:05;joao;[~piotrszul] For the fix there is a workaround that I can use in my python script ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkUI.setAppName() has no effect,SPARK-2169,12721775,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,17/Jun/14 20:52,20/Aug/14 23:20,14/Jul/23 06:25,20/Aug/14 23:20,1.0.0,,,,,,,,1.1.0,,,,,Web UI,,,,,0,,,,,,"{{SparkUI.setAppName()}} does not do anything useful.

It overwrites the instance's {{appName}} fields, but all places where that field is used have already read that value into their own copies by the time that happens.

e.g.

StagePage.scala copies {{parent.appName}} into its own private {{appName}} in the constructor, which is called as part of SparkUI's constructor. So when you call {{SparkUI.setAppName}} it does not overwrite StagePage's copy, and so the UI still shows the old value.",,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399971,,,Fri Jun 27 22:19:44 UTC 2014,,,,,,,,,,"0|i1wur3:",400078,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/14 22:19;vanzin;https://github.com/apache/spark/pull/1252;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History Server renered page not suitable for load balancing,SPARK-2168,12721774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,elyast,elyast,elyast,17/Jun/14 20:45,05/Jan/16 21:25,14/Jul/23 06:25,05/Jan/16 21:24,1.0.0,,,,,,,,1.3.2,1.4.2,,,,Spark Core,,,,,0,,,,,,"Small issue but still.

I run history server through Marathon and balance it through haproxy. The problem is that links generated by HistoryPage (links to completed applications) are absolute, e.g. <a href=""http://some-server:port/history/..."">completedApplicationName</a> , but instead they should be relative, e.g.  <a hfref=""/history/..."">completedApplicationName</a>, so they can be load balanced. ",,apachespark,elyast,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399970,,,Thu Feb 26 04:18:47 UTC 2015,,,,,,,,,,"0|i1wuqv:",400077,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/14 20:46;elyast;I can open pull request for this, let me know what do you think;;;","01/Jul/14 04:01;elyast;PR opened;;;","26/Feb/15 04:18;apachespark;User 'elyast' has created a pull request for this issue:
https://github.com/apache/spark/pull/4778;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Applying UDF on a struct throws a MatchError,SPARK-2164,12721653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,marmbrus,marmbrus,17/Jun/14 11:15,17/Jun/14 11:15,14/Jul/23 06:25,17/Jun/14 11:15,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399849,,,Tue Jun 17 11:15:48 UTC 2014,,,,,,,,,,"0|i1wu07:",399957,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/14 11:15;marmbrus;Fixed by: https://github.com/apache/spark/pull/796;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
possible to read from removed block in blockmanager,SPARK-2162,12721594,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,colorant,colorant,colorant,17/Jun/14 06:18,17/May/20 18:21,14/Jul/23 06:25,18/Jun/14 19:50,,,,,,,,,1.1.0,,,,,Block Manager,Spark Core,,,,0,,,,,,"In BlockManager's doGetLocal method,  there are chance that info get removed when info.synchronized block is entered. thus  it will either read in vain in memory level case, or throw exception in disk level case when it believe the block is there while actually it had been removed.
",,colorant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399790,,,Thu Jun 19 00:33:22 UTC 2014,,,,,,,,,,"0|i1wtmv:",399898,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/14 00:33;colorant;patch merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileAppenderSuite is not cleaning up after itself,SPARK-2158,12721545,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,markhamstra,markhamstra,markhamstra,16/Jun/14 23:11,13/Jul/14 18:14,14/Jul/23 06:25,13/Jul/14 18:14,1.1.0,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,FileAppenderSuite is leaving behind the file core/stdout,,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399741,,,Sun Jul 13 18:14:20 UTC 2014,,,,,,,,,,"0|i1wtc7:",399850,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/14 16:32;srowen;I tried to clean this up a while ago, but I think it predates your comment.  I don't see this file however after running tests. Is it maybe due to an unusual termination in the test? I don't see this file created either.;;;","13/Jul/14 18:14;markhamstra;This is fixed at 4cb33a83e0 from https://github.com/apache/spark/pull/1100
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't write tight firewall rules for Spark,SPARK-2157,12721513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ash211@gmail.com,aash,aash,16/Jun/14 21:24,13/Dec/14 19:36,14/Jul/23 06:25,06/Aug/14 07:10,1.0.0,,,,,,,,1.1.0,,,,,Deploy,Spark Core,,,,0,,,,,,"In order to run Spark in places with strict firewall rules, you need to be able to specify every port that's used between all parts of the stack.

Per the [network activity section of the docs|http://spark.apache.org/docs/latest/spark-standalone.html#configuring-ports-for-network-security] most of the ports are configurable, but there are a few ports that aren't configurable.

We need to make every port configurable to a particular port, so that we can run Spark in highly locked-down environments.",,aash,andrewor,apachespark,epahomov,pwendell,,,,,,,,,,,,,,,,,,,,,,SPARK-1176,SPARK-1174,,,,,,,,,,,,SPARK-1174,SPARK-4837,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399709,,,Wed Aug 06 07:26:18 UTC 2014,,,,,,,,,,"0|i1wt53:",399818,,,,,,,,,,,,,1.1.0,,,,,,,,,,"16/Jun/14 21:30;aash;[~epakhomov] you were looking at making the ports for HttpBroadcast and HttpFileServer configurable in SPARK-1174 and SPARK-1176.  From looking at your pull requests these were never merged though.

Have you been running a patched version of Spark instead?  I'm interested in how you've been dealing with this issue.

Many thanks!
Andrew;;;","17/Jun/14 18:01;aash;I pulled together Egor's work for HttpBroadcast and HttpFileServer and added configuration options for the block manager and the repl class server in this PR: https://github.com/apache/spark/pull/1107;;;","18/Jun/14 10:24;epahomov;Yep, I used patched version of spark. ;;;","05/Aug/14 05:41;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/1777;;;","05/Aug/14 05:42;andrewor;[~aash] I have built my changes on top of yours in my PR. It would be good if you could take a look.;;;","05/Aug/14 07:34;aash;Looks great!  I made one comment, but this definitely captures the spirit of my changes and I'm very happy to see this effort getting attention.  Many thanks Andrew!;;;","06/Aug/14 07:10;pwendell;Issue resolved by pull request 1777
[https://github.com/apache/spark/pull/1777];;;","06/Aug/14 07:26;aash;[~epahomov] does this look good?  [~pwendell] I think we can close subtasks SPARK-1174 and SPARK-1176 now that this is in as well.

Thanks Andrew for finishing this off!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"When the size of serialized results for one partition is slightly smaller than 10MB (the default akka.frameSize), the execution blocks",SPARK-2156,12721507,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mengxr,xiaocai,xiaocai,16/Jun/14 20:57,17/Jul/14 14:32,14/Jul/23 06:25,17/Jul/14 04:31,0.9.1,1.0.0,,,,,,,0.9.2,1.0.1,1.1.0,,,Spark Core,,,,,0,,,,,," I have done some experiments when the frameSize is around 10MB .

1) spark.akka.frameSize = 10
If one of the partition size is very close to 10MB, say 9.97MB, the execution blocks without any exception or warning. Worker finished the task to send the serialized result, and then throw exception saying hadoop IPC client connection stops (changing the logging to debug level). However, the master never receives the results and the program just hangs.
But if sizes for all the partitions less than some number btw 9.96MB amd 9.97MB, the program works fine.
2) spark.akka.frameSize = 9
when the partition size is just a little bit smaller than 9MB, it fails as well.
This bug behavior is not exactly what spark-1112 is about.",AWS EC2 1 master 2 slaves with the instance type of r3.2xlarge,DjvuLee,gq,mengxr,pwendell,sinisa_lyh,xiaocai,,,,,,,,,,,,,1814400,1814400,,0%,1814400,1814400,,,,,,,,,,,,,,,,SPARK-1712,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399703,,,Thu Jul 17 14:32:33 UTC 2014,,,,,,,,,,"0|i1wt3r:",399812,,,,,,,,,,,,,0.9.2,1.0.1,1.1.0,,,,,,,,"18/Jun/14 15:05;gq;The problem here: [Executor.scala#L215|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala#L215]

The difference between the size of the serialized {{StatusUpdate}}  and serialized {{DirectResult}} may exceed 1024 .;;;","19/Jun/14 01:57;xiaocai;[~gq] Thanks a lot for looking into this. The difference is around 30~40KB, which is much larger than 1024B. In practice, the partition size is arbitrary, and may fall into this critical range. Probably at least better error or exception would be helpful. ;;;","19/Jun/14 02:08;gq;[~pwendell]   This seems to is not the same as   [SPARK-1112|https://issues.apache.org/jira/browse/SPARK-1112];;;","19/Jun/14 02:11;pwendell;Ah I see - thanks! I'm going to re-open it. ;;;","19/Jun/14 02:12;pwendell;[~xiaocai] Are you calling collect() on the RDD? Or what action are you running?;;;","19/Jun/14 02:25;xiaocai;Yes, I am calling collect() at the end. ;;;","19/Jun/14 02:32;pwendell;[~xiaocai] Would you mind trying with the patch here?

https://github.com/apache/spark/pull/1126/files;;;","19/Jun/14 02:35;mengxr;PR: https://github.com/apache/spark/pull/1124;;;","19/Jun/14 02:37;pwendell;I closed my patch in favor of Xiangrui's.;;;","19/Jun/14 03:30;xiaocai;[~pwendell] [~mengxr][~gq], Thanks a lot, I will give it a try. ;;;","23/Jun/14 02:49;pwendell;This is fixed in the 1.0 branch via:

https://github.com/apache/spark/pull/1172;;;","25/Jun/14 02:07;pwendell;Fixed in 1.1.0 via: 
https://github.com/apache/spark/pull/1132;;;","10/Jul/14 01:21;mengxr;Is it solved?;;;","10/Jul/14 01:22;mengxr;Oh, I see. We are waiting for 0.9.2.;;;","17/Jul/14 04:01;mengxr;PR for branch-0.9: https://github.com/apache/spark/pull/1455;;;","17/Jul/14 14:32;DjvuLee;I see this fixed in the spark branch-0.9 in the github, but does it updated in the spark v0.9.1 in the http://spark.apache.org/ site?;;;",,,,,,,,,,,,,,,,,
Support effectful / non-deterministic key expressions in CASE WHEN statements,SPARK-2155,12721470,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cloud_fan,ConcreteVitamin,ConcreteVitamin,16/Jun/14 18:10,16/May/15 10:59,14/Jul/23 06:25,07/May/15 23:28,,,,,,,,,1.4.0,,,,,SQL,,,,,0,,,,,,"Currently we translate CASE KEY WHEN to CASE WHEN, hence incurring redundant evaluations of the key expression. Relevant discussions here: https://github.com/apache/spark/pull/1055/files#r13784248

If we are very in need of support for effectful key expressions, at least we can resort to the baseline approach of having both CaseWhen and CaseKeyWhen as expressions, which seem to introduce much code duplication (e.g. see https://github.com/concretevitamin/spark/blob/47d406a58d129e5bba68bfadf9dd1faa9054d834/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala#L216 for a sketch implementation). ",,apachespark,ConcreteVitamin,gvramana,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399666,,,Thu May 07 23:28:09 UTC 2015,,,,,,,,,,"0|i1wsvj:",399775,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/14 05:49;rxin;Can you include a link to some page that explains CASE KEY WHEN?;;;","19/Jun/14 05:56;ConcreteVitamin;Sure, here is the link I used: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-ConditionalFunctions

FWIW, CaseWhen is ""WHEN"" in Hive, and CaseKeyWhen is ""CASE"". It might be worthwhile to follow the convention at some point.;;;","13/Oct/14 12:40;gvramana;we can separate CASE KEY WHEN  and CASE WHEN into two expressions and can have an common abstract base class containing common code.
This will address redundant evaluation problem without duplicating code.;;;","07/May/15 12:52;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/5979;;;","07/May/15 23:28;marmbrus;Issue resolved by pull request 5979
[https://github.com/apache/spark/pull/5979];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker goes down.,SPARK-2154,12721410,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adav,talk2siva8,talk2siva8,16/Jun/14 13:25,25/Jul/14 23:20,14/Jul/23 06:25,16/Jul/14 21:17,0.8.1,0.9.0,1.0.0,,,,,,1.0.2,1.1.0,,,,Spark Core,,,,,0,patch,,,,,"Worker dies when i try to submit drivers more than the allocated cores. When I submit 9 drivers with one core for each driver on a cluster having 8 cores all together the worker dies as soon as i submit the 9 the driver. It works fine until it reaches 8 cores, As soon as i submit 9th driver the driver status remains ""Submitted"" and the worker crashes. I understand that we cannot run  drivers more than the allocated cores but the problem here is instead of the 9th driver being in queue it is being executed and as a result it is crashing the worker. Let me know if there is a way to get around this issue or is it being fixed in the upcoming version?

Cluster Details:
Spark 1.00
2 nodes with 4 cores each.",Spark on cluster of three nodes on Ubuntu 12.04.4 LTS,andrewor,ilikerps,nchammas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2350,,,,,,,,"14/Jul/14 15:21;talk2siva8;Sccreenhot at various states of driver ..jpg;https://issues.apache.org/jira/secure/attachment/12655558/Sccreenhot+at+various+states+of+driver+..jpg",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399606,,,Fri Jul 25 23:20:48 UTC 2014,,,,,,,,,,"0|i1wsi7:",399715,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/14 15:00;talk2siva8;It looks it has been fixed in 1.0.1, But as a result of this fix there is another bug which is created.
If we launch a new driver when there are drivers already running in the cluster. It shows the driver is submitted, But it does not go to running stage even after existing drivers have completed . The driver changes state from submitted state to running only when we submit another driver.;;;","14/Jul/14 16:22;ilikerps;Created this PR to hopefully fix that: https://github.com/apache/spark/pull/1405;;;","16/Jul/14 21:17;pwendell;Issue resolved by pull request 1405
[https://github.com/apache/spark/pull/1405];;;","16/Jul/14 21:21;talk2siva8;Fixed in the future releases;;;","16/Jul/14 21:51;pwendell;[~talk2siva8] Yes, that's correct.;;;","25/Jul/14 23:20;nchammas;Can we edit the title of this issue to something more descriptive? Say, something like (cribbed from the description): ""Worker dies when # of submitted drivers is more than allocated cores"";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
the error of comput rightNodeAgg about  Decision tree algorithm  in Spark MLlib ,SPARK-2152,12721365,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jsondag,mathcao,mathcao,16/Jun/14 09:14,05/Aug/14 19:32,14/Jul/23 06:25,09/Jul/14 02:18,1.0.0,,,,,,,,1.0.2,1.1.0,,,,,,,,,0,features,,,,," the error of comput rightNodeAgg about  Decision tree algorithm  in Spark MLlib  about  the function extractLeftRightNodeAggregates() ,when compute rightNodeAgg  used bindata index is error. in the DecisionTree.scala file about  Line 980:

             rightNodeAgg(featureIndex)(2 * (numBins - 2 - splitIndex)) =
                binData(shift + (2 * (numBins - 2 - splitIndex))) +
                  rightNodeAgg(featureIndex)(2 * (numBins - 1 - splitIndex))    

 the   binData(shift + (2 * (numBins - 2 - splitIndex)))  index compute is error, so the result of rightNodeAgg  include  repeated data about ""bins""  ","windows7 ,32 operator,and 3G mem",jsondag,mathcao,mengxr,pwendell,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,SPARK-2160,,,,,,SPARK-2417,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399561,,,Wed Jul 09 13:43:06 UTC 2014,,,,,,,,,,"0|i1ws87:",399670,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/14 17:58;jsondag;https://github.com/apache/spark/pull/1316 (also resolves SPARK-2160);;;","09/Jul/14 02:18;mengxr;Issue resolved by pull request 1316
[https://github.com/apache/spark/pull/1316];;;","09/Jul/14 12:12;pwendell;FYI this caused some new test failures, I created SPARK-2417 to track it.;;;","09/Jul/14 13:43;jsondag;Submitted a PR to fix those tests.


On Wed, Jul 9, 2014 at 8:12 AM, Patrick Wendell (JIRA) <jira@apache.org>

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-submit issue (int format expected for memory parameter),SPARK-2151,12721342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nravi,nravi,nravi,16/Jun/14 06:07,20/Jun/14 00:11,14/Jul/23 06:25,20/Jun/14 00:11,1.0.0,,,,,,,,1.0.1,1.1.0,,,,,,,,,0,,,,,,"Get this exception when invoking spark-submit in standalone cluster mode:

{code}
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""38g""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at org.apache.spark.deploy.ClientArguments.parse(ClientArguments.scala:55)
	at org.apache.spark.deploy.ClientArguments.<init>(ClientArguments.scala:47)
	at org.apache.spark.deploy.Client$.main(Client.scala:148)
	at org.apache.spark.deploy.Client.main(Client.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}",,nravi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399538,,,Mon Jun 16 06:11:07 UTC 2014,,,,,,,,,,"0|i1ws33:",399647,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/14 06:11;nravi;PR: https://github.com/apache/spark/pull/1095/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master UI forgets about Executors when application exits cleanly,SPARK-2147,12721274,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,ilikerps,ilikerps,15/Jun/14 02:37,05/Nov/14 10:45,14/Jul/23 06:25,17/Jun/14 19:39,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Web UI,,,,,0,,,,,,"When an application exits cleanly, the Master will remove all executors from the application's ApplicationInfo, causing the historic ""Completed Applications"" page to report that there were no executors associated with that application. 

On the contrary, if the application exits uncleanly, then the Master will remove the application FIRST, and will not actually remove the executors from the ApplicationInfo page. This causes the executors to show up correctly in the ""Completed Applications"" page.

The correct behavior would probably be to gather a history of all executors (so we'd retain executors that we had at one point but were removed during the job), and not remove lost executors.",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2015,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399470,,,Tue Jun 17 19:39:44 UTC 2014,,,,,,,,,,"0|i1wro7:",399579,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/14 19:39;ilikerps;https://github.com/apache/spark/pull/1102;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the takeOrdered doc,SPARK-2146,12721266,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sandyr,sandyr,sandyr,14/Jun/14 21:40,17/Jun/14 19:04,14/Jul/23 06:25,17/Jun/14 19:04,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Documentation,,,,,0,,,,,,,,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399462,,,Tue Jun 17 19:04:17 UTC 2014,,,,,,,,,,"0|i1wrmf:",399571,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/14 19:04;pwendell;Issue resolved by pull request 1086
[https://github.com/apache/spark/pull/1086];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkUI Executors tab displays incorrect RDD blocks,SPARK-2144,12721205,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,13/Jun/14 22:05,05/Nov/14 10:45,14/Jul/23 06:25,17/Jun/14 08:28,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Web UI,,,,,0,,,,,,"If a block is dropped because of memory pressure, this is not reflected in the ""RDD Blocks"" column on the Executors page.

This is because StorageStatusListener updates the StorageLevel of the dropped block to StorageLevel.None, but does not remove it from the list. This is a simple fix.",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399401,,,Tue Jun 17 08:28:48 UTC 2014,,,,,,,,,,"0|i1wr9b:",399510,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/14 08:28;pwendell;Issue resolved by pull request 1080
[https://github.com/apache/spark/pull/1080];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn stable client doesn't properly handle MEMORY_OVERHEAD for AM,SPARK-2140,12721125,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,copester,tgraves,tgraves,13/Jun/14 15:16,11/Sep/14 13:14,14/Jul/23 06:25,11/Sep/14 13:14,1.0.0,,,,,,,,1.1.1,1.2.0,,,,YARN,,,,,0,,,,,,"The yarn stable client doesn't properly remove the MEMORY_OVERHEAD amount from the java heap size, the code to handle that is commented out (see function calculateAMMemory).  We should fix this.",,copester,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1287,,,,,,SPARK-2604,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399322,,,Fri Aug 22 21:14:13 UTC 2014,,,,,,,,,,"0|i1wqrz:",399431,,,,,,,,,,,,,1.1.0,,,,,,,,,,"12/Aug/14 15:47;copester;From Client and ClientBase, it looks like the commented-out code already makes the proper memory overhead arithmetic. Could you clarify the description? If that's right, then the simple PR would should suffice.;;;","12/Aug/14 15:55;tgraves;The commented out code was sufficient at the time of filing.   ;;;","12/Aug/14 16:14;copester;But what about Client.scala L87:
{code}memoryResource.setMemory(args.amMemory + memoryOverhead){code}
Does calculateAMMemory need this logic? I'm sure I'm confusing this more, but I'm looking at this ticket and the history of the code(commenting of the code) to see what is needed for v1.0.1;;;","13/Aug/14 17:07;tgraves;ah it seems things have changed. Its now actually the opposite problem now where yarn alpha is getting less then it should be. 

line 87 is what its actually asking from YARN.  the calculateAMMemory is what its using for heap.  So it appears yarn stable is correct right now.  It appears yarn alpha in calculateAMmemory subtracts out the memory overhead when it shouldn't.;;;","22/Aug/14 21:14;vanzin;Same as SPARK-1287?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timestamp UDFs broken,SPARK-2137,12721053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,marmbrus,marmbrus,13/Jun/14 07:54,14/Jun/14 06:29,14/Jul/23 06:25,14/Jun/14 06:29,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{code}
create table wiki (id int, title string, mod timestamp, text string, username string)
SELECT year(mod) FROM wiki LIMIT 10
{code}

Error:
{code}
No matching wrapper found, options: WrappedArray(public org.apache.hadoop.hive.serde2.io.TimestampWritable(byte[],int), public org.apache.hadoop.hive.serde2.io.TimestampWritable(), public org.apache.hadoop.hive.serde2.io.TimestampWritable(org.apache.hadoop.hive.serde2.io.TimestampWritable), public org.apache.hadoop.hive.serde2.io.TimestampWritable(java.sql.Timestamp)).
{code}",,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399251,,,Fri Jun 13 23:04:45 UTC 2014,,,,,,,,,,"0|i1wqcf:",399361,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/14 23:04;yhuai;Attaching the Pull Request link.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InMemoryColumnarScan does not get planned correctly,SPARK-2135,12721016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,13/Jun/14 00:13,13/Jun/14 06:54,14/Jul/23 06:25,13/Jun/14 06:54,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"Right now we do not have a logical concept for in memory data.  This breaks planning in several ways:

This sort of query fails to analyze:
{code}
    TestSQLContext.sql(""SELECT * FROM testData"").registerAsTable(""selectStar"")
    TestSQLContext.cacheTable(""selectStar"")
    TestSQLContext.sql(""SELECT * FROM selectStar WHERE key = 1"").collect()
{code}

Also, we read all of the columns even when we don't need to.",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399214,,,Fri Jun 13 06:54:01 UTC 2014,,,,,,,,,,"0|i1wq4v:",399326,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"13/Jun/14 06:54;marmbrus;https://github.com/apache/spark/pull/1072;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No plan for DESCRIBE,SPARK-2128,12720813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,12/Jun/14 07:49,13/Jun/14 20:09,14/Jul/23 06:25,13/Jun/14 20:06,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{code}
java.lang.AssertionError: assertion failed: No plan for NativeCommand DESCRIBE src

	at scala.Predef$.assert(Predef.scala:179)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:302)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:302)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:303)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:303)
	at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:379)
	at .<init>(<console>:40)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
{code}",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399012,,,Fri Jun 13 20:06:25 UTC 2014,,,,,,,,,,"0|i1woxb:",399129,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/14 20:06;marmbrus;https://github.com/apache/spark/pull/1071;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading Parquet InputSplits dominates query execution time when reading off S3,SPARK-2119,12720770,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,marmbrus,marmbrus,12/Jun/14 01:38,17/Jul/14 08:17,14/Jul/23 06:25,16/Jul/14 16:46,1.0.0,,,,,,,,1.1.0,,,,,SQL,,,,,0,,,,,,"Here's the relevant stack trace where things are hanging:

{code}
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:326)
	at parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:370)
	at parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:344)
	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:90)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
{code}

We should parallelize or cache or something here.",,lian cheng,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2551,,,PARQUET-16,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398969,,,Thu Jul 17 08:17:50 UTC 2014,,,,,,,,,,"0|i1wonj:",399086,,,,,,,,,,,,,1.1.0,,,,,,,,,,"11/Jul/14 06:09;lian cheng;PR: https://github.com/apache/spark/pull/1370;;;","16/Jul/14 16:46;marmbrus;I'm going to mark this as closed, but [~lian cheng], do you think we should open a follow-up JIRA to remove hacks once the changes have been accepted to parquet upstream?;;;","17/Jul/14 08:17;lian cheng;Agree. 
Created SPARK-2551 for removing those hacks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
awaitTermination() after stop() will hang in Spark Stremaing,SPARK-2113,12720648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pwendell,pwendell,11/Jun/14 17:54,11/Jun/14 17:56,14/Jul/23 06:25,11/Jun/14 17:56,1.0.0,,,,,,,,,,,,,DStreams,,,,,0,,,,,,,,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398847,,,Wed Jun 11 17:56:52 UTC 2014,,,,,,,,,,"0|i1wnyv:",398969,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"11/Jun/14 17:56;pwendell;Fixed by:
https://github.com/apache/spark/pull/1001;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetTypesConverter should not create its own conf,SPARK-2112,12720639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,schumach,marmbrus,marmbrus,11/Jun/14 17:32,26/Jun/14 21:17,14/Jul/23 06:25,26/Jun/14 21:17,1.0.0,,,,,,,,,,,,,SQL,,,,,0,,,,,,"[~adav]: ""this actually makes it so that we can't use S3 credentials set in the SparkContext, or add new FileSystems at runtime, for instance.""",,marmbrus,rxin,schumach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398838,,,Fri Jun 20 10:35:10 UTC 2014,,,,,,,,,,"0|i1wnwv:",398960,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/14 10:35;schumach;Since commit
https://github.com/apache/spark/commit/f479cf3743e416ee08e62806e1b34aff5998ac22
the SparkContext's Hadoop configuration should be used when reading metadata from the file source. I wasn't yet able to test this with say S3 bucket names.

Are the the S3 credentials copied from SparkConfig to its Hadoop configuration?  If someone could confirm this to be working we could close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark errors when SPARK_PRINT_LAUNCH_COMMAND=1,SPARK-2111,12720611,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,tgraves,tgraves,11/Jun/14 16:03,16/Jul/14 13:25,14/Jul/23 06:25,16/Jul/14 13:24,1.0.0,,,,,,,,1.0.1,1.1.0,,,,PySpark,,,,,0,,,,,,"If you set SPARK_PRINT_LAUNCH_COMMAND=1 to see what java command is being used to launch spark and then try to run pyspark it errors out with a very non-useful error message:

Traceback (most recent call last):
  File ""/homes/tgraves/test/hadoop2/y-spark-git/python/pyspark/shell.py"", line 43, in <module>
    sc = SparkContext(appName=""PySparkShell"", pyFiles=add_files)
  File ""/homes/tgraves/test/hadoop2/y-spark-git/python/pyspark/context.py"", line 94, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File ""/homes/tgraves/test/hadoop2/y-spark-git/python/pyspark/context.py"", line 184, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway()
  File ""/homes/tgraves/test/hadoop2/y-spark-git/python/pyspark/java_gateway.py"", line 51, in launch_gateway
    gateway_port = int(proc.stdout.readline())
ValueError: invalid literal for int() with base 10: 'Spark Command: /home/gs/java/jdk/bin/java -cp :/home/gs/hadoop/current/share/hadoop/common/hadoop-gpl-compression.jar:/home/gs/hadoop/current/share/hadoop/hdfs/lib/YahooDNSToSwitchMapping-0.2.14020207'",,farrellee,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2313,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398810,,,Wed Jul 16 13:19:12 UTC 2014,,,,,,,,,,"0|i1wnqn:",398932,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/14 21:48;farrellee;i'll take a look at this;;;","27/Jun/14 22:01;farrellee;the issue is java_gateway.py expects the only thing on stdout to be a port number and SPARK_PRINT_LAUNCH_COMMAND writes the command to stdout

fix is to write the command to stderr

pull request - https://github.com/apache/spark/pull/1251;;;","28/Jun/14 01:15;pwendell;I was thinking that SPARK-2313 might be a better general solution to this.;;;","28/Jun/14 11:59;farrellee;the port needs to be owned by the child and is ephemeral. getting a guaranteed available port is something that os can provide, but a parent cannot (unless the parent opens the port and somehow passes it to the child). so, i agree not using stdout for passing port information from child to parent is desirable, it's not exactly simple and using a command-line to pass parent->child is also error prone.

push this to resolve the SPARK_PRINT_LAUNCH_COMMAND issue and i'll take a look at better ways to setup the gateway;;;","16/Jul/14 13:18;farrellee;this was resolved by https://github.com/apache/spark/pull/1050;;;","16/Jul/14 13:19;farrellee;[~pwendell] please close this issue as resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading help displayed for interactive mode pyspark --help,SPARK-2110,12720547,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,prashant,prashant,11/Jun/14 10:28,15/Aug/14 19:20,14/Jul/23 06:25,15/Aug/14 19:20,1.1.0,,,,,,,,1.1.0,,,,,Documentation,PySpark,,,,0,,,,,,The help displayed with command pyspark --help is not relevant for interactive mode.,,joshrosen,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2678,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398746,,,Fri Aug 15 19:20:08 UTC 2014,,,,,,,,,,"0|i1wncn:",398868,,,,,,,,,,,,,1.1.0,,,,,,,,,,"07/Aug/14 21:02;joshrosen;I was just affected by this issue, too, and found it really confusing.

I launched a new Spark EC2 cluster and tried to launch a PySpark shell in {{local}} mode using the command {{./bin/pyspark \-\-master local}}, which makes sense after reading the current {{\-\-help}}.  The {{\-\-master}} option seems to be ignored here; it seems that the only way to use a different master is to add {{spark.master local}} to my SparkConf file.

This is a regression from 1.02 since it  breaks the pyspark shell examples in the documentation (Spark Programming Guide).  To reproduce the problem locally with 1.1.0-snapshot1, try running {{./bin/pyspark --master local-cluster\[2,2,512\]}}; you'll see that no extra executors are started because the option is ignored, so we end up running in local mode.;;;","15/Aug/14 19:20;joshrosen;I think this was fixed by SPARK-2678: these options now take effect when launching pyspark shells.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting SPARK_MEM for bin/pyspark does not work. ,SPARK-2109,12720544,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,prashant,prashant,prashant,11/Jun/14 10:16,10/Dec/15 15:05,14/Jul/23 06:25,03/Jul/14 22:09,1.0.0,,,,,,,,1.0.1,1.1.0,,,,,,,,,0,,,,,,"
prashant@sc:~/work/spark$ SPARK_MEM=10G bin/pyspark 
Python 2.7.6 (default, Mar 22 2014, 22:59:56) 
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File ""/home/prashant/work/spark/python/pyspark/shell.py"", line 43, in <module>
    sc = SparkContext(appName=""PySparkShell"", pyFiles=add_files)
  File ""/home/prashant/work/spark/python/pyspark/context.py"", line 94, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File ""/home/prashant/work/spark/python/pyspark/context.py"", line 190, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway()
  File ""/home/prashant/work/spark/python/pyspark/java_gateway.py"", line 51, in launch_gateway
    gateway_port = int(proc.stdout.readline())
ValueError: invalid literal for int() with base 10: 'Warning: SPARK_MEM is deprecated, please use a more specific config option\n'",,apachespark,prashant,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398743,,,Thu Dec 10 15:05:51 UTC 2015,,,,,,,,,,"0|i1wnbz:",398865,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/14 10:21;prashant;https://github.com/apache/spark/pull/1050;;;","03/Jul/14 22:09;pwendell;Fixed in master and 1.0 via https://github.com/apache/spark/pull/1050/files;;;","10/Dec/15 15:05;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/1050;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mark SparkContext methods that return block information as developer API's,SPARK-2108,12720516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,pwendell,pwendell,11/Jun/14 05:29,10/Dec/15 15:06,14/Jul/23 06:25,11/Jun/14 17:50,1.0.0,,,,,,,,1.0.1,1.1.0,,,,,,,,,0,,,,,,"We should really avoid marking things as developer API's post 1.0 (it's super annoying when projects do this), but in this case we have these getters that do nothing but return classes which are themselves developer API's, so to make it more clear I'd like to mark this for users.

The methods are `getExecutorStorageStatus` and `getRDDStorageInfo`.",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398715,,,Thu Dec 10 15:06:25 UTC 2015,,,,,,,,,,"0|i1wn5r:",398837,,,,,,,,,,,,,1.0.0,1.0.1,,,,,,,,,"11/Jun/14 17:50;pwendell;Issue resolved by pull request 1047
[https://github.com/apache/spark/pull/1047];;;","10/Dec/15 15:06;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/1047;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FilterPushdownSuite imports Junit and leads to compilation error,SPARK-2107,12720508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,qiuzhuang.lian,qiuzhuang.lian,11/Jun/14 03:58,11/Jun/14 07:40,14/Jul/23 06:25,11/Jun/14 07:38,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Build,SQL,,,,0,,,,,,,,qiuzhuang.lian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398707,,,Wed Jun 11 05:24:10 UTC 2014,,,,,,,,,,"0|i1wn3z:",398829,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/14 05:24;qiuzhuang.lian;PR: https://github.com/apache/spark/pull/1046;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkUI doesn't remove active stages that failed,SPARK-2105,12720499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,andrewor14,andrewor,11/Jun/14 02:21,24/Jan/15 22:59,14/Jul/23 06:25,24/Jan/15 22:59,1.0.0,,,,,,,,1.1.0,,,,,Spark Core,Web UI,,,,0,,,,,,"If a stage fails because its tasks cannot be serialized, for instance, the failed stage remains in the Active Stages section forever. This is because the StageCompleted event is never posted.",,andrewor,andrewor14,joshrosen,mubarak.seyed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398698,,,Sat Jan 24 22:59:50 UTC 2015,,,,,,,,,,"0|i1wn2f:",398822,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/14 19:04;joshrosen;I tried and failed to reproduce this: https://github.com/apache/spark/commit/bf589fc717c842d1998e3c3a523bc8775cb30269#diff-f346ada4cd59416756b6dd36b6c2605aR97

That doesn't mean that we've fixed the issue, though.  In my tests, the stage never becomes active because the ClosureCleaner detects that the task isn't serializable.  Maybe there's some UDF that manages to slip through the closure cleaning step and fails once the stage is submitted to the scheduler, so it's still possible that we could hit this bug.;;;","26/Oct/14 19:08;andrewor14;Hey Josh I think this was fixed by this commit:
https://github.com/apache/spark/commit/d934801d53fc2f1d57d3534ae4e1e9384c7dda99

The root cause is because we were dropping events, and that happened because one of the listeners was taking all the time to process the events. We may run into this only if the application attaches arbitrary listeners to Spark and these listeners perform expensive operations, but from Spark's side I don't think there's anything we can do about that.;;;","24/Jan/15 22:59;srowen;It appears this is considered fixed by that commit, for 1.1.0, and it has not been reproducible otherwise.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangePartitioner should use user specified serializer to serialize range bounds,SPARK-2104,12720489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,rxin,rxin,11/Jun/14 00:35,30/Jun/14 06:00,14/Jul/23 06:25,30/Jun/14 06:00,,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"Otherwise it is pretty annoying to do a sort on types that are not java serializable. 

To reproduce, just set the serializer to Kryo, and run the following job:
{code}
class JavaNonSerializableClass extends Comparable { override def compareTo(o: JavaNonSerializableClass) = 0 }

sc.parallelize(Seq(new JavaNonSerializableClass, new JavaNonSerializableClass), 2).map(x => (x,x)).sortByKey()
{code}

Basically the partitioner will always be serialized using Java (by the task closure serializer). However, the rangeBounds variable in RangePartitioner should be serialized with the user specified serializer. 

",,jerryshao,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398688,,,Fri Jun 27 05:04:45 UTC 2014,,,,,,,,,,"0|i1wn07:",398812,,,,,,,,,,,,,1.1.0,,,,,,,,,,"27/Jun/14 01:23;jerryshao;Hi Reynold, is that annoying message you mentioned above?

{quote}
org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$JavaNonSerializableClass
{quote}

Seems we can implement writeObject and readObject to use specified serializer like {{ParallelCollectionPartition}}, is that what you want?
;;;","27/Jun/14 01:35;rxin;That sounds good!;;;","27/Jun/14 01:48;jerryshao;Ok, got it. I will try to fix this issue :);;;","27/Jun/14 04:17;rxin;BTW I have some old code I wrote -- you can do your changes based on this
{code}

/**
 * A [[org.apache.spark.Partitioner]] that partitions sortable records by range into roughly
 * equal ranges. The ranges are determined by sampling the content of the RDD passed in.
 *
 * Note that the actual number of partitions created by the RangePartitioner might not be the same
 * as the `partitions` parameter, in the case where the number of sampled records is less than
 * the value of `partitions`.
 */
class RangePartitioner[K : Ordering : ClassTag, V](
    var partitions: Int,
    @transient rdd: RDD[_ <: Product2[K,V]],
    private val ascending: Boolean = true)
  extends Partitioner {

  private var ordering = implicitly[Ordering[K]]

  // An array of upper bounds for the first (partitions - 1) partitions
  var rangeBounds: Array[K] = {
    if (partitions == 1) {
      Array()
    } else {
      val rddSize = rdd.count()
      val maxSampleSize = partitions * 20.0
      val frac = math.min(maxSampleSize / math.max(rddSize, 1), 1.0)
      val rddSample = rdd.sample(false, frac, 1).map(_._1).collect().sorted
      if (rddSample.length == 0) {
        Array()
      } else {
        val bounds = new Array[K](partitions - 1)
        for (i <- 0 until partitions - 1) {
          val index = (rddSample.length - 1) * (i + 1) / partitions
          bounds(i) = rddSample(index)
        }
        bounds
      }
    }
  }

  @throws(classOf[IOException])
  private def writeObject(out: ObjectOutputStream): Unit = {
    val sfactory = SparkEnv.get.serializer
    // Treat java serializer with default action rather than going thru serialization, to avoid a
    // separate serialization header.
    sfactory match {
      case js: JavaSerializer => out.defaultWriteObject()
      case _ =>
        out.writeInt(partitions)
        val ser = sfactory.newInstance()
        Utils.serializeViaNestedStream(out, ser) { stream =>
          stream.writeObject(ordering)
          stream.writeObject(scala.reflect.classTag[K])
          stream.writeObject(rangeBounds)
        }
    }
  }

  @throws(classOf[IOException])
  private def readObject(in: ObjectInputStream): Unit = {

    val sfactory = SparkEnv.get.serializer
    sfactory match {
      case js: JavaSerializer => in.defaultReadObject()
      case _ =>
        partitions = in.readInt()

        val ser = sfactory.newInstance()
        Utils.deserializeViaNestedStream(in, ser) { ds =>
          println(ds)
          ordering = ds.readObject[Ordering[K]]()
          implicit val classTag = ds.readObject[ClassTag[Array[K]]]()
          rangeBounds = ds.readObject[Array[K]]()(classTag)
          binarySearch = CollectionsUtils.makeBinarySearch[K]
        }
    }
  }

  def numPartitions = rangeBounds.length + 1

  private var binarySearch: ((Array[K], K) => Int) = CollectionsUtils.makeBinarySearch[K]

  def getPartition(key: Any): Int = {
    val k = key.asInstanceOf[K]
    var partition = 0
    if (rangeBounds.length < 1000) {
      // If we have less than 100 partitions naive search
      while (partition < rangeBounds.length && ordering.gt(k, rangeBounds(partition))) {
        partition += 1
      }
    } else {
      // Determine which binary search method to use only once.
      partition = binarySearch(rangeBounds, k)
      // binarySearch either returns the match location or -[insertion point]-1
      if (partition < 0) {
        partition = -partition-1
      }
      if (partition > rangeBounds.length) {
        partition = rangeBounds.length
      }
    }
    if (ascending) {
      partition
    } else {
      rangeBounds.length - partition
    }
  }

  override def equals(other: Any): Boolean = other match {
    case r: RangePartitioner[_,_] =>
      r.rangeBounds.sameElements(rangeBounds) && r.ascending == ascending
    case _ =>
      false
  }

  override def hashCode(): Int = {
    val prime = 31
    var result = 1
    var i = 0
    while (i < rangeBounds.length) {
      result = prime * result + rangeBounds(i).hashCode
      i += 1
    }
    result = prime * result + ascending.hashCode
    result
  }
}
{code};;;","27/Jun/14 04:59;jerryshao;Hi Reynold, thanks a lot for your code. At first glance seems it is quite OK for the problem you mentioned, I'm not sure is there any hiding corners I missed?;;;","27/Jun/14 05:01;rxin;I don't really remember ... :) It's been a while

I guess adding tests would be important.;;;","27/Jun/14 05:04;jerryshao;OK, got it. Thanks a lot;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Java + Kafka + Spark Streaming NoSuchMethodError in java.lang.Object.<init>,SPARK-2103,12720474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,srowen,srowen,10/Jun/14 22:55,01/Aug/14 11:34,14/Jul/23 06:25,01/Aug/14 11:34,1.0.0,,,,,,,,1.1.0,,,,,DStreams,,,,,2,,,,,,"This has come up a few times, from user venki-kratos:
http://apache-spark-user-list.1001560.n3.nabble.com/NoSuchMethodError-in-KafkaReciever-td2209.html

and I ran into it a few weeks ago:
http://mail-archives.apache.org/mod_mbox/spark-dev/201405.mbox/%3CCAMAsSdLzS6ihcTxepUsphRyXxA-wp26ZGBxx83sM6niRo0q4Rg@mail.gmail.com%3E

and yesterday user mpieck:

{quote}
When I use the createStream method from the example class like
this:

KafkaUtils.createStream(jssc, ""zookeeper:port"", ""test"", topicMap);

everything is working fine, but when I explicitely specify message decoder
classes used in this method with another overloaded createStream method:

KafkaUtils.createStream(jssc, String.class, String.class,
StringDecoder.class, StringDecoder.class, props, topicMap,
StorageLevels.MEMORY_AND_DISK_2);

the applications stops with an error:

14/06/10 22:28:06 ERROR kafka.KafkaReceiver: Error receiving data
java.lang.NoSuchMethodException:
java.lang.Object.<init>(kafka.utils.VerifiableProperties)
        at java.lang.Class.getConstructor0(Unknown Source)
        at java.lang.Class.getConstructor(Unknown Source)
        at
org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:108)
        at
org.apache.spark.streaming.dstream.NetworkReceiver.start(NetworkInputDStream.scala:126)
{quote}

Something is making it try to instantiate java.lang.Object as if it's a Decoder class.

I suspect that the problem is to do with
https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala#L148

{code}
    implicit val keyCmd: Manifest[U] =
implicitly[Manifest[AnyRef]].asInstanceOf[Manifest[U]]
    implicit val valueCmd: Manifest[T] =
implicitly[Manifest[AnyRef]].asInstanceOf[Manifest[T]]
{code}

... where U and T are key/value Decoder types. I don't know enough Scala to fully understand this, but is it possible this causes the reflective call later to lose the type and try to instantiate Object? The AnyRef made me wonder.

I am sorry to say I don't have a PR to suggest at this point.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398673,,,Mon Jul 21 06:41:01 UTC 2014,,,,,,,,,,"0|i1wmwv:",398797,,,,,,,,,,,,,1.1.0,,,,,,,,,,"21/Jul/14 06:41;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/1508;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caching with GENERIC column type causes query execution to slow down significantly,SPARK-2102,12720467,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,10/Jun/14 22:35,23/Jul/14 23:32,14/Jul/23 06:25,23/Jul/14 23:32,1.0.0,,,,,,,,1.1.0,,,,,SQL,,,,,0,,,,,,"It is likely that we are doing something wrong with Kryo.

[~adav] has a twitter dataset that reproduces the issue.",,marmbrus,mubarak.seyed,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398666,,,2014-06-10 22:35:10.0,,,,,,,,,,"0|i1wmvb:",398790,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python unit tests fail on Python 2.6 because of lack of unittest.skipIf(),SPARK-2101,12720464,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,laserson,laserson,10/Jun/14 22:05,09/Jan/15 17:50,14/Jul/23 06:25,11/Aug/14 18:56,1.0.0,,,,,,,,0.9.3,1.0.3,1.1.0,,,PySpark,,,,,0,,,,,,"PySpark tests fail with Python 2.6 because they currently depend on {{unittest.skipIf}}, which was only introduced in Python 2.7.",,apachespark,laserson,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2910,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398663,,,Sun Aug 10 07:33:24 UTC 2014,,,,,,,,,,"0|i1wmun:",398787,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/14 22:06;laserson;If there are no objections, I will propose a patch that depends imports {{unittest2}} when the interpreter is Python 2.6.  This module backports the relevant improvements to the {{unittest}} module.;;;","11/Jun/14 00:02;pwendell;That seems like a good idea!;;;","10/Aug/14 07:33;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1874;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF Support,SPARK-2097,12720399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,10/Jun/14 17:52,02/Aug/14 23:34,14/Jul/23 06:25,02/Aug/14 23:34,,,,,,,,,1.1.0,,,,,SQL,,,,,0,,,,,,Right now we only support UDFs that are written against the Hive API or are called directly as expressions in the DSL.  It would be nice to have native support for registering scala/python functions as well.,,kzhang,marmbrus,ravipesala,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398598,,,Thu Jun 12 17:54:24 UTC 2014,,,,,,,,,,"0|i1wmgf:",398723,,,,,,,,,,,,,1.1.0,,,,,,,,,,"12/Jun/14 17:54;marmbrus;Started working on this here: https://github.com/marmbrus/spark/tree/udfs;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly parse dot notations for accessing an array of structs,SPARK-2096,12720367,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,yhuai,yhuai,10/Jun/14 15:53,11/Jan/15 08:26,14/Jul/23 06:25,10/Sep/14 19:59,1.0.0,,,,,,,,1.2.0,,,,,SQL,,,,,1,starter,,,,,"For example, ""arrayOfStruct"" is an array of structs and every element of this array has a field called ""field1"". ""arrayOfStruct[0].field1"" means to access the value of ""field1"" for the first element of ""arrayOfStruct"", but the SQL parser (in sql-core) treats ""field1"" as an alias. Also, ""arrayOfStruct.field1"" means to access all values of ""field1"" in this array of structs and the returns those values as an array. But, the SQL parser cannot resolve it.",,apachespark,chuxi,cloud_fan,gvramana,jonathak,marmbrus,miccagiann,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5166,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398566,,,Tue Sep 16 04:40:25 UTC 2014,,,,,,,,,,"0|i1wm9b:",398691,,,,,,,,,,,,,1.3.0,,,,,,,,,,"10/Jun/14 15:54;yhuai;Also, we need to check the behavior of Hive parser. ;;;","15/Aug/14 01:12;miccagiann;Hi Yin,

If nobody is working in this issue, I can help!

Thanks,
Michael ;;;","15/Aug/14 01:22;yhuai;[~miccagiann] Yeah, sure. Thank you. We have fixed it in Hive parser (https://issues.apache.org/jira/browse/SPARK-2483). We just need to fix it in the SqlParser in Catalyst.;;;","21/Aug/14 08:59;chuxi;I think i almost have solved the issue. 

I have passed the test case in JsonSuite (""Complex field and type inferring (Ignored)"") which is ignored, by a little modified.

modified test part :
checkAnswer(
      sql(""select arrayOfStruct.field1, arrayOfStruct.field2 from jsonTable""),
      (Seq(true, false, null), Seq(""str1"", null, null)) :: Nil
    )

However, another question is repeated nested structure is a problem, like arrayOfStruct.field1.arrayOfStruct.field1 or arrayOfStruct[0].field1.arrayOfStruct[0].field1

I plan to ignore this problem and try to add  ""select arrayOfStruct.field1, arrayOfStruct.field2 from jsonTable where arrayOfStruct.field1==true ""

Besides, my friend anyweil (Wei Li) solved the problem of arrayOfStruct.field1 and its Filter part( means where parsing).

I am fresh here but will continue working on spark :)
;;;","21/Aug/14 12:09;chuxi;I checked the problem "" where arrayOfStruct.field1==true "" 

this problem will lead to modify every kind of comparisonExpression. And I think it makes no sense to add this function. So I discard it. 

Over.;;;","21/Aug/14 13:07;apachespark;User 'chuxi' has created a pull request for this issue:
https://github.com/apache/spark/pull/2082;;;","01/Sep/14 12:35;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/2230;;;","10/Sep/14 19:59;marmbrus;I'm going to mark this as fixed as I think the parsing issues are fixed.  If we want to add support for calling .fieldName of arrays of structs we should open another PR.;;;","16/Sep/14 04:40;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/2405;;;",,,,,,,,,,,,,,,,,,,,,,,,
Ensure exactly once semantics for DDL / Commands,SPARK-2094,12720299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,10/Jun/14 08:33,13/Jun/14 20:05,14/Jul/23 06:25,13/Jun/14 20:05,,,,,,,,,1.1.0,,,,,SQL,,,,,0,,,,,,"From [~lian cheng]...
The constraints presented here are:

 * The side effect of a command SchemaRDD should take place eagerly;
 * The side effect of a command SchemaRDD should take place once and only once;
 * When .collect() method is called, something meaningful, usually the output message lines of the command, should be presented.

Then how about adding a lazy field inside all the physical command nodes to wrap up the side effect and hold the command output? Take the SetCommandPhysical as an example:
{code}
trait PhysicalCommand(@transient context: SQLContext) {
   lazy val commandOutput: Any
}

case class SetCommandPhysical(
    key: Option[String], value: Option[String], output: Seq[Attribute])(
    @transient context: SQLContext)
  extends PhysicalCommand(context)
  with PhysicalCommand {

  override lazy val commandOutput = {
    // Perform the side effect, and record appropriate output
    ???
  }

  def execute(): RDD[Row] = {
    val row = new GenericRow(Array[Any](commandOutput))
    context.sparkContext.parallelize(row, 1)
  }
}
{code}
In this way, all the constraints are met:

 * Eager evaluation: done by the toRdd call in SchemaRDDLike (PR #948),
 * Side effect should take place once and only once: ensured by the lazy commandOutput field,
 * Present meaningful output as RDD contents: command output is held by commandOutput and returned in execute().

An additional benefit is that, side effect logic of all the commands can be implemented within their own physical command nodes, instead of adding special cases inside SQLContext.toRdd and/or HiveContext.toRdd.",,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398498,,,Fri Jun 13 20:05:54 UTC 2014,,,,,,,,,,"0|i1wlu7:",398623,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/14 20:55;lian cheng;The basic idea is illustrated in {{CacheCommandPhysical}}: https://github.com/apache/spark/pull/1038/files#diff-726d84ece1e6f6197b98a5868c881ac7R68;;;","12/Jun/14 19:43;lian cheng;PR: https://github.com/apache/spark/pull/1071;;;","13/Jun/14 20:05;marmbrus;https://github.com/apache/spark/pull/1071;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPropagation should use exact type value.,SPARK-2093,12720295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,10/Jun/14 08:00,11/Jun/14 06:15,14/Jul/23 06:25,11/Jun/14 06:15,,,,,,,,,1.0.1,1.1.0,,,,,,,,,0,,,,,,{{NullPropagation}} should use exact type value when transform {{Count}} or {{Sum}}.,,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398494,,,Tue Jun 10 08:05:53 UTC 2014,,,,,,,,,,"0|i1wltb:",398619,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/14 08:05;ueshin;PRed: https://github.com/apache/spark/pull/1034;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark/mllib is not compatible with numpy-1.4,SPARK-2091,12720283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,10/Jun/14 06:30,11/Jun/14 07:54,14/Jul/23 06:25,11/Jun/14 07:54,1.0.0,,,,,,,,1.0.1,,,,,MLlib,PySpark,,,,0,,,,,,"pyspark/mllib is not compatible with numpy 1.4. If the required changes are small, we should support numpy 1.4.",,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398482,,,Tue Jun 10 08:18:33 UTC 2014,,,,,,,,,,"0|i1wlqn:",398607,,,,,,,,,,,,,1.0.1,,,,,,,,,,"10/Jun/14 08:18;mengxr;PR: https://github.com/apache/spark/pull/1035;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in toString when creationSiteInfo is null after deserialization,SPARK-2088,12720261,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dorx,dorx,dorx,10/Jun/14 02:06,12/Jun/14 19:54,14/Jul/23 06:25,12/Jun/14 19:54,1.0.0,,,,,,,,1.0.1,,,,,,,,,,0,,,,,,"After deserialization, the transient field creationSiteInfo does not get backfilled with the default value, but the toString method, which is invoked by the serializer, expects the field to always be non-null. The following issue is encountered during serialization:

java.lang.NullPointerException
	at org.apache.spark.rdd.RDD.getCreationSite(RDD.scala:1198)
	at org.apache.spark.rdd.RDD.toString(RDD.scala:1263)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1418)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at scala.collection.immutable.$colon$colon.writeObject(List.scala:379)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
	at org.apache.spark.scheduler.ResultTask$.serializeInfo(ResultTask.scala:46)
	at org.apache.spark.scheduler.ResultTask.writeExternal(ResultTask.scala:125)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1458)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1429)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:71)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:771)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:717)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:701)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1180)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",,dorx,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398460,,,Thu Jun 12 19:54:47 UTC 2014,,,,,,,,,,"0|i1wllr:",398585,,,,,,,,,,,,,1.0.1,,,,,,,,,,"12/Jun/14 19:54;mengxr;PR: https://github.com/apache/spark/pull/1028;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Yarn: history UI link missing, wrong reported user",SPARK-2080,12720146,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,09/Jun/14 16:50,12/Jun/14 21:28,14/Jul/23 06:25,12/Jun/14 21:28,1.0.0,,,,,,,,1.0.1,1.1.0,,,,YARN,,,,,0,,,,,,"In Yarn client mode, the History UI link is not set for finished applications (it is for cluster mode). In Yarn cluster mode, the user reported by the application is wrong - it reports the user running the Yarn service, not the user running the Yarn application.

PR is up:
https://github.com/apache/spark/pull/1002",,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398345,,,Thu Jun 12 21:13:31 UTC 2014,,,,,,,,,,"0|i1wkwn:",398472,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 16:51;vanzin;Patrick / someone, I can't seem to be able to assign bugs to myself anymore, could someone do that? Thanks.;;;","12/Jun/14 21:13;tgraves;Note that this is only fixing it for running on in insecure Yarn cluster.  For a secure cluster the proxy stuff has to be setup properly. That will fall under SPARK-1291;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log serializer in use on application startup,SPARK-2077,12719119,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ash211@gmail.com,aash,aash,09/Jun/14 06:32,30/Jun/14 06:29,14/Jul/23 06:25,30/Jun/14 06:29,1.0.0,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"In a recent mailing list thread a user was uncertain that their {{spark.serializer}} setting was in effect.

Let's log the serializer being used to protect against typos on the setting.",,aash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397318,,,Mon Jun 09 06:35:41 UTC 2014,,,,,,,,,,"0|i1wekf:",397445,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 06:35;aash;https://github.com/apache/spark/pull/1017;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anonymous classes are missing from Spark distribution,SPARK-2075,12719084,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,paulrbrown,paulrbrown,08/Jun/14 19:44,22/Dec/14 10:12,14/Jul/23 06:25,22/Dec/14 06:10,1.0.0,,,,,,,,1.2.1,1.3.0,,,,Build,Spark Core,,,,2,,,,,,"Running a job built against the Maven dep for 1.0.0 and the hadoop1 distribution produces:

{code}
java.lang.ClassNotFoundException:
org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1
{code}

Here's what's in the Maven dep as of 1.0.0:

{code}
jar tvf ~/.m2/repository/org/apache/spark/spark-core_2.10/1.0.0/spark-core_2.10-1.0.0.jar | grep 'rdd/RDD' | grep 'saveAs'
  1519 Mon May 26 13:57:58 PDT 2014 org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
  1560 Mon May 26 13:57:58 PDT 2014 org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
{code}

And here's what's in the hadoop1 distribution:

{code}
jar tvf spark-assembly-1.0.0-hadoop1.0.4.jar| grep 'rdd/RDD' | grep 'saveAs'
{code}

I.e., it's not there.  It is in the hadoop2 distribution:

{code}
jar tvf spark-assembly-1.0.0-hadoop2.2.0.jar| grep 'rdd/RDD' | grep 'saveAs'
  1519 Mon May 26 07:29:54 PDT 2014 org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
  1560 Mon May 26 07:29:54 PDT 2014 org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
{code}",,aash,andrewor,apachespark,dragos,glenn.strycker@gmail.com,huitseeker,maropu,NoamBarcay,paulrbrown,pferrel,pwendell,shivaram,sunrui,zsxwing,,,,,,,,,,,,,,,,,,,,,,,SPARK-2292,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397283,,,Mon Dec 22 10:12:43 UTC 2014,,,,,,,,,,"0|i1wecn:",397410,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/14 22:11;pwendell;Okay I did some more digging. I think the issue is that the anonymous classes used by saveAsTextFile are not guaranteed to be compiled to the same name every time you compile them in Scala. In the Hadoop 1 build these end up being shortened wheras in the Hadoop 2 build they use the longer names. saveAsTextFile seems to, strangely, be the only affected function. I confirmed this by looking at the difference in the hadoop 1 and 2 jars:

{code}
$ jar tvf spark-1.0.0-bin-hadoop1/lib/spark-assembly-1.0.0-hadoop*.jar |grep ""rdd\/RDD\\$"" | awk '{ print $8;}' | sort > hadoop1
$ jar tvf spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop*.jar |grep ""rdd\/RDD\\$"" | awk '{ print $8;}' | sort > hadoop2
$ diff hadoop1 hadoop2
23a24
> org/apache/spark/rdd/RDD$$anonfun$28$$anonfun$apply$13.class
27,29d27
< org/apache/spark/rdd/RDD$$anonfun$30$$anonfun$apply$13.class
< org/apache/spark/rdd/RDD$$anonfun$30.class
< org/apache/spark/rdd/RDD$$anonfun$31.class
90a89,90
> org/apache/spark/rdd/RDD$$anonfun$saveAsTextFile$1.class
> org/apache/spark/rdd/RDD$$anonfun$saveAsTextFile$2.class
{code}

This strangely only seems to affect the saveAsTextFile function.

I'm still a bit confused though because I didn't think these anonymous classes would show up in the byte code of the user application, so I don't think it should matter (i.e. this is why Scala probably allows this).

{code}
javap RDD | grep saveAsText
  public void saveAsTextFile(java.lang.String);
  public void saveAsTextFile(java.lang.String, java.lang.Class<? extends org.apache.hadoop.io.compress.CompressionCodec>);
{code}

[~paulrbrown] could you explain how you are bundling and submitting your application to the Spark cluster?;;;","09/Jun/14 15:04;paulrbrown;The job is run by a Java client that connects to the master (using a SparkContext).

Bundling is performed by a Maven build with two shade plugin invocations, one to package a ""driver"" uberjar and one to packager a ""worker"" uberjar.  The worker flavor is sent to the worker nodes, the driver contains the code to connect to the master and run the job.  The Maven build runs against the JAR from Maven Central, and the deployment uses the Spark 1.0.0 hadoop1 download.  (The Spark is staged to S3 once and then downloaded onto master/worker nodes and set up during cluster provisioning.)

The Maven build uses the usual Scala setup with the library as a dependency and the plugin:

{code}
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.10.3</version>
        </dependency>
{code}

{code}
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <scalaVersion>2.10.3</scalaVersion>
                    <jvmArgs>
                      <jvmArg>-Xms64m</jvmArg>
                      <jvmArg>-Xmx4096m</jvmArg>
                    </jvmArgs>
                </configuration>
            </plugin>
{code}
;;;","09/Jun/14 15:36;paulrbrown;As food for thought, [here|http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.7.6] is the {{InnerClass}} section of the JVM spec.  It looks like there have been some changes from 2.10.3 to 2.10.4 (e.g., [SI-6546|https://issues.scala-lang.org/browse/SI-6546]), but I didn't dig in.

I think the thing most likely to work is to ensure that exactly the same bits are used by all of the distributions and posted to Maven Central.  (For some discussion on inner class naming stability, there was quite a bit of it on the Java 8 lambda discussion list, e.g., [this message|http://mail.openjdk.java.net/pipermail/lambda-spec-experts/2013-July/000316.html].);;;","09/Jun/14 17:46;pwendell;I see - so the issue is that your closures are compiled to call that anonymous class in your driver. Then on the cluster the anonymous class is named differently. I wonder if scala can produce stable naming for anonymous classes, that would be the best solution because otherwise it sort of breaks closures. I looked around the compiler flags but I don't see anything useful.

One solution here is to see if we can avoid re-compiling spark-core when we make the different builds. The tricky bit though is that if users compile their own downstream version of Spark we'd have to make sure they don't end up with different versions as well.;;;","15/Oct/14 14:12;dragos;The Scala compiler produces stable names for anonymous functions. In fact, that's the reason why the name of the enclosing method is part of the name: so that adding or removing an anonymous function in another method does not change the numbering of the others. Names are assigned by using a per-compilation unit counter and a prefix. Looking at the diff, there's quite a different picture in the two cases (anonymous functions vs. anonymous classes). Are you sure the two jars are built from the same sources?

I don't know how the `assembly` jar is produced, but if it's using some sort of whole-program analysis and dead-code elimination, it might erroneously remove them. It might help to look at the inputs to the assembly and see if the class is already missing.

Another possibility is running `scalac -optimize` in only one of the two builds. However, looking at current sources I can't see why the inliner would remove those closures (the class is not final, and `map` is not final either, so they can't be resolved and inlined).. ;;;","21/Oct/14 00:55;pferrel;Is there any more on this?

Building Spark from the 1.1.0 tar for Hadoop 1.2.1--all is well. Trying to upgrade Mahout to use Spark 1.1.0. The Mahout 1.0-snapshot source builds and build tests pass with spark 1.1.0 as a maven dependency. Running the Mahout build on some bigger data using my dev machine as a standalone single node Spark cluster. So the same code is running as executed the build tests, just in single node cluster mode. Also since I built Spark i assume it is using the artifact from my .m2 maven cache, but not 100% on that. Anyway I get the class not found error below.

I assume the missing function is the anon function passed to the 

{code}
    rdd.map(
      {anon function}
    )saveAsTextFile ???? 
{code}

so shouldn't the function be in the Mahout jar (it isn't)? Isn't this function passed in from Mahout so I don't understand why it matters how Spark was built. 

Several other users are getting this for Spark 1.0.2. If we are doing something wrong in our build process we'd appreciate a pointer.

Here's the error I get:

14/10/20 17:21:36 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 8.0 (TID 16, 192.168.0.2): java.lang.ClassNotFoundException: org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1
        java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        java.security.AccessController.doPrivileged(Native Method)
        java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        java.lang.Class.forName0(Native Method)
        java.lang.Class.forName(Class.java:249)
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:59)
        java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1591)
        java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1496)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1750)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        java.lang.Thread.run(Thread.java:695)
  ;;;","21/Oct/14 12:44;dragos;The name of the missing class is {{org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1}}, so unless Mahout is putting classes inside Spark packages, that class should be somewhere *in Spark jars*.;;;","21/Oct/14 17:30;pferrel;Oops, right. But the function name is being constructed at Mahout build time, right? So the rules for constructing the name are different when building Spark and Mahout OR the function is not being put in the Spark jars?

Rebuilding yet again so I can't check the jars just yet. This error was from a clean build of Spark 1.1.0 and Mahout in that sequence. I cleaned the .m2 repo and see that it is filled in only when Mahout is build and is filled in with a repo version of Spark 1.1.0

Could the problem be related to the fact that Spark as executed on my standalone cluster is from a local build but as Mahout is built it uses the repo version of Spark? Since I am using hadoop 1.2.1 I suspect the repo version may not be exactly what I build.;;;","21/Oct/14 17:45;pferrel;trying mvn install instead of the documented mvn package to put Spark in the maven cache so that when building Mahout it will get exactly the same bits as will later be run on the cluster--WAG;;;","21/Oct/14 19:00;pferrel;OK solved. The WAG worked.

Instead of 'mvn package ...' use 'mvn install ...' so you'll get the exact version of Spark needed in your local maven cache at ~/.m2

This should be changed in the instructions until some way to reliable point to the right version of Spark in the Maven repos is implemented. In my case I had to build Spark to target Hadoop 1.2.1 but the Maven repo was not built for that.;;;","10/Dec/14 06:59;srowen;It looks like in the end this is a problem caused by mismatching Spark versions at the client/server end, which isn't supported.;;;","10/Dec/14 09:05;paulrbrown;It's not a version of Spark that's in question; it's incompatibility between what's published to Maven Central and what's offered for download from the Apache site for what is nominally the *same* version.

That said, I don't object to closing the issue, but I agree with [~pferrel] above that it's a documentation issue.  The perfect-world solution would be to publish *multiple* Spark artifacts with Maven classifiers identifying the expected backplane.  This issue will continue to afflict people who build Spark applications with Maven Central artifacts and attempt to connect to Spark clusters build against a different backplane.;;;","10/Dec/14 09:33;srowen;Hm on re-reading more closely, you are right, this is not attributable to version mismatch per se. I think it's better to leave it open even if I don't know of any action here. Sorry for the noise.

The same thing appears to occur in 1.1.1. Maybe this goes away, accidentally, when people all use Hadoop 2.x.

I don't think there will be multiple versions deployed to Maven as I think the theory is that the artifacts only exist for the API, and not whatever other libs they happen to link against. This is a leak in that theory though.;;;","10/Dec/14 20:45;pferrel;If the explanation is correct this needs to be filed against Spark as putting the wrong or not enough artifacts into maven repos. There would need to be a different artifact for every config option that will change internal naming.

I can't understand why lots of people aren't running into this, all it requires is that you link against the repo artifact and run against a user compiled Spark.;;;","18/Dec/14 11:00;sunrui;I met the same issue. I had a post in the Spark user mailing list but it does not get archived in http://apache-spark-user-list.1001560.n3.nabble.com/, so I have to describe the issue here:

Steps to reproduce:
1.	Download the official pre-built Spark binary 1.1.1 at http://d3kbcqa49mib13.cloudfront.net/spark-1.1.1-bin-hadoop1.tgz 
2.	Launch the Spark cluster in pseudo cluster mode
3.	A small scala APP which calls RDD.saveAsObjectFile()
scalaVersion := ""2.10.4""

libraryDependencies ++= Seq(
  ""org.apache.spark"" %% ""spark-core"" % ""1.1.1""
)

val sc = new SparkContext(args(0), ""test"") //args[0] is the Spark master URI
  val rdd = sc.parallelize(List(1, 2, 3))
  rdd.saveAsObjectFile(""/tmp/mysaoftmp"")
          sc.stop

throws an exception as follows:
[error] (run-main-0) org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 6, ray-desktop.sh.intel.com): java.lang.ClassCastException: scala.Tuple2 cannot be cast to scala.collection.Iterator
[error]         org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
[error]         org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
[error]         org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
[error]         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
[error]         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
[error]         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
[error]         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
[error]         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
[error]         org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
[error]         org.apache.spark.scheduler.Task.run(Task.scala:54)
[error]         org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
[error]         java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
[error]         java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[error]         java.lang.Thread.run(Thread.java:701)

After investigation, I found that this is caused by bytecode incompatibility issue between RDD.class in spark-core_2.10-1.1.1.jar and the pre-built spark assembly respectively.

This issue also happens with spark 1.1.0.
;;;","18/Dec/14 11:01;zsxwing;Dig deeply and found weird things:

If I used `mvn -Dhadoop.version=1.2.1 -DskipTests clean package -pl core -am` to compile, the `saveAsTextFile` will be:
{noformat}
public void saveAsTextFile(java.lang.String);
  Code:
   0:   aload_0
   1:   new     #1577; //class org/apache/spark/rdd/RDD$$anonfun$27
   4:   dup
   5:   aload_0
   6:   invokespecial   #1578; //Method org/apache/spark/rdd/RDD$$anonfun$27.""<init>"":(Lorg/apache/spark/rdd/RDD;)V
   9:   getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   12:  ldc_w   #441; //class scala/Tuple2
   15:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   18:  invokevirtual   #447; //Method map:(Lscala/Function1;Lscala/reflect/ClassTag;)Lorg/apache/spark/rdd/RDD;
   21:  astore_2
   22:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   25:  ldc_w   #1580; //class org/apache/hadoop/io/NullWritable
   28:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   31:  astore_3
   32:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   35:  ldc_w   #1582; //class org/apache/hadoop/io/Text
   38:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   41:  astore  4
   43:  getstatic       #21; //Field org/apache/spark/rdd/RDD$.MODULE$:Lorg/apache/spark/rdd/RDD$;
   46:  aload_2
   47:  invokevirtual   #23; //Method org/apache/spark/rdd/RDD$.rddToPairRDDFunctions$default$4:(Lorg/apache/spark/rdd/RDD;)Lscala/runtime/Null$;
   50:  astore  5
   52:  getstatic       #21; //Field org/apache/spark/rdd/RDD$.MODULE$:Lorg/apache/spark/rdd/RDD$;
   55:  aload_2
   56:  aload_3
   57:  aload   4
   59:  aload   5
   61:  pop
   62:  aconst_null
   63:  invokevirtual   #47; //Method org/apache/spark/rdd/RDD$.rddToPairRDDFunctions:(Lorg/apache/spark/rdd/RDD;Lscala/reflect/ClassTag;Lscala/reflect/ClassTag;Lscala/math/Ordering;)Lorg/apache/spark/rdd/PairRDDFunctions;
   66:  aload_1
   67:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   70:  ldc_w   #1584; //class org/apache/hadoop/mapred/TextOutputFormat
   73:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   76:  invokevirtual   #1588; //Method org/apache/spark/rdd/PairRDDFunctions.saveAsHadoopFile:(Ljava/lang/String;Lscala/reflect/ClassTag;)V
   79:  return
{noformat}

If I used `mvn -Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package -pl core -am` to compile, the `saveAsTextFile` is different:
{noformat}
public void saveAsTextFile(java.lang.String);
  Code:
   0:   getstatic       #21; //Field org/apache/spark/rdd/RDD$.MODULE$:Lorg/apache/spark/rdd/RDD$;
   3:   aload_0
   4:   new     #1577; //class org/apache/spark/rdd/RDD$$anonfun$saveAsTextFile$1
   7:   dup
   8:   aload_0
   9:   invokespecial   #1578; //Method org/apache/spark/rdd/RDD$$anonfun$saveAsTextFile$1.""<init>"":(Lorg/apache/spark/rdd/RDD;)V
   12:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   15:  ldc_w   #441; //class scala/Tuple2
   18:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   21:  invokevirtual   #447; //Method map:(Lscala/Function1;Lscala/reflect/ClassTag;)Lorg/apache/spark/rdd/RDD;
   24:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   27:  ldc_w   #1580; //class org/apache/hadoop/io/NullWritable
   30:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   33:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   36:  ldc_w   #1582; //class org/apache/hadoop/io/Text
   39:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   42:  getstatic       #1587; //Field scala/math/Ordering$.MODULE$:Lscala/math/Ordering$;
   45:  getstatic       #471; //Field scala/Predef$.MODULE$:Lscala/Predef$;
   48:  invokevirtual   #1591; //Method scala/Predef$.conforms:()Lscala/Predef$$less$colon$less;
   51:  invokevirtual   #1595; //Method scala/math/Ordering$.ordered:(Lscala/Function1;)Lscala/math/Ordering;
   54:  invokevirtual   #47; //Method org/apache/spark/rdd/RDD$.rddToPairRDDFunctions:(Lorg/apache/spark/rdd/RDD;Lscala/reflect/ClassTag;Lscala/reflect/ClassTag;Lscala/math/Ordering;)Lorg/apache/spark/rdd/PairRDDFunctions;
   57:  aload_1
   58:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   61:  ldc_w   #1597; //class org/apache/hadoop/mapred/TextOutputFormat
   64:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   67:  invokevirtual   #1601; //Method org/apache/spark/rdd/PairRDDFunctions.saveAsHadoopFile:(Ljava/lang/String;Lscala/reflect/ClassTag;)V
   70:  return
{noformat}

Note: in hadoop 1.2.1, saveAsTextFile use the default `Ordering` value `null`, while in hadoop 2.2.0, saveAsTextFile will use `Ordering.ordered` to create a new `Ordering`.
;;;","18/Dec/14 11:11;sunrui;I think we need to address this issue, at least we need to document it.

In order to guarrantee the binary compatibility, I think we may enforce the release steps to build the assembly as follows for an official release:
1. Build all module jars except the assembly module
2. Publish all modules jars to the central maven repository
3. Then build the assembly module to assemble the module jars into the final assembly by depending on modular jars in the maven repo
4. May also push the assembly jar to the central maven repository
;;;","18/Dec/14 11:16;srowen;[~zsxwing] I don't expect the byte code to be identical when compiled against different versions of the underlying library, not necessarily. That's not quite the issue, but rather how these differences change the naming of anonymous functions. [~sunrui] I don't think that's quite the issue either. The assembly *is* made from the module JARs. All are published to Maven Central; please search http://search.maven.org/ . ;;;","18/Dec/14 11:26;sunrui;Owen, if the official assembly is made from the exact published jars, then the binaries should be exactly the same, why we met this issue? You can see that the RDD.class from the maven spark-core 1.1.0 is different from that in the Spark pre-built 1.1.0 assembly (they have different file size, and in-compatible bytecode ).

So I doubt there is some mistake in the release step of Spark. ;;;","18/Dec/14 12:12;srowen;(Sean) But my understanding is that you are comparing to binaries compiled for a potentially different underlying Hadoop distribution. That is the issue at hand here.;;;","18/Dec/14 19:03;shivaram;So just to make sure I understand things correctly, is it the case that the jar published to maven (spark-core-1.1.1) is built using Hadoop2 dependencies while the Hadoop1 assembly jar that is distributed is built using Hadoop 1 (obviously...) ?

[~srowen] While I see that we officially support submitting jobs using spark-submit, it is surprising to me that other deployment methods would fail this way (from the user's perspective the Spark versions presumably at compile time and run time presumably match up ?). We should at the very least document this, but it would also be good to see if there is a work around.;;;","18/Dec/14 20:31;srowen;[~shivaram] No, I don't think that's the case. Certainly not Hadoop 1 vs 2. Here is how the downloadable distros are built:

{code}
make_binary_release ""hadoop1"" ""-Phive -Phive-thriftserver -Dhadoop.version=1.0.4"" &
make_binary_release ""hadoop1-scala2.11"" ""-Phive -Dscala-2.11"" &
make_binary_release ""cdh4"" ""-Phive -Phive-thriftserver -Dhadoop.version=2.0.0-mr1-cdh4.2.0"" &
make_binary_release ""hadoop2.3"" ""-Phadoop-2.3 -Phive -Phive-thriftserver -Pyarn"" &
make_binary_release ""hadoop2.4"" ""-Phadoop-2.4 -Phive -Phive-thriftserver -Pyarn"" &
make_binary_release ""mapr3"" ""-Pmapr3 -Phive -Phive-thriftserver"" &
make_binary_release ""mapr4"" ""-Pmapr4 -Pyarn -Phive -Phive-thriftserver"" &
make_binary_release ""hadoop2.4-without-hive"" ""-Phadoop-2.4 -Pyarn"" &
{code}

A default {{mvn release}} would use {{hadoop.version=1.0.4}}. Somebody can correct me if this isn't the case, but I assume that this is what goes to Maven Central.

Of course, this behavior is not desirable and not by design, and worth a mention. As far as I can tell it has only been observed arising in Hadoop 1 vs Hadoop 2-compiled artifacts. Although the intended public API is identical, it's not actually 100% compatible. Ideally you could get away with using any copy of the public API. 

In these particular cases, the safe practice of always harmonizing binaries on client and server is actually necessary, which is no terrible thing I think. Nothing about this requires using {{spark-submit}}, although, that's a way to make sure you're using the same Spark in your app and cluster.;;;","18/Dec/14 21:57;shivaram;Hmm -- looking at the release steps it looks like the release on maven should be from Hadoop 1.0.4 [~pwendell] or [~andrewor14] might be able to throw more light on this. (BTW I wonder if we can trace the source of this mismatch for the case reported by [~sunrui] where the distribution with Hadoop1 of Spark 1.1.1 doesn't work with the Maven central jar)

I see your high level point that this is not about spark-submit per se, but about having the exact same binary on the server and as a compile-time dependency. Its just unfortunate that having the same Spark version number isn't sufficient. Also is the workaround right now to rebuild Spark from source using `make-distribution`, do `mvn install`, rebuild the application and deploy Spark using the assembly jar ?;;;","19/Dec/14 02:35;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3740;;;","19/Dec/14 04:21;zsxwing;Please take a look at my PR;;;","19/Dec/14 05:32;sunrui;Since `mvn release` is built for Hadoop 1.0.4, I don't understand the reason why there is difference in RDD.class bytecode from mvn spark-core and the pre-built binary for Hadoop 1.x, because they are both built for Hadoop 1.x and has the same version 1.1.0.

According to [~zsxwing]'s PR, it seems that it's diffcult to guanranttee same bytecode for Hadoop1.x and 2.x. So maybe we need to pubish two versions of a module to mvn, one is for Hadoop 1.x, the other is for Hadoop 2.x, for example, spark-core_2.10-1.1.0-hadoop1.jar and spark-core_2.10-1.1.0-hadoop2.jar?
;;;","19/Dec/14 10:05;srowen;[~sunrui] From digging in to the various reports of this issue, it seemed to me that in each case the Hadoop version did not match. That is, I do not know that it's true that the issue manifests when the Hadoop version matches; that would indeed be strange. I could have missed it; this is a bit hard to follow. But do you see evidence of this?

I don't think publishing two versions fixes anything, really. The PR might get at the heart of the difference here and resolve it for real. It doesn't happen if you match binaries, which is good practice anyway.;;;","20/Dec/14 13:07;sunrui;[~srowen] I assume that mvn jars were built for Hadoop 1.x as you said that ""A default mvn release would use hadoop.version=1.0.4. Somebody can correct me if this isn't the case, but I assume that this is what goes to Maven Central."" :) so it is very possible that mvn jars were actually built for Hadoop 2.x, which is the cause for this issue.

Yes, I agree that we should using match binaries. The problem is that I thought I was using the matching binaries but actually not. So we can 
1. For exising releases,  document the fact that mvn jars are for Hadoop 2.x. If app depends on mvn jars, it should be       used with Hadoop 2.x. If the app is intended to work with Hadoop 1.x, one way is to rebuild Spark source against Hadoop 1.x and publish the module jars to local mvn repo.
2. For futrure release, we may eliminate the Spark core bytecode incompatibility between Hadoop1.x and 2.x just as Shixong's PR is trying to do. ;;;","21/Dec/14 10:41;srowen;[~sunrui] You are asking if the Maven Central artifacts are built for Hadoop 2? No, why do you say that? It is not true that you need to use Hadoop 2, or need to build a custom version. What you should do ideally is match the version you compile against the version you deploy against -- good practice, even if in reality it should not be so strict. 

But the best outcome indeed is to allow compiling against any artifact, and having it work against any build of the same public API version, no matter what the 'backend'. That was intended behavior and hopefully the PRs here get at the nature of the problem.;;;","22/Dec/14 02:06;sunrui;[~srowen] Yes, I totally agree with you  matching version between compilation and depolyment. That's my point also. What I am confused is that an app depends on maven spark-core_2.10-1.1.0.jar (suppose it is built for Hadoop 1.x as you said by default is for Hadoop 1.x for mvn release) does not match the spark-core included in the Spark pre-built binary 1.1.0 for hadoop 1.x. I suppose that Spark pre-built binary 1.1.0 for Hadoop 1.x be assembled from MVN jars, so there should be no mismatching, Am I right? If I am wrong, could anyone tell me the reason of the binary mismatch and tell me the best practice for me to solve my problem?

;;;","22/Dec/14 02:30;shivaram;[~sunrui] What I can see from this JIRA discussion (and [~srowen] please correct me if I am wrong) is that Hadoop 1 vs. Hadoop 2 is one of the causes of incompatibility. It is _not the only_ reason and I don't think we exactly know why the pre-built binary for 1.1.0 is different from the maven version.  

I think the best practice advice is to use the exact same jar in the application and in the runtime. Marking Spark a provided dependency in the application build and using spark-submit is one way of achieving this. Or one can publish a local build to maven and use the same local build to start the cluster etc.;;;","22/Dec/14 06:21;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3758;;;","22/Dec/14 10:12;srowen;[~sunrui] Yes, but I am still not clear that anyone has observed the problem with two binaries built for the same version of Hadoop. Most of the situations listed here do not match that description. I might not understand someone's issue report here. In any event, it sounds like an underlying cause has been fixed already anyway.;;;"
MIMA false positives (umbrella),SPARK-2069,12719053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,prashant,pwendell,pwendell,08/Jun/14 08:38,10/Dec/15 15:05,14/Jul/23 06:25,27/Dec/14 07:45,1.0.0,,,,,,,,1.2.0,,,,,Build,,,,,0,,,,,,Since we started using MIMA more actively in core we've been running into situations were we get false positives. We should address these ASAP as they require having manual excludes in our build files which is pretty tedious.,,apachespark,darabos,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397252,,,Thu Dec 10 15:05:52 UTC 2015,,,,,,,,,,"0|i1we5r:",397379,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/14 20:44;srowen;May we resolve this? the sub-tasks are all Resolved.;;;","10/Dec/15 15:05;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/1021;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark logo in application UI uses absolute path,SPARK-2067,12719027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,neville,sinisa_lyh,sinisa_lyh,07/Jun/14 18:58,09/Jun/14 06:19,14/Jul/23 06:25,09/Jun/14 06:18,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Web UI,,,,,0,,,,,,"Link of the Spark logo in application UI (top left corner) is hard coded to ""/"", and points to the wrong page when running with YARN proxy. Should use uiRoot instead.",,pwendell,sinisa_lyh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397226,,,Mon Jun 09 06:18:48 UTC 2014,,,,,,,,,,"0|i1wdzz:",397353,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"07/Jun/14 19:17;sinisa_lyh;A simple fix: https://github.com/apache/spark/pull/1006;;;","09/Jun/14 06:18;pwendell;Issue resolved by pull request 1006
[https://github.com/apache/spark/pull/1006];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better error message for non-aggregated attributes with aggregates,SPARK-2066,12719000,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,rxin,rxin,07/Jun/14 07:02,13/Oct/14 20:37,14/Jul/23 06:25,13/Oct/14 20:36,1.0.0,,,,,,,,1.2.0,,,,,SQL,,,,,0,,,,,,"[~marmbrus]

Run the following query
{code}
scala> c.hql(""select key, count(*) from src"").collect()
{code}

Got the following exception at runtime
{code}
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: No function to evaluate expression. type: AttributeReference, tree: key#61
	at org.apache.spark.sql.catalyst.expressions.AttributeReference.eval(namedExpressions.scala:157)
	at org.apache.spark.sql.catalyst.expressions.Projection.apply(Projection.scala:35)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$1.apply(Aggregate.scala:154)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$1.apply(Aggregate.scala:134)
	at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:558)
	at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:558)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:261)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:228)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:261)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:228)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}

This should either fail in analysis time, or pass at runtime. Definitely shouldn't fail at runtime.",,apachespark,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2059,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397199,,,Mon Oct 13 20:37:38 UTC 2014,,,,,,,,,,"0|i1wdtb:",397322,,,,,,,,,,,,,1.2.0,,,,,,,,,,"13/Oct/14 20:36;marmbrus;Issue resolved by pull request 2774
[https://github.com/apache/spark/pull/2774];;;","13/Oct/14 20:37;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2774;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating a SchemaRDD via sql() does not correctly resolve nested types,SPARK-2063,12718942,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,ilikerps,ilikerps,06/Jun/14 21:31,15/Sep/15 20:46,14/Jul/23 06:25,15/Sep/15 20:46,1.0.0,,,,,,,,1.5.0,,,,,SQL,,,,,0,,,,,,"For example, from the typical twitter dataset:

{code}
scala> val popularTweets = sql(""SELECT retweeted_status.text, MAX(retweeted_status.retweet_count) AS s FROM tweets WHERE retweeted_status is not NULL GROUP BY retweeted_status.text ORDER BY s DESC LIMIT 30"")

scala> popularTweets.toString
14/06/06 21:27:48 INFO analysis.Analyzer: Max iterations (2) reached for batch MultiInstanceRelations
14/06/06 21:27:48 INFO analysis.Analyzer: Max iterations (2) reached for batch CaseInsensitiveAttributeReferences
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to qualifiers on unresolved object, tree: 'retweeted_status.text
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.qualifiers(unresolved.scala:51)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.qualifiers(unresolved.scala:47)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:67)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:65)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:65)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$3$$anonfun$applyOrElse$2.applyOrElse(Analyzer.scala:100)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$3$$anonfun$applyOrElse$2.applyOrElse(Analyzer.scala:97)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:51)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1$$anonfun$apply$1.apply(QueryPlan.scala:65)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:64)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:69)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:40)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$3.applyOrElse(Analyzer.scala:97)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$3.applyOrElse(Analyzer.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:217)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:94)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:93)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:60)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:60)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:52)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:52)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:265)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:265)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:266)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:266)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:268)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:268)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:269)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:269)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:272)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:272)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:154)
{code}",,gvramana,ilikerps,marmbrus,nitin2goyal,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3414,,,,,,,SPARK-2775,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397141,,,Tue Sep 15 20:46:39 UTC 2015,,,,,,,,,,"0|i1wdgf:",397264,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/14 01:55;marmbrus;This seems to work for me:
{code}
scala> val popularTweets = sql(""SELECT retweeted_status.text, MAX(retweeted_status.retweet_count) AS s FROM tweetTable WHERE retweeted_status is not NULL GROUP BY retweeted_status.text ORDER BY s DESC LIMIT 30"") 
popularTweets: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[13] at RDD at SchemaRDD.scala:117
== Query Plan ==
TakeOrdered 30, [s#26:1 DESC]
 Aggregate false, [text#27], [text#27,MAX(PartialMax#30) AS s#26]
  Exchange (HashPartitioning [text#27:0], 150)
   Aggregate true, [retweeted_status#23.text AS text#27], [retweeted_status#23.text AS text#27,MAX(retweeted_status#23.retweet_count AS retweet_count#29) AS PartialMax#30]
    Project [retweeted_status#23:23]
     Filter IS NOT NULL retweeted_status#23:23
      ExistingRdd [contributors#0,created_at#1,favorite_count#2,favorited#3,filter_level#4,id#5L,id_str#6,in_reply_to_screen_name#7,in_reply_to_status_id#8L,in_reply_to_status_id_str#9,in_reply_to_user_id#10,in_reply_to_user_id_str#11,lang#12,possibly_sensitive#13,retweet_count#14,retwee...
scala> popularTweets.collect()
res3: Array[org.apache.spark.sql.Row] = Array([Four more years. http://t.co/bAJE6Vom,793368], [388726,388726], [Thank you all for helping me through this time with your enormous love &amp; support. Cory will forever be in my heart. http://t.co/XVlZnh9vOc,388719], [Yesss ! I'm 20 ! Wohooo ! No more teens!,369389], [Never been more happy in my life ! Thank you to everyone that has been so lovely about my engagement to my beautiful fiancé ! Big love z X,320358], [Note to self. Don't 'twerk'.,314666], [Harry wake up !! :D http://t.co/cuhD5bC5,311770], [In honor of Kim and Kanye's baby ""North West"" I will be naming my first son ""Taco"",311575], [ITS NIALL BITCH!! HAHA,283976], [what makes you so beautiful is that you dont know how beautiful you are... to me,279850], [279578,279578], [applied ...
{code};;;","07/Jun/14 19:58;marmbrus;Here is a reproducible test case:
{code}
 case class TableName(tableName: String)
 TestSQLContext.sparkContext.parallelize(TableName(""test"") :: Nil).registerAsTable(""tableName"")

 case class NestedData(a: String)
 case class TopLevelRecord(n: NestedData)
 val nestedData =
   TestSQLContext.sparkContext.parallelize(
     TopLevelRecord(NestedData(""value1"")) ::
     TopLevelRecord(NestedData(""value2"")) :: Nil)
 nestedData.registerAsTable(""nestedData"")
 
 test(""nested data"") {
   val query1 = sql(""SELECT n, n.a FROM nestedData GROUP BY a ORDER BY a LIMIT 10"")
   //query1.collect()
   val query2 = query1.select('a)
   checkAnswer(
     query2,
     ""test"")
 }
{code}
;;;","08/Jul/14 17:48;yhuai;Tried two queries.

The following one works
{code}
val query1 = sql(""SELECT n.a as aa FROM nestedData GROUP BY n.a ORDER BY aa LIMIT 10"")
query1: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[33] at RDD at SchemaRDD.scala:104
== Query Plan ==
TakeOrdered 10, [aa#19:0 ASC]
 Aggregate false, [a#20], [a#20 AS aa#19]
  Exchange (HashPartitioning [a#20], 200)
   Aggregate true, [n#0.a AS a#20], [n#0.a AS a#20]
    ExistingRdd [n#0], MapPartitionsRDD[1] at mapPartitions at basicOperators.scala:176
{code}

But, this one does not work
{code}
val query1 = sql(""SELECT n.a FROM nestedData GROUP BY n.a ORDER BY n.a LIMIT 10"")
query1: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[34] at RDD at SchemaRDD.scala:104
== Query Plan ==
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Unresolved attributes: 'n.a ASC, tree:
Sort ['n.a ASC]
 Aggregate [n#0.a AS a#22], [n#0.a AS a#23]
  Subquery nestedData
   SparkLogicalPlan (ExistingRdd [n#0], MapPartitionsRDD[1] at mapPartitions at basicOperators.scala:176), nestedData
{code}

Seems the issue is that when we resolve a reference to a field of a StructType, we use the last part of the reference as the alias.
This logic can be found in ""LogicalPlan.resolve"".
{code}
case StructType(fields) =>
            Some(Alias(nestedFields.foldLeft(a: Expression)(GetField), nestedFields.last)())
{code}
For example, if we have ""n.a"", we will use ""a"" as the alias. ;;;","30/Sep/14 19:08;yhuai;I tried 
{code}
sql(""SELECT n.a FROM nestedData GROUP BY n.a ORDER BY n.a LIMIT 10"")
{code}
It still cannot be analyzed. I am reopening it. ;;;","30/Sep/14 19:10;yhuai;Also, seems it is not duplicated with SPARK-3414.;;;","15/Sep/15 20:46;marmbrus;This is fixed in 1.5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
VertexRDD.apply does not use the mergeFunc,SPARK-2062,12718937,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,larryxiao,ankurd,ankurd,06/Jun/14 20:55,26/Nov/14 04:23,14/Jul/23 06:25,22/Sep/14 19:37,,,,,,,,,1.1.1,1.2.0,,,,GraphX,,,,,0,,,,,,Here: https://github.com/apache/spark/blob/b1feb60209174433262de2a26d39616ba00edcc8/graphx/src/main/scala/org/apache/spark/graphx/VertexRDD.scala#L410,,ankurd,apachespark,larryxiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397136,,,Mon Sep 22 19:37:44 UTC 2014,,,,,,,,,,"0|i1wdfb:",397259,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,"11/Aug/14 03:05;larryxiao;Is anyone working on it? I want to take it.
My plan is to add a pass to do the merge, is it ok? [~ankurd];;;","12/Aug/14 08:47;apachespark;User 'larryxiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/1903;;;","22/Sep/14 19:37;ankurd;Resolved by https://github.com/apache/spark/pull/1903;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate `splits` in JavaRDDLike and add `partitions`,SPARK-2061,12718923,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,slcclimber,pwendell,pwendell,06/Jun/14 20:01,21/Jun/14 01:57,14/Jul/23 06:25,21/Jun/14 01:57,,,,,,,,,1.1.0,,,,,Java API,,,,,0,starter,,,,,Most of spark has used over to consistently using `partitions` instead of `splits`. We should do likewise and add a `partitions` method to JavaRDDLike and have `splits` just call that. We should also go through all cases where other API's (e.g. Python) call `splits` and we should change those to use the newer API.,,pwendell,slcclimber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397122,,,Sat Jun 21 01:57:38 UTC 2014,,,,,,,,,,"0|i1wdc7:",397245,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/14 16:32;slcclimber;Could I be added as the Assignee to this task? I am currently working on a fix.;;;","11/Jun/14 17:58;pwendell;Go for it!;;;","12/Jun/14 06:42;slcclimber;Proposed fixed can be found at : https://github.com/apache/spark/pull/1062;;;","21/Jun/14 01:57;pwendell;Issue resolved by pull request 1062
[https://github.com/apache/spark/pull/1062];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unresolved Attributes should cause a failure before execution time,SPARK-2059,12718901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,marmbrus,marmbrus,06/Jun/14 18:20,04/Jul/14 07:56,14/Jul/23 06:25,04/Jul/14 06:42,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,Here's a partial solution: https://github.com/marmbrus/spark/tree/analysisChecks,,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2066,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397100,,,2014-06-06 18:20:44.0,,,,,,,,,,"0|i1wd7b:",397223,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run-example can only be run within spark_home,SPARK-2057,12718798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maji2014,maji2014,maji2014,06/Jun/14 08:50,08/Jun/14 22:17,14/Jul/23 06:25,08/Jun/14 22:17,,,,,,,,,1.0.0,1.0.1,,,,Spark Core,,,,,0,,,,,,"Old code can only be ran under spark_home and use ""bin/run-example"".
 Error ""./run-example: line 55: ./bin/spark-submit: No such file or directory"" appears when running in other place. 

this code should be """"$FWDIR""/bin/spark-submit ""
",,maji2014,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396997,,,Sun Jun 08 22:16:47 UTC 2014,,,,,,,,,,"0|i1wcjb:",397115,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/14 22:16;pwendell;Fixed via this patch:
https://github.com/apache/spark/pull/1011;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"LIKE, RLIKE, IN, BETWEEN and DIV in HQL should not be case sensitive",SPARK-2050,12718766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,06/Jun/14 02:12,08/Jun/14 07:02,14/Jul/23 06:25,08/Jun/14 07:02,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396965,,,Sun Jun 08 07:02:01 UTC 2014,,,,,,,,,,"0|i1wcc7:",397083,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/14 06:33;marmbrus;Also BETWEEN and DIV.;;;","08/Jun/14 07:02;marmbrus;https://github.com/apache/spark/pull/994
https://github.com/apache/spark/pull/989;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExternalAppendOnlyMap doesn't always find matching keys,SPARK-2043,12718734,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,matei,matei,matei,05/Jun/14 22:01,06/Jun/14 06:03,14/Jul/23 06:25,06/Jun/14 06:03,0.9.0,0.9.1,1.0.0,,,,,,0.9.2,1.0.1,1.1.0,,,Spark Core,,,,,0,,,,,,"The current implementation reads one key with the next hash code as it finishes reading the keys with the current hash code, which may cause it to miss some matches of the next key. This can cause operations like join to give the wrong result when reduce tasks spill to disk and there are hash collisions, as values won't be matched together.",,glenn.strycker@gmail.com,lukas.nalezenec,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396933,,,Fri Jun 06 01:12:18 UTC 2014,,,,,,,,,,"0|i1wc53:",397051,,,,,,,,,,,,,0.9.2,1.0.1,1.1.0,,,,,,,,"06/Jun/14 01:12;matei;https://github.com/apache/spark/pull/986;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception when querying when tableName == columnName,SPARK-2041,12718721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,05/Jun/14 20:53,06/Jun/14 06:35,14/Jul/23 06:25,06/Jun/14 06:35,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{code}
[info]   java.util.NoSuchElementException: next on empty iterator
[info]   at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
[info]   at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
[info]   at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:64)
[info]   at scala.collection.IterableLike$class.head(IterableLike.scala:91)
[info]   at scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:108)
[info]   at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:120)
[info]   at scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:108)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:68)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:65)
[info]   at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
[info]   at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
[info]   at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:65)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$3$$anonfun$applyOrElse$2.applyOrElse(Analyzer.scala:100)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$3$$anonfun$applyOrElse$2.applyOrElse(Analyzer.scala:97)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
[info]   at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:51)
[info]   at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1$$anonfun$apply$1.apply(QueryPlan.scala:65)
[info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
{code}",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396920,,,Fri Jun 06 06:35:32 UTC 2014,,,,,,,,,,"0|i1wc2f:",397039,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/14 06:35;marmbrus;https://github.com/apache/spark/pull/985;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn client mode doesn't support spark.yarn.max.executor.failures,SPARK-2037,12718692,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,tgraves,tgraves,05/Jun/14 18:08,24/Jul/14 19:46,14/Jul/23 06:25,24/Jul/14 19:46,1.0.0,,,,,,,,1.1.0,,,,,YARN,,,,,0,,,,,,yarn client mode doesn't support the config spark.yarn.max.executor.failures.  We should investigate if we need it.,,devaraj,diederik,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396891,,,Wed Jul 23 02:02:43 UTC 2014,,,,,,,,,,"0|i1wbvz:",397010,,,,,,,,,,,,,1.1.0,,,,,,,,,,"23/Jul/14 02:02;tgraves;https://github.com/apache/spark/pull/1180;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CaseConversionExpression should check if the evaluated value is null.,SPARK-2036,12718663,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,05/Jun/14 16:26,06/Jun/14 06:34,14/Jul/23 06:25,06/Jun/14 06:07,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,{{CaseConversionExpression}} should check if the evaluated value is {{null}}.,,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396862,,,Thu Jun 05 16:32:57 UTC 2014,,,,,,,,,,"0|i1wbpj:",396981,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/14 16:32;ueshin;PRed: https://github.com/apache/spark/pull/982;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaInputDStream doesn't close resources and may prevent JVM shutdown,SPARK-2034,12718584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,05/Jun/14 10:06,15/Jan/15 09:08,14/Jul/23 06:25,22/Jun/14 08:12,1.0.0,,,,,,,,1.0.1,1.1.0,,,,DStreams,,,,,0,,,,,,"Tobias noted today on the mailing list:

{quote}
I am trying to use Spark Streaming with Kafka, which works like a
charm -- except for shutdown. When I run my program with ""sbt
run-main"", sbt will never exit, because there are two non-daemon
threads left that don't die.

I created a minimal example at
<https://gist.github.com/tgpfeiffer/b1e765064e983449c6b6#file-kafkadoesntshutdown-scala>.
It starts a StreamingContext and does nothing more than connecting to
a Kafka server and printing what it receives. Using the `future { ...
}` construct, I shut down the StreamingContext after some seconds and
then print the difference between the threads at start time and at end
time. The output can be found at
<https://gist.github.com/tgpfeiffer/b1e765064e983449c6b6#file-output1>.
There are a number of threads remaining that will prevent sbt from
exiting.

When I replace `KafkaUtils.createStream(...)` with a call that does
exactly the same, except that it calls `consumerConnector.shutdown()`
in `KafkaReceiver.onStop()` (which it should, IMO), the output is as
shown at <https://gist.github.com/tgpfeiffer/b1e765064e983449c6b6#file-output2>.

Does anyone have *any* idea what is going on here and why the program
doesn't shut down properly? The behavior is the same with both kafka
0.8.0 and 0.8.1.1, by the way.
{quote}

Something similar was noted last year:

http://mail-archives.apache.org/mod_mbox/spark-dev/201309.mbox/%3C1380220041.2428.YahooMailNeo@web160804.mail.bf1.yahoo.com%3E 

KafkaInputDStream doesn't close ConsumerConnector in onStop(), and does not close the Executor it creates. The latter leaves non-daemon threads and can prevent the JVM from shutting down even if streaming is closed properly.",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396783,,,Sun Jun 22 08:12:36 UTC 2014,,,,,,,,,,"0|i1wb7z:",396902,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/14 08:12;pwendell;Issue resolved by pull request 980
[https://github.com/apache/spark/pull/980];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump SparkBuild.scala version number of branch-1.0 to 1.0.1-SNAPSHOT.,SPARK-2030,12718539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,05/Jun/14 05:39,05/Jun/14 18:29,14/Jul/23 06:25,05/Jun/14 18:28,,,,,,,,,1.0.1,,,,,,,,,,0,,,,,,Bump SparkBuild.scala version number of branch-1.0 to 1.0.1-SNAPSHOT.,,pwendell,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396738,,,Thu Jun 05 18:28:58 UTC 2014,,,,,,,,,,"0|i1waxz:",396857,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/14 05:43;ueshin;PRed: https://github.com/apache/spark/pull/975;;;","05/Jun/14 18:28;pwendell;Issue resolved by pull request 975
[https://github.com/apache/spark/pull/975];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump pom.xml version number of master branch to 1.1.0-SNAPSHOT.,SPARK-2029,12718538,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,05/Jun/14 05:25,05/Jun/14 18:28,14/Jul/23 06:25,05/Jun/14 18:28,,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,Bump pom.xml version number of master branch to 1.1.0-SNAPSHOT.,,pwendell,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396737,,,Thu Jun 05 18:28:00 UTC 2014,,,,,,,,,,"0|i1waxr:",396856,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/14 05:34;ueshin;PRed: https://github.com/apache/spark/pull/974;;;","05/Jun/14 18:28;pwendell;Issue resolved by pull request 974
[https://github.com/apache/spark/pull/974];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EdgeRDD persists after pregel iteration,SPARK-2025,12718512,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,tweninge,tweninge,04/Jun/14 23:46,06/Jun/14 02:01,14/Jul/23 06:25,06/Jun/14 02:01,1.0.0,1.0.1,,,,,,,1.0.1,1.1.0,,,,GraphX,,,,,0,Pregel,,,,,"Symptoms: During execution of a pregel script/function a copy of an intermediate EdgeRDD object persists after each iteration as shown by the Spark WebUI - storage.

This is like a memory leak that affects in the Pregel function.

For example, after the first iteration I will have an EdgeRDD in addition to the EdgeRDD and VertexRDD that are kept for the next iteration. After 15 iterations I will have 15 EdgeRDDs in addition to the current/correct state represented by a single set of 1 EdgeRDD and 1 VertexRDD.

At the end of a Pregel loop the old EdgeRDD and VertexRDD are unpersisted, but there seems to be another EdgeRDD that is created somewhere that does not get unpersisted.

i _think_ this is from the replicateVertex function, but I cannot be sure.

Update - Dave Ankur says, in comments on SPARK-2011 - 
{quote}
... is a bug introduced by https://github.com/apache/spark/pull/497.
It occurs because unpersistVertices used to unpersist both the vertices and the replicated vertices, but after unifying replicated vertices with edges, there was no way to unpersist only one of them. I think the solution is just to unpersist both the vertices and the edges in Pregel.{quote}",RHEL6 on local and on spark cluster,ankurd,tweninge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396711,,,Thu Jun 05 01:36:43 UTC 2014,,,,,,,,,,"0|i1warz:",396830,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/14 01:03;tweninge;adding 

{{prevG.edges.unpersist(blocking=false)}}

after line 152 in Pregel.scala fixes the issue;;;","05/Jun/14 01:04;tweninge;I'll leave it to you to make the bug fix. You seem to be a pro.;;;","05/Jun/14 01:36;ankurd;Proposed fix: https://github.com/apache/spark/pull/972;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 1.0.0 is failing if mesos.coarse set to true,SPARK-2022,12718466,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,chengt,mwiewiorka,mwiewiorka,04/Jun/14 20:54,18/Sep/14 02:44,14/Jul/23 06:25,18/Sep/14 02:44,1.0.0,,,,,,,,,,,,,Mesos,,,,,1,,,,,,"more stderr
-------------------

WARNING: Logging before InitGoogleLogging() is written to STDERR
I0603 16:07:53.721132 61192 exec.cpp:131] Version: 0.18.2
I0603 16:07:53.725230 61200 exec.cpp:205] Executor registered on slave 201405220917-134217738-5050-27119-0
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""sparkseq003.cloudapp.net""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:492)
        at java.lang.Integer.parseInt(Integer.java:527)
        at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
        at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:135)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)

more stdout
-----------------------
Registered executor on sparkseq003.cloudapp.net
Starting task 5
Forked command at 61202
sh -c '""/home/mesos/spark-1.0.0/bin/spark-class"" org.apache.spark.executor.CoarseGrainedExecutorBackend -Dspark.mesos.coarse=true akka.tcp://spark@sparkseq001.cloudapp.net:40312/user/CoarseG
rainedScheduler 201405220917-134217738-5050-27119-0 sparkseq003.cloudapp.net 4'
Command exited with status 1 (pid: 61202)",,apachespark,mwiewiorka,nchammas,tnachen,tstclair,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2020,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396665,,,Tue Sep 16 16:19:05 UTC 2014,,,,,,,,,,"0|i1wahz:",396784,,,,,,,,,,,,,1.1.0,,,,,,,,,,"17/Jun/14 20:06;srainville;I'm seeing the same behavior when trying to set spark.executor.extraLibraryPath:

in conf/spark-defaults.conf:

{noformat}
spark.executor.extraLibraryPath /usr/lib/hadoop/lib/native
{noformat}

the error message in stderr:

{noformat}
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0617 16:00:55.592289 27091 fetcher.cpp:73] Fetching URI 'hdfs://ca1-dcc1-0071:9200/user/sebastien/spark-1.0.0-bin-cdh4-sebr.tgz'
I0617 16:00:55.592428 27091 fetcher.cpp:99] Downloading resource from 'hdfs://ca1-dcc1-0071:9200/user/sebastien/spark-1.0.0-bin-cdh4-sebr.tgz' to '/u05/app/mesos/work/slaves/201311011608-1369465866-5050-9189-86/frameworks/20140416-011500-1369465866-5050-26096-0449/executors/9/runs/ba87d7b6-56c1-4892-9ed8-18fa8f8364d2/spark-1.0.0-bin-cdh4-sebr.tgz'
I0617 16:01:05.170714 27091 fetcher.cpp:61] Extracted resource '/u05/app/mesos/work/slaves/201311011608-1369465866-5050-9189-86/frameworks/20140416-011500-1369465866-5050-26096-0449/executors/9/runs/ba87d7b6-56c1-4892-9ed8-18fa8f8364d2/spark-1.0.0-bin-cdh4-sebr.tgz' into '/u05/app/mesos/work/slaves/201311011608-1369465866-5050-9189-86/frameworks/20140416-011500-1369465866-5050-26096-0449/executors/9/runs/ba87d7b6-56c1-4892-9ed8-18fa8f8364d2'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0617 16:01:06.105363 27166 exec.cpp:131] Version: 0.18.0
I0617 16:01:06.112191 27175 exec.cpp:205] Executor registered on slave 201311011608-1369465866-5050-9189-86
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""ca1-dcc1-0106.lab.mtl""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:135)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
{noformat}

here is the command in stdout:

{noformat}
Registered executor on ca1-dcc1-0106.lab.mtl
Starting task 9
Forked command at 27178
sh -c 'cd spark-1*; ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend -Djava.library.path=/usr/lib/hadoop/lib/native akka.tcp://spark@ca1-dcc1-0071.lab.mtl:32789/user/CoarseGrainedScheduler 201311011608-1369465866-5050-9189-86 ca1-dcc1-0106.lab.mtl 1'
Command exited with status 1 (pid: 27178)
{noformat}

In fact, this behavior occurs whenever a jvmarg is set, so setting spark.executor.extraJavaOptions triggers it too.

The problem is that CoarseMesosSchedulerBackend is passing the jvmargs to CoarseGrainedExecutorBackend instead of the jvm itself:

{code}
val uri = conf.get(""spark.executor.uri"", null)
    if (uri == null) {
      val runScript = new File(sparkHome, ""./bin/spark-class"").getCanonicalPath
      command.setValue(
        ""\""%s\"" org.apache.spark.executor.CoarseGrainedExecutorBackend %s %s %s %s %d"".format(
          runScript, extraOpts, driverUrl, offer.getSlaveId.getValue, offer.getHostname, numCores))
    } else {
      // Grab everything to the first '.'. We'll use that and '*' to
      // glob the directory ""correctly"".
      val basename = uri.split('/').last.split('.').head
      command.setValue(
        (""cd %s*; "" +
          ""./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend %s %s %s %s %d"")
          .format(basename, extraOpts, driverUrl, offer.getSlaveId.getValue,
            offer.getHostname, numCores))
      command.addUris(CommandInfo.URI.newBuilder().setValue(uri))
    }
{code}

as a reference, here's the main method in CoarseGrainedExecutorBackend:

{code}
def main(args: Array[String]) {
    args.length match {
      case x if x < 4 =>
        System.err.println(
          // Worker url is used in spark standalone mode to enforce fate-sharing with worker
          ""Usage: CoarseGrainedExecutorBackend <driverUrl> <executorId> <hostname> "" +
          ""<cores> [<workerUrl>]"")
        System.exit(1)
      case 4 =>
        run(args(0), args(1), args(2), args(3).toInt, None)
      case x if x > 4 =>
        run(args(0), args(1), args(2), args(3).toInt, Some(args(4)))
    }
  }
{code}

;;;","21/Jul/14 23:29;tnachen;this seems duplicate of SPARK-2020;;;","28/Jul/14 23:16;tnachen;Github PR: https://github.com/apache/spark/pull/1622;;;","28/Jul/14 23:16;apachespark;User 'tnachen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1622;;;","01/Sep/14 17:29;tnachen;This should be resolved now, [~pwendell@gmail.com] please help close this.;;;","16/Sep/14 15:23;tstclair;*this appears to be done, could we please close the JIRA. ;;;","16/Sep/14 16:19;nchammas;Pinging [~pwendell] about closing this issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Big-Endian (IBM Power7)  Spark Serialization issue,SPARK-2018,12718348,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tellison,gaoyanjie55,gaoyanjie55,04/Jun/14 09:21,13/May/15 01:18,14/Jul/23 06:25,13/May/15 01:18,1.0.0,,,,,,,,1.3.2,1.4.0,,,,Deploy,,,,,0,,,,,,"We have an application run on Spark on Power7 System .
But we meet an important issue about serialization.
The example HdfsWordCount can meet the problem.
./bin/run-example      org.apache.spark.examples.streaming.HdfsWordCount localdir
We used Power7 (Big-Endian arch) and Redhat  6.4.
Big-Endian  is the main cause since the example ran successfully in another Power-based Little Endian setup.

here is the exception stack and log:

Spark Executor Command: ""/opt/ibm/java-ppc64-70//bin/java"" ""-cp"" ""/home/test1/spark-1.0.0-bin-hadoop2/lib::/home/test1/src/spark-1.0.0-bin-hadoop2/conf:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop2.2.0.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-rdbms-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-api-jdo-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-core-3.2.2.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/"" ""-XX:MaxPermSize=128m""  ""-Xdebug"" ""-Xrunjdwp:transport=dt_socket,address=99999,server=y,suspend=n"" ""-Xms512M"" ""-Xmx512M"" ""org.apache.spark.executor.CoarseGrainedExecutorBackend"" ""akka.tcp://spark@9.186.105.141:60253/user/CoarseGrainedScheduler"" ""2"" ""p7hvs7br16"" ""4"" ""akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker"" ""app-20140604023054-0000""
========================================

14/06/04 02:31:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/06/04 02:31:21 INFO spark.SecurityManager: Changing view acls to: test1,yifeng
14/06/04 02:31:21 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(test1, yifeng)
14/06/04 02:31:22 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/06/04 02:31:22 INFO Remoting: Starting remoting
14/06/04 02:31:22 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@p7hvs7br16:39658]
14/06/04 02:31:22 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkExecutor@p7hvs7br16:39658]
14/06/04 02:31:22 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://spark@9.186.105.141:60253/user/CoarseGrainedScheduler
14/06/04 02:31:22 INFO worker.WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker
14/06/04 02:31:23 INFO worker.WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker
14/06/04 02:31:24 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
14/06/04 02:31:24 INFO spark.SecurityManager: Changing view acls to: test1,yifeng
14/06/04 02:31:24 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(test1, yifeng)
14/06/04 02:31:24 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/06/04 02:31:24 INFO Remoting: Starting remoting
14/06/04 02:31:24 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@p7hvs7br16:58990]
14/06/04 02:31:24 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@p7hvs7br16:58990]
14/06/04 02:31:24 INFO spark.SparkEnv: Connecting to MapOutputTracker: akka.tcp://spark@9.186.105.141:60253/user/MapOutputTracker
14/06/04 02:31:25 INFO spark.SparkEnv: Connecting to BlockManagerMaster: akka.tcp://spark@9.186.105.141:60253/user/BlockManagerMaster
14/06/04 02:31:25 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20140604023125-3f61
14/06/04 02:31:25 INFO storage.MemoryStore: MemoryStore started with capacity 307.2 MB.
14/06/04 02:31:25 INFO network.ConnectionManager: Bound socket to port 39041 with id = ConnectionManagerId(p7hvs7br16,39041)
14/06/04 02:31:25 INFO storage.BlockManagerMaster: Trying to register BlockManager
14/06/04 02:31:25 INFO storage.BlockManagerMaster: Registered BlockManager
14/06/04 02:31:25 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-7bce4e43-2833-4666-93af-bd97c327497b
14/06/04 02:31:25 INFO spark.HttpServer: Starting HTTP Server
14/06/04 02:31:25 INFO server.Server: jetty-8.y.z-SNAPSHOT
14/06/04 02:31:26 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:39958
14/06/04 02:31:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
14/06/04 02:31:26 INFO executor.Executor: Running task ID 2
14/06/04 02:31:26 ERROR executor.Executor: Exception in task ID 2
java.io.InvalidClassException: scala.reflect.ClassTag$$anon$1; local class incompatible: stream classdesc serialVersionUID = -8102093212602380348, local class serialVersionUID = -4937928798201944954
        at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:678)
        at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1678)
        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1573)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1827)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:607)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)

at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:607)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:607)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:607)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:607)

   at java.lang.reflect.Method.invoke(Method.java:607)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
        at org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61)
        at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)
        at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1893)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1852)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
        at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:781)
14/06/04 02:31:26 ERROR executor.CoarseGrainedExecutorBackend: Driver Disassociated [akka.tcp://sparkExecutor@p7hvs7br16:39658] -> [akka.tcp://spark@9.186.105.141:60253] disassociated! Shutting down.




","hardware : IBM Power7
OS:Linux version 2.6.32-358.el6.ppc64 (mockbuild@ppc-017.build.eng.bos.redhat.com) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-3) (GCC) ) #1 SMP Tue Jan 29 11:43:27 EST 2013


JDK: Java(TM) SE Runtime Environment (build pxp6470sr5-20130619_01(SR5))
IBM J9 VM (build 2.6, JRE 1.7.0 Linux ppc64-64 Compressed References 20130617_152572 (JIT enabled, AOT enabled)


Hadoop:Hadoop-0.2.3-CDH5.0
Spark:Spark-1.0.0 or Spark-0.9.1

spark-env.sh:
export JAVA_HOME=/opt/ibm/java-ppc64-70/
export SPARK_MASTER_IP=9.114.34.69
export SPARK_WORKER_MEMORY=10000m
export SPARK_CLASSPATH=/home/test1/spark-1.0.0-bin-hadoop2/lib
export  STANDALONE_SPARK_MASTER_HOST=9.114.34.69
#export SPARK_JAVA_OPTS=' -Xdebug -Xrunjdwp:transport=dt_socket,address=99999,server=y,suspend=n '",apachespark,dougb,gaoyanjie55,gireesh,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3603,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396547,,,Tue May 12 19:49:25 UTC 2015,,,,,,,,,,"0|i1w9s7:",396668,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/14 09:38;srowen;The meaning of the error is that Java thinks two serializable classes are not mutually compatible. This is because two different serialVersioUIDs get computed for two copies of what may be the same class. If I understand you correctly, you are communicating between different JVM versions, or reading one's output from the other? I don't think it's guaranteed that the auto-generated serialVersionUID will be the same. If so, it's nothing to do with big-endian-ness per se. Does it happen entirely within the same machine / JVM? ;;;","05/Jun/14 01:49;gaoyanjie55;Thanks for your quick reply!
I believe they  use the same jvm

Do you think this may have another reason?

How can I  debug it  to find the reason ?

Best regards !
Yanjie Gao
here is the ps -aux |grep java log

 test1      349  0.5  3.7 2945280 195456 pts/7  Sl   02:30   0:22 /opt/ibm/java-ppc64-70//bin/java -cp /home/test1/spark-1.0.0-bin-hadoop2/lib::/home/test1/src/spark-1.0.0-bin-hadoop2/conf:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop2.2.0.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-rdbms-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-api-jdo-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-core-3.2.2.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/ -XX:MaxPermSize=128m -Dspark.akka.logLifecycleEvents=true -Xms512m -Xmx512m org.apache.spark.deploy.master.Master --ip 9.114.34.69 --port 7077 --webui-port 8080
test1      492  0.4  3.7 2946496 194432 ?      Sl   02:30   0:19 /opt/ibm/java-ppc64-70//bin/java -cp /home/test1/spark-1.0.0-bin-hadoop2/lib::/home/test1/src/spark-1.0.0-bin-hadoop2/conf:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop2.2.0.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-rdbms-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-api-jdo-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-core-3.2.2.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/ -XX:MaxPermSize=128m -Dspark.akka.logLifecycleEvents=true -Xms512m -Xmx512m org.apache.spark.deploy.worker.Worker spark://9.114.34.69:7077
test1     3160  0.0  0.0 104832  2816 pts/10   S+   03:40   0:00 grep java
test1    13163  0.1  2.7 1631232 144256 ?      Sl   Jun02   2:00 /opt/ibm/java-ppc64-70/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop-test1-namenode-p7hvs7br16.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode
test1    13328  0.0  2.1 1636160 113152 ?      Sl   Jun02   1:39 /opt/ibm/java-ppc64-70/bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop-test1-datanode-p7hvs7br16.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode
test1    13474  0.0  2.1 1624960 113408 ?      Sl   Jun02   0:35 /opt/ibm/java-ppc64-70/bin/java -Dproc_secondarynamenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop-test1-secondarynamenode-p7hvs7br16.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
test1    13702  0.3  2.4 1666112 124544 ?      Sl   Jun02   6:47 /opt/ibm/java-ppc64-70/bin/java -Dproc_resourcemanager -Xmx1000m -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dyarn.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=yarn-test1-resourcemanager-p7hvs7br16.log -Dyarn.log.file=yarn-test1-resourcemanager-p7hvs7br16.log -Dyarn.home.dir= -Dyarn.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dyarn.policy.file=hadoop-policy.xml -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dyarn.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=yarn-test1-resourcemanager-p7hvs7br16.log -Dyarn.log.file=yarn-test1-resourcemanager-p7hvs7br16.log -Dyarn.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -classpath /home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/common/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/common/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/*.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/*.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/*.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/*.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop//rm-config/log4j.properties org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
test1    13800  0.1  1.9 1633664 98560 ?       Sl   Jun02   3:03 /opt/ibm/java-ppc64-70/bin/java -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dyarn.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=yarn-test1-nodemanager-p7hvs7br16.log -Dyarn.log.file=yarn-test1-nodemanager-p7hvs7br16.log -Dyarn.home.dir= -Dyarn.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dyarn.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=yarn-test1-nodemanager-p7hvs7br16.log -Dyarn.log.file=yarn-test1-nodemanager-p7hvs7br16.log -Dyarn.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -classpath /home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/common/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/common/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/*.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/*.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/lib/*:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop//nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager
songdm   29650  0.4  5.1 3032704 264704 ?      Sl   May27  50:13 /opt/ibm/java-ppc64-70//bin/java -cp ::/home/songdm/spark/conf:/home/songdm/spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop2.2.0.jar:/home/songdm/hadoop-2.2.0/etc/hadoop/:/home/songdm/hadoop-2.2.0/etc/hadoop/ -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --master local[*] --class org.apache.spark.examples.streaming.HdfsWordCount /home/songdm/spark/examples/target/scala-2.10/spark-examples-1.0.0-SNAPSHOT-hadoop2.2.0.jar ./kk;;;","11/Jun/14 13:31;gireesh;I face this same issue for a different class:

org.apache.spark.SerializableWritable; local class incompatible: stream classdesc serialVersionUID = 6301214776158303468, local class serialVersionUID = -7785455416944904980

All the entities are in the same system, using same JVM version.

Bad  value in hex: 57726974 225BB4EC
Good value in hex: 93F4818C 225BB4EC

Please note that the lower word (in big endian) is same while the higher word is different.

With little debugging, I extracted the byte buffer from which the de-serialization was being performed, and it looks like below. Apparently, offsets 48,49,50,51 contain the right value for the lower word (in big endian) of the SUID, while offsets 41,42,43,44 seem to be containing portion of the class name ('Writ', which is part of 'SerializableWritable'). Looks like data corruption in the serializer?

Is there any configurable option through which I can disable custom serializations, and fall back (compltely) to Java serialization?

In a spark configuration which runs HdfsWordCount example, which process is responsible for serializing this object? With my little understanding of the whole framework, I can see a master, a worker, an execute runner (spawned by worker) and the client who issues the test case.

The client reports the execption, but I believe it just prints whatever the execute runner reports to it. ExecutorRunner actually invokes a org.apache.spark.executor.CoarseGrainedExecutorBackend to  perform its task. This process never serialized any objects. is the serialization performed by the master? Or is the example exercising a pre-serialized disc file? How do we know that?

Is there any debug option which reports messages on step-by-step serialization / de-serialization process? I have all the tools and debug capabilities at the JVM / native level, but lack of understanding of the spark topology makes it hard to move further.

Finally, if I modify a scala file, how do I effect the changes into the class file and the parent jar file? when I performed sbt assembly or sbt package it indeed compiled the scala file, but the .class file ( org/apache/spark/SerializableWritable.class) or the .jar file (spark-core_2.10-0.9.1.jar) do not seem to have updated. Is there any other way of doing it?


    [0] = 172, 0xac
    [1] = 116, 0x74, 't'
    [2] =  31, 0x1f
    [3] = 172, 0xac
    [4] = 237, 0xed
    [5] =   0, 0x00
    [6] =   5, 0x05
    [7] = 115, 0x73, 's'
    [8] = 114, 0x72, 'r'
    [9] =   0, 0x00
    [10] =  37, 0x25, '%'  // indicates next 37 bytes are for the class name? then probably multiple over-writes happened: i) the classname over the SUID, ii) something else on the classname (bytes  45 through 48)

    [11] = 111, 0x6f, 'o'
    [12] = 114, 0x72, 'r'
    [13] = 103, 0x67, 'g'
    [14] =  46, 0x2e, '.'
    [15] =  97, 0x61, 'a'
    [16] = 112, 0x70, 'p'
    [17] =  97, 0x61, 'a'
    [18] =  99, 0x63, 'c'
    [19] = 104, 0x68, 'h'
    [20] = 101, 0x65, 'e'
    [21] =  46, 0x2e, '.'
    [22] = 115, 0x73, 's'
    [23] = 112, 0x70, 'p'
    [24] =  97, 0x61, 'a'
    [25] = 114, 0x72, 'r'
    [26] = 107, 0x6b, 'k'
    [27] =  46, 0x2e, '.'
    [28] =  83, 0x53, 'S'
    [29] = 101, 0x65, 'e'
    [30] = 114, 0x72, 'r'
    [31] = 105, 0x69, 'i'
    [32] =  97, 0x61, 'a'
    [33] = 108, 0x6c, 'l'
    [34] = 105, 0x69, 'i'
    [35] =   8, 0x08
    [36] = 122, 0x7a, 'z'
    [37] =  97, 0x61, 'a'
    [38] =  98, 0x62, 'b'
    [39] = 108, 0x6c, 'l'
    [40] = 101, 0x65, 'e'

    [41] =  87, 0x57, 'W'
    [42] = 114, 0x72, 'r'
    [43] = 105, 0x69, 'i'
    [44] = 116, 0x74, 't'

    [45] = 192, 0xc0
    [46] =   7, 0x07
    [47] =  15, 0x0f

    [48] =  34, 0x22, '""'
    [49] =  91, 0x5b, '['
    [50] = 180, 0xb4
    [51] = 236, 0xec

    [52] =   3, 0x03
    [53] =   0, 0x00
    [54] =   0, 0x00
    [55] = 120, 0x78, 'x'
    [56] = 112, 0x70, 'p'
    [57] = 122, 0x7a, 'z'
    [58] =   0, 0x00
    [59] =   0, 0x00
    [60] =   4, 0x04
    [61] =   0, 0x00
    [62] =   0, 0x00
    [63] =  32, 0x20, ' '
    [64] = 224, 0xe0
...;;;","11/Jun/14 14:25;mridulm80;Ah ! This is an interesting bug.
Default spark uses java serialization ... so should not be an issue : but yet you are facing it ! (I am assuming you have not customized serialization).
Is it possible for you to dump data written and read at both ends ? The env vars and jvm details ?
Actually, spark does not do anything fancy for default serialization : so a simple example code without spark in picture could also be tried (write to file on master node, and read from the file in slave node - and see if it works);;;","11/Jun/14 14:37;gireesh;Mridul,

Thanks for responding.

1. As I explained earlier, I am just exercising the HdfsWordCount example, exactly following the documentation. So there is no specific JVM arguments in my case either.
2. Dumping data from either side: this is the problem - I don't know what is being written. I am able to capture only what is read (de-serialization end). My all efforts are to capture who writes this object.
3. so a simple example code without spark in picture: Can you please elaborate the steps involved? I am not familiar even with the basic operations on the hadoop platform. The current process I follow is as follows:
spark# sbin/start-all.sh
spark# ./bin/run-example org.apache.spark.streaming.examples.HdfsWordCount spark://<IP>:7077 localdir

Thanks once again!
;;;","12/Jun/14 00:28;gaoyanjie55;Thanks a lot.
Now  I have some new detection.


(1) Compare
        server                Spark exec mode          pass or not                                                                   jdk                                   
x86(Little Endian)    Local+cluster                 pass                                                                               x86
p8(Little Endian)     Local+cluster                 pass                                                                               IBM(little endian)
P7(Big Endian)      Local   mode                   pass(I change some jar classpath then can't pass)        IBM(Big  endian)
P7 (Big Endian)    Cluster mode                   not                                                                                  IBM(Big endian)


(2) The Exception priciple
2.1 Main Error : Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on host arlab105.austin.ibm.com: java.io.InvalidClassException: org.apache.spark.SerializableWritable; local class incompatible: stream classdesc serialVersionUID = 6301214776158303468, local class serialVersionUID = -7785455416944904980

(other  may has the same reason)
2.2 Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0:0 failed 1 times, most recent failure: Exception failure in TID 1 on host localhost: java.io.InvalidClassException: scala.Tuple2; invalid descriptor for field _1


Now we analysis 2.1 Bug .
refer:
 serialVersionUID has two generate method
1   default 1Lprivate static final long serialVersionUID = 1L
2    Generated by  hash.       Class name ,interface name ,method ,attribute can affect the result.
Our error is not 1L .So it generated by method 2.

UID is used  when the process deserialize the byte array .the   process read the local class file ,and find the class's UID.If it is diff with the array .Then throw the Exception. 

Let's  see the work flow of  Spark Serilization

Local mode 
once serialize 
object -----serialize(thread1 or thread2)---->array------deserialize(thread2 or process2)---->object

Cluster mode
twice serialize
object ----serialize(thread1 or thread2)---->array---Actor send message serialize --->message---->Actor receive and deserialize it ----->array  ------deserialize(thread2 or process2)---->object 




summary:
let't compare (1) 's four situation.
I think the reason is that IBM jdk and (scala lib and akka lib)  may have some intersection of some class.  But they compile in diff platform use diff javac .They may generate diff UID. 
In run time .jvm may load the same class from diff .class file. 


(3)Method to fix it.

I think
The reason is the same class load diff class file.
There are two method  .May be there are other better method.
4.1 Let the two file has the same version UID:Compile  scala lib  and akka lib  in P7 platform
4.2 Let the two loader load the same Jar. Use some method like extend class loader or  OSGI .We force the jvm to load the same class file.(The difficult thing is that  classes is in jar and class num is too large .)

Best Regards
Yanjie Gao

;;;","16/Jun/14 09:23;gireesh;I was able to identify the root cause. Please see https://github.com/ning/compress/issues/37 for details.;;;","12/May/15 08:44;apachespark;User 'tellison' has created a pull request for this issue:
https://github.com/apache/spark/pull/6077;;;","12/May/15 19:49;srowen;Issue resolved by pull request 6077
[https://github.com/apache/spark/pull/6077];;;",,,,,,,,,,,,,,,,,,,,,,,,
Key not found exception when slow receiver starts,SPARK-2009,12718273,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vchekan,vchekan,vchekan,03/Jun/14 23:33,12/Apr/16 07:04,14/Jul/23 06:25,18/Jun/14 05:04,1.0.0,,,,,,,,1.0.1,1.1.0,,,,DStreams,,,,,0,,,,,,"I got ""java.util.NoSuchElementException: key not found: 1401756085000 ms"" exception when using kafka stream and 1 sec batchPeriod.

Investigation showed that the reason is that ReceiverLauncher.startReceivers is asynchronous (started in a thread).
https://github.com/vchekan/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala#L206

In case of slow starting receiver, such as Kafka, it easily takes more than 2sec to start. In result, no single ""compute"" will be called on ReceiverInputDStream before first batch job is executed and receivedBlockInfo remains empty (obviously). Batch job will cause ReceiverInputDStream.getReceivedBlockInfo call and ""key not found"" exception.

The patch makes getReceivedBlockInfo more robust by tolerating missing values.",,apachespark,bli,kdang,pwendell,tdas,vchekan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396475,,,Tue Apr 12 07:04:04 UTC 2016,,,,,,,,,,"0|i1w9c7:",396596,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"06/Jun/14 04:38;bli;Hi Vadim, I have exactly the same problem. Could you share the patch? Thanks. ;;;","06/Jun/14 06:20;kdang;I have exactly the same problem.I'm also seeing that data is being read from Kafka and offsets are being incremented even though we are failing to process the data. Another concern is the Spark application monitoring UI is not showing any such exception.;;;","06/Jun/14 17:07;vchekan;Folks,

Patch is in progress of review here:
https://github.com/apache/spark/pull/961#issuecomment-45125185;;;","18/Jun/14 05:04;pwendell;Issue resolved by pull request 961
[https://github.com/apache/spark/pull/961];;;","12/Apr/16 07:04;apachespark;User 'vchekan' has created a pull request for this issue:
https://github.com/apache/spark/pull/961;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on YARN picks up hadoop log4j.properties even if SPARK_LOG4J_CONF is set,SPARK-2007,12718252,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ibuenros,ibuenros,03/Jun/14 21:12,25/Nov/14 12:44,14/Jul/23 06:25,25/Nov/14 12:44,1.0.0,,,,,,,,,,,,,YARN,,,,,0,,,,,,"When running Spark on YARN, with SPARK_LOG4J_CONF environment variable set, containers might still pick up a different log4j configuration.

spark-yarn will create a local resource in YARN called log4j.properties, which is included in the classpath. However, there might be other log4j.properties files in the classpath which might be chosen instead.",,amalakar,ibuenros,qwertymaniac,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396454,,,Tue Nov 25 12:44:49 UTC 2014,,,,,,,,,,"0|i1w97r:",396576,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/14 21:14;ibuenros;This can be solved by changing the name of the local resource pointing to the log4j configuration (for example to spark.log4j.properties). In a local version of Spark I have implemented this fix.

Other options would be to name the local resource with the same name as the original file, or make this name configurable.;;;","10/Jun/14 20:29;vanzin;I believe https://github.com/apache/spark/pull/560 fixes this.;;;","25/Nov/14 12:44;srowen;Looks like this is resolved per the method Marcelo alludes to in the PR. That seems to be the correct current way to handle log4j.properties on YARN.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Sql example throws ClassCastException: Long -> Int,SPARK-2006,12718248,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,ktham,ktham,03/Jun/14 20:31,03/Jun/14 21:16,14/Jul/23 06:25,03/Jun/14 21:15,1.0.0,,,,,,,,,,,,,Examples,,,,,0,,,,,,"getInt() is being called on a spark sql COUNT query whose output datatype is of type Long. Casting a Long to Int is an illegal operation and the example should use getLong() instead.
",,ktham,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396450,,,Tue Jun 03 21:15:35 UTC 2014,,,,,,,,,,"0|i1w96v:",396572,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/14 20:31;ktham;https://github.com/apache/spark/pull/954;;;","03/Jun/14 21:15;ktham;Actually there was an earlier pull request that just got merged in, https://github.com/apache/spark/pull/949. so no longer an issue.;;;","03/Jun/14 21:15;ktham;Fixed in https://github.com/apache/spark/pull/949;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove docs/spark-debugger.md from master since it is obsolete,SPARK-2001,12718210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hsaputra,hsaputra,hsaputra,03/Jun/14 18:17,03/Jun/14 20:04,14/Jul/23 06:25,03/Jun/14 20:04,,,,,,,,,,,,,,Documentation,,,,,0,,,,,,"Per discussion in dev list:
""
Seemed like the spark-debugger.md is no longer accurate (see
http://spark.apache.org/docs/latest/spark-debugger.html) and since it
was originally written Spark has evolved that makes the doc obsolete.

There are already work pending for new replay debugging (I could not
find the PR links for it) so I

With version control we could always reinstate the old doc if needed,
but as of today the doc is no longer reflect the current state of
Spark's RDD.
""",,hsaputra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396412,,,Tue Jun 03 18:30:12 UTC 2014,,,,,,,,,,"0|i1w8yf:",396534,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"03/Jun/14 18:17;hsaputra;Working on this;;;","03/Jun/14 18:30;hsaputra;See https://github.com/apache/spark/pull/953;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UI : StorageLevel in storage tab and RDD Storage Info never changes ,SPARK-1999,12718121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,crazyjvm,crazyjvm,crazyjvm,03/Jun/14 11:00,16/Jun/14 06:26,14/Jul/23 06:25,16/Jun/14 06:26,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Web UI,,,,,0,,,,,,StorageLevel in 'storage tab' and 'RDD Storage Info' never changes even if you call rdd.unpersist() and then you give the rdd another different storage level.,,crazyjvm,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396323,,,Mon Jun 16 06:26:00 UTC 2014,,,,,,,,,,"0|i1w8f3:",396445,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"04/Jun/14 09:21;crazyjvm;PR：https://github.com/apache/spark/pull/968;;;","16/Jun/14 06:26;pwendell;Fixed in:
https://github.com/apache/spark/pull/968;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkFlumeEvent with body bigger than 1020 bytes are not read properly,SPARK-1998,12718087,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sunsc,sunsc,sunsc,03/Jun/14 08:21,11/Jun/14 00:27,14/Jul/23 06:25,11/Jun/14 00:27,0.9.1,1.0.0,,,,,,,0.9.2,1.0.1,1.1.0,,,,,,,,0,,,,,,,,joyyoj,pwendell,sunsc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1916,,,,,,,,"03/Jun/14 08:23;sunsc;patch.diff;https://issues.apache.org/jira/secure/attachment/12648098/patch.diff",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396289,,,Wed Jun 11 00:27:06 UTC 2014,,,,,,,,,,"0|i1w87j:",396411,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/14 08:26;sunsc;it seems not completed fixed by the patch in spark-1916;;;","03/Jun/14 08:55;srowen;(Can you make a pull request versus a patch here? this looks important, I know we have some Flume streaming users.);;;","03/Jun/14 13:27;sunsc;I'm new here, thanks for reminding me of pulling request. ;;;","10/Jun/14 02:33;joyyoj;I've pulled a request here(https://github.com/apache/spark/pull/951)
Does anyone can submit and resolve it ?
;;;","11/Jun/14 00:27;pwendell;Issue resolved by pull request 951
[https://github.com/apache/spark/pull/951];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add native support for UPPER() LOWER() and MIN() MAX(),SPARK-1995,12717981,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,marmbrus,marmbrus,02/Jun/14 20:50,03/Jun/14 01:04,14/Jul/23 06:25,03/Jun/14 01:04,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,https://github.com/apache/spark/pull/936,,egraldlo,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396183,,,Tue Jun 03 00:56:41 UTC 2014,,,,,,,,,,"0|i1w7k7:",396306,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/14 00:56;egraldlo;upper() and lower() supported, max() and min() added to hive parser.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregates return incorrect results on first execution,SPARK-1994,12717980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,02/Jun/14 20:37,15/Jul/14 06:21,14/Jul/23 06:25,08/Jun/14 07:02,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"[~adav] has a full reproduction but he has found a case where the first run returns corrupted results, but the second case does not.  The same does not occur when reading from HDFS a second time...

{code}
sql(""SELECT lang, COUNT(*) AS cnt FROM tweetTable GROUP BY lang ORDER BY cnt DESC"").collect.foreach(println)
[bg,16636]
[16266,16266]
[16223,16223]
[16161,16161]
[16047,16047]
[lt,11405]
[hu,11380]
[el,10845]
[da,10289]
[fi,10261]
[9897,9897]
[9765,9765]
[9751,9751]
{code}",,marmbrus,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,396182,,,Sun Jun 08 07:02:30 UTC 2014,,,,,,,,,,"0|i1w7jz:",396305,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"08/Jun/14 07:02;marmbrus;https://github.com/apache/spark/pull/1004;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven build requires SCALA_HOME to be set even though it's not needed,SPARK-1984,12717782,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,01/Jun/14 18:16,15/Jul/14 07:21,14/Jul/23 06:25,15/Jul/14 07:21,,,,,,,,,1.0.1,1.1.0,,,,Build,,,,,0,,,,,,,,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395986,,,Tue Jul 15 07:21:28 UTC 2014,,,,,,,,,,"0|i1w6d3:",396112,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"01/Jun/14 18:19;pwendell;master: https://github.com/apache/spark/commit/d8c005d5371f81a2a06c5d27c7021e1ae43d7193

1.0:
https://github.com/apache/spark/commit/a54a48f83674bb3c6f9aca9f736448338b029dfd;;;","15/Jul/14 07:21;pwendell;this was fixed a while ago, not sure why it's open;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saveToParquetFile doesn't support ByteType,SPARK-1982,12717752,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,schumach,marmbrus,marmbrus,31/May/14 22:17,26/Jun/14 21:20,14/Jul/23 06:25,26/Jun/14 21:20,1.0.0,,,,,,,,,,,,,SQL,,,,,0,,,,,,"{code}
java.lang.RuntimeException: Unsupported datatype ByteType
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetRelation.scala:201)
...
{code}",,marmbrus,rxin,schumach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395956,,,Sun Jun 01 09:38:15 UTC 2014,,,,,,,,,,"0|i1w66n:",396082,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/14 09:38;schumach;It turns out that ByteType primitive types weren't correctly treated earlier. Since Parquet doesn't have these one fix is to use fixed-length byte arrays (which are treated as primitives also). This is fine until there will be support for nested types. Even then I think one may want to treat these as actual arrays and not primitives.

Anyway.. PR available here: https://github.com/apache/spark/pull/934;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In some cases, spark-yarn does not automatically restart the failed container",SPARK-1978,12717719,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gq,gq,gq,31/May/14 07:36,10/Jun/14 18:44,14/Jul/23 06:25,10/Jun/14 18:44,0.9.1,1.0.0,,,,,,,1.0.1,1.1.0,,,,YARN,,,,,0,,,,,,,,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395923,,,Sat May 31 07:37:14 UTC 2014,,,,,,,,,,"0|i1w5zb:",396049,,,,,,,,,,,,,,,,,,,,,,,"31/May/14 07:37;gq;The PR: https://github.com/apache/spark/pull/921;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mutable.BitSet in ALS not serializable with KryoSerializer,SPARK-1977,12717622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,sinisa_lyh,sinisa_lyh,30/May/14 18:58,14/Nov/14 06:30,14/Jul/23 06:24,07/Jul/14 22:08,1.0.0,,,,,,,,1.0.2,1.1.0,,,,MLlib,,,,,2,,,,,,"OutLinkBlock in ALS.scala has an Array[mutable.BitSet] member.
KryoSerializer uses AllScalaRegistrar from Twitter chill but it doesn't register mutable.BitSet.

Right now we have to register mutable.BitSet manually. A proper fix would be using immutable.BitSet in ALS or register mutable.BitSet in upstream chill.

{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1724.0:9 failed 4 times, most recent failure: Exception failure in TID 68548 on host lon4-hadoopslave-b232.lon4.spotify.net: com.esotericsoftware.kryo.KryoException: java.lang.ArrayStoreException: scala.collection.mutable.HashSet
Serialization trace:
shouldSend (org.apache.spark.mllib.recommendation.OutLinkBlock)
        com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
        com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
        org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:115)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:125)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:155)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        java.lang.Thread.run(Thread.java:662)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,apachespark,coderxiang,gen,huasanyelao,ilganeli,mengxr,sinisa_lyh,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3990,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395826,,,Fri Nov 14 06:30:08 UTC 2014,,,,,,,,,,"0|i1w5br:",395943,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/14 05:54;mengxr;I cannot reproduce this error in v1.0.0. There is an example called `MovieLensALS.scala` under `examples/`, which runs fine with kryo enabled. Did you include other dependencies in your application?;;;","04/Jun/14 07:16;sinisa_lyh;Yeah that example worked fine for me in standalone mode but failed in YARN cluster mode with the same error.
Maybe serialization wasn't needed/triggered in standalone mode?;;;","04/Jun/14 08:07;mengxr;This is more likely a version conflict in your dependencies. From the Spark WebUI, you can find the system classpath in the environment tab. Please verify that you don't have two different versions of spark, kryo, or any other related library. Classes may hide inside an assembly jar.;;;","04/Jun/14 08:44;sinisa_lyh;We submit 1 spark-assembly and 1 job assembly jar via spark-submit and there are no other obvious scala/spark/kryo jars in the global classpath. I can reproduce the same exception locally with the following snippet, when kryo.register() is commented out.

I just added mutable BitSet to Twitter chill: https://github.com/twitter/chill/pull/185

{code}
import com.twitter.chill._
import org.apache.spark.serializer.{KryoSerializer, KryoRegistrator}
import org.apache.spark.SparkConf
import scala.collection.mutable

class MyRegistrator extends KryoRegistrator {
  override def registerClasses(kryo: Kryo) {
    // kryo.register(classOf[mutable.BitSet])
  }
}

case class OutLinkBlock(elementIds: Array[Int], shouldSend: Array[mutable.BitSet])

object KryoTest {
  def main(args: Array[String]) {
    println(""hello"")
    val conf = new SparkConf()
      .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
      .set(""spark.kryo.registrator"", classOf[MyRegistrator].getName)
    val serializer = new KryoSerializer(conf).newInstance()

    val bytes = serializer.serialize(OutLinkBlock(Array(1, 2, 3), Array(mutable.BitSet(2, 4, 6))))
    serializer.deserialize(bytes).asInstanceOf[OutLinkBlock]
  }
}
{code};;;","04/Jun/14 20:50;mengxr;In our example code, we only register `Rating` and it works. Could you try adding the following:

{code}
kryo.register(classOf[Rating])
{code}

I need to reproduce this problem with `ALS.train`.;;;","04/Jun/14 20:53;sinisa_lyh;We are already doing that :)
Our job works on YARN with ""register(classOf[mutable.BitSet])"". Without it we get the reported exception.;;;","04/Jun/14 21:43;mengxr;Did you register `Rating`? I think this is necessary.;;;","04/Jun/14 21:56;sinisa_lyh;Yes we did register 'Rating'. And we had to ""register(classOf[mutable.BitSet])"" in addition to make it work.;;;","04/Jun/14 23:25;coderxiang;Hi [~neville], I just run the MovieLens example on my YARN cluster (hadoop-2.0.5-alpha) with kryo enabled and it works. I use the following command:

bin/spark-submit --master yarn-cluster  --class org.apache.spark.examples.mllib.MovieLensALS  --num-executors ** --driver-memory ** --executor-memory ** --executor-cores 1  spark-examples-1.0.0-hadoop2.0.5-alpha.jar  --rank 5 --numIterations 20 --lambda 1.0 --kryo /path/to/sample_movielens_data.txt;;;","05/Jun/14 01:04;sinisa_lyh;Our YARN cluster runs 2.2.0. We built spark-assembly and spark-examples jars with 1.0.0 release source and the bundled make_distribution.sh. And here's my command:

{code}
spark-submit --master yarn-cluster --class org.apache.spark.examples.mllib.MovieLensALS --num-executors 2 --executor-memory 2g --driver-memory 2g dist/lib/spark-examples-1.0.0-hadoop2.2.0.jar --kryo --implicitPrefs sample_movielens_data.txt
{code}

Here's a complete list of classpath from the environment tab.
{code}
/etc/hadoop/conf
/usr/lib/hadoop-hdfs/hadoop-hdfs-2.2.0.2.0.6.0-76-tests.jar
/usr/lib/hadoop-hdfs/hadoop-hdfs-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-hdfs/hadoop-hdfs-nfs-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-hdfs/lib/asm-3.2.jar
/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar
/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar
/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar
/usr/lib/hadoop-hdfs/lib/commons-el-1.0.jar
/usr/lib/hadoop-hdfs/lib/commons-io-2.1.jar
/usr/lib/hadoop-hdfs/lib/commons-lang-2.5.jar
/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.1.jar
/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar
/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar
/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar
/usr/lib/hadoop-hdfs/lib/jasper-runtime-5.5.23.jar
/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar
/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar
/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.jar
/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.jar
/usr/lib/hadoop-hdfs/lib/jsp-api-2.1.jar
/usr/lib/hadoop-hdfs/lib/jsr305-1.3.9.jar
/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar
/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar
/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar
/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar
/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar
/usr/lib/hadoop-mapreduce/hadoop-archives-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-datajoin-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-distcp-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-extras-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-gridmix-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-app-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-common-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-core-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-hs-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-hs-plugins-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.2.0.2.0.6.0-76-tests.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-shuffle-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-rumen-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-streaming-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar
/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar
/usr/lib/hadoop-mapreduce/lib/avro-1.7.4.jar
/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar
/usr/lib/hadoop-mapreduce/lib/commons-io-2.1.jar
/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar
/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar
/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.1.jar
/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar
/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar
/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar
/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar
/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar
/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar
/usr/lib/hadoop-mapreduce/lib/junit-4.10.jar
/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar
/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar
/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar
/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar
/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar
/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar
/usr/lib/hadoop-yarn/hadoop-yarn-api-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-applications-distributedshell-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-client-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-common-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-common-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-nodemanager-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-resourcemanager-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-tests-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-web-proxy-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-site-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar
/usr/lib/hadoop-yarn/lib/asm-3.2.jar
/usr/lib/hadoop-yarn/lib/avro-1.7.4.jar
/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar
/usr/lib/hadoop-yarn/lib/commons-io-2.1.jar
/usr/lib/hadoop-yarn/lib/guice-3.0.jar
/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar
/usr/lib/hadoop-yarn/lib/hamcrest-core-1.1.jar
/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.8.8.jar
/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.8.8.jar
/usr/lib/hadoop-yarn/lib/javax.inject-1.jar
/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar
/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar
/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar
/usr/lib/hadoop-yarn/lib/junit-4.10.jar
/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar
/usr/lib/hadoop-yarn/lib/netty-3.6.2.Final.jar
/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar
/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar
/usr/lib/hadoop-yarn/lib/snappy-java-1.0.4.1.jar
/usr/lib/hadoop-yarn/lib/xz-1.0.jar
/usr/lib/hadoop/hadoop-annotations-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop/hadoop-auth-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop/hadoop-common-2.2.0.2.0.6.0-76-tests.jar
/usr/lib/hadoop/hadoop-common-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop/hadoop-nfs-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop/lib/activation-1.1.jar
/usr/lib/hadoop/lib/asm-3.2.jar
/usr/lib/hadoop/lib/avro-1.7.4.jar
/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar
/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar
/usr/lib/hadoop/lib/commons-cli-1.2.jar
/usr/lib/hadoop/lib/commons-codec-1.4.jar
/usr/lib/hadoop/lib/commons-collections-3.2.1.jar
/usr/lib/hadoop/lib/commons-compress-1.4.1.jar
/usr/lib/hadoop/lib/commons-configuration-1.6.jar
/usr/lib/hadoop/lib/commons-digester-1.8.jar
/usr/lib/hadoop/lib/commons-el-1.0.jar
/usr/lib/hadoop/lib/commons-httpclient-3.1.jar
/usr/lib/hadoop/lib/commons-io-2.1.jar
/usr/lib/hadoop/lib/commons-lang-2.5.jar
/usr/lib/hadoop/lib/commons-logging-1.1.1.jar
/usr/lib/hadoop/lib/commons-math-2.1.jar
/usr/lib/hadoop/lib/commons-net-3.1.jar
/usr/lib/hadoop/lib/guava-11.0.2.jar
/usr/lib/hadoop/lib/jackson-core-asl-1.8.8.jar
/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar
/usr/lib/hadoop/lib/jackson-mapper-asl-1.8.8.jar
/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar
/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar
/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar
/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar
/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar
/usr/lib/hadoop/lib/jersey-core-1.9.jar
/usr/lib/hadoop/lib/jersey-json-1.9.jar
/usr/lib/hadoop/lib/jersey-server-1.9.jar
/usr/lib/hadoop/lib/jets3t-0.6.1.jar
/usr/lib/hadoop/lib/jettison-1.1.jar
/usr/lib/hadoop/lib/jetty-6.1.26.jar
/usr/lib/hadoop/lib/jetty-util-6.1.26.jar
/usr/lib/hadoop/lib/jsch-0.1.42.jar
/usr/lib/hadoop/lib/jsp-api-2.1.jar
/usr/lib/hadoop/lib/jsr305-1.3.9.jar
/usr/lib/hadoop/lib/junit-4.8.2.jar
/usr/lib/hadoop/lib/log4j-1.2.17.jar
/usr/lib/hadoop/lib/mockito-all-1.8.5.jar
/usr/lib/hadoop/lib/native/*
/usr/lib/hadoop/lib/netty-3.6.2.Final.jar
/usr/lib/hadoop/lib/paranamer-2.3.jar
/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar
/usr/lib/hadoop/lib/servlet-api-2.5.jar
/usr/lib/hadoop/lib/slf4j-api-1.7.5.jar
/usr/lib/hadoop/lib/slf4j-log4j12-1.7.5.jar
/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar
/usr/lib/hadoop/lib/stax-api-1.0.1.jar
/usr/lib/hadoop/lib/xmlenc-0.52.jar
/usr/lib/hadoop/lib/xz-1.0.jar
/usr/lib/hadoop/lib/zookeeper-3.4.5.jar
{code};;;","06/Jun/14 07:53;smolav;I can reproduce this depending on the size of the dataset:

{noformat}
spark-submit mllib-movielens-evaluation-assembly-1.0.jar --master spark://mllib1:7077
--class com.example.MovieLensALS --rank 10 --numIterations 20 --lambda 1.0 --kryo
hdfs:/movielens/oversampled.dat
{noformat}

The exception will not be thrown for small datasets. It will successfully run with MovieLens 100k and 10M. However, when I run it on a 100M dataset, the exception will be thrown.

My MovieLensALS is mostly the same as the one shipped with Spark. I just added cross-validation. Rating is registered in Kryo just as in the stock example.

{noformat}
# cat RELEASE 
Spark 1.0.0 built for Hadoop 2.2.0
{noformat}

;;;","06/Jun/14 16:48;coderxiang;Update: I also reproduce similar error message for a larger data set (~ 3GB).;;;","06/Jun/14 22:28;mengxr;[~smolav] and [~coderxiang]:

Thanks for testing it! Could you post the exact error message you got with stack trace? Based on your description, it should be caused by the default serialization of kryo. It may treat BitSet as a general Java collection, then run into error in ser/de.;;;","09/Jun/14 09:36;smolav;Xiangrui Meng, I can't reproduce it at the moment. It takes a quite big dataset to reproduce and I have my machines busy. But I'm pretty sure the stacktrace is exactly the same as the one posted by Neville Li. My bet is that this will be fixed with next Twitter Chill release: https://github.com/twitter/chill/commit/b47512c2c75b94b7c5945985306fa303576bf90d;;;","07/Jul/14 18:40;mengxr;I think now I understand when it happens. We use storage level MEMORY_AND_DISK for user/product in/out links, which contains BitSet objects. If the dataset is large, these RDDs will be pushed from in memory storage to on disk storage, where the latter requires serialization. So the easiest way to re-produce this error is changing the storage level of inLinks/outLinks to DISK_ONLY and run with kryo.

[~neville] Instead of mapping mutable.BitSet to immutable.BitSet, which introduces overhead, we can register mutable.BitSet in our MovieLensALS example code and wait for the next Chill release. Does it sound good to you?;;;","07/Jul/14 18:42;sinisa_lyh;[~mengxr] sounds good to me.;;;","07/Jul/14 18:45;mengxr;Do you mind creating a PR registering mutable.BitSet in MovieLensALS.scala and close PR #925? Thanks!;;;","07/Jul/14 19:10;sinisa_lyh;There you go:
https://github.com/apache/spark/pull/1319;;;","07/Jul/14 22:08;mengxr;Issue resolved by pull request 1319
[https://github.com/apache/spark/pull/1319];;;","23/Oct/14 08:26;gen;[~sinisa_lyh]
Sorry to bother you.
According to https://github.com/twitter/chill/pull/185, the twitter.chill have already had the support of mutable BitSet. However, I tried your code, it still doesn't work, if we make kryo as a comment. The task fails in the last line:
{code}
serializer.deserialize(bytes).asInstanceOf[OutLinkBlock]
{code}
Have you any ideas how it happens? The error information is as follow:
{code}
[error] (run-main) com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.BitSet field OutLinkBlock.elementIds to scala.collection.mutable.HashSet
[error] Serialization trace:
[error] elementIds (OutLinkBlock)
com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.BitSet field OutLinkBlock.elementIds to scala.collection.mutable.HashSet
Serialization trace:
elementIds (OutLinkBlock)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:162)
	at KroTest$.main(helloworld.scala:25)
	at KroTest.main(helloworld.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
Caused by: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.BitSet field OutLinkBlock.elementIds to scala.collection.mutable.HashSet
	at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:164)
	at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:168)
	at sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.set(UnsafeQualifiedObjectFieldAccessorImpl.java:83)
	at java.lang.reflect.Field.set(Field.java:736)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:619)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:162)
	at KroTest$.main(helloworld.scala:25)
	at KroTest.main(helloworld.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)

{code};;;","23/Oct/14 12:37;gen;In fact, the problem about transformation between HashSet and BitSet in Kyro happens, if we don't register BitSet manually. 
{code}
com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.BitSet field OutLinkBlock.elementIds to scala.collection.mutable.HashSet
{code}
This will also cause the collapse of spark when we use spark HIVE.;;;","27/Oct/14 16:04;ilganeli;Hi all - concise writeup on how to fix this bug here:
http://tbertinmahieux.com/wp/?author=1

Also related to:

http://apache-spark-user-list.1001560.n3.nabble.com/ALS-implicit-error-pyspark-td16595.html

Thanks. 
;;;","14/Nov/14 06:30;apachespark;User 'nevillelyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/925;;;",,,,,,,,,,
misleading streaming document,SPARK-1976,12717601,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,codingcat,30/May/14 18:20,31/May/14 05:07,14/Jul/23 06:25,31/May/14 05:07,1.0.0,,,,,,,,1.0.1,1.1.0,,,,,,,,,0,,,,,,"Spark streaming requires at least two working thread, but the document gives the example like 

import org.apache.spark.api.java.function._
import org.apache.spark.streaming._
import org.apache.spark.streaming.api._
// Create a StreamingContext with a local master
val ssc = new StreamingContext(""local"", ""NetworkWordCount"", Seconds(1))

http://spark.apache.org/docs/latest/streaming-programming-guide.html

",,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395805,,,Fri May 30 18:27:45 UTC 2014,,,,,,,,,,"0|i1w573:",395922,,,,,,,,,,,,,,,,,,,,,,,"30/May/14 18:27;codingcat;PR : https://github.com/apache/spark/pull/924 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI throws NPE on trying to load the app page for non-existent app,SPARK-1965,12717339,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,kayousterhout,kayousterhout,29/May/14 19:23,28/Feb/15 15:34,14/Jul/23 06:25,28/Feb/15 15:34,1.0.0,,,,,,,,1.4.0,,,,,Web UI,,,,,0,,,,,,"If you try to load the Spark UI for an application that doesn't exist:

sparkHost:8080/app/?appId=foobar

The UI throws a NPE.  The problem is in ApplicationPage.scala -- Spark proceeds even if the ""app"" variable is null.  We should handle this more gracefully.",,apachespark,kayousterhout,liuchang0812,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395543,,,Sat Feb 28 15:34:18 UTC 2015,,,,,,,,,,"0|i1w3mn:",395668,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/15 01:17;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4777;;;","28/Feb/15 15:34;srowen;Issue resolved by pull request 4777
[https://github.com/apache/spark/pull/4777];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timestamp missing from HiveMetastore types parser,SPARK-1964,12717292,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,29/May/14 16:46,19/Jun/14 05:37,14/Jul/23 06:25,19/Jun/14 05:37,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{code}


---------- Forwarded message ----------
From: dataginjaninja <rickett.stephanie@gmail.com>
Date: Thu, May 29, 2014 at 8:54 AM
Subject: Timestamp support in v1.0
To: dev@spark.incubator.apache.org


Can anyone verify which rc  [SPARK-1360] Add Timestamp Support for SQL #275
<https://github.com/apache/spark/pull/275>   is included in? I am running
rc3, but receiving errors with TIMESTAMP as a datatype in my Hive tables
when trying to use them in åçpyspark.

*The error I get:
*
14/05/29 15:44:47 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
aol
14/05/29 15:44:48 INFO ParseDriver: Parse Completed
14/05/29 15:44:48 INFO metastore: Trying to connect to metastore with URI
thrift:
14/05/29 15:44:48 INFO metastore: Waiting 1 seconds before next connection
attempt.
14/05/29 15:44:49 INFO metastore: Connected to metastore.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/spark-1.0.0-rc3/python/pyspark/sql.py"", line 189, in hql
    return self.hiveql(hqlQuery)
  File ""/opt/spark-1.0.0-rc3/python/pyspark/sql.py"", line 183, in hiveql
    return SchemaRDD(self._ssql_ctx.hiveql(hqlQuery), self)
  File
""/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"",
line 537, in __call__
  File
""/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"", line
300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o14.hiveql.
: java.lang.RuntimeException: Unsupported dataType: timestamp

*The table I loaded:*
DROP TABLE IF EXISTS aol;
CREATE EXTERNAL TABLE aol (
        userid STRING,
        query STRING,
        query_time TIMESTAMP,
        item_rank INT,
        click_url STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/tmp/data/aol';

*The pyspark commands:*
from pyspark.sql import HiveContext
hctx= HiveContext(sc)
results = hctx.hql(""SELECT COUNT(*) FROM aol"").collect()
{code}",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395496,,,Thu May 29 17:05:05 UTC 2014,,,,,,,,,,"0|i1w3cv:",395624,,,,,,,,,,,,,,,,,,,,,,,"29/May/14 17:05;marmbrus;https://github.com/apache/spark/pull/913;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"String ""NULL"" is interpreted as null value",SPARK-1959,12717203,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,29/May/14 03:12,28/Oct/14 22:06,14/Jul/23 06:25,31/May/14 05:13,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"The {{HiveTableScan}} operator unwraps string ""NULL"" (case insensitive) into null values even if the column type is {{STRING}}.

To reproduce the bug, we use {{sql/hive/src/test/resources/groupby_groupingid.txt}} as test input, copied to {{/tmp/groupby_groupingid.txt}}.

Hive session:

{code}
hive> CREATE TABLE test_null(key INT, value STRING);
hive> LOAD DATA LOCAL INPATH '/tmp/groupby_groupingid.txt' INTO table test_null;
hive> SELECT * FROM test_null WHERE value IS NOT NULL;
...
OK
1       NULL
1       1
2       2
3       3
3       NULL
4       5
{code}

We can see that the {{NULL}} cells in the original input file are interpreted as string {{""NULL""}} in Hive.

Spark SQL session ({{sbt/sbt hive/console}}):

{code}
scala> hql(""CREATE TABLE test_null(key INT, value STRING)"")
scala> hql(""LOAD DATA LOCAL INPATH '/tmp/groupby_groupingid.txt' INTO table test_null"")
scala> hql(""SELECT * FROM test_null WHERE value IS NOT NULL"").foreach(println)
...
[1,1]
[2,2]
[3,3]
[4,5]
{code}

As we can see, string {{""NULL""}} is interpreted as null values in Spark SQL.",,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3683,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395407,,,Thu May 29 06:31:44 UTC 2014,,,,,,,,,,"0|i1w2tb:",395536,,,,,,,,,,,,,,,,,,,,,,,"29/May/14 03:54;lian cheng;To [~marmbrus]: The problematic line should be [this one|https://github.com/apache/spark/blob/master/sql%2Fhive%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2Fsql%2Fhive%2FhiveOperators.scala#L154]. I wonder under what circumstances, would Hive return a Java string {{""NULL""}} to represent a null value? Is it safe to simply remove this line?;;;","29/May/14 04:36;marmbrus;If all the hive tests still pass with that line removed then I'm okay with it.;;;","29/May/14 06:31;lian cheng;Pull request: https://github.com/apache/spark/pull/909;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling .collect() on a SchemaRDD should call executeCollect() on the underlying query plan.,SPARK-1958,12717197,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,29/May/14 02:54,02/Jun/14 21:41,14/Jul/23 06:25,02/Jun/14 21:41,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,In some cases (like LIMIT) executeCollect() makes optimizations that execute().collect() will not.,,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395401,,,Mon Jun 02 03:02:53 UTC 2014,,,,,,,,,,"0|i1w2s7:",395531,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/14 03:02;lian cheng;PR: https://github.com/apache/spark/pull/939;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VertexRDD can incorrectly assume index sharing,SPARK-1955,12717153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,boyork,ankurd,ankurd,28/May/14 21:57,25/Feb/15 22:15,14/Jul/23 06:25,25/Feb/15 22:15,0.9.0,0.9.1,1.0.0,,,,,,1.2.2,1.3.0,,,,GraphX,,,,,0,,,,,,"Many VertexRDD operations (diff, leftJoin, innerJoin) can use a fast zip join if both operands are VertexRDDs sharing the same index (i.e., one operand is derived from the other). This check is implemented by matching on the operand type and using the fast join strategy if both are VertexRDDs.

This is clearly fine when both do in fact share the same index. It is also fine when the two VertexRDDs have the same partitioner but different indexes, because each VertexPartition will detect the index mismatch and fall back to the slow but correct local join strategy.

However, when they have different numbers of partitions or different partition functions, an exception or even silently incorrect results can occur.

For example:

{code}
import org.apache.spark._
import org.apache.spark.graphx._

// Construct VertexRDDs with different numbers of partitions
val a = VertexRDD(sc.parallelize(List((0L, 1), (1L, 2)), 1))
val b = VertexRDD(sc.parallelize(List((0L, 5)), 8))
// Try to join them. Appears to work...
val c = a.innerJoin(b) { (vid, x, y) => x + y }
// ... but then fails with java.lang.IllegalArgumentException: Can't zip RDDs with unequal numbers of partitions
c.collect

// Construct VertexRDDs with different partition functions
val a = VertexRDD(sc.parallelize(List((0L, 1), (1L, 2))).partitionBy(new HashPartitioner(2)))
val bVerts = sc.parallelize(List((1L, 5)))
val b = VertexRDD(bVerts.partitionBy(new RangePartitioner(2, bVerts)))
// Try to join them. We expect (1L, 7).
val c = a.innerJoin(b) { (vid, x, y) => x + y }
// Silent failure: we get an empty set!
c.collect
{code}

VertexRDD should check equality of partitioners before using the fast zip join. If the partitioners are different, the two datasets should be automatically co-partitioned.",,ankurd,apachespark,boyork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2365,,SPARK-5790,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395360,,,Wed Feb 25 22:15:48 UTC 2015,,,,,,,,,,"0|i1w2jb:",395490,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/14 22:46;ankurd;IndexedRDD fixes this issue.;;;","13/Feb/15 06:34;boyork;[~ankurdave] if you haven't started on this I can take it since it relates heavily to [SPARK-4600|https://issues.apache.org/jira/browse/SPARK-4600] and [SPARK-5790|https://issues.apache.org/jira/browse/SPARK-5790]. If you've already started work though let me know :);;;","13/Feb/15 07:01;ankurd;[~boyork] Thanks, it would be great if you could take this. I think the fix should be pretty simple: in each of the VertexRDD functions, check partitioner equality and repartition the other RDD if unequal, as in https://github.com/amplab/spark-indexedrdd/blob/master/src/main/scala/edu/berkeley/cs/amplab/spark/indexedrdd/IndexedRDDLike.scala#L206;;;","20/Feb/15 17:46;apachespark;User 'brennonyork' has created a pull request for this issue:
https://github.com/apache/spark/pull/4705;;;","25/Feb/15 22:15;ankurd;Issue resolved by pull request 4705
https://github.com/apache/spark/pull/4705;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn client mode Application Master memory size is same as driver memory size,SPARK-1953,12717134,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTaoTheTonic,tgraves,tgraves,28/May/14 21:10,17/Feb/15 13:01,14/Jul/23 06:25,09/Jan/15 21:23,1.0.0,,,,,,,,1.3.0,,,,,YARN,,,,,0,,,,,,"With Spark on yarn in client mode, the application master that gets created to allocated containers gets the same amount of memory as the driver running on the client. (--driver-memory option through spark-submit)  This could definitely be more then what is really needed, thus wasting resources.  The Application Master should be very small and require very little memory since all its doing is allocating and starting containers.  

We should allow the memory for the application master to be configurable separate from the driver in client mode. 

 We probably need to be careful about how we do this as to not cause confusion about what the options do in the various modes.",,apachespark,sandyr,sb58,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5861,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395342,,,Thu Dec 04 13:58:38 UTC 2014,,,,,,,,,,"0|i1w2fb:",395472,,,,,,,,,,,,,1.3.0,,,,,,,,,,"04/Dec/14 13:58;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3607;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApproxCountDistinctMergeFunction should return Int value.,SPARK-1938,12716769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,27/May/14 10:28,28/May/14 05:18,14/Jul/23 06:25,28/May/14 05:18,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,{{ApproxCountDistinctMergeFunction}} should return {{Int}} value because the {{dataType}} of {{ApproxCountDistinct}} is {{IntegerType}}.,,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394977,,,Tue May 27 10:33:50 UTC 2014,,,,,,,,,,"0|i1w07j:",395112,,,,,,,,,,,,,,,,,,,,,,,"27/May/14 10:33;ueshin;PRed: https://github.com/apache/spark/pull/893;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tasks can be submitted before executors are registered,SPARK-1937,12716744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lirui,lirui,lirui,27/May/14 07:00,24/Jun/14 18:42,14/Jul/23 06:25,24/Jun/14 18:42,1.0.0,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"During construction, TaskSetManager will assign tasks to several pending lists according to the tasks’ preferred locations. If the desired location is unavailable, it’ll then assign this task to “pendingTasksWithNoPrefs”, a list containing tasks without preferred locations.
The problem is that tasks may be submitted before the executors get registered with the driver, in which case TaskSetManager will assign all the tasks to pendingTasksWithNoPrefs. Later when it looks for a task to schedule, it will pick one from this list and assign it to arbitrary executor, since TaskSetManager considers the tasks can run equally well on any node.
This problem deprives benefits of data locality, drags the whole job slow and can cause imbalance between executors.
I ran into this issue when running a spark program on a 7-node cluster (node6~node12). The program processes 100GB data.
Since the data is uploaded to HDFS from node6, this node has a complete copy of the data and as a result, node6 finishes tasks much faster, which in turn makes it complete dis-proportionally more tasks than other nodes.
To solve this issue, I think we shouldn't check availability of executors/hosts when constructing TaskSetManager. If a task prefers a node, we simply add the task to that node’s pending list. When later on the node is added, TaskSetManager can schedule the task according to proper locality level. If unfortunately the preferred node(s) never gets added, TaskSetManager can still schedule the task at locality level “ANY”.",,devaraj,drankye,junping_du,lirui,yiyao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/14 08:28;lirui;After-patch.PNG;https://issues.apache.org/jira/secure/attachment/12646858/After-patch.PNG","27/May/14 08:27;lirui;Before-patch.png;https://issues.apache.org/jira/secure/attachment/12646857/Before-patch.png","27/May/14 08:42;lirui;RSBTest.scala;https://issues.apache.org/jira/secure/attachment/12646863/RSBTest.scala",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394952,,,Tue May 27 08:42:10 UTC 2014,,,,,,,,,,"0|i1w01z:",395087,,,,,,,,,,,,,1.1.0,,,,,,,,,,"27/May/14 08:28;lirui;Here's a quick fix:
https://github.com/apache/spark/pull/892;;;","27/May/14 08:42;lirui;The program that triggers the problem.
With the patch, the whole execution time of the job reduces from nearly 600s to around 250s.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explicitly add commons-codec 1.5 as a dependency,SPARK-1935,12716721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yhuai,yhuai,yhuai,27/May/14 03:23,23/Jul/14 07:14,14/Jul/23 06:25,29/May/14 16:10,0.9.1,,,,,,,,0.9.2,1.0.1,1.1.0,,,Build,,,,,0,,,,,,"Right now, commons-codec is a transitive dependency. When Spark is built by maven for Hadoop 1, jets3t 0.7.1 will pull in commons-codec 1.3 which is an older version (Hadoop 1.0.4 depends on 1.4). This older version can cause problems because 1.4 introduces incompatible changes and new methods.",,gq,pwendell,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394929,,,Thu May 29 16:29:46 UTC 2014,,,,,,,,,,"0|i1vzwv:",395064,,,,,,,,,,,,,,,,,,,,,,,"27/May/14 04:03;yhuai;PR's link added.;;;","27/May/14 05:11;pwendell;Does commons-codec 1.4 really break compatibility with commons-codec 1.3? Or is the issue just that Hadoop is compiled against 1.4 but maven is selecting 1.3 and so the new 1.4 functions aren't available.

Also, would you mind giving the exact permutation of the build that is causing this error? I just want to see if it's also a problem in Spark 1.0 or if it's only in 0.9.;;;","27/May/14 05:14;rxin;If you build Spark with Maven, commons-codec 1.3 is included. If you build Spark with SBT, commons-codec 1.4 is included.

Hive uses a Base64 decode([String]) that is introduced in 1.4. ;;;","27/May/14 05:35;yhuai;Thanks, [~rxin]. Let me add more info. 

Commands I used:
{code}
mvn clean -DskipTests clean package
{code}
{code}
sbt/sbt assembly
{code}
You can also check the pre-built Hadoop 1 package which has the 1.3 codec. 

There are a few methods in the class of Base64 that were introduced with 1.4 (http://commons.apache.org/proper/commons-codec/apidocs/org/apache/commons/codec/binary/Base64.html).
I noticed the problem when Hive was calling 
{code}
public static byte[] decodeBase64(String base64String)
{code};;;","27/May/14 07:50;srowen;Yeah, I think  this is a matter of Maven's nearest-first vs SBT's latest-first conflict resolution strategy.
It should be safe to manually manage this to 1.5, I believe.;;;","27/May/14 16:26;yhuai;[~srowen] yes, it is the reason. 

I did a quick search in parquet-mr and seems parquet does not have any import related to commons-codec. Also, I checked the release note of codec 1.5. It seems 1.5 is fine. So, do we upgrade to 1.5?;;;","27/May/14 20:40;yhuai;I have updated my PR to use codec 1.5.;;;","29/May/14 16:10;pwendell;I merged this fix into Master and 1.0.1 form this PR:
https://github.com/apache/spark/pull/889

[~yhuai] Did you want this in 0.9.2 as well (based on your PR I was wondering)? If so we'll need a new PR because it didn't merge cleanly at all with branch-0.9. I'm resolving it for now, but we can re-open.;;;","29/May/14 16:29;yhuai;[~pwendell] Yes, let's add this into 0.9.2. I have opened a new PR for the 0.9 branch (https://github.com/apache/spark/pull/912). Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"""this"" reference escape to ""selectorThread"" during construction in ConnectionManager",SPARK-1934,12716720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,zsxwing,zsxwing,27/May/14 03:20,28/Jan/15 20:44,14/Jul/23 06:25,28/Jan/15 20:44,,,,,,,,,1.3.0,,,,,Spark Core,,,,,0,,,,,,"`selectorThread` starts in the construction of `org.apache.spark.network.ConnectionManager`, which may cause `writeRunnableStarted` and `readRunnableStarted` are uninitialized before them are used.

Indirectly, `BlockManager.this` also escape since it calls `new ConnectionManager(...)` and will be used in some threads of `ConnectionManager`. Some threads may view an uninitialized `BlockManager`.

In summary, it's dangerous and hard to analyse the correctness of concurrency. Such escape should be avoided.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394928,,,Tue Jan 27 17:28:39 UTC 2015,,,,,,,,,,"0|i1vzwn:",395063,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/15 17:28;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4225;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException when a directory is passed to SparkContext.addJar/addFile,SPARK-1933,12716718,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,27/May/14 03:17,18/Jun/14 07:39,14/Jul/23 06:25,18/Jun/14 07:39,,,,,,,,,1.0.1,1.1.0,,,,Spark Core,,,,,0,,,,,,"When SparkContext.addJar/addFile is used to add a directory (which is not supported), the runtime exception is 
{code}
java.io.FileNotFoundException: [file] (No such file or directory)
{code}

This exception is extremely confusing because the directory does exist. We should throw a more meaningful exception when a directory is passed to addJar/addFile.",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394926,,,Tue May 27 03:19:28 UTC 2014,,,,,,,,,,"0|i1vzw7:",395061,,,,,,,,,,,,,,,,,,,,,,,"27/May/14 03:19;rxin;Pull request added https://github.com/apache/spark/pull/888;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race conditions in BlockManager.cachedPeers and ConnectionManager.onReceiveCallback,SPARK-1932,12716716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,zsxwing,zsxwing,27/May/14 02:50,27/May/14 06:26,14/Jul/23 06:25,27/May/14 06:26,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,easyfix,,,,,BlockManager.cachedPeers and  ConnectionManager.onReceiveCallback are read and written in different threads without proper protection.,,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394924,,,Tue May 27 06:26:48 UTC 2014,,,,,,,,,,"0|i1vzvr:",395059,,,,,,,,,,,,,,,,,,,,,,,"27/May/14 06:26;zsxwing;PR: https://github.com/apache/spark/pull/887;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Graph.partitionBy does not reconstruct routing tables,SPARK-1931,12716675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,26/May/14 17:45,29/May/14 00:11,14/Jul/23 06:25,27/May/14 01:31,1.0.0,,,,,,,,1.0.1,,,,,GraphX,,,,,0,,,,,,"Commit 905173df57b90f90ebafb22e43f55164445330e6 introduced a bug in partitionBy where, after repartitioning the edges, it reuses the VertexRDD without updating the routing tables to reflect the new edge layout. This causes the following test to fail:

{code}
      import org.apache.spark.graphx._
      val g = Graph(
        sc.parallelize(List((0L, ""a""), (1L, ""b""), (2L, ""c""))),
        sc.parallelize(List(Edge(0L, 1L, 1), Edge(0L, 2L, 1)), 2))
      assert(g.triplets.collect.map(_.toTuple).toSet ==
        Set(((0L, ""a""), (1L, ""b""), 1), ((0L, ""a""), (2L, ""c""), 1)))
      val gPart = g.partitionBy(PartitionStrategy.EdgePartition2D)
      assert(gPart.triplets.collect.map(_.toTuple).toSet ==
        Set(((0L, ""a""), (1L, ""b""), 1), ((0L, ""a""), (2L, ""c""), 1)))
{code}",,ankurd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394883,,,Tue May 27 21:47:34 UTC 2014,,,,,,,,,,"0|i1vzmn:",395018,,,,,,,,,,,,,,,,,,,,,,,"26/May/14 18:12;ankurd;The fix is in PR #885: https://github.com/apache/spark/pull/885;;;","27/May/14 21:47;ankurd;Since the fix didn't make it into Spark 1.0.0, a workaround is to partition the edges before constructing the graph, as follows:

{code}
// Define our own version of partitionBy to work around SPARK-1931
import org.apache.spark.HashPartitioner
def partitionBy[ED](edges: RDD[Edge[ED]], partitionStrategy: PartitionStrategy): RDD[Edge[ED]] = {
  val numPartitions = edges.partitions.size
  edges.map(e => (partitionStrategy.getPartition(e.srcId, e.dstId, numPartitions), e))
    .partitionBy(new HashPartitioner(numPartitions))
    .mapPartitions(_.map(_._2), preservesPartitioning = true)
}

val vertices = ...
val edges = ...

// Instead of:
val g = Graph(vertices, edges).partitionBy(PartitionStrategy.EdgePartition2D) // broken in Spark 1.0.0

// Use:
val g = Graph(vertices, partitionBy(edges, PartitionStrategy.EdgePartition2D))
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The Container is running beyond physical memory limits, so as to be killed.",SPARK-1930,12716648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,26/May/14 13:50,06/Jul/14 02:00,14/Jul/23 06:25,16/Jun/14 19:28,,,,,,,,,1.0.1,1.1.0,,,,YARN,,,,,0,,,,,,"When the containers occupies 8G memory ,the containers were killed
yarn node manager log:
{code}
2014-05-23 13:35:30,776 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Container [pid=4947,containerID=container_1400809535638_0015_01_000005] is running beyond physical memory limits. Current usage: 8.6 GB of 8.5 GB physical memory used; 10.0 GB of 17.8 GB virtual memory used. Killing container.
Dump of the process-tree for container_1400809535638_0015_01_000005 :
        |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
        |- 4947 25417 4947 4947 (bash) 0 0 110804992 335 /bin/bash -c /usr/java/jdk1.7.0_45-cloudera/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms8192m -Xmx8192m  -Xss2m -Djava.io.tmpdir=/yarn/nm/usercache/spark/appcache/application_1400809535638_0015/container_1400809535638_0015_01_000005/tmp  -Dlog4j.configuration=log4j-spark-container.properties -Dspark.akka.askTimeout=""120"" -Dspark.akka.timeout=""120"" -Dspark.akka.frameSize=""20"" org.apache.spark.executor.CoarseGrainedExecutorBackend akka.tcp://spark@10dian71.domain.test:45477/user/CoarseGrainedScheduler 3 10dian72.domain.test 4 1> /var/log/hadoop-yarn/container/application_1400809535638_0015/container_1400809535638_0015_01_000005/stdout 2> /var/log/hadoop-yarn/container/application_1400809535638_0015/container_1400809535638_0015_01_000005/stderr 
        |- 4957 4947 4947 4947 (java) 157809 12620 10667016192 2245522 /usr/java/jdk1.7.0_45-cloudera/bin/java -server -XX:OnOutOfMemoryError=kill %p -Xms8192m -Xmx8192m -Xss2m -Djava.io.tmpdir=/yarn/nm/usercache/spark/appcache/application_1400809535638_0015/container_1400809535638_0015_01_000005/tmp -Dlog4j.configuration=log4j-spark-container.properties -Dspark.akka.askTimeout=120 -Dspark.akka.timeout=120 -Dspark.akka.frameSize=20 org.apache.spark.executor.CoarseGrainedExecutorBackend akka.tcp://spark@10dian71.domain.test:45477/user/CoarseGrainedScheduler 3 10dian72.domain.test 4 

2014-05-23 13:35:30,776 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Removed ProcessTree with root 4947
2014-05-23 13:35:30,776 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1400809535638_0015_01_000005 transitioned from RUNNING to KILLING
2014-05-23 13:35:30,777 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1400809535638_0015_01_000005
2014-05-23 13:35:30,788 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1400809535638_0015_01_000005 is : 143
2014-05-23 13:35:30,829 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1400809535638_0015_01_000005 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2014-05-23 13:35:30,830 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /yarn/nm/usercache/spark/appcache/application_1400809535638_0015/container_1400809535638_0015_01_000005
2014-05-23 13:35:30,830 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=spark        OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1400809535638_0015    CONTAINERID=container_1400809535638_0015_01_000005
2014-05-23 13:35:30,830 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1400809535638_0015_01_000005 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2014-05-23 13:35:30,830 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Removing container_1400809535638_0015_01_000005 from application application_1400809535638_0015
{code}
I think it should be related with {{YarnAllocationHandler.MEMORY_OVERHEA}}  
https://github.com/apache/spark/blob/master/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala#L562

Relative to 8G, 384 MB is too small",,devaraj,gq,pwendell,qiaohaijun,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394856,,,Fri May 30 06:17:56 UTC 2014,,,,,,,,,,"0|i1vzgn:",394991,,,,,,,,,,,,,,,,,,,,,,,"27/May/14 21:00;pwendell;PySpark might also be an issue with this, because it launches Python VM's that consume memory.;;;","30/May/14 06:17;gq;The PR: https://github.com/apache/spark/pull/894;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler suspended by local task OOM,SPARK-1928,12716641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhpengg,zhpengg,zhpengg,26/May/14 13:12,17/May/20 17:47,14/Jul/23 06:25,15/May/15 13:43,0.9.0,,,,,,,,1.1.0,,,,,Scheduler,Spark Core,,,,0,,,,,,"DAGScheduler does not handle local task OOM properly, and will wait for the job result forever.",,gq,zhpengg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394849,,,Fri May 15 13:43:50 UTC 2015,,,,,,,,,,"0|i1vzf3:",394984,,,,,,,,,,,,,,,,,,,,,,,"26/May/14 13:51;gq;How to reproduce the issue?;;;","27/May/14 07:58;zhpengg;https://github.com/apache/spark/pull/883 ;;;","27/May/14 09:34;zhpengg;[~gq] I met this case in our local mode spark streaming application. 
And in the UT, I have added a test case to simulate this.;;;","15/May/15 13:43;srowen;Resolved long ago by https://github.com/apache/spark/pull/883;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nullability of Max/Min/First should be true.,SPARK-1926,12716587,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,26/May/14 07:19,27/May/14 21:57,14/Jul/23 06:25,27/May/14 21:57,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,Nullability of {{Max}}/{{Min}}/{{First}} should be {{true}} because they return {{null}} if there are no rows.,,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394795,,,Mon May 26 07:23:49 UTC 2014,,,,,,,,,,"0|i1vz33:",394930,,,,,,,,,,,,,,,,,,,,,,,"26/May/14 07:23;ueshin;PRed: https://github.com/apache/spark/pull/881;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in org.apache.spark.mllib.tree.DecisionTree.isSampleValid,SPARK-1925,12716578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,zsxwing,zsxwing,26/May/14 05:18,27/May/14 02:44,14/Jul/23 06:25,27/May/14 02:44,,,,,,,,,1.0.0,,,,,MLlib,,,,,0,easyfix,,,,,"I believe this is a typo:

{code}
      if ((level > 0) & (parentFilters.length == 0)) {
        return false
      }
{code}

Should use ""&&"" here.",,mengxr,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394786,,,Mon May 26 05:21:19 UTC 2014,,,,,,,,,,"0|i1vz13:",394921,,,,,,,,,,,,,,,,,,,,,,,"26/May/14 05:21;zsxwing;PR: https://github.com/apache/spark/pull/879;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException when running with sbt and Scala 2.10.3,SPARK-1923,12716564,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,25/May/14 20:18,19/Jun/15 23:42,14/Jul/23 06:25,25/May/14 20:21,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"I just wanted to document this for posterity. I had an issue when running a Spark 1.0 app locally with sbt. The issue was that if you both:

1. Reference a scala class (e.g. None) inside of a closure.
2. Run your program with 'sbt run'

It throws an exception. Upgrading the scalaVersion to 2.10.4 in sbt solved this issue. Somehow scala classes were not being loaded correctly inside of the executors:

Application:
{code}
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object Test {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster(""local[4]"").setAppName(""Test"")
    val sc = new SparkContext(conf)
    sc.makeRDD(1 to 1000, 10).map(x => Some(x)).count
    sc.stop()
  }
{code}

Exception:
{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0:1 failed 1 times, most recent failure: Exception failure in TID 1 on host localhost: java.lang.ClassNotFoundException: scala.None$
        java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        java.security.AccessController.doPrivileged(Native Method)
        java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        java.lang.Class.forName0(Native Method)
        java.lang.Class.forName(Class.java:270)
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:60)
        java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
        java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
{code}
",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8494,,,,,,,SPARK-1410,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394772,,,2014-05-25 20:18:12.0,,,,,,,,,,"0|i1vyxz:",394907,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hql query throws ""RuntimeException: Unsupported dataType"" if struct field of a table has a column with underscore in name",SPARK-1922,12716543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,llai,llai,llai,25/May/14 12:16,27/May/14 23:09,14/Jul/23 06:25,27/May/14 23:09,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"If table A has a struct field A_strct <a_col: string>, when doing an hql query like ""select"", ""RuntimeException: Unsupported dataType"" is thrown.

Running a query wiht ""sbt/sbt hive/console"":
{code}
scala> hql(""SELECT utc_time, pkg FROM pkg_table where year=2014 and month=1 limit 10"").collect().foreach(x => println(x(1)))
{code}
Console output:
{code}
14/05/25 19:50:27 INFO parse.ParseDriver: Parsing command: SELECT utc_time, pkg FROM pkg_table where year=2014 and month=1 limit 10
14/05/25 19:50:27 INFO parse.ParseDriver: Parse Completed
14/05/25 19:50:28 INFO analysis.Analyzer: Max iterations (2) reached for batch MultiInstanceRelations
14/05/25 19:50:28 INFO analysis.Analyzer: Max iterations (2) reached for batch CaseInsensitiveAttributeReferences
14/05/25 19:50:28 INFO hive.metastore: Trying to connect to metastore with URI thrift://xxxxx
14/05/25 19:50:28 INFO hive.metastore: Waiting 1 seconds before next connection attempt.
14/05/25 19:50:29 INFO hive.metastore: Connected to metastore.
java.lang.RuntimeException: Unsupported dataType: struct<adv_id:string,u_lat:double,u_lon:double>
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.hive.HiveMetastoreTypes$.toDataType(HiveMetastoreCatalog.scala:219)
	at org.apache.spark.sql.hive.MetastoreRelation$SchemaAttribute.toAttribute(HiveMetastoreCatalog.scala:273)
	at org.apache.spark.sql.hive.MetastoreRelation$$anonfun$8.apply(HiveMetastoreCatalog.scala:283)
	at org.apache.spark.sql.hive.MetastoreRelation$$anonfun$8.apply(HiveMetastoreCatalog.scala:283)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
{code}",,llai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394751,,,Sun May 25 12:20:40 UTC 2014,,,,,,,,,,"0|i1vytb:",394886,,,,,,,,,,,,,,,,,,,,,,,"25/May/14 12:20;llai;My table uses avro serde. Checked Hive [LanguageManual DDL|https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTable] and [Avro spec|https://avro.apache.org/docs/1.7.6/spec.html#Names], underscore is valid.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In Windows, Spark shell cannot load classes in spark.jars (--jars)",SPARK-1919,12716475,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,24/May/14 00:04,05/Nov/14 10:43,14/Jul/23 06:25,09/Sep/14 22:28,1.0.0,,,,,,,,1.1.1,1.2.0,,,,Windows,,,,,0,,,,,,"Not sure what the issue is, but Spark submit does not have the same problem, even if the jars specified are the same.",,andrewor,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394683,,,Tue Sep 09 22:28:37 UTC 2014,,,,,,,,,,"0|i1vyen:",394820,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,"29/Aug/14 22:40;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2211;;;","02/Sep/14 17:47;andrewor14;Here's a reminder for myself to back port this into branch-1.1 after the release.;;;","09/Sep/14 22:28;andrewor14;Ok, backported to 1.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark shell --py-files does not work for zip files,SPARK-1918,12716468,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,andrewor14,andrewor,23/May/14 23:21,05/Nov/14 10:43,14/Jul/23 06:25,25/May/14 20:52,1.0.0,,,,,,,,1.0.0,,,,,PySpark,,,,,0,,,,,,"For pyspark shell, we never add --py-files to the python path. This is specific to non-python files, because python does not automatically look into zip files even if they are also uploaded to the HTTP server through `sc.addFile`.",,andrewor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394676,,,Fri May 23 23:29:31 UTC 2014,,,,,,,,,,"0|i1vydb:",394814,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 23:29;andrewor;https://github.com/apache/spark/pull/853;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark fails to import functions from {{scipy.special}},SPARK-1917,12716438,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,laserson,laserson,laserson,23/May/14 20:52,31/May/14 22:05,14/Jul/23 06:25,31/May/14 22:05,0.9.0,1.0.0,,,,,,,0.9.2,1.0.1,,,,PySpark,,,,,0,,,,,,"PySpark is able to load {{numpy}} functions, but not {{scipy.special}} functions.  For example  take this snippet:

{code}
from numpy import exp
from scipy.special import gammaln

a = range(1, 11)
b = sc.parallelize(a)
c = b.map(exp)
d = b.map(special.gammaln)
{code}

Calling {{c.collect()}} will return the expected result.  However, calling {{d.collect()}} will fail with

{code}
KeyError: (('gammaln',), <function _getobject at 0x10c0879b0>, ('scipy.special', 'gammaln'))
{code}

in {{cloudpickle.py}} module in {{_getobject}}.

The reason is that {{_getobject}} executes {{__import__(modname)}}, which only loads the top-level package {{X}} in case {{modname}} is like {{X.Y}}.  It is failing because {{gammaln}} is not a member of {{scipy}}.  The fix (for which I will shortly submit a PR) is to add {{fromlist=[attribute]}} to the {{__import__}} call, which will load the innermost module.

See 
[https://docs.python.org/2/library/functions.html#__import__]
and
[http://stackoverflow.com/questions/9544331/from-a-b-import-x-using-import]",,laserson,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394646,,,Fri May 23 21:35:26 UTC 2014,,,,,,,,,,"0|i1vy6v:",394785,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 21:35;laserson;https://github.com/apache/spark/pull/866;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkFlumeEvent with body bigger than 1020 bytes are not read properly,SPARK-1916,12716413,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lemieud,lemieud,lemieud,23/May/14 19:28,03/Jun/14 08:21,14/Jul/23 06:25,28/May/14 22:52,0.9.0,,,,,,,,0.9.2,1.0.1,,,,DStreams,,,,,0,,,,,,"The readExternal implementation on SparkFlumeEvent will read only the first 1020 bytes of the actual body when streaming data from flume.

This means that any event sent to Spark via Flume will be processed properly if the body is small, but will fail if the body is bigger than 1020.
Considering that the default max size for a Flume Avro Event is 32K, the implementation should be updated to read more.

The following is related : http://apache-spark-user-list.1001560.n3.nabble.com/Spark-Streaming-using-Flume-body-size-limitation-tt6127.html",,lemieud,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1998,,,,,,,,,,,"23/May/14 19:35;lemieud;SPARK-1916.diff;https://issues.apache.org/jira/secure/attachment/12646579/SPARK-1916.diff",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394621,,,Wed May 28 22:52:36 UTC 2014,,,,,,,,,,"0|i1vy1r:",394762,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 19:35;lemieud;Attaching a diff for now. I'll create a pull request shortly.;;;","28/May/14 22:52;pwendell;Issue resolved by pull request 865
[https://github.com/apache/spark/pull/865];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AverageFunction should not count if the evaluated value is null.,SPARK-1915,12716311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,23/May/14 11:23,27/May/14 23:59,14/Jul/23 06:25,27/May/14 21:58,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"Average values are difference between the calculation is done partially or not partially.

Because {{AverageFunction}} (in not-partially calculation) counts even if the evaluated value is null.

To reproduce this bug, run the following in {{sbt/sbt hive/console}}:

{code}
scala> sql(""SELECT AVG(key) FROM src1"").collect().foreach(println)
...
== Query Plan ==
Aggregate false, [], [(CAST(SUM(PartialSum#648), DoubleType) / CAST(SUM(PartialCount#649), DoubleType)) AS c0#644]
 Exchange SinglePartition
  Aggregate true, [], [COUNT(key#646) AS PartialCount#649,SUM(key#646) AS PartialSum#648]
   HiveTableScan [key#646], (MetastoreRelation default, src1, None), None), which is now runnable
14/05/28 07:04:33 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 8 (SchemaRDD[45] at RDD at SchemaRDD.scala:98
== Query Plan ==
Aggregate false, [], [(CAST(SUM(PartialSum#648), DoubleType) / CAST(SUM(PartialCount#649), DoubleType)) AS c0#644]
 Exchange SinglePartition
  Aggregate true, [], [COUNT(key#646) AS PartialCount#649,SUM(key#646) AS PartialSum#648]
   HiveTableScan [key#646], (MetastoreRelation default, src1, None), None)
...
[237.06666666666666]

scala> sql(""SELECT AVG(key), COUNT(DISTINCT key) FROM src1"").collect().foreach(println)
...
== Query Plan ==
Aggregate false, [], [AVG(key#672) AS c0#668,COUNT(DISTINCT key#672}) AS c1#669]
 Exchange SinglePartition
  HiveTableScan [key#672], (MetastoreRelation default, src1, None), None), which is now runnable
14/05/28 07:21:31 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 12 (SchemaRDD[67] at RDD at SchemaRDD.scala:98
== Query Plan ==
Aggregate false, [], [AVG(key#672) AS c0#668,COUNT(DISTINCT key#672}) AS c1#669]
 Exchange SinglePartition
  HiveTableScan [key#672], (MetastoreRelation default, src1, None), None)
...
[142.24,15]
{code}

In the first query, {{AVG}} is broke into partial aggregation, and gives the right answer (null values ignored). In the second query, since {{COUNT(DISTINCT key)}} can't be turned into partial aggregation, {{AVG}} isn't either, and the bug is triggered.",,lian cheng,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394519,,,Tue May 27 23:59:01 UTC 2014,,,,,,,,,,"0|i1vxf3:",394660,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 13:01;ueshin;Pull-requested: https://github.com/apache/spark/pull/862;;;","27/May/14 23:59;lian cheng;Added bug reproduction steps.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify CountFunction not to traverse to evaluate all child expressions.,SPARK-1914,12716301,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,23/May/14 10:27,27/May/14 22:01,14/Jul/23 06:25,26/May/14 07:17,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"{{CountFunction}} should count up only if the child's evaluated value is not null.

Because it traverses to evaluate all child expressions, even if the child is null, it counts up if one of the all children is not null.

To reproduce this bug in {{sbt hive/console}}:

{code}
scala> hql(""SELECT COUNT(*) FROM src1"").collect()
res1: Array[org.apache.spark.sql.Row] = Array([25])

scala> hql(""SELECT COUNT(*) FROM src1 WHERE key IS NULL"").collect()
res2: Array[org.apache.spark.sql.Row] = Array([10])

scala> hql(""SELECT COUNT(key + 1) FROM src1"").collect()
res3: Array[org.apache.spark.sql.Row] = Array([25])
{code}

{{res3}} should be 15 since there are 10 null keys.",,lian cheng,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394509,,,Mon May 26 07:10:54 UTC 2014,,,,,,,,,,"0|i1vxd3:",394650,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 10:45;ueshin;Pull-requested: https://github.com/apache/spark/pull/861;;;","26/May/14 07:10;lian cheng;Added steps to reproduce this bug in {{sbt hive/console}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet table column pruning error caused by filter pushdown,SPARK-1913,12716288,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,crazyjvm,crazyjvm,23/May/14 08:39,14/Jun/14 06:27,14/Jul/23 06:25,29/May/14 05:14,1.1.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"When scanning Parquet tables, attributes referenced only in predicates that are pushed down are not passed to the `ParquetTableScan` operator and causes exception. Verified in the {{sbt hive/console}}:

{code}
loadTestTable(""src"")
table(""src"").saveAsParquetFile(""src.parquet"")
parquetFile(""src.parquet"").registerAsTable(""src_parquet"")
hql(""SELECT value FROM src_parquet WHERE key < 10"").collect().foreach(println)
{code}

Exception
{code}
parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/scratch/rxin/spark/src.parquet/part-r-2.parquet
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:177)
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:130)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:122)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:717)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:717)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1080)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1080)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IllegalArgumentException: Column key does not exist.
	at parquet.filter.ColumnRecordFilter$1.bind(ColumnRecordFilter.java:51)
	at org.apache.spark.sql.parquet.ComparisonFilter.bind(ParquetFilters.scala:306)
	at parquet.io.FilteredRecordReader.<init>(FilteredRecordReader.java:46)
	at parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:74)
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:110)
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:172)
	... 28 more
{code}",mac os 10.9.2,crazyjvm,lian cheng,rxin,schumach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394496,,,Sat May 24 19:34:16 UTC 2014,,,,,,,,,,"0|i1vxa7:",394637,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 10:32;lian cheng;Attributes referenced only in those filters that are pushed down are not considered when building the {{ParquetTableScan}} operator in {{ParquetOperations}}. Will submit a PR for this.;;;","23/May/14 11:34;lian cheng;Corresponding PR: https://github.com/apache/spark/pull/863;;;","23/May/14 21:13;rxin;I added the exception.;;;","24/May/14 19:34;schumach;Sigh.. can't believe I missed this when adding the original tests. Good that there is a fix now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression memory issue during reduce,SPARK-1912,12716259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,23/May/14 03:54,28/Aug/14 08:07,14/Jul/23 06:25,03/Jun/14 20:19,,,,,,,,,0.9.2,1.0.1,1.1.0,,,Spark Core,,,,,0,,,,,,"When we need to read a compressed block, we will first create a compress stream instance(LZF or Snappy) and use it to wrap that block.
Let's say a reducer task need to read 1000 local shuffle blocks, it will first prepare to read that 1000 blocks, which means create 1000 compression stream instance to wrap them. But the initialization of compression instance will allocate some memory and when we have many compression instance at the same time, it is a problem.
Actually reducer reads the shuffle blocks one by one, so why we create compression instance at the first time? Can we do it lazily that when a block is first read, create compression instance for it.",,aash,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394468,,,Thu Aug 28 08:07:31 UTC 2014,,,,,,,,,,"0|i1vx3z:",394609,,,,,,,,,,,,,0.9.2,1.0.1,1.1.0,,,,,,,,"23/May/14 15:13;aash;https://github.com/apache/spark/pull/860;;;","28/Aug/14 08:07;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2179;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warn users if their assembly jars are not built with Java 6,SPARK-1911,12716236,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,andrewor14,andrewor,22/May/14 23:57,11/Oct/17 01:20,14/Jul/23 06:25,04/Mar/15 11:44,1.1.0,,,,,,,,1.2.2,1.3.0,,,,Documentation,,,,,0,,,,,,"The root cause of the problem is detailed in: https://issues.apache.org/jira/browse/SPARK-1520.

In short, an assembly jar built with Java 7+ is not always accessible by Python or other versions of Java (especially Java 6). If the assembly jar is not built on the cluster itself, this problem may manifest itself in strange exceptions that are not trivial to debug. This is an issue especially for PySpark on YARN, which relies on the python files included within the assembly jar.

Currently we warn users only in make-distribution.sh, but most users build the jars directly. At the very least we need to emphasize this in the docs (currently missing entirely). The next step is to add a warning prompt in the mvn scripts whenever Java 7+ is detected.",,airhorns,andrewor,apachespark,donnchadh,dougb,naven084k,stevel@apache.org,Swaapnika,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1520,,,SPARK-1753,SPARK-1703,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394445,,,Wed Oct 11 01:20:27 UTC 2017,,,,,,,,,,"0|i1vwz3:",394586,,,,,,,,,,,,,,,,,,,,,,,"25/May/14 01:36;tdas;https://github.com/apache/spark/pull/859/ adds a warning in make-distribution.sh if they try to compile spark with java 6.

Note that the actual problem of making Java 7 compiled JARs work with Python still needs to be solved.;;;","29/May/14 23:26;airhorns;Do we know why Python is unable to read Java 7 jars, and only sometimes? ;;;","30/May/14 00:29;tdas;As far as I think, it is because Java 7 uses Zip64 encoding when making JARs with more 2^16 files and python (at least 2.x) is not able to read Zip64. So it fails in those times when the Spark assembly JAR has more than 65k files, which in turn depends on whether it has been generated with YARN and/or Hive enabled.

Java 6 uses the traditional Zip format to create JARs, even if it has more than 65k files. So python always seems to work with Java 6 Jars

Caveat: I cant claim 100% certainty on this interpretation because there is so little documentation on this on the net.;;;","24/Jun/14 18:45;andrewor;Looks like we still haven't fixed this. At the very least we should explicitly add this warning in the documentation. The next step that is more involved is to add a warning prompt in the mvn build scripts whenever Java 7+ is detected.;;;","01/Jan/15 13:27;naven084k;Hi,  I have built spark assembly jar with java 6 using make-distribution.sh and started spark cluster on 2 nodes(which are on unix boxes)  I am able to execute java programs on the cluster.  Now I am able to connect to the cluster from my windows machine using pyspark interactive shell  Bin> pyspark –master spark://master:7078  And then I am trying to execute following commands at interactive shell  lines = sc.textFile(""hdfs://master/data/spark/SINGLE.TXT"") lineLengths = lines.map(lambda s: len(s)) totalLength = lineLengths.reduce(lambda a, b: a + b)  It is throwing the following error  Traceback (most recent call last): File """", line 1, in  File ""C:\Users\npokala\Downloads\spark-java\spark-master\python\pyspark\rdd.py"", line 715, in reduce vals = self.mapPartitions(func).collect() File ""C:\Users\npokala\Downloads\spark-java\spark-master\python\pyspark\rdd.py"", line 676, in collect bytesInJava = self.jrdd.collect().iterator() File ""C:\Users\npokala\Downloads\spark-java\spark-master\python\lib\py4j-0.8.2.1-src.zip\py4j\java_gateway.py"", line 538, in __call_ 15/01/01 18:29:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on master:34586 (size: 3.9 KB, free: 1060.0 MB) File ""C:\Users\npokala\Downloads\spark-java\spark-master\python\lib\py4j-0.8.2.1-src.zip\py4j\protocol.py"", line 300, in get_return_value py4j.protocol.Py4JJavaError: An error occurred while calling o24.collect. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 7, master): org.apache.spark.SparkException: Error from python worker: python: module pyspark.daemon not found PYTHONPATH was: /home/npokala/data/spark-install/spark-java-1.6/spark-master/python:/home/npokala/data/spark-install/spark-java-1.6/spark-master/python/lib/py4j-0.8.2.1-src.zip:/home/npokala/data/spark-install/spark-java-1.6/spark-master/assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop2.4.0.jar:/home/npokala/data/spark-install/spark-java-1.6/spark-master/sbin/../python/lib/py4j-0.8.2.1-src.zip:/home/npokala/data/spark-install/spark-java-1.6/spark-master/sbin/../python: java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:392) at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163) at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:86) at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:62) at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:102) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:265) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61) at org.apache.spark.scheduler.Task.run(Task.scala:56) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)  Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696) at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) at akka.dispatch.Mailbox.run(Mailbox.scala:220) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  Is there any pyspark daemons do I need to start on cluster or am I missing something else;;;","03/Mar/15 14:40;srowen;[~andrewor14] {{compute-classpath.sh}} will now show a warning in this situation (cf. SPARK-1703). I will send a PR for the doc change. Is this the same issue as SPARK-1753?;;;","03/Mar/15 14:41;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4874;;;","04/Mar/15 11:09;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4888;;;","20/Apr/15 09:51;stevel@apache.org;This doesn't fix the problem, merely documents it.

It should be doable by using Ant's <zip> task, which doesn't use the JDK zip routines. The assembly would be unzipped first, then zipped with zip63 option set to never

see [https://ant.apache.org/manual/Tasks/zip.html]

;;;","20/Apr/15 09:55;srowen;[~stevel@apache.org] Yeah this is mostly duplicating https://issues.apache.org/jira/browse/SPARK-1703 which has an actual check and warning. I think this JIRA/PR ended up just being about the follow-on doc change.;;;","11/Oct/17 01:20;Swaapnika;Does this issue still exist with Spark-2.2.? ;;;",,,,,,,,,,,,,,,,,,,,,,
Spark shell prints error when :4040 port already in use,SPARK-1902,12716160,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ash211@gmail.com,aash,aash,22/May/14 18:35,12/May/17 17:37,14/Jul/23 06:25,21/Jun/14 01:26,1.0.0,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"When running two shells on the same machine, I get the below error.  The issue is that the first shell takes port 4040, then the next tries tries 4040 and fails so falls back to 4041, then a third would try 4040 and 4041 before landing on 4042, etc.

We should catch the error and instead log as ""Unable to use port 4041; already in use.  Attempting port 4042...""

{noformat}
14/05/22 11:31:54 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: Address already in use
java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:444)
        at sun.nio.ch.Net.bind(Net.java:436)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
        at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
        at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
        at org.eclipse.jetty.server.Server.doStart(Server.java:293)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
        at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
        at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
        at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
        at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
        at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:217)
        at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:957)
        at $line3.$read$$iwC$$iwC.<init>(<console>:8)
        at $line3.$read$$iwC.<init>(<console>:14)
        at $line3.$read.<init>(<console>:16)
        at $line3.$read$.<init>(<console>:20)
        at $line3.$read$.<clinit>(<console>)
        at $line3.$eval$.<init>(<console>:7)
        at $line3.$eval$.<clinit>(<console>)
        at $line3.$eval.$print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:788)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1056)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:614)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:645)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
        at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:121)
        at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:120)
        at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:263)
        at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:120)
        at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:913)
        at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:142)
        at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:104)
        at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:930)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:983)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:278)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{noformat}",,aash,dud,fathersson,hhadi,pwendell,sinisa_lyh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13451,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394369,,,Fri May 12 17:37:33 UTC 2017,,,,,,,,,,"0|i1vwiv:",394508,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 20:51;pwendell;Yeah, this would be a good one to fix. IIRC I spent a long time trying to figure out how to silence only this message, but I couldn't do anything except for silencing all jetty WARN logs (which we don't want). I also considered first checking if the port is free before trying to bind to it, but that has race conditions. If someone figures out a better way to do this, that would be great.;;;","09/Jun/14 07:05;aash;https://github.com/apache/spark/pull/1019;;;","21/Jun/14 01:26;pwendell;Issue resolved by pull request 1019
[https://github.com/apache/spark/pull/1019];;;","14/Jul/15 17:51;hhadi;I am still seeing this error , I have spark running on Mesos. Any idea how to fix it ?
15/07/14 17:41:05 WARN AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use

java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:437)
        at sun.nio.ch.Net.bind(Net.java:429)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
        at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
        at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
        at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
        at org.spark-project.jetty.server.Server.doStart(Server.java:293)
        at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
        at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)
        at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
        at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
        at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
        at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)
        at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)
        at org.apache.spark.ui.WebUI.bind(WebUI.scala:117)
        at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
        at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:448)
        at com.myclass.MyClassTest$class.build_context(MyClassTes.scala:233)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/07/14 17:41:05 WARN AbstractLifeCycle: FAILED org.spark-project.jetty.server.Server@35fe2125: java.net.BindException: Address already in use

Here is the line of code that is causing it :  val sparkContext = new org.apache.spark.SparkContext(conf)

;;;","12/May/17 17:37;dud;Hello

I'm also getting this long warning message on Spark 2.1.1.
I just copied log4j.properties.template to log4j.properties and this long stackstrace is now gone.
I don't know why this log4j configuration is not applied by default.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone worker update exector's state ahead of executor process exit,SPARK-1901,12715946,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhpengg,zhpengg,zhpengg,22/May/14 08:52,30/May/14 17:13,14/Jul/23 06:25,30/May/14 17:13,0.9.0,,,,,,,,1.0.1,,,,,Deploy,,,,,0,,,,,,"Standalone worker updates executor's state prematurely, making the resource status in an inconsistent state until the executor process really died.

In our cluster, we found this situation may cause new submitted applications removed by Master for launching executor fail.",spark-1.0 rc10,zhpengg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394231,,,Thu May 22 09:04:56 UTC 2014,,,,,,,,,,"0|i1vvnz:",394369,,,,,,,,,,,,,,,,,,,,,,,"22/May/14 09:04;zhpengg;https://github.com/apache/spark/pull/854;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark shell --jars (or spark.jars) doesn't work,SPARK-1897,12715852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,andrewor14,andrewor,21/May/14 21:04,05/Nov/14 10:43,14/Jul/23 06:25,23/May/14 03:26,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"This is being overridden by ADD_JARS in SparkILoop.scala. However, ADD_JARS is outdated and not documented anywhere.",,andrewor,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394137,,,Fri May 23 03:26:44 UTC 2014,,,,,,,,,,"0|i1vv3b:",394275,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 03:26;tdas;https://github.com/apache/spark/pull/849;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MASTER masks spark.master in spark-shell,SPARK-1896,12715824,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,andrewor14,andrewor,21/May/14 18:30,05/Nov/14 10:43,14/Jul/23 06:25,23/May/14 03:33,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"For bin/spark-shell, the priority hierarchy is
{code}
MASTER > --master > spark.master (spark-defaults.conf)
{code}

Not sure if this was the intended behavior, but it was surprising to me when I tried to run the shell on local mode, and it didn't take effect. Note that this is inconsistent with running applications, where the hierarchy is

{code}
--master > spark.master (spark-defaults.conf) > MASTER
{code}

I think the MASTER is a good cluster default, but if the user explicitly specifies a master we should obey it, like we do in normal applications.",,andrewor,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394109,,,Fri May 23 03:33:32 UTC 2014,,,,,,,,,,"0|i1vux3:",394247,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 03:33;tdas;https://github.com/apache/spark/pull/846/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apply splitConjunctivePredicates to join condition while finding join keys.,SPARK-1889,12715384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,20/May/14 09:02,21/May/14 22:38,14/Jul/23 06:25,21/May/14 22:38,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"When tables are equi-joined by multiple-keys {{HashJoin}} should be used, but {{CartesianProduct}} and then {{Filter}} are used.
The join keys are paired by {{And}} expression so we need to apply {{splitConjunctivePredicates}} to join condition while finding join keys.",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393670,,,Tue May 20 10:42:46 UTC 2014,,,,,,,,,,"0|i1vsen:",393833,,,,,,,,,,,,,,,,,,,,,,,"20/May/14 10:42;ueshin;Pull-requested: https://github.com/apache/spark/pull/836;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
workers keep dying for uncaught exception of executor id not found ,SPARK-1886,12715343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhpengg,zhpengg,zhpengg,20/May/14 01:26,25/May/14 03:45,14/Jul/23 06:25,25/May/14 03:45,0.9.0,,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"14/05/19 15:43:30 ERROR OneForOneStrategy: key not found: app-20140519154218-0132/6
java.util.NoSuchElementException: key not found: app-20140519154218-0132/6
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at scala.collection.AbstractMap.default(Map.scala:58)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:64)
        at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.applyOrElse(Worker.scala:266)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",spark-1.0-rc8,zhpengg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393629,,,Tue May 20 01:30:28 UTC 2014,,,,,,,,,,"0|i1vs4n:",393788,,,,,,,,,,,,,,,,,,,,,,,"20/May/14 01:30;zhpengg;https://github.com/apache/spark/pull/827;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark graph.triplets does not return correct values,SPARK-1883,12715278,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,glenn.strycker,glenn.strycker,19/May/14 20:16,29/May/14 06:54,14/Jul/23 06:25,19/May/14 20:46,,,,,,,,,,,,,,,,,,,0,,,,,,"graph.triplets does not work -- it returns incorrect results 

I have a graph with the following edges: 

orig_graph.edges.collect 
=  Array(Edge(1,4,1), Edge(1,5,1), Edge(1,7,1), Edge(2,5,1), Edge(2,6,1), Edge(3,5,1), Edge(3,6,1), Edge(3,7,1), Edge(4,1,1), Edge(5,1,1), Edge(5,2,1), Edge(5,3,1), Edge(6,2,1), Edge(6,3,1), Edge(7,1,1), Edge(7,3,1)) 

When I run triplets.collect, I only get the last edge repeated 16 times: 

orig_graph.triplets.collect 
= Array(((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1)) 

I've also tried writing various map steps first before calling the triplet function, but I get the same results as above. 

Similarly, the example on the graphx programming guide page (http://spark.apache.org/docs/0.9.0/graphx-programming-guide.html) is incorrect. 

val facts: RDD[String] = 
  graph.triplets.map(triplet => 
    triplet.srcAttr._1 + "" is the "" + triplet.attr + "" of "" + triplet.dstAttr._1) 

does not work, but 

val facts: RDD[String] = 
  graph.triplets.map(triplet => 
    triplet.srcAttr + "" is the "" + triplet.attr + "" of "" + triplet.dstAttr) 

does work, although the results are meaningless.  For my graph example, I get the following line repeated 16 times: 

1 is the 1 of 1",,glenn.strycker,glenn.strycker@gmail.com,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,SPARK-1188,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393564,,,Mon May 19 20:46:29 UTC 2014,,,,,,,,,,"0|i1vrqn:",393725,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 20:45;glenn.strycker;Sorry, this has been fixed -- https://issues.apache.org/jira/browse/SPARK-1188

Thanks to rxin for pointing this out on my email list question http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-td6693.html

-----


This was an optimization that reuses a triplet object in GraphX, and when 
you do a collect directly on triplets, the same object is returned. 

It has been fixed in Spark 1.0 here: 
https://issues.apache.org/jira/browse/SPARK-1188

To work around in older version of Spark, you can add a copy step to it, 
e.g. 

graph.triplets.map(_.copy()).collect() ;;;","19/May/14 20:46;glenn.strycker;already fixed -- user is running an old version of Spark;;;","19/May/14 20:46;glenn.strycker;issue fixed -- user was running an older version of Spark;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default PermGen size too small when using Hadoop2 and Hive,SPARK-1879,12715108,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,matei,matei,matei,19/May/14 07:20,20/May/14 01:44,14/Jul/23 06:25,20/May/14 01:44,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"If you launch a spark-shell with Hadoop 2 and Hive on the classpath, and try to use Hive therein, the PermGen quickly reaches 85 MB after a few commands, at which point Java gives up and freezes. We should pass a MaxPermSize to prevent this. Unfortunately passing this results in a warning on Java 8, but that's still better than not passing it.

I don't think this affects stuff other than the shell; it's just the combination of Scala compiler + Hive + Hadoop 2 that pushes things over the edge.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393394,,,Mon May 19 07:37:27 UTC 2014,,,,,,,,,,"0|i1vqpb:",393556,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 07:25;matei;BTW the warning on Java 8 is the following:

{code}
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0
{code}

Not *that* scary. I think we can document it and live with this for a bit and possibly test java -version in a later release.;;;","19/May/14 07:37;matei;https://github.com/apache/spark/pull/823;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect initialization order in JavaStreamingContext,SPARK-1878,12715103,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zsxwing,zsxwing,19/May/14 07:02,20/May/14 01:27,14/Jul/23 06:25,20/May/14 01:27,0.9.0,0.9.1,1.0.0,,,,,,1.0.0,,,,,DStreams,,,,,0,easyfix,,,,,"sc will be null because it is initialized before sparkContext

  @deprecated(""use sparkContext"", ""0.9.0"")
  val sc: JavaSparkContext = sparkContext

  /** The underlying SparkContext */
  val sparkContext = new JavaSparkContext(ssc.sc)",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393389,,,Mon May 19 07:07:36 UTC 2014,,,,,,,,,,"0|i1vqo7:",393551,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 07:07;zsxwing;PR: https://github.com/apache/spark/pull/822;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException when loading RDD with serialized objects,SPARK-1877,12715100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ghidi,ghidi,19/May/14 06:35,20/May/14 05:36,14/Jul/23 06:25,20/May/14 05:36,1.0.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"When I load a RDD that has custom serialized objects, Spark throws ClassNotFoundException. This happens only when Spark is deployed as a standalone cluster, it works fine when Spark is local.

I debugged the issue and I noticed that ObjectInputStream.resolveClass does not use ExecutorURLClassLoader set by SparkSubmit. You have to explicitly set the classloader in SparkContext.objectFile for ObjectInputStream when deserializing objects.
Utils.deserialize[Array[T]](...., Thread.currentThread.getContextClassLoader)

I will attach a patch shortly...","standalone Spark cluster, jdk 1.7",darabos,ghidi,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393386,,,Mon May 19 23:52:55 UTC 2014,,,,,,,,,,"0|i1vqnj:",393548,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 06:43;ghidi;Submitted the patch as a GitHub pull request.
https://github.com/apache/spark/pull/821;;;","19/May/14 23:52;tdas;Can you please give us the steps to reproduce this problem. I am guessing this can be reproduced using a local standalone cluster.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Windows scripts to deal with latest distribution layout changes,SPARK-1876,12715069,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,matei,matei,matei,19/May/14 00:24,20/May/14 01:26,14/Jul/23 06:25,20/May/14 01:26,,,,,,,,,1.0.0,,,,,Windows,,,,,0,,,,,,,,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393355,,,2014-05-19 00:24:35.0,,,,,,,,,,"0|i1vqgn:",393517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoClassDefFoundError: StringUtils when building against Hadoop 1,SPARK-1875,12715068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gq,matei,matei,19/May/14 00:16,20/May/14 02:41,14/Jul/23 06:25,20/May/14 02:41,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"Maybe I missed something, but after building an assembly with Hadoop 1.2.1 and Hive enabled, if I go into it and run spark-shell, I get this:

{code}
java.lang.NoClassDefFoundError: org/apache/commons/lang/StringUtils
	at org.apache.hadoop.metrics2.lib.MetricMutableStat.<init>(MetricMutableStat.java:59)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.<init>(MetricsSystemImpl.java:75)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.<init>(MetricsSystemImpl.java:120)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<init>(DefaultMetricsSystem.java:37)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<clinit>(DefaultMetricsSystem.java:34)
	at org.apache.hadoop.security.UgiInstrumentation.create(UgiInstrumentation.java:51)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:216)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:184)
	at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:236)
	at org.apache.hadoop.security.KerberosName.<clinit>(KerberosName.java:79)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:209)
	at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:226)
	at org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:36)
	at org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:109)
	at org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:228)
{code}",,gq,matei,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393354,,,Mon May 19 07:46:21 UTC 2014,,,,,,,,,,"0|i1vqgf:",393516,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 00:47;matei;This may have been broken by https://issues.apache.org/jira/browse/SPARK-1629 / https://github.com/apache/spark/pull/569, which added an explicit dependency on commons-lang, though it's not clear.;;;","19/May/14 00:48;matei;Sorry I meant https://github.com/apache/spark/pull/635;;;","19/May/14 00:49;matei;It may also just be that security needs other JARs on Hadoop 1.;;;","19/May/14 01:19;matei;Actually, upon a closer look, it seems to be caused by including Hive. Hive probably brings in a newer version of either hadoop-client or something that depends on, which brings in new classes here without their dependencies. I'm fairly sure that Hive 0.12 can work on Hadoop 1, but if it doesn't, it means we have to relegate the Hive support to only newer versions of Hadoop.;;;","19/May/14 02:08;pwendell;The issue was caused by this patch. I need to look further to figure out what was going on.

https://github.com/apache/spark/pull/754;;;","19/May/14 02:17;gq;[~pwendell]
This PR 754  should not associated with hadoop 1, concerned only with hadoop 2 and hive.;;;","19/May/14 02:47;gq;[~matei]  I can not reproduce the bug.;;;","19/May/14 03:25;pwendell;[~witgo]. Here is how I reproduced it:

{code}
./make-distribution.sh --with-hive --tgz
{code}

Then run spark-shell from the distribution. This is mostly equivalent to running

{code}
mvn package -Phive -DskipTests
{code};;;","19/May/14 04:08;pwendell;The issue here is that somehow the commons-lang exclusion from the hive project is being respected when building an assembly for Hadoop 1. So it's excluded from hadoop-client even though hadoop-client 1.0.4 depends on it.

{code}
mvn -Phive install
mvn -pl assembly -Phive  dependency:tree 
[INFO] Scanning for projects...
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Assembly 1.0.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ spark-assembly_2.10 ---
[INFO] org.apache.spark:spark-assembly_2.10:pom:1.0.1-SNAPSHOT
[INFO] +- org.apache.spark:spark-core_2.10:jar:1.0.1-SNAPSHOT:compile
[INFO] |  +- org.apache.hadoop:hadoop-client:jar:1.0.4:compile
[INFO] |  |  \- org.apache.hadoop:hadoop-core:jar:1.0.4:compile
[INFO] |  |     +- xmlenc:xmlenc:jar:0.52:compile
[INFO] |  |     +- org.apache.commons:commons-math:jar:2.1:compile
[INFO] |  |     +- commons-el:commons-el:jar:1.0:compile
[INFO] |  |     +- hsqldb:hsqldb:jar:1.8.0.10:compile
[INFO] |  |     \- oro:oro:jar:2.0.8:compile
[INFO] |  +- net.java.dev.jets3t:jets3t:jar:0.7.1:runtime
[INFO] |  |  +- commons-codec:commons-codec:jar:1.3:compile
[INFO] |  |  \- commons-httpclient:commons-httpclient:jar:3.1:runtime
[INFO] |  +- org.apache.curator:curator-recipes:jar:2.4.0:compile
[INFO] |  |  +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO] |  |  |  \- org.apache.curator:curator-client:jar:2.4.0:compile
[INFO] |  |  \- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO] |  +- org.eclipse.jetty:jetty-plus:jar:8.1.14.v20131031:compile
[INFO] |  |  +- org.eclipse.jetty.orbit:javax.transaction:jar:1.1.1.v201105210645:compile
[INFO] |  |  +- org.eclipse.jetty:jetty-webapp:jar:8.1.14.v20131031:compile
[INFO] |  |  |  +- org.eclipse.jetty:jetty-xml:jar:8.1.14.v20131031:compile
[INFO] |  |  |  \- org.eclipse.jetty:jetty-servlet:jar:8.1.14.v20131031:compile
[INFO] |  |  \- org.eclipse.jetty:jetty-jndi:jar:8.1.14.v20131031:compile
[INFO] |  |     \- org.eclipse.jetty.orbit:javax.mail.glassfish:jar:1.4.1.v201005082020:compile
[INFO] |  |        \- org.eclipse.jetty.orbit:javax.activation:jar:1.1.0.v201105071233:compile
[INFO] |  +- org.eclipse.jetty:jetty-security:jar:8.1.14.v20131031:compile
[INFO] |  +- org.eclipse.jetty:jetty-util:jar:8.1.14.v20131031:compile
[INFO] |  +- org.eclipse.jetty:jetty-server:jar:8.1.14.v20131031:compile
[INFO] |  |  +- org.eclipse.jetty.orbit:javax.servlet:jar:3.0.0.v201112011016:compile
[INFO] |  |  +- org.eclipse.jetty:jetty-continuation:jar:8.1.14.v20131031:compile
[INFO] |  |  \- org.eclipse.jetty:jetty-http:jar:8.1.14.v20131031:compile
[INFO] |  |     \- org.eclipse.jetty:jetty-io:jar:8.1.14.v20131031:compile
{code}

If you run
{code}
mvn -pl assembly dependency:tree 
{code}
it includes commons-lang correctly.;;;","19/May/14 04:23;gq;[~pwendell], [~matei]
Do you have time to review the code?
https://github.com/apache/spark/pull/820;;;","19/May/14 06:36;srowen;Here's my recap of what I understand:

- All the original changes about `commons-lang` were to ensure that Spark itself uses `commons-lang3` and declares the dependency. That's OK and not related.
- The PR https://github.com/apache/spark/pull/754 was intended to mirror my changes in https://github.com/apache/spark/pull/746/files for SBT, but I didn't look closely enough: it actually also excludes `commons-lang`
- I don't know of a reason we need to deal with `commons-lang` directly. It *should* exist in the built assembly since dependencies need it. The version resolution among various versions of 2.x is all fine AFAICT, so we don't need (or do) any manual version setting.

In short I do not see why `commons-lang` is excluded? I think this exclusion should simply be reverted.;;;","19/May/14 06:41;matei;I see, it might be fine to just remove the exclusion then. I agree it doesn't seem necessary to add it. In particular I believe lang3 and lang actually use different package names.;;;","19/May/14 06:43;srowen;(That's correct that commons-lang and commons-lang3 use separate packages.);;;","19/May/14 06:44;matei;BTW feel free to just try it and send a pull request. The important things to test are:

- make-distribution.sh with the settings above works, either with or without hive (specifically check that the build lets you actually launch spark-shell)
- apps built against spark-core_2.10 1.0.0-SNAPSHOT work
- apps built against spark-core_2.10 1.0.0-SNAPSHOT plus spark-hive_2.10 also work

I'll take a closer look at it tomorrow but this is a fair number of things to test. In the future we can automate some of this testing. I've usually just built some apps manually against the new version.;;;","19/May/14 07:24;gq;||hadoop or hive ||commons-lang||
|hive 0.12     | 2.4|
|hadoop-core 1.x| 2.4|
|hadoop-common 0.23,2.2| 2.5|
|hadoop-common 2.3,2.4 | 2.6|
 
 ;;;","19/May/14 07:31;srowen;Yeah, my experience ""in the trenches"" suggests that commons-lang 2.x is well-behaved with respect to minor versions. It will be OK for dependency resolution to pick 2.6, for example, and have it work with components that want 2.4. ;;;","19/May/14 07:46;gq;[~srowen] 
How about [the PR 824 |https://github.com/apache/spark/pull/824] ?
;;;",,,,,,,,,,,,,,,,
Clean up MLlib sample data,SPARK-1874,12715061,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,matei,matei,18/May/14 22:01,20/May/14 04:30,14/Jul/23 06:25,20/May/14 04:30,,,,,,,,,1.0.0,,,,,MLlib,,,,,0,,,,,,"- Replace logistic regression example data with linear to make mllib.LinearRegression example easier to run
- Move files from mllib/data into data/mllib to make them easier to find
- Add a simple MovieLens data file",,matei,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393347,,,Mon May 19 22:55:29 UTC 2014,,,,,,,,,,"0|i1vqev:",393509,,,,,,,,,,,,,,,,,,,,,,,"18/May/14 23:17;mengxr;Is `data/mllib` a better place than `mllib/data`?;;;","19/May/14 20:51;matei;Yes, cause there's other stuff in `data`. I think it's a more obvious location.;;;","19/May/14 22:55;mengxr;There are three files under `data/`: `kmeans_data.txt`, `lr_data.txt`, and `pagerank_data.txt`, while more files under `mllib/data`. It feels more natural to me to keep the sample data under `mllib/data`. Anyway, I will create sample data first.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`spark-shell --help` fails if called from outside spark home,SPARK-1869,12714995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,pwendell,pwendell,18/May/14 05:31,18/May/14 20:39,14/Jul/23 06:25,18/May/14 20:39,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Spark Core,,,,,0,,,,,,"When a user runs the shell with `--help` from outside of the Spark directory, it doesn't call spark-submit in the direct location:

{code}
$ /home/patrick/Documents/spark/bin/spark-shell --help
Usage: ./bin/spark-shell [options]
/home/patrick/Documents/spark/bin/spark-shell: line 33: ./bin/spark-submit: No such file or directory
{code}

The fix is simple, we should just use the full path as in other places where we invoke the shell.",,pwendell,techaddict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393308,,,Sun May 18 20:39:22 UTC 2014,,,,,,,,,,"0|i1vq67:",393470,,,,,,,,,,,,,,,,,,,,,,,"18/May/14 20:24;techaddict;https://github.com/apache/spark/pull/817;;;","18/May/14 20:39;pwendell;Fixed by: https://github.com/apache/spark/pull/812;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Classpath not correctly sent to executors.,SPARK-1864,12714921,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,17/May/14 01:13,17/May/14 06:11,14/Jul/23 06:25,17/May/14 03:25,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,,,marmbrus,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393234,,,Sat May 17 03:25:43 UTC 2014,,,,,,,,,,"0|i1vppr:",393396,,,,,,,,,,,,,,,,,,,,,,,"17/May/14 01:20;marmbrus;https://github.com/apache/spark/pull/808;;;","17/May/14 03:25;pwendell;Issue resolved by pull request 808
[https://github.com/apache/spark/pull/808];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add build support for MapR,SPARK-1862,12714703,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,16/May/14 05:02,16/May/14 06:32,14/Jul/23 06:25,16/May/14 06:32,,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"Would be nice to add support for some of the other distro's in the build. MapR is one that I've done before, so it's a starting point.",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393016,,,Fri May 16 06:32:36 UTC 2014,,,,,,,,,,"0|i1voev:",393183,,,,,,,,,,,,,,,,,,,,,,,"16/May/14 06:32;pwendell;Issue resolved by pull request 803
[https://github.com/apache/spark/pull/803];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone Worker cleanup should not clean up running executors,SPARK-1860,12714679,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,ilikerps,ilikerps,16/May/14 00:38,26/Feb/15 01:01,14/Jul/23 06:25,03/Oct/14 21:26,1.0.0,,,,,,,,1.2.0,,,,,Deploy,,,,,1,,,,,,"The default values of the standalone worker cleanup code cleanup all application data every 7 days. This includes jars that were added to any executors that happen to be running for longer than 7 days, hitting streaming jobs especially hard.

Executor's log/data folders should not be cleaned up if they're still running. Until then, this behavior should not be enabled by default.",,aash,apachespark,cfregly,ian.springer,ilikerps,markhamstra,mcheah,mkim,pwendell,romi-totango,,,,,,,,,,,,,,,,,,,SPARK-3805,,SPARK-786,,SPARK-1154,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392992,,,Sun Oct 05 21:31:08 UTC 2014,,,,,,,,,,"0|i1vo9r:",393160,,,,,,,,,,,,,1.2.0,,,,,,,,,,"16/May/14 04:38;pwendell;Issue resolved by pull request 800
[https://github.com/apache/spark/pull/800];;;","16/May/14 04:56;pwendell;The PR actually just disabled it, it didn't fix this.;;;","16/May/14 04:59;pwendell;I think it would be better to only start the TTL once an executor has finished and to only delete the specific folder used by the executor.;;;","18/May/14 18:26;aash;[~mkim] is going to take a look at this after discussion at https://issues.apache.org/jira/browse/SPARK-1154

I think the correct fix as Patrick outlines would be:

{code}
// pseudocode
for folder in onDiskFolders:
    if folder is owned by a running application:
        continue
    if folder contains any folder/file (recursively) that is more recently touched (mtime) than the TTS:
        continue
    cleanUp(folder)
{code}

Schedule that to run periodically (interval configured by setting) and this should be all fixed up.

Is that right?

An alternative approach could be to have executor clean up the application's work directory when the application terminates, but un-clean executor shutdown could still leave work directories around so a TTL approach still needs to be included as well.;;;","21/May/14 15:10;mkim;[~aash], is there a reliable way to check ""folder is owned by a running application""? I thought that's not possible, so I was just going to have the second if statement, which means folder for running applications that just haven't been active for TTS will also get wiped out, assuming that executor is writing out something to either stdout or stderr when it runs some computation.

This also means that if you have a long-running inactive application, the application should send a ""heartbeat"" by running a trivial computation once every while.

Any suggestions?;;;","22/May/14 04:16;aash;So the Spark master webui shows the running applications, so it at least knows what's running.  I guess since this is running on a worker it may need to be told by the master what the active applications are.  Not sure the internals of Spark very well but there's got to be a way to determine this.;;;","29/Jun/14 07:49;mkim;[~pwendell], would there be an easy way to tell from the worker node whether an app directory is active or not? In other words, can a worker node get the list of active application ids from the master? I thought this was not doable, so was just going to wipe out all app directories that haven't been used (i.e. no jobs have run even if the the application is still alive) based on the last modified date of the log files. What do you think?;;;","28/Jul/14 05:28;mkim;Friendly ping? [~pwendell] Can you let me know if there is an easy way to tell from the worker node whether an app is active? Or, should we just go with the rather-fragile design as I proposed right above?;;;","28/Jul/14 17:10;ilikerps;There's not an easy way to tell if an application is still running. However, the Worker has state about which executors are still running. This is really what I intended originally -- we must not clean up an Executor's own state from underneath it. I will change the title to reflect this intention.;;;","28/Jul/14 20:29;markhamstra;I don't think that there is much in the way of conflict, but something to be aware of is that the proposed fix to SPARK-2425 does modify Executor state transitions and cleanup: https://github.com/apache/spark/pull/1360;;;","29/Sep/14 22:22;mcheah;Apologies for any naivety - this will be the first issue I tackle as a Spark contributor.

Mingyu and I had a short chat and we thought it would be reasonable for the Executor to simply clean up its own state when it shuts down. Is there anything preventing Executor.stop() from cleaning up the app directory it was using?;;;","29/Sep/14 23:54;aash;Cleanup on executor shutdown is part of the solution (and should be done IMO) but not all of it.

Particularly it won't cover when an executor dies from an OOM or a kill -9 or any other unclean shutdown.  The perfect solution would do the event-based cleanup self on executor shutdown, and also a periodic cleaner to get rid of directories that were shutdown uncleanly.;;;","30/Sep/14 00:45;mcheah;ExecutorRunner seems to have various cases corresponding to how the Executor exited. ExecutorRunner also creates the directory in fetchAndRunExecutor(). We can catch all of the exit cases there and delete the directory in any case.

In the case that the executor failed to exit, however, it would be best to preserve the logs. instead of blindly killing the whole directory.

On that note, one other thought is that perhaps we actually want to preserve the directory entirely upon crash since preserving the state will allow us to better understand what happened, i.e. what jars and files were present and so on.;;;","30/Sep/14 02:49;ilikerps;Note that there are two separate forms of cleanup: application data cleanup (jars and logs) and shuffle data cleanup. Standalone Worker cleanup deals with the former, Executor termination handlers deal with the latter. The purpose is not to deal with executors that have terminated ungracefully, but to actually clean up old application directories.

Here the idea is that a Worker may be running for a very long time (weeks, months) and over time accumulates hundreds of application directories. We want to delete these directories after several days of them being terminated (today we'll clean them up whether or not they're terminated, which loses their jars and logs), after which we presumably don't care anymore. We do not want to clean them up immediately after application termination.

The Worker performing shuffle data cleanup for ungracefully terminated Executors is not a bad idea, but is a (smallish) feature onto itself, as the Worker does not currently know where a particular Executor is storing its data.;;;","30/Sep/14 03:14;mcheah;I agree we should focus the scope on cleaning up things that have successfully finished. Preserving state is beneficial in erroneous cases.

However, should it not be the case that when an Executor shuts down, it cleans up all of the files it created? As you stated, the Worker doesn't know where a particular Executor is storing its data, but the Executor should know where it is storing its own data, and be managing it and cleaning up when completed. This is regardless of the distinction between application data and shuffle data.

The Executor class has a record of the files and jars added through the SparkContext (currentFiles and currentJars fields) for that Executor's use, and these should naturally expire and be cleaned up when the Executor terminates. The top level application directories may still remain for a short time (Executors can only delete the subdirectory they work with) but the Worker can do a pass and remove empty directories that were all cleaned up by the completed Executor task.;;;","30/Sep/14 06:09;ilikerps;The Executor could clean up its own jars when it terminates normally, that seems fine. The impact of this seems limited, though, and it's a good idea to limit the scope of shutdown hooks as much as possible.

There are three classes of things to delete:
1. Shuffle files / block manager blocks -- large -- deleted by graceful Executor termination. Can be deleted immediately.
2. Uploaded jars / files -- usually small -- deleted by Worker cleanup. Can be deleted immediately.
3. Logs -- small to medium -- deleted by Worker cleanup. Should not be deleted immediately.

Number 1 is most critical in terms of impact on the system. Numbers 2 and 3 are of the same order of magnitude in size, so cleaning up 2 and not 3 is not expected to improve the system's stability by more than a factor of ~2x applications.

Note that the intentions of this particular JIRA are very simple: cleanup 2 and 3 for all executors several days after they have terminated, rather than after they have started. If you wish to expand the scope of the Worker or Executor cleanup, that should be covered in a separate JIRA (which is welcome -- I just want to make sure we're on the same page about this particular issue!).;;;","30/Sep/14 16:47;mcheah;Cool, I see where you're coming from now. I'll whip up something. Thanks for the input!

The cleanup for jars and added files is more for cosmetic sake than performance, I agree.;;;","30/Sep/14 18:25;mcheah;The change I am going to make is that when the cleanup task runs, it deletes an app directory inside the work directory if both the timestamp on the app directory and the timestamps on all of the app directory's files are older than the app directory retention time.

If any files inside the app directory are recently modified, the app directory is not touched.

Let me know if this change suffices to address this issue and I'll open a pull request.;;;","30/Sep/14 21:08;aash;That matches my expectations for this ticket Matt -- improve the timed cleanup task to only delete applications that have terminated instead of running ones as well.;;;","30/Sep/14 22:31;mcheah;Would like confirmation from [~adav] for the proposed change.;;;","01/Oct/14 01:54;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/2608;;;","01/Oct/14 02:01;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/2609;;;","01/Oct/14 02:05;ilikerps;Your logic SGTM, but I would add one additional check to avoid deleting the directory for an Application which still has running Executors on that node, just to make absolutely sure that we don't delete app directories that just happen to sit idle for a while. This check can be performed by iterating over the ""executors"" map in Worker.scala and matching the appId with the app directory's name.
;;;","01/Oct/14 16:49;mcheah;This might be a silly question, but are we guaranteed that the application folder will always be labeled by appid? I looked at ExecutorRunner and it certainly generates the folder by application ID and executor ID, but code comments in ExecutorRunner indicate it is only used by the standalone cluster mode. Hence I didn't tie any logic to the actual naming of the folders.;;;","01/Oct/14 19:03;ilikerps;The Worker itself is solely a Standalone mode construct, so AFAIK, this is not an issue.;;;","03/Oct/14 21:26;ilikerps;Fixed by mccheah in https://github.com/apache/spark/pull/2609;;;","03/Oct/14 23:04;aash;[~ilikerps] this ticket mentioned turning the cleanup code on by default once this ticket was fixed.  Should we change the defaults to have this on by default?;;;","04/Oct/14 23:23;ilikerps;Agreed, that sounds good. Would you or [~mccheah] be able to create a quick PR for this?;;;","05/Oct/14 21:31;aash;Filed as SPARK-3805;;;",,,,
SparkSQL Queries with Sorts run before the user asks them to,SPARK-1852,12714603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,15/May/14 20:30,31/Mar/15 00:43,14/Jul/23 06:25,13/Jun/14 20:06,,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,,,,,,"This is related to [SPARK-1021] but will not be fixed by that since we do our own partitioning.

Part of the problem here is that we calculate the range partitioning too eagerly.  Though this could also be alleviated by avoiding the call to toRdd for non DDL queries.",,kzhang,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1021,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392916,,,Fri Jun 13 20:06:53 UTC 2014,,,,,,,,,,"0|i1vntb:",393086,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/14 20:06;marmbrus;https://github.com/apache/spark/pull/1071;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad exception if multiple jars exist when running PySpark,SPARK-1850,12714591,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,andrewor14,andrewor,15/May/14 19:39,05/Nov/14 10:43,14/Jul/23 06:25,02/Jul/14 16:53,1.0.0,,,,,,,,1.0.1,,,,,PySpark,,,,,0,,,,,,"{code}
Found multiple Spark assembly jars in /Users/andrew/Documents/dev/andrew-spark/assembly/target/scala-2.10:
Traceback (most recent call last):
  File ""/Users/andrew/Documents/dev/andrew-spark/python/pyspark/shell.py"", line 43, in <module>
    sc = SparkContext(os.environ.get(""MASTER"", ""local[*]""), ""PySparkShell"", pyFiles=add_files)
  File ""/Users/andrew/Documents/dev/andrew-spark/python/pyspark/context.py"", line 94, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File ""/Users/andrew/Documents/dev/andrew-spark/python/pyspark/context.py"", line 180, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway()
  File ""/Users/andrew/Documents/dev/andrew-spark/python/pyspark/java_gateway.py"", line 49, in launch_gateway
    gateway_port = int(proc.stdout.readline())
ValueError: invalid literal for int() with base 10: 'spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4-deps.jar\n'
{code}

It's trying to read the Java gateway port as an int from the sub-process' STDOUT. However, what it read was an error message, which is clearly not an int. We should differentiate between these cases and just propagate the original message if it's not an int. Right now, this exception is not very helpful.",,andrewor,farrellee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2242,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392904,,,Wed Jul 02 16:53:05 UTC 2014,,,,,,,,,,"0|i1vnqn:",393074,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/14 13:23;farrellee;[~andrewor14] -

i think this should be closed as resolved in SPARK-2242

the current output for the error is,

{noformat}
$ ./dist/bin/pyspark
Python 2.7.5 (default, Feb 19 2014, 13:47:28) 
[GCC 4.8.2 20131212 (Red Hat 4.8.2-7)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/shell.py"", line 43, in <module>
    sc = SparkContext(appName=""PySparkShell"", pyFiles=add_files)
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/context.py"", line 95, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/context.py"", line 191, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway()
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/java_gateway.py"", line 66, in launch_gateway
    raise Exception(error_msg)
Exception: Launching GatewayServer failed with exit code 1!(Warning: unexpected output detected.)

Found multiple Spark assembly jars in /home/matt/Documents/Repositories/spark/dist/lib:
spark-assembly-1.1.0-SNAPSHOT-hadoop1.0.4-.jar
spark-assembly-1.1.0-SNAPSHOT-hadoop1.0.4.jar
Please remove all but one jar.
{noformat};;;","02/Jul/14 16:53;andrewor;Ye, I will change it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RAT checks should exclude logs/ directory,SPARK-1846,12714535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,aash,aash,15/May/14 17:09,23/Jun/14 17:14,14/Jul/23 06:25,23/Jun/14 17:13,1.0.0,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"When there are logs in the logs/ directory, the rat check from ./dev/check-license fails.

```
aash@aash-mbp ~/git/spark$ find logs -type f
logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out
logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.1
logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.2
logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.3
logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.4
logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.5
logs/spark-aash-org.apache.spark.deploy.worker.Worker--aash-mbp.local.out
logs/spark-aash-org.apache.spark.deploy.worker.Worker--aash-mbp.local.out.1
logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out
logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.1
logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.2
logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.3
logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.4
logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.5
logs/spark-aash-spark.deploy.master.Master-1-aash-mbp.local.out
logs/spark-aash-spark.deploy.master.Master-1-aash-mbp.local.out.1
logs/spark-aash-spark.deploy.master.Master-1-aash-mbp.local.out.2
logs/spark-aash-spark.deploy.worker.Worker-1-aash-mbp.local.out
logs/spark-aash-spark.deploy.worker.Worker-1-aash-mbp.local.out.1
logs/spark-aash-spark.deploy.worker.Worker-1-aash-mbp.local.out.2
aash@aash-mbp ~/git/spark$ ./dev/check-license
Could not find Apache license headers in the following files:
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.1
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.2
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.3
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.4
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.master.Master-1-aash-mbp.local.out.5
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.worker.Worker--aash-mbp.local.out
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.worker.Worker--aash-mbp.local.out.1
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.1
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.2
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.3
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.4
 !????? /Users/aash/git/spark/logs/spark-aash-org.apache.spark.deploy.worker.Worker-1-aash-mbp.local.out.5
 !????? /Users/aash/git/spark/logs/spark-aash-spark.deploy.master.Master-1-aash-mbp.local.out
 !????? /Users/aash/git/spark/logs/spark-aash-spark.deploy.master.Master-1-aash-mbp.local.out.1
 !????? /Users/aash/git/spark/logs/spark-aash-spark.deploy.master.Master-1-aash-mbp.local.out.2
 !????? /Users/aash/git/spark/logs/spark-aash-spark.deploy.worker.Worker-1-aash-mbp.local.out
 !????? /Users/aash/git/spark/logs/spark-aash-spark.deploy.worker.Worker-1-aash-mbp.local.out.1
 !????? /Users/aash/git/spark/logs/spark-aash-spark.deploy.worker.Worker-1-aash-mbp.local.out.2
aash@aash-mbp ~/git/spark$
```",,aash,ash211,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392848,,,Mon Jun 23 17:14:26 UTC 2014,,,,,,,,,,"0|i1vnf3:",393022,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/14 20:56;srowen;Just looking over some old JIRAs. This appears to be resolved already. logs is excluded.;;;","23/Jun/14 17:14;ash211;Yep this was merged in commit 3abe2b734a5578966f671c34f1de34b4446b90f1 for
master and aa5f989a537ec616b30ce8f7e134959eb1bbdc11 for branch-1.0

Good to close



;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use AllScalaRegistrar for SparkSqlSerializer to register serializers of Scala collections.,SPARK-1845,12714452,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,15/May/14 11:11,15/May/14 18:23,14/Jul/23 06:25,15/May/14 18:23,,,,,,,,,,,,,,SQL,,,,,0,,,,,,"When I execute {{orderBy}} or {{limit}} for {{SchemaRDD}} including {{ArrayType}} or {{MapType}}, {{SparkSqlSerializer}} throws the following exception:

{quote}
com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor): scala.collection.immutable.$colon$colon
{quote}

or

{quote}
com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor): scala.collection.immutable.Vector
{quote}

or

{quote}
com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor): scala.collection.immutable.HashMap$HashTrieMap
{quote}

and so on.

This is because registrations of serializers for each concrete collections are missing in {{SparkSqlSerializer}}.
I believe it should use {{AllScalaRegistrar}}.
{{AllScalaRegistrar}} covers a lot of serializers for concrete classes of {{Seq}}, {{Map}} for {{ArrayType}}, {{MapType}}.",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392765,,,Thu May 15 11:19:29 UTC 2014,,,,,,,,,,"0|i1vmxj:",392941,,,,,,,,,,,,,,,,,,,,,,,"15/May/14 11:19;ueshin;Pull-requested: https://github.com/apache/spark/pull/790;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkListenerBus prints out scary error message when terminating normally,SPARK-1840,12714384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,andrewor14,andrewor,15/May/14 01:36,05/Nov/14 10:43,14/Jul/23 06:25,15/May/14 04:15,1.0.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"This is because the Scala's NonLocalReturnControl (which extends ControlThrowable) is being logged. However, this is expected when the SparkContext terminates.

(OP is TD)",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392697,,,Thu May 15 04:15:14 UTC 2014,,,,,,,,,,"0|i1vmin:",392873,,,,,,,,,,,,,,,,,,,,,,,"15/May/14 04:15;pwendell;Issue resolved by pull request 783
[https://github.com/apache/spark/pull/783];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark take() does not launch a Spark job when it has to,SPARK-1839,12714356,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,falaki,falaki,14/May/14 23:45,31/May/14 20:06,14/Jul/23 06:25,31/May/14 20:06,1.0.0,,,,,,,,1.1.0,,,,,PySpark,,,,,0,,,,,,"If you call take() or first() on a large FilteredRDD, the driver attempts to scan all partitions to find the first valid item. If the RDD is large this would fail or hang.",,falaki,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392669,,,2014-05-14 23:45:45.0,,,,,,,,,,"0|i1vmd3:",392847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NumericRange should be partitioned in the same way as other sequences,SPARK-1837,12714309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kzhang,kzhang,kzhang,14/May/14 21:23,14/Jun/14 21:32,14/Jul/23 06:25,14/Jun/14 21:32,1.0.0,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"Otherwise, RDD.zip() would behave unexpectedly. For example, as given in SPARK-1817:

scala> sc.parallelize(1L to 2L,4).zip(sc.parallelize(11 to 12,4)).collect
res1: Array[(Long, Int)] = Array((2,11))",,kzhang,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1817,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392622,,,Wed May 14 21:28:59 UTC 2014,,,,,,,,,,"0|i1vm3b:",392802,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 21:28;kzhang;PR: https://github.com/apache/spark/pull/776;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Created forked version of hive-exec that doesn't bundle other dependencies,SPARK-1828,12714133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,14/May/14 07:52,15/Aug/14 17:20,14/Jul/23 06:25,14/May/14 16:51,1.0.0,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"The hive-exec jar includes a bunch of Hive's dependencies in addition to hive itself (protobuf, guava, etc). See HIVE-5733. This breaks any attempt in Spark to manage those dependencies.

The only solution to this problem is to publish our own version of hive-exec 0.12.0 that behaves correctly. While we are doing this, we might as well re-write the protobuf dependency to use the shaded version of protobuf 2.4.1 that we already have for Akka.",,pwendell,rossmohax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5733,SPARK-1802,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392446,,,Fri Aug 15 17:20:23 UTC 2014,,,,,,,,,,"0|i1vl27:",392631,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 16:51;pwendell;Issue resolved by pull request 767
[https://github.com/apache/spark/pull/767];;;","15/Aug/14 15:12;rossmohax;Because of this change any incompatibilities with Hive in Hadoop distros are hidden untile you run job on an actual cluster. Unless you are willing to keep your fork up to date with every major Hadoop distro of course. 

Right now we see incompatibility with CDH5.0.2 Hive, but I'd rather have it failing to compile rather that seeing problems at runtime;;;","15/Aug/14 16:57;pwendell;Maxim - I think what you are pointing out is unrated to this exact issue. Spark hard-codes a specific version of Hive in our build. This is true whether or not we are pointing to a slightly modified version of Hive 0.12 or the actual Hive 0.12.

The issue is that Hive does not have stable API's so we can't provide a version of Spark that is cross-compatible with different versions of Hive. We are trying to simplify our dependency on Hive to fix this.

Are you proposing a specific change here?;;;","15/Aug/14 17:20;rossmohax;I don't have a pull request at hand if you are askin that ;) But IMHO proper solution is to tinker with maven shade plugin, to drop classes pulled by hive dependency in favor of those specified in Spark POM. 

If it is done that way, then it would be possible to specify hive version using ""-D"" param in the same way we can specify hadoop version and be sure (to some extent of course :) ) that if it builds,it works.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LICENSE and NOTICE files need a refresh to contain transitive dependency info,SPARK-1827,12714125,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,srowen,srowen,14/May/14 07:07,15/Jan/15 09:08,14/Jul/23 06:25,14/May/14 16:39,0.9.1,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"(Pardon marking it a blocker, but think it needs doing before 1.0 per chat with [~pwendell])

The LICENSE and NOTICE files need to cover all transitive dependencies, since these are all distributed in the assembly jar. (c.f. http://www.apache.org/dev/licensing-howto.html )

I don't believe the current files cover everything. It's possible to mostly-automatically generate these. I will generate this and propose a patch to both today.",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392438,,,Wed May 14 16:39:12 UTC 2014,,,,,,,,,,"0|i1vl0n:",392623,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 10:41;srowen;LICENSE and NOTICE policy is explained here:

http://www.apache.org/dev/licensing-howto.html
http://www.apache.org/legal/3party.html

This leads to the following changes.

First, this change enables two extensions to maven-shade-plugin in assembly/ that will try to include and merge all NOTICE and LICENSE files. This can't hurt.

This generates a consolidated NOTICE file that I manually added to NOTICE.

Next, a list of all dependencies and their licenses was generated:

mvn ... license:aggregate-add-third-party

to create: target/generated-sources/license/THIRD-PARTY.txt

Each dependency is listed with one or more licenses. Determine the most-compatible license for each if there is more than one.

For ""unknown"" license dependencies, I manually evaluateD their license. Many are actually Apache projects or components of projects covered already. The only non-trivial one was Colt, which has its own (compatible) license.

I ignored Apache-licensed and public domain dependencies as these require no further action (beyond NOTICE above).

BSD and MIT licenses (permissive Category A licenses) are evidently supposed to be mentioned in LICENSE, so I added a section without output from the THIRD-PARTY.txt file appropriately.

Everything else, Category B licenses, are evidently mentioned in NOTICE (?) Same there.

LICENSE contained some license statements for source code that is redistributed. I left this as I think that is the right place to put it.
;;;","14/May/14 16:39;pwendell;Fixed by:
https://github.com/apache/spark/pull/770;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some bad head notations in sparksql ,SPARK-1826,12714099,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,14/May/14 03:03,15/May/14 18:14,14/Jul/23 06:25,15/May/14 18:14,1.0.0,,,,,,,,1.0.1,,,,,SQL,,,,,0,,,,,,There are some obvious bad notations， such as sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala.  ,,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392412,,,Thu May 15 18:14:05 UTC 2014,,,,,,,,,,"0|i1vkuv:",392597,,,,,,,,,,,,,,,,,,,,,,,"15/May/14 18:14;marmbrus;Fixed in: https://github.com/apache/spark/pull/765;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows Spark fails to work with Linux YARN,SPARK-1825,12714098,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tsudukim,zeodtr,zeodtr,14/May/14 02:38,02/Feb/15 02:30,14/Jul/23 06:25,02/Feb/15 02:30,1.0.0,,,,,,,,1.3.0,,,,,YARN,,,,,4,,,,,,"Windows Spark fails to work with Linux YARN.
This is a cross-platform problem.

This error occurs when 'yarn-client' mode is used.
(yarn-cluster/yarn-standalone mode was not tested.)

On YARN side, Hadoop 2.4.0 resolved the issue as follows:
https://issues.apache.org/jira/browse/YARN-1824

But Spark YARN module does not incorporate the new YARN API yet, so problem persists for Spark.

First, the following source files should be changed:
- /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala
- /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnableUtil.scala

Change is as follows:
- Replace .$() to .$$()
- Replace File.pathSeparator for Environment.CLASSPATH.name to ApplicationConstants.CLASS_PATH_SEPARATOR (import org.apache.hadoop.yarn.api.ApplicationConstants is required for this)

Unless the above are applied, launch_container.sh will contain invalid shell script statements(since they will contain Windows-specific separators), and job will fail.
Also, the following symptom should also be fixed (I could not find the relevant source code):
- SPARK_HOME environment variable is copied straight to launch_container.sh. It should be changed to the path format for the server OS, or, the better, a separate environment variable or a configuration variable should be created.
- '%HADOOP_MAPRED_HOME%' string still exists in launch_container.sh, after the above change is applied. maybe I missed a few lines.

I'm not sure whether this is all, since I'm new to both Spark and YARN.",,aaditya.prodigy,angel2014,apachespark,cmccabe,donnchadh,radoophp,roji,tsudukim,zeodtr,ztoth,zzhan,,,,,,,,,,,,,,,,,,,,SPARK-5164,,,,,,YARN-2929,,,,,,,,,,,"11/Nov/14 12:56;angel2014;SPARK-1825.patch;https://issues.apache.org/jira/secure/attachment/12680784/SPARK-1825.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392411,,,Thu Jan 08 05:56:04 UTC 2015,,,,,,,,,,"0|i1vkun:",392596,,,,,,,,,,,,,1.3.0,,,,,,,,,,"11/Nov/14 12:56;angel2014;Is it really necessary to change the file ""ExecutorRunnableUtil.scala""?

I'd just changed the file ""ClientBase.scala"" and it (apparently) works for Spark 1.1.

In order to make it work, you'll have to add the following configuration

	- Program arguments: --master yarn-cluster
	- VM arguments: -Dspark.app-submission.cross-platform=true
	
;;;","20/Nov/14 15:34;angel2014;I've had the following problems to make Windows+Pyspark+YARN work properly:

1. net.ScriptBasedMapping: Exception running /etc/hadoop/conf.cloudera.yarn/topology.py

FIX? Comment the ""net.topology.script.file.name"" property configuration in the file core-site.xml.

2. Error from python worker:
  /usr/bin/python: No module named pyspark
PYTHONPATH was:
  /yarn/nm/usercache/bigdata/filecache/63/spark-assembly-1.1.0-hadoop2.3.0-cdh5.0.1.jar

FIX? Add the environment variable SPARK_YARN_USER_ENV to my client (Eclipse) launch configuration. Assign this value to the env var:

PYTHONPATH=/opt/cloudera/parcels/CDH/lib/spark/python:/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip	


is there any way to do it simpler? am I doing something wrong?;;;","08/Jan/15 05:47;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/3943;;;","08/Jan/15 05:56;tsudukim;It is necessary to use $$() to solve this problem but as discussed on PR #899 if we use $$() build for hadoop<2.4 will fail.
So PR #3943 uses reflection to avoid build failure for every version of hadoop.
Windows clilents works fine with Linux YARN cluetr only when we use hadoop 2.4+. But it doesn't work under hadoop<2.4 even after this patch.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix GetField.nullable.,SPARK-1819,12713853,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,13/May/14 08:51,15/May/14 18:50,14/Jul/23 06:25,15/May/14 18:50,,,,,,,,,,,,,,SQL,,,,,0,,,,,,{{GetField.nullable}} should be {{true}} not only when {{field.nullable}} is {{true}} but also when {{child.nullable}} is {{true}}.,,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392169,,,Tue May 13 09:01:04 UTC 2014,,,,,,,,,,"0|i1vjdj:",392363,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 09:01;ueshin;Pull-requested: https://github.com/apache/spark/pull/757;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD zip erroneous when partitions do not divide RDD count,SPARK-1817,12713827,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kzhang,michaelmalak,michaelmalak,13/May/14 05:26,04/Jun/14 16:16,14/Jul/23 06:25,04/Jun/14 05:47,0.9.0,1.0.0,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"Example:

scala> sc.parallelize(1L to 2L,4).zip(sc.parallelize(11 to 12,4)).collect
res1: Array[(Long, Int)] = Array((2,11))

But more generally, it's whenever the number of partitions does not evenly divide the total number of elements in the RDD.

See https://groups.google.com/forum/#!msg/spark-users/demrmjHFnoc/Ek3ijiXHr2MJ
",,kzhang,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1837,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392143,,,Wed Jun 04 16:10:42 UTC 2014,,,,,,,,,,"0|i1vj7r:",392337,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 22:30;kzhang;I opened SPARK-1837 as a specific fix for the error reported in the description.;;;","04/Jun/14 16:10;kzhang;There are 2 issues related to this bug. One is that we partition numeric ranges (e.g., Long and Double ranges) differently from other types of sequences (i.e, at different indexes). This causes elements to be dropped when zipping with numeric ranges since we zip by partition and partitions for numeric ranges may have different sizes from other sequences (even if the total length and the number of partitions are the same). This is fixed in SPARK-1837. One caveat is currently partitioning Double ranges still doesn't work properly due to a Scala bug that breaks {{take}} and {{drop}} on Double ranges (https://issues.scala-lang.org/browse/SI-8518).

The other issue is instead of dropping elements silently, we should throw an error during zipping when we found out that partition sizes are not the same between 2 sequences. This is fixed by https://github.com/apache/spark/pull/944;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LiveListenerBus dies if a listener throws an exception,SPARK-1816,12713807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,ilikerps,ilikerps,13/May/14 01:36,22/Dec/14 22:11,14/Jul/23 06:25,14/May/14 01:32,1.0.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,The exception isn't even printed.,,andrewor,ilikerps,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4859,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392123,,,Wed May 14 01:32:59 UTC 2014,,,,,,,,,,"0|i1vj3b:",392317,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 00:14;andrewor;https://github.com/apache/spark/pull/759;;;","14/May/14 01:32;pwendell;Issue resolved by pull request 759
[https://github.com/apache/spark/pull/759];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/pyspark does not load default configuration properties,SPARK-1808,12713705,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,12/May/14 19:14,05/Nov/14 10:45,14/Jul/23 06:25,17/May/14 05:35,1.0.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"... because it doesn't go through spark-submit. Either we make it go through spark-submit (hard), or we extract the load default configurations logic and set them for the JVM that launches the py4j GatewayServer (easier).

Right now, the only way to set config values for bin/pyspark is to do it through SPARK_JAVA_OPTS in spark-env.sh, which is supposedly deprecated.",,andrewor,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392021,,,Sat May 17 05:35:12 UTC 2014,,,,,,,,,,"0|i1vihb:",392218,,,,,,,,,,,,,,,,,,,,,,,"16/May/14 00:44;andrewor;https://github.com/apache/spark/pull/799;;;","17/May/14 05:35;pwendell;Issue resolved by pull request 799
[https://github.com/apache/spark/pull/799];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error launching cluster when master and slave machines are of different virtualization types,SPARK-1805,12713607,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nchammas,darkjh,darkjh,12/May/14 11:38,15/Apr/15 16:03,14/Jul/23 06:25,10/Feb/15 15:45,0.9.0,0.9.1,1.0.0,1.1.1,1.2.0,,,,1.4.0,,,,,EC2,,,,,1,,,,,,"In the current EC2 script, the AMI image object is loaded only once. This is OK when the master and slave machines are of the same virtualization type (pv or hvm). But this won't work if, say, the master is pv and the slaves are hvm since the AMI is not compatible across these two kinds of virtualization.",,apachespark,darkjh,grzegorz-dubicki,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5086,,,,,,SPARK-6935,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391923,,,Tue Feb 10 15:45:57 UTC 2015,,,,,,,,,,"0|i1vhxb:",392126,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/15 03:10;apachespark;User 'nchammas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4455;;;","09/Feb/15 22:56;nchammas;I've created a PR to catch this error early.

We can create a separate PR later to actually support launching clusters where the master and slaves have different virtualization types.;;;","10/Feb/15 15:45;srowen;Issue resolved by pull request 4455
[https://github.com/apache/spark/pull/4455];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Audit dependency graph when Spark is built with -Phive,SPARK-1802,12713568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,pwendell,pwendell,12/May/14 06:27,15/Jan/15 09:08,14/Jul/23 06:25,19/May/14 18:43,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"I'd like to have binary release for 1.0 include Hive support. Since this isn't enabled by default in the build I don't think it's as well tested, so we should dig around a bit and decide if we need to e.g. add any excludes.

{code}
$ mvn install -Phive -DskipTests && mvn dependency:build-classpath -pl assembly | grep -v INFO | tr "":"" ""\n"" |  awk ' { FS=""/""; print ( $(NF) ); }' | sort > without_hive.txt

$ mvn install -Phive -DskipTests && mvn dependency:build-classpath -Phive -pl assembly | grep -v INFO | tr "":"" ""\n"" |  awk ' { FS=""/""; print ( $(NF) ); }' | sort > with_hive.txt

$ diff without_hive.txt with_hive.txt
< antlr-2.7.7.jar
< antlr-3.4.jar
< antlr-runtime-3.4.jar
10,14d6
< avro-1.7.4.jar
< avro-ipc-1.7.4.jar
< avro-ipc-1.7.4-tests.jar
< avro-mapred-1.7.4.jar
< bonecp-0.7.1.RELEASE.jar
22d13
< commons-cli-1.2.jar
25d15
< commons-compress-1.4.1.jar
33,34d22
< commons-logging-1.1.1.jar
< commons-logging-api-1.0.4.jar
38d25
< commons-pool-1.5.4.jar
46,49d32
< datanucleus-api-jdo-3.2.1.jar
< datanucleus-core-3.2.2.jar
< datanucleus-rdbms-3.2.1.jar
< derby-10.4.2.0.jar
53,57d35
< hive-common-0.12.0.jar
< hive-exec-0.12.0.jar
< hive-metastore-0.12.0.jar
< hive-serde-0.12.0.jar
< hive-shims-0.12.0.jar
60,61d37
< httpclient-4.1.3.jar
< httpcore-4.1.3.jar
68d43
< JavaEWAH-0.3.2.jar
73d47
< javolution-5.5.1.jar
76d49
< jdo-api-3.0.1.jar
78d50
< jetty-6.1.26.jar
87d58
< jetty-util-6.1.26.jar
93d63
< json-20090211.jar
98d67
< jta-1.1.jar
103,104d71
< libfb303-0.9.0.jar
< libthrift-0.9.0.jar
112d78
< mockito-all-1.8.5.jar
136d101
< servlet-api-2.5-20081211.jar
139d103
< snappy-0.2.jar
144d107
< spark-hive_2.10-1.0.0.jar
151d113
< ST4-4.0.4.jar
153d114
< stringtemplate-3.2.1.jar
156d116
< velocity-1.7.jar
158d117
< xz-1.0.jar
{code}

Some initial investigation suggests we may need to take some precaution surrounding (a) jetty and (b) servlet-api.",,gq,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1828,,,,,,,,,,,,,,,,,"12/May/14 23:35;srowen;hive-exec-jar-problems.txt;https://issues.apache.org/jira/secure/attachment/12644517/hive-exec-jar-problems.txt",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391884,,,Tue May 13 11:26:13 UTC 2014,,,,,,,,,,"0|i1vhon:",392087,,,,,,,,,,,,,,,,,,,,,,,"12/May/14 10:34;srowen;[~pwendell] You can see my start on it here:

https://github.com/srowen/spark/commits/SPARK-1802
https://github.com/srowen/spark/commit/a856604cfc67cb58146ada01fda6dbbb2515fa00

This resolves the new issues you note in your diff.


Next issue is that hive-exec, quite awfully, includes a copy of all of its transitive dependencies in its artifact. See https://issues.apache.org/jira/browse/HIVE-5733 and note the warnings you'll get during assembly:

{code}
[WARNING] hive-exec-0.12.0.jar, libthrift-0.9.0.jar define 153 overlappping classes: 
[WARNING]   - org.apache.thrift.transport.TSaslTransport$SaslResponse
...
{code}

hive-exec is in fact used in this module. Aside from actual surgery on the artifact with the shade plugin, you can't control the dependencies as a result. This may be simply ""the best that can be done"" right now. If it has worked, it has worked.


Am I right that the datanucleus JARs *are* meant to be in the assembly, only for the Hive build?
https://github.com/apache/spark/pull/688
https://github.com/apache/spark/pull/610

That's good if so since that's what your diff shows.


Finally, while we're here, I note that there are still a few JAR conflicts that turn up when you build the assembly *without* Hive. (I'm going to ignore conflicts in examples; these can be cleaned up but aren't really a big deal given its nature.)  We could touch those up too.

This is in the normal build (and I know how to zap most of this problem):
{code}
[WARNING] commons-beanutils-core-1.8.0.jar, commons-beanutils-1.7.0.jar define 82 overlappping classes: 
{code}

These turn up in the Hadoop 2.x + YARN build:
{code}
[WARNING] servlet-api-2.5.jar, javax.servlet-3.0.0.v201112011016.jar define 42 overlappping classes: 
...
[WARNING] jcl-over-slf4j-1.7.5.jar, commons-logging-1.1.3.jar define 6 overlappping classes: 
...
[WARNING] activation-1.1.jar, javax.activation-1.1.0.v201105071233.jar define 17 overlappping classes: 
...
[WARNING] servlet-api-2.5.jar, javax.servlet-3.0.0.v201112011016.jar define 42 overlappping classes: 
{code}

These should be easy to track down. Shall I?;;;","12/May/14 21:17;pwendell;Issue resolved by pull request 744
[https://github.com/apache/spark/pull/744];;;","12/May/14 23:35;srowen;(Edited to fix comment about protobuf versions)

I looked further into just what might go wrong by including hive-exec into the assembly, since it includes its dependencies directly (i.e. Maven can't manage around it.)

Attached is a full dump of the conflicts.

The ones that are potential issues appear to be the following, and one looks like it could be a deal-breaker -- protobuf -- since it's neither forwards nor backwards compatible. That is, I recommend testing this assembly with an *newer* Hadoop that needs 2.5 and see if it croaks.

The rest might be worked around but need some additional mojo to make sure the right version wins in the packaging.

Certainly having hive-exec in the build is making me queasy!


[WARNING] hive-exec-0.12.0.jar, libthrift-0.9.0.jar define 153 overlappping classes: 

HBase includes libthrift-0.8.0, but it's in examples, and so figure this is ignorable.


[WARNING] hive-exec-0.12.0.jar, commons-lang-2.4.jar define 2 overlappping classes: 

Probably ignorable, but we have to make sure commons-lang-3.3.2 'wins' in the build.


[WARNING] hive-exec-0.12.0.jar, jackson-core-asl-1.9.11.jar define 117 overlappping classes: 
[WARNING] hive-exec-0.12.0.jar, jackson-mapper-asl-1.8.8.jar define 432 overlappping classes: 

Believe this are ignorable. (Not sure why the jackson versions are mismatched? another todo)


[WARNING] hive-exec-0.12.0.jar, guava-14.0.1.jar define 1087 overlappping classes: 

Should be OK. Hive uses 11.0.2 like Hadoop; the build is already taking that particular risk. We need 14.0.1 to win.


[WARNING] hive-exec-0.12.0.jar, protobuf-java-2.4.1.jar define 204 overlappping classes: 

Oof. Hive has protobuf *2.4.1*. This has got to be a problem for newer Hadoop builds?

(Edited to fix comment about protobuf versions);;;","13/May/14 02:46;pwendell;Let's keep this open given the ongoing discussion.;;;","13/May/14 03:06;gq;[Related work|https://github.com/apache/spark/pull/754];;;","13/May/14 08:17;pwendell;This protobuf thing is very troubling. The options here are pretty limited since they publish this assembly jar. I see a few:

1. Publish a Hive 0.12 that uses our shaded protobuf 2.4.1 (we already published a shaded version of protobuf 2.4.1). I actually have this working in a local build of Hive 0.12, but I haven't tried to push it to sonatype yet:
https://github.com/pwendell/hive/commits/branch-0.12-shaded-protobuf

2. Upgrade our use of hive to 0.13 (which bumps to protobuf 2.5.0) and only support Spark SQL with Hadoop 2+ - that is, versions of Hadoop that have also bumped to protobuf 2.5.0. I'm not sure how big of an effort that would be in terms of the code changes between 0.12 and 0.13. Spark didn't recompile trivially. I can talk to Michael Armbrust tomorrow morning about this.

One thing I don't totally understand is how Hive itself deals with this conflict. For instance, if someone wants to run Hive 0.12 with Hadoop 2. Presumably both the Hive protobuf 2.4.1 and the HDFS client protobuf 2.5.0 will be in the JVM at the same time... I'm not sure how they are isolated from each-other. HDP 2.1 for instance, seems to have both (http://hortonworks.com/hdp/whats-new/);;;","13/May/14 11:26;srowen;First, I apologize, I misspoke above. Actually hive-exec 0.12.0 depends on protobuf 2.4.1. I got this wrong since I was evaluating dependencies under the Hadoop 2.3 profile.

That leaves the same but opposite problem, meaning it probably won't work with later versions of Hadoop at some level.

The 'right' place to solve this is the hive-exec build, and I know there's a Hive JIRA open for this. Maybe people who are interested in making this work can push that through? But that's more of a medium-term answer.

I suppose it's possible to *also* vary the Hive version with Hadoop version? Assuming the Spark code can be made to work both ways, and I don't know that.This may or may not work as I don't know if Hive-Hadoop are compatible in exactly the same versions that they agree on protobuf.

You could go ahead with testing the Hive-enabled build anyway, with crossed fingers.
You could look at deploying two artifacts, with and without Hive, and simply give in to combinatorial explosion for the short term.
You could say Hive builds are do-it-yourself for the short-term, and bring it back in the medium term if/when the project drops support for older Hadoop -- which may be becoming more attractive all the time -- and combinatorial mess is much less

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Tests should clean up temp files,SPARK-1798,12713524,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,11/May/14 15:23,15/Jan/15 09:08,14/Jul/23 06:25,12/May/14 21:16,0.9.1,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"Three issues related to temp files that tests generate -- these should be touched up for hygiene but are not urgent.

Modules have a log4j.properties which directs the unit-test.log output file to a directory like [module]/target/unit-test.log. But this ends up creating [module]/[module]/target/unit-test.log instead of former.

The work/ directory is not deleted by ""mvn clean"", in the parent and in modules. Neither is the checkpoint/ directory created under the various external modules.

Many tests create a temp directory, which is not usually deleted. This can be largely resolved by calling deleteOnExit() at creation and trying to call Utils.deleteRecursively consistently to clean up, sometimes in an ""@After"" method.

(If anyone seconds the motion, I can create a more significant change that introduces a new test trait along the lines of LocalSparkContext, which provides management of temp directories for subclasses to take advantage of.)",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391840,,,Mon May 12 21:16:48 UTC 2014,,,,,,,,,,"0|i1vhev:",392043,,,,,,,,,,,,,,,,,,,,,,,"12/May/14 21:16;pwendell;Issue resolved by pull request 732
[https://github.com/apache/spark/pull/732];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing Spark-Shell Configure Options,SPARK-1792,12713483,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jegonzal,jegonzal,10/May/14 23:29,17/Jan/20 19:19,14/Jul/23 06:25,08/Oct/16 04:30,,,,,,,,,1.1.0,,,,,Documentation,Spark Core,,,,1,,,,,,"The `conf/spark-env.sh.template` does not have configure options for the spark shell.   For example to enable Kryo for GraphX when using the spark shell in stand alone mode it appears you must add:

{code}
SPARK_SUBMIT_OPTS=""-Dspark.serializer=org.apache.spark.serializer.KryoSerializer ""
SPARK_SUBMIT_OPTS+=""-Dspark.kryo.registrator=org.apache.spark.graphx.GraphKryoRegistrator  ""
{code}

However SPARK_SUBMIT_OPTS is not documented anywhere.  Perhaps the spark-shell should have its own options (e.g., SPARK_SHELL_OPTS).

",,andrewor,donnchadh,holden,jegonzal,jiml,kadiyalakc,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30557,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391799,,,Sat Oct 08 04:24:41 UTC 2016,,,,,,,,,,"0|i1vh5z:",392002,,,,,,,,,,,,,,,,,,,,,,,"12/May/14 03:33;rxin;[~matei] [~pwendell];;;","12/May/14 04:46;pwendell;It would be nice if the spark-submit script could automatically set arbitrary spark options. The way we have it now is you can set them in the following ways:

1. You explicitly set an option when you create a SparkConf()
2. You have default values for spark conf options in spark-defaults.conf.;;;","12/May/14 04:49;andrewor;Spark shell calls spark submit, which loads configuration options from conf/spark-defaults.conf. This is documented here, but not clearly: https://github.com/apache/spark/blob/master/docs/cluster-overview.md

We should add this to the configuration page itself (https://github.com/apache/spark/blob/master/docs/configuration.md). I already have a PR on the docs (https://github.com/apache/spark/pull/701) so maybe I can add that.;;;","12/May/14 05:00;jegonzal;That makes a lot of sense!  Perhaps the template should list more options (e.g., how to enable kryo and disable/enable kryo reference tracking)?

Also it might be helpful to print a message on launch of the spark-shell stating that additional configuration options can be set in conf/spark-defaults.conf.

;;;","27/Jan/16 22:01;jiml;Was there any movement on this? I am seeing an unresolved status and a date of 2014? Does it really exist and it not deprecated? Thanks

BACKGROUND FWIW: As a newb to Spark for a month or two I didn't realize there was a SPARK_SUBMIT_OPTS that can be set in spark-env.sh. 

I found this in this blog page: https://abrv8.wordpress.com/2014/10/06/debugging-java-spark-applications/

That blog pages was linked from this SO question: http://stackoverflow.com/questions/29090745/debugging-spark-applications;;;","08/Oct/16 04:24;holden;It feels like we've already got a pretty good mechanism for handling this with `spark-defaults.conf` and we've made a lot of progress with documenting many of the configuration enviroment variables in http://spark.apache.org/docs/latest/submitting-applications.html and http://spark.apache.org/docs/latest/configuration.html , so personally I think we should probably mark this issue as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple versions of Netty dependencies cause FlumeStreamSuite failure,SPARK-1789,12713475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,10/May/14 20:00,15/Jan/15 09:08,14/Jul/23 06:25,11/May/14 03:51,0.9.1,,,,,,,,1.0.0,,,,,Build,,,,,0,flume,netty,test,,,"TL;DR is there is a bit of JAR hell trouble with Netty, that can be mostly resolved and will resolve a test failure.


I hit the error described at http://apache-spark-user-list.1001560.n3.nabble.com/SparkContext-startup-time-out-td1753.html while running FlumeStreamingSuite, and have for a short while (is it just me?)

velvia notes:
""I have found a workaround.  If you add akka 2.2.4 to your dependencies, then everything works, probably because akka 2.2.4 brings in newer version of Jetty."" 

There are at least 3 versions of Netty in play in the build:

- the new Flume 1.4.0 dependency brings in io.netty:netty:3.4.0.Final, and that is the immediate problem
- the custom version of akka 2.2.3 depends on io.netty:netty:3.6.6.
- but, Spark Core directly uses io.netty:netty-all:4.0.17.Final

The POMs try to exclude other versions of netty, but are excluding org.jboss.netty:netty, when in fact older versions of io.netty:netty (not netty-all) are also an issue.

The org.jboss.netty:netty excludes are largely unnecessary. I replaced many of them with io.netty:netty exclusions until everything agreed on io.netty:netty-all:4.0.17.Final.

But this didn't work, since Akka 2.2.3 doesn't work with Netty 4.x. Down-grading to 3.6.6.Final across the board made some Spark code not compile.

If the build *keeps* io.netty:netty:3.6.6.Final as well, everything seems to work. Part of the reason seems to be that Netty 3.x used the old `org.jboss.netty` packages. This is less than ideal, but is no worse than the current situation. 


So this PR resolves the issue and improves the JAR hell, even if it leaves the existing theoretical Netty 3-vs-4 conflict:

- Remove org.jboss.netty excludes where possible, for clarity; they're not needed except with Hadoop artifacts
- Add io.netty:netty excludes where needed -- except, let akka keep its io.netty:netty
- Change a bit of test code that actually depended on Netty 3.x, to use 4.x equivalent
- Update SBT build accordingly

A better change would be to update Akka far enough such that it agrees on Netty 4.x, but I don't know if that's feasible.
",,pwendell,willbenton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391791,,,Wed May 14 14:50:31 UTC 2014,,,,,,,,,,"0|i1vh47:",391994,,,,,,,,,,,,,,,,,,,,,,,"11/May/14 03:51;pwendell;Issue resolved by pull request 723
[https://github.com/apache/spark/pull/723];;;","12/May/14 19:57;willbenton;Sean, we're currently building against Akka 2.3.0 in Fedora (it's a trivial source patch against 0.9.1; I haven't investigated the delta against 1.0 yet).  Are there reasons why Akka 2.3.0 is a bad idea for Spark in general?  If not, I'm happy to file a JIRA for updating the dependency and contribute my patch upstream.;;;","14/May/14 06:07;srowen;I don't have any info either way on that. Later is always better no? probably OK to consider post-1.0? 

The issue here was to do with Netty, and the comment about Akka that I quoted was really meant to suggest that it was Netty (as it happens being imported by Akka) that was relevant.;;;","14/May/14 14:50;willbenton;Yes, this is absolutely a post-1.0 thing.  I'm just saying that by updating the version of Akka to 2.3 we'd eliminate one of Spark's dependencies that can't work with Netty 4.  The issue of only transitively depending on at most one version of Netty 3 and at most one version of Netty 4 (and choosing ones that can work different coordinates) is orthogonal, but still an issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kryo Serialization Error in GraphX,SPARK-1786,12713430,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jegonzal,jegonzal,jegonzal,10/May/14 01:29,12/May/14 02:22,14/Jul/23 06:25,12/May/14 02:22,1.0.0,,,,,,,,1.0.0,,,,,GraphX,,,,,0,,,,,,"The following code block will generate a serialization error when run in the spark-shell with Kryo enabled:

{code}
import org.apache.spark.storage._
import org.apache.spark.graphx._
import org.apache.spark.graphx.util._

val g = GraphGenerators.gridGraph(sc, 100, 100)
val e = g.edges
e.persist(StorageLevel.MEMORY_ONLY_SER)
e.collect().foreach(println(_)) // <- Runs successfully the first time.

// The following line will fail:
e.collect().foreach(println(_))
{code}

The following error is generated:

{code}
scala> e.collect().foreach(println(_))
14/05/09 18:31:13 INFO SparkContext: Starting job: collect at EdgeRDD.scala:59
14/05/09 18:31:13 INFO DAGScheduler: Got job 1 (collect at EdgeRDD.scala:59) with 8 output partitions (allowLocal=false)
14/05/09 18:31:13 INFO DAGScheduler: Final stage: Stage 1(collect at EdgeRDD.scala:59)
14/05/09 18:31:13 INFO DAGScheduler: Parents of final stage: List()
14/05/09 18:31:13 INFO DAGScheduler: Missing parents: List()
14/05/09 18:31:13 INFO DAGScheduler: Submitting Stage 1 (MappedRDD[15] at map at EdgeRDD.scala:59), which has no missing parents
14/05/09 18:31:13 INFO DAGScheduler: Submitting 8 missing tasks from Stage 1 (MappedRDD[15] at map at EdgeRDD.scala:59)
14/05/09 18:31:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:0 as TID 8 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:0 as 1779 bytes in 3 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:1 as TID 9 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:1 as 1779 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:2 as TID 10 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:2 as 1779 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:3 as TID 11 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:3 as 1779 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:4 as TID 12 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:4 as 1779 bytes in 3 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:5 as TID 13 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:5 as 1782 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:6 as TID 14 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:6 as 1783 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:7 as TID 15 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:7 as 1783 bytes in 4 ms
14/05/09 18:31:13 INFO Executor: Running task ID 9
14/05/09 18:31:13 INFO Executor: Running task ID 8
14/05/09 18:31:13 INFO Executor: Running task ID 11
14/05/09 18:31:13 INFO Executor: Running task ID 14
14/05/09 18:31:13 INFO Executor: Running task ID 10
14/05/09 18:31:13 INFO Executor: Running task ID 13
14/05/09 18:31:13 INFO Executor: Running task ID 15
14/05/09 18:31:13 INFO Executor: Running task ID 12
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_6 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_4 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_2 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_7 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_1 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_3 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_0 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_5 locally
14/05/09 18:31:13 ERROR Executor: Exception in task ID 13
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 10
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 11
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 12
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 15
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 8
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 9
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 14
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 WARN TaskSetManager: Lost TID 11 (task 1.0:3)
14/05/09 18:31:13 WARN TaskSetManager: Loss was due to java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR TaskSetManager: Task 1.0:3 failed 1 times; aborting job
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 1]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 2]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 3]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 4]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO DAGScheduler: Failed to run collect at EdgeRDD.scala:59
14/05/09 18:31:13 INFO TaskSchedulerImpl: Cancelling stage 1
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 5]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 6]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 7]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0:3 failed 1 times, most recent failure: Exception failure in TID 11 on host localhost: java.lang.NullPointerException
        org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
        org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        scala.collection.AbstractIterator.to(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
        org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:744)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107
{code}

We believe the error is associated with serialization of the EdgePartition.",,jegonzal,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391746,,,Mon May 12 02:22:01 UTC 2014,,,,,,,,,,"0|i1vguv:",391952,,,,,,,,,,,,,,,,,,,,,,,"12/May/14 02:22;matei;https://github.com/apache/spark/pull/724;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming requires receivers to be serializable,SPARK-1785,12713429,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,hshreedharan,hshreedharan,10/May/14 01:07,10/Jul/14 20:22,14/Jul/23 06:25,10/Jul/14 20:22,0.9.0,,,,,,,,,,,,,DStreams,,,,,0,,,,,,"When the ReceiverTracker starts the receivers it creates a temporary RDD to  send the receivers over to the workers. Then they are started on the workers  using a the startReceivers method.

Looks like this means that the receivers have to really be serializable. In case of the Flume receiver, the Avro IPC components are not serializable causing an error that looks like this:
{code}
Exception in thread ""Thread-46"" org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: org.apache.avro.ipc.specific.SpecificResponder
	- field (class ""org.apache.spark.streaming.flume.FlumeReceiver"", name: ""responder"", type: ""class org.apache.avro.ipc.specific.SpecificResponder"")
	- object (class ""org.apache.spark.streaming.flume.FlumeReceiver"", org.apache.spark.streaming.flume.FlumeReceiver@5e6bbb36)
	- element of array (index: 0)
	- array (class ""[Lorg.apache.spark.streaming.receiver.Receiver;"", size: 1)
	- field (class ""scala.collection.mutable.WrappedArray$ofRef"", name: ""array"", type: ""class [Ljava.lang.Object;"")
	- object (class ""scala.collection.mutable.WrappedArray$ofRef"", WrappedArray(org.apache.spark.streaming.flume.FlumeReceiver@5e6bbb36))
	- field (class ""org.apache.spark.rdd.ParallelCollectionPartition"", name: ""values"", type: ""interface scala.collection.Seq"")
	- custom writeObject data (class ""org.apache.spark.rdd.ParallelCollectionPartition"")
	- object (class ""org.apache.spark.rdd.ParallelCollectionPartition"", org.apache.spark.rdd.ParallelCollectionPartition@691)
	- writeExternal data
	- root object (class ""org.apache.spark.scheduler.ResultTask"", ResultTask(0, 0))
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:770)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:713)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1176)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

A way out of this is to simply send the class name (or .class) to the workers in the tempRDD and have the workers instantiate and start the receiver.

My analysis maybe wrong. but if it makes sense, I will submit a PR to fix this.",,hshreedharan,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391745,,,Wed Jul 09 22:30:46 UTC 2014,,,,,,,,,,"0|i1vgun:",391951,,,,,,,,,,,,,,,,,,,,,,,"10/May/14 01:18;hshreedharan;The problem with sending only the class file across is that any configuration of the receivers is lost. We might need to find another way around it - the first being making the receivers explicitly extend Serializable. 

In the Flume receiver case, we can fix this in some way by instantiating the requestor and the server only later. I am actually working on a different receiver anyway, so I will fix the current one too - but we need to look at other receivers as well.;;;","10/May/14 07:30;hshreedharan;This really is a documentation jira. In Flume's case, having internal implementation being initialized lazily seems to save the day (I removed the lazy which resulted in the error).;;;","09/Jul/14 22:30;tdas;Yeah, this will be good to document. This can be added the guide for creating custom receivers. I am adding this to this overarching documentation JIRA.
https://issues.apache.org/jira/browse/SPARK-2419;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-existent SPARK_DAEMON_OPTS is referred to in a few places,SPARK-1780,12713374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,09/May/14 19:28,05/Nov/14 10:45,14/Jul/23 06:25,13/May/14 02:43,0.9.1,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,SparkConf.scala and spark-env.sh refer to a non-existent SPARK_DAEMON_OPTS. What they really mean SPARK_DAEMON_JAVA_OPTS.,,andrewor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391690,,,Tue May 13 01:03:35 UTC 2014,,,,,,,,,,"0|i1vgjz:",391902,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 01:03;andrewor;https://github.com/apache/spark/pull/751;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unneeded lock in ShuffleMapTask.deserializeInfo,SPARK-1775,12713234,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,techaddict,matei,matei,09/May/14 02:06,10/May/14 06:11,14/Jul/23 06:25,09/May/14 05:31,0.9.0,0.9.1,1.0.0,,,,,,0.9.2,1.0.0,,,,Spark Core,,,,,0,Starter,,,,,"This was used in the past to have a cache of deserialized ShuffleMapTasks, but that's been removed, so there's no need for a lock. It slows down Spark when task descriptions are large, e.g. due to large lineage graphs or local variables.",,matei,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391550,,,Fri May 09 05:31:15 UTC 2014,,,,,,,,,,"0|i1vfov:",391762,,,,,,,,,,,,,,,,,,,,,,,"09/May/14 05:31;pwendell;Issue resolved by pull request 707
[https://github.com/apache/spark/pull/707];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmit --jars not working for yarn-client,SPARK-1774,12713226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor,09/May/14 00:49,05/Nov/14 10:45,14/Jul/23 06:25,11/May/14 03:58,0.9.1,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"If the mode is yarn-client, SparkSubmit ignores --jars altogether. The other thing is that the application jar is not automatically added the --jars, which leads to spurious ClassNotFoundExceptions. (This latter point is already implemented, but only for Standalone mode)",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391542,,,Sun May 11 03:58:23 UTC 2014,,,,,,,,,,"0|i1vfn3:",391754,,,,,,,,,,,,,,,,,,,,,,,"11/May/14 03:58;pwendell;Issue resolved by pull request 710
[https://github.com/apache/spark/pull/710];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone cluster docs should be updated to reflect Spark Submit,SPARK-1773,12713225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,pwendell,pwendell,09/May/14 00:36,05/Nov/14 10:45,14/Jul/23 06:25,13/May/14 02:45,,,,,,,,,1.0.0,,,,,Documentation,,,,,0,,,,,,,,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391541,,,2014-05-09 00:36:39.0,,,,,,,,,,"0|i1vfmv:",391753,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark executors do not successfully die on OOM,SPARK-1772,12713224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ilikerps,ilikerps,09/May/14 00:33,13/May/14 03:11,14/Jul/23 06:25,12/May/14 18:09,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Executor catches Throwable, and does not always die when JVM fatal exceptions occur. This is a problem because any subsequent use of these Executors are very likely to fail.",,ilikerps,mridulm80,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391540,,,Tue May 13 03:11:26 UTC 2014,,,,,,,,,,"0|i1vfmn:",391752,,,,,,,,,,,,,,,,,,,,,,,"12/May/14 18:09;pwendell;Issue resolved by pull request 715
[https://github.com/apache/spark/pull/715];;;","12/May/14 20:42;mridulm80;BTW, for specific case of OOM, there is another way to handle it - via -XX flags : which is what we do in yarn case.;;;","13/May/14 03:11;ilikerps;Could you point me to the flag you're referring to? The only one I knew about was the one that prints a heap dump on error.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repartition and coalesce(shuffle=true) put objects with the same key in the same bucket,SPARK-1770,12713220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,matei,matei,08/May/14 23:59,27/May/14 01:02,14/Jul/23 06:25,12/May/14 00:13,0.9.0,0.9.1,1.0.0,,,,,,1.0.0,,,,,,,,,,0,Starter,,,,,This is bad when you have many identical objects. We should assign each one a random key.,,ilikerps,matei,pwendell,techaddict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1784,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391536,,,Sun May 11 00:15:41 UTC 2014,,,,,,,,,,"0|i1vflr:",391748,,,,,,,,,,,,,,,,,,,,,,,"09/May/14 09:10;techaddict;I think this is fixed in PR https://github.com/apache/spark/pull/704 by [~pwendell];;;","09/May/14 20:56;ilikerps;Ah, that PR seems unrelated.;;;","09/May/14 21:04;ilikerps;Sorry, you're right, it was somehow committed when that PR was merged (https://github.com/apache/spark/commit/06b15baab25951d124bbe6b64906f4139e037deb) though the change doesn't actually show up in the PR itself.;;;","11/May/14 00:15;pwendell;It was a mistake on my part - I accidentally pulled it into a different merge.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor loss can cause race condition in Pool,SPARK-1769,12713212,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,ilikerps,ilikerps,08/May/14 23:34,05/Nov/14 10:45,14/Jul/23 06:25,14/May/14 07:55,1.0.0,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"Loss of executors (in this case due to OOMs) exposes a race condition in Pool.scala, evident from this stack trace:

{code}
14/05/08 22:41:48 ERROR OneForOneStrategy:
java.lang.NullPointerException
        at org.apache.spark.scheduler.Pool$$anonfun$executorLost$1.apply(Pool.scala:87)
        at org.apache.spark.scheduler.Pool$$anonfun$executorLost$1.apply(Pool.scala:87)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.Pool.executorLost(Pool.scala:87)
        at org.apache.spark.scheduler.Pool$$anonfun$executorLost$1.apply(Pool.scala:87)
        at org.apache.spark.scheduler.Pool$$anonfun$executorLost$1.apply(Pool.scala:87)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.Pool.executorLost(Pool.scala:87)
        at org.apache.spark.scheduler.TaskSchedulerImpl.removeExecutor(TaskSchedulerImpl.scala:412)
        at org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:385)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor.removeExecutor(CoarseGrainedSchedulerBackend.scala:160)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1$$anonfun$applyOrElse$5.apply(CoarseGrainedSchedulerBackend.scala:123)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1$$anonfun$applyOrElse$5.apply(CoarseGrainedSchedulerBackend.scala:123)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:123)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

Note that the line of code that throws this exception is here:
{code}
schedulableQueue.foreach(_.executorLost(executorId, host))
{code}

By the stack trace, it's not schedulableQueue that is null, but an element therein. As far as I could tell, we never add a null element to this queue. Rather, I could see that removeSchedulable() and executorLost() were called at about the same time (via log messages), and suspect that since this ArrayBuffer is in no way synchronized, that we iterate through the list while it's in an incomplete state.",,andrewor,codingcat,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391528,,,Wed May 14 00:15:14 UTC 2014,,,,,,,,,,"0|i1vfk7:",391741,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 00:15;andrewor;https://github.com/apache/spark/pull/762;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move reduceByKey definitions next to each other in PairRDDFunctions,SPARK-1766,12713177,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,copester,sandyr,sandyr,08/May/14 20:25,12/Aug/14 15:46,14/Jul/23 06:25,10/Aug/14 04:08,1.0.0,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,,,,,,"Sorry, I know this is pedantic, but I've been browsing the source multiple times and gotten fooled into thinking reduceByKey always requires a partitioner.",,apachespark,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391493,,,Tue Aug 12 15:42:37 UTC 2014,,,,,,,,,,"0|i1vfcf:",391706,,,,,,,,,,,,,1.1.0,,,,,,,,,,"08/Aug/14 20:46;apachespark;User 'copester' has created a pull request for this issue:
https://github.com/apache/spark/pull/1859;;;","10/Aug/14 04:08;pwendell;Resolved by: https://github.com/apache/spark/pull/1859;;;","12/Aug/14 15:42;apachespark;User 'copester' has created a pull request for this issue:
https://github.com/apache/spark/pull/1906;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify a typo in monitoring.md,SPARK-1765,12713142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,sarutak,sarutak,08/May/14 18:18,15/May/14 17:59,14/Jul/23 06:25,15/May/14 17:59,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"There is a word 'JXM' In monitoring.md.
I guess, it's a typo for 'JMX'.",,aash,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391458,,,Tue May 13 03:51:38 UTC 2014,,,,,,,,,,"0|i1vf4v:",391672,,,,,,,,,,,,,,,,,,,,,,,"08/May/14 18:28;sarutak;I've pull-requested for PR-698.;;;","13/May/14 03:51;aash;https://github.com/apache/spark/pull/698

This can now be closed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOF reached before Python server acknowledged,SPARK-1764,12713136,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,bouk,bouk,08/May/14 17:52,15/Sep/14 17:23,14/Jul/23 06:25,12/Sep/14 00:01,1.0.0,,,,,,,,1.1.0,,,,,Mesos,PySpark,,,,3,mesos,pyspark,,,,"I'm getting ""EOF reached before Python server acknowledged"" while using PySpark on Mesos. The error manifests itself in multiple ways. One is:

{noformat}
14/05/08 18:10:40 ERROR DAGSchedulerActorSupervisor: eventProcesserActor failed due to the error EOF reached before Python server acknowledged; shutting down SparkContext
{noformat}

And the other has a full stacktrace:

{noformat}
14/05/08 18:03:06 ERROR OneForOneStrategy: EOF reached before Python server acknowledged
org.apache.spark.SparkException: EOF reached before Python server acknowledged
	at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:416)
	at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:387)
	at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:71)
	at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:279)
	at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:277)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.Accumulators$.add(Accumulators.scala:277)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:818)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1204)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}

This error causes the SparkContext to shutdown. I have not been able to reliably reproduce this bug, it seems to happen randomly, but if you run enough tasks on a SparkContext it'll hapen eventually",,ajatix,berngp,bouk,coderfi,davies,skrasser,therealnb,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391452,,,Fri Sep 12 00:01:22 UTC 2014,,,,,,,,,,"0|i1vf3j:",391666,,,,,,,,,,,,,,,,,,,,,,,"08/May/14 18:29;bouk;I can semi-reliably recreate this by just running this code:

{quote}
while True:
  sc.parallelize(range(100)).map(lambda n: n * 2).collect()
{quote}

Running this on Mesos will eventually crash with 

Py4JJavaError: An error occurred while calling o1142.collect.
: org.apache.spark.SparkException: Job 101 cancelled as part of cancellation of all jobs
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
	at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:998)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:499)
	at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1151)
	at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1147)
	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295)
	at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:253)
	at akka.actor.ActorCell.handleFailure(ActorCell.scala:338)
	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:423)
	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)
	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)
	at akka.dispatch.Mailbox.run(Mailbox.scala:218)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


I0508 18:29:03.623627  7868 sched.cpp:730] Stopping framework '20140508-173240-16842879-5050-24645-0032'
14/05/08 18:29:04 ERROR OneForOneStrategy: EOF reached before Python server acknowledged
org.apache.spark.SparkException: EOF reached before Python server acknowledged
	at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:416)
	at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:387)
	at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:71)
	at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:279)
	at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:277)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.Accumulators$.add(Accumulators.scala:277)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:818)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1204)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107);;;","12/May/14 18:33;bouk;I did some more digging into this and I have no idea what's the exact issue. The write to the Python server succeeds (which I checked from the Python side) but the Scala side doesn't seem to be able to read the acknowledgement. 

I have also confirmed that it isn't an issue with the Python broadcast server dying, as commented out the exception makes it work fine (!) ;;;","12/May/14 19:07;berngp;We just ran `sc.parallelize(range(100)).map(lambda n: n * 2).collect()` on a Mesos 0.18.1 cluster with the latest spark and it worked. Could you confirm the Spark & Mesos version you are using (if using master please include the sha/commit hash).;;;","12/May/14 21:08;bouk;Interesting, including SPARK-1806 in our build made it stop failing... I guess this can be considered fixed then;;;","01/Jul/14 12:27;therealnb;I have seen this issue in a cut of the main branch take on friday 27th June (last commit https://github.com/srowen/spark/commit/18f29b96c7e0948f5f504e522e5aa8a8d1ab163e )

It does run successfully for a time (~800 requests) but then fails. This has happened on two different clusters, mesos 0.18.0 and 0.18.2, different sites.

One interesting side effect is as this is running, the number of file handles in use on the slaves goes up to many thousands. This could be the resource that is running out. One of the slaves (different machines) always dies and the job is eventually aborted.

This happens with a real app that is actually doing stuff with 18GB RDDs, but also with the simple, but brutal while true command below. A similar loop in scala seems to work fine, indefinitely.

I would be interested to hear if anyone can run this command for a significant amount of time because that would point to our environment in some way.

Using Python version 2.7.6 (default, Jan 17 2014 10:13:17)
SparkContext available as sc.

In [1]: while True:                       
    sc.parallelize(range(100)).map(lambda n: n * 2).collect()
   ...:     

it ran for a while then...

14/06/27 13:59:08 INFO TaskSetManager: Finished TID 778 in 63 ms on guavcpt-ch2-a28p.sys.comcast.net (progress: 1/8)
14/06/27 13:59:08 INFO DAGScheduler: Completed ResultTask(97, 2)
14/06/27 13:59:08 INFO TaskSetManager: Finished TID 777 in 70 ms on guavcpt-ch2-a32p.sys.comcast.net (progress: 2/8)
14/06/27 13:59:08 INFO DAGScheduler: Completed ResultTask(97, 1)
14/06/27 13:59:08 INFO TaskSetManager: Finished TID 782 in 104 ms on guavcpt-ch2-a28p.sys.comcast.net (progress: 3/8)
14/06/27 13:59:08 INFO DAGScheduler: Completed ResultTask(97, 6)
14/06/27 13:59:08 INFO TaskSetManager: Finished TID 781 in 105 ms on guavcpt-ch2-a32p.sys.comcast.net (progress: 4/8)
14/06/27 13:59:08 INFO DAGScheduler: Completed ResultTask(97, 5)
14/06/27 13:59:08 ERROR DAGSchedulerActorSupervisor: eventProcesserActor failed due to the error EOF reached before Python server acknowledged; shutting down SparkContext
14/06/27 13:59:08 INFO TaskSchedulerImpl: Cancelling stage 97
14/06/27 13:59:08 INFO DAGScheduler: Could not cancel tasks for stage 97
java.lang.UnsupportedOperationException
at org.apache.spark.scheduler.SchedulerBackend$class.killTask(SchedulerBackend.scala:32)
at org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend.killTask(MesosSchedulerBackend.scala:41)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:185)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:183)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:183)
at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:183)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:176)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:176)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1066)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1052)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1052)
at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1052)
at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1005)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:502)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:502)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:502)
at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:502)
at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1167)
at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1162)
at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295)
at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:253)
at akka.actor.ActorCell.handleFailure(ActorCell.scala:338)
at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:423)
at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)
at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)
at akka.dispatch.Mailbox.run(Mailbox.scala:218)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/06/27 13:59:08 INFO TaskSetManager: Finished TID 776 in 151 ms on guavcpt-ch2-a31p.sys.comcast.net (progress: 5/8)
14/06/27 13:59:08 INFO TaskSetManager: Finished TID 780 in 147 ms on guavcpt-ch2-a31p.sys.comcast.net (progress: 6/8)
14/06/27 13:59:08 INFO TaskSetManager: Finished TID 779 in 188 ms on guavcpt-ch2-a27p.sys.comcast.net (progress: 7/8)
14/06/27 13:59:08 INFO TaskSetManager: Finished TID 783 in 193 ms on guavcpt-ch2-a27p.sys.comcast.net (progress: 8/8)
14/06/27 13:59:08 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool 
14/06/27 13:59:09 WARN QueuedThreadPool: 5 threads could not be stopped
14/06/27 13:59:09 INFO SparkUI: Stopped Spark web UI at http://guavcpt-ch2-a26p.sys.comcast.net:4041
14/06/27 13:59:09 INFO DAGScheduler: Stopping DAGScheduler
I0627 13:59:09.129179 14267 sched.cpp:730] Stopping framework '20140605-200137-607132588-5050-19211-0073'
14/06/27 13:59:09 INFO MesosSchedulerBackend: driver.run() returned with code DRIVER_STOPPED
14/06/27 13:59:10 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
14/06/27 13:59:10 INFO ConnectionManager: Selector thread was interrupted!
14/06/27 13:59:10 INFO ConnectionManager: ConnectionManager stopped
14/06/27 13:59:10 INFO MemoryStore: MemoryStore cleared
14/06/27 13:59:10 INFO BlockManager: BlockManager stopped
14/06/27 13:59:10 INFO BlockManagerMasterActor: Stopping BlockManagerMaster
14/06/27 13:59:10 INFO BlockManagerMaster: BlockManagerMaster stopped
14/06/27 13:59:10 INFO SparkContext: Successfully stopped SparkContext
14/06/27 13:59:10 ERROR OneForOneStrategy: EOF reached before Python server acknowledged
org.apache.spark.SparkException: EOF reached before Python server acknowledged
at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:613)
at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:584)
at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:72)
at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:280)
at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:278)
at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
at org.apache.spark.Accumulators$.add(Accumulators.scala:278)
at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:825)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1223)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/06/27 13:59:10 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
14/06/27 13:59:10 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
14/06/27 13:59:10 INFO Remoting: Remoting shut down

;;;","17/Jul/14 22:47;tnachen;I'm not sure how this is related to Mesos, is this reproable using YARN or standalone?;;;","18/Jul/14 06:07;therealnb;Hi;
Never used yarn. Doesn't happen on standalone.

;;;","25/Aug/14 19:01;davies;This issue should be fixed in SPARK-2282 [1], I had ran the jobs above against mesos-0.19.1 after more than a hour without problems.

[~therealnb] Could you also verify this?

[1] https://github.com/apache/spark/commit/ef4ff00f87a4e8d38866f163f01741c2673e41da;;;","25/Aug/14 19:15;therealnb;Hi;
Sadly I moved jobs and I don't have a working Spark environment at the moment (I will be doing some Spark work soon :-). I'll pass this on to the guys that are still there and get them to confirm. 
Cheers;;;","11/Sep/14 23:44;coderfi;FYI the workaround described in SPARK-2282 got me past the same ""EOF reached"" issue.

i.e.

echo ""1"" > /proc/sys/net/ipv4/tcp_tw_reuse
echo ""1"" > /proc/sys/net/ipv4/tcp_tw_recycle

then restart your spark shell (or program).

Apparently a fix is slated for Spark 1.1

I am running Spark 1.0.2 under Mesos 0.18.2
;;;","12/Sep/14 00:01;davies;This is fixed by #2282;;;",,,,,,,,,,,,,,,,,,,,,,
 mvn  -Dsuites=*  test throw an ClassNotFoundException,SPARK-1760,12713096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,08/May/14 14:29,09/May/14 09:53,14/Jul/23 06:25,09/May/14 08:51,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"{{mvn -Dhadoop.version=0.23.9 -Phadoop-0.23 -Dsuites=org.apache.spark.repl.ReplSuite test}} => 
{code}
*** RUN ABORTED ***
  java.lang.ClassNotFoundException: org.apache.spark.repl.ReplSuite
  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
  at org.scalatest.tools.Runner$$anonfun$21.apply(Runner.scala:1470)
  at org.scalatest.tools.Runner$$anonfun$21.apply(Runner.scala:1469)
  at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264)
  at scala.collection.immutable.List.foreach(List.scala:318)
  ...
{code}",,gq,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391412,,,Fri May 09 09:53:30 UTC 2014,,,,,,,,,,"0|i1vev3:",391627,,,,,,,,,,,,,,,,,,,,,,,"08/May/14 15:18;srowen;Yeah I think you would have to run this from the `repl/` module for it it work. At least it does for me, and that makes sense. I think the docs just need to note that, at: https://spark.apache.org/docs/0.9.1/building-with-maven.html  (and that suite name can be updated to include ""org.apache"");;;","08/May/14 16:05;gq;Hi, [~srowen]
Is there a perfect solution?
The [building-with-maven.md|https://github.com/apache/spark/blob/master/docs/building-with-maven.md] has been updated;;;","09/May/14 08:27;gq;{{mvn -Dhadoop.version=0.23.9 -Phadoop-0.23 -DwildcardSuites=org.apache.spark.rdd.RDDSuite test}} is OK;;;","09/May/14 08:51;pwendell;Issue resolved by pull request 712
[https://github.com/apache/spark/pull/712];;;","09/May/14 09:10;srowen;If `wildcardSuites` lets you invoke specific suites across the whole project, then that sounds like an ideal solution. If it works then I'd propose that as a small doc change?;;;","09/May/14 09:53;gq;Yes, [PR 712 |https://github.com/apache/spark/pull/712] have been merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support saving null primitives with .saveAsParquetFile(),SPARK-1757,12713024,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,aash,aash,08/May/14 06:24,14/May/14 09:04,14/Jul/23 06:25,13/May/14 03:49,1.0.0,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"See stack trace below:

{noformat}
14/05/07 21:45:51 INFO analysis.Analyzer: Max iterations (2) reached for batch MultiInstanceRelations
14/05/07 21:45:51 INFO analysis.Analyzer: Max iterations (2) reached for batch CaseInsensitiveAttributeReferences
14/05/07 21:45:51 INFO optimizer.Optimizer$: Max iterations (2) reached for batch ConstantFolding
14/05/07 21:45:51 INFO optimizer.Optimizer$: Max iterations (2) reached for batch Filter Pushdown
java.lang.RuntimeException: Unsupported datatype StructType(List())
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetRelation.scala:201)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$1.apply(ParquetRelation.scala:235)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$1.apply(ParquetRelation.scala:235)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertFromAttributes(ParquetRelation.scala:234)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.writeMetaData(ParquetRelation.scala:267)
        at org.apache.spark.sql.parquet.ParquetRelation$.createEmpty(ParquetRelation.scala:143)
        at org.apache.spark.sql.parquet.ParquetRelation$.create(ParquetRelation.scala:122)
        at org.apache.spark.sql.execution.SparkStrategies$ParquetOperations$.apply(SparkStrategies.scala:139)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:264)
        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:264)
        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:265)
        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:265)
        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:268)
        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:268)
        at org.apache.spark.sql.SchemaRDDLike$class.saveAsParquetFile(SchemaRDDLike.scala:66)
        at org.apache.spark.sql.SchemaRDD.saveAsParquetFile(SchemaRDD.scala:96)
{noformat}",,aash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391340,,,Tue May 13 03:49:52 UTC 2014,,,,,,,,,,"0|i1veg7:",391560,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 03:49;aash;https://github.com/apache/spark/pull/690;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-submit --name does not resolve to application name on YARN,SPARK-1755,12713010,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor,08/May/14 04:15,05/Nov/14 10:45,14/Jul/23 06:25,09/May/14 03:45,0.9.1,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"In YARN client mode, --name is ignored because the deploy mode is client, and the name is for some reason a [cluster config|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L170)].

In YARN cluster mode, --name is passed to the org.apache.spark.deploy.yarn.Client as a command line argument. The Client class, however, uses this name only as the [app name for the RM|https://github.com/apache/spark/blob/master/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L80], but not for Spark. In other words, when SparkConf attempts to load default configs, application name is not set.

In both cases, passing --name to SparkSubmit does not actually cause Spark to adopt it as its application name, despite what the usage promises.",,andrewor,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1664,,,,SPARK-1631,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391326,,,Fri May 09 03:45:46 UTC 2014,,,,,,,,,,"0|i1ved3:",391546,,,,,,,,,,,,,,,,,,,,,,,"08/May/14 04:22;andrewor;SPARK-1631 is concerned with fixing the case in which the appName is set manually through the SparkConf, while SPARK-1755 is concerned with SparkSubmit.;;;","08/May/14 21:06;tgraves;I believe this is a dup of SPARK-1664
spark-submit --name doesn't work in yarn-client mode;;;","08/May/14 22:24;andrewor;You are correct. However, it also doesn't work for yarn-cluster.;;;","08/May/14 22:25;andrewor;https://github.com/apache/spark/pull/699;;;","09/May/14 03:45;pwendell;Issue resolved by pull request 699
[https://github.com/apache/spark/pull/699];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark on YARN does not work on assembly jar built on Red Hat based OS,SPARK-1753,12712986,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,08/May/14 00:36,03/Mar/15 14:40,14/Jul/23 06:25,13/May/14 02:44,1.0.0,,,,,,,,1.0.0,,,,,PySpark,Spark Core,,,,0,,,,,,"If the jar is built on a Red Hat based OS, the additional python files included in the jar cannot be accessed. This means PySpark doesn't work on YARN because in this mode it relies on the python files within this jar.

I have confirmed that my Java, Scala, and maven versions are all exactly the same on my CentOS environment and on my local OSX environment, and the former does not work. Thomas Graves also struggled with the same problem.

Until a fix is found, we should at the very least document this peculiarity.",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1911,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391302,,,Tue May 13 02:44:49 UTC 2014,,,,,,,,,,"0|i1ve7r:",391522,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 02:44;pwendell;Issue resolved by pull request 701
[https://github.com/apache/spark/pull/701];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark ec2 scripts should check for SSh to be up,SPARK-1751,12712943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,nchammas,holden,holden,07/May/14 21:15,13/Dec/14 22:21,14/Jul/23 06:25,13/Dec/14 22:21,,,,,,,,,,,,,,EC2,,,,,1,,,,,,When launching a large cluster you currently need large delay for after launch so that ssh has come up on the hosts. Instead we could poll for ssh being up on the hosts.,,holden,joshrosen,slcclimber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3398,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391259,,,Sat Dec 13 22:21:11 UTC 2014,,,,,,,,,,"0|i1vdy7:",391479,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/14 22:21;joshrosen;This was addressed by SPARK-3398, so I'm marking this issue as 'Fixed'.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EdgePartition is not serialized properly,SPARK-1750,12712938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jegonzal,ankurd,ankurd,07/May/14 20:54,26/May/14 20:20,14/Jul/23 06:25,26/May/14 18:15,0.9.0,0.9.1,1.0.0,,,,,,1.0.0,,,,,GraphX,,,,,1,,,,,,"The GraphX design attempts to avoid moving edges across the network, instead shipping the vertices to the edge partitions. However, Spark sometimes needs to move the edges, such as for straggler mitigation.

All EdgePartition fields are currently declared transient, so the edges will not be serialized properly. Even if they are not marked transient, Kryo is unable to serialize the EdgePartition, failing with the following error:

{code}
java.lang.IllegalArgumentException: Can not set final org.apache.spark.graphx.util.collection.PrimitiveKeyOpenHashMap field org.apache.spark.graphx.impl.EdgePartition.index to scala.collection.immutable.$colon$colon
{code}

A workaround is to discourage Spark from moving the edges by setting {{spark.locality.wait}} to a high value such as 100000.",,ankurd,holden,jegonzal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391254,,,Mon May 26 18:15:35 UTC 2014,,,,,,,,,,"0|i1vdx3:",391474,,,,,,,,,,,,,,,,,,,,,,,"12/May/14 03:16;jegonzal;I believe this issue is resolved with PR #724.

;;;","26/May/14 18:15;ankurd;https://github.com/apache/spark/pull/742;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler supervisor strategy broken with Mesos,SPARK-1749,12712920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,markhamstra,bouk,bouk,07/May/14 19:19,26/Jun/14 03:58,14/Jul/23 06:25,26/Jun/14 03:58,1.0.0,,,,,,,,1.0.1,1.1.0,,,,Mesos,Spark Core,,,,1,mesos,scheduler,scheduling,,,"Any bad Python code will trigger this bug, for example `sc.parallelize(range(100)).map(lambda n: undefined_variable * 2).collect()` will cause a `undefined_variable isn't defined`, which will cause spark to try to kill the task, resulting in the following stacktrace:

java.lang.UnsupportedOperationException
	at org.apache.spark.scheduler.SchedulerBackend$class.killTask(SchedulerBackend.scala:32)
	at org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend.killTask(MesosSchedulerBackend.scala:41)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:184)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:182)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:182)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:182)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:175)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:175)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1058)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1045)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1045)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1045)
	at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:998)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:499)
	at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1151)
	at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1147)
	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295)
	at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:253)
	at akka.actor.ActorCell.handleFailure(ActorCell.scala:338)
	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:423)
	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)
	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)
	at akka.dispatch.Mailbox.run(Mailbox.scala:218)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

This is because killTask isn't implemented for the MesosSchedulerBackend. I assume this isn't pyspark-specific, as there will be other instances where you might want to kill the task ",,aash,berngp,bouk,codingcat,markhamstra,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391236,,,Thu Jun 26 03:58:15 UTC 2014,,,,,,,,,,"0|i1vdtb:",391457,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"07/May/14 20:54;markhamstra;The failure is bad, but not all that bad, since it occurs when the Supervisor actor in the DAGScheduler is already trying to cancel running jobs and shut down the system after having caught an exception thrown during the processing of DAGScheduler event.

I'm not going to try to fix anything in PySpark in the PR addressing this issue -- in part because it isn't clear to me from the stack trace just what PySpark is doing to cause the failure in the DAGScheduler; but once the DAGScheduler does catch the exception thrown by the eventProcessActor, it should handle it better when Mesos is doing the task scheduling, and I can fix that.

Another issue specific to Python may need to be filed as a follow up.  ;;;","09/May/14 14:40;bouk;This isn't really PySpark specific, this works fine on other backends which will mark the task as failed and just keep the SparkContext running.

It shouldn't be shutting down the whole SparkContext just because a single job failed;;;","26/Jun/14 03:58;pwendell;Issue resolved by pull request 1219
[https://github.com/apache/spark/pull/1219];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark cancellation kills unrelated pyspark workers,SPARK-1740,12712657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,ilikerps,ilikerps,06/May/14 19:28,03/Aug/14 22:53,14/Jul/23 06:25,03/Aug/14 22:53,1.0.0,,,,,,,,1.1.0,,,,,PySpark,,,,,0,,,,,,"PySpark cancellation calls SparkEnv#destroyPythonWorker. Since there is one python worker per process, this would seem like a sensible thing to do. Unfortunately, this method actually destroys a python daemon, and all associated workers, which generally means that we can cause failures in unrelated Pyspark jobs.

The severity of this bug is limited by the fact that the Pyspark daemon is easily recreated, so the tasks will succeed after being restarted.",,adgaudio,apachespark,davies,ilikerps,joshrosen,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390973,,,Wed Jul 30 00:15:58 UTC 2014,,,,,,,,,,"0|i1vc9b:",391205,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/14 17:07;tgraves;On yarn when I run pyspark it kills the executors after a single action.  Perhaps this is caused by this same issue. 

This could be a much bigger deal on yarn since when it kills the executors, it has to go back to the resource manager to get more containers.  This is an aweful lot of thrashing of containers and could cause major headaches.;;;","26/Jun/14 18:23;joshrosen;The ""Python daemon -> multiple workers"" architecture was motivated by the high cost of forking a JVM with a large heap, so this issue could also lead to performance problems if we attempt to re-launch the daemon once a huge amount of data has been cached in the Spark worker JVM.;;;","30/Jul/14 00:15;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/1643;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warn rather than fail when Java 7+ is used to create distributions,SPARK-1737,12712584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,dkanoafry,dkanoafry,06/May/14 14:24,06/May/14 22:42,14/Jul/23 06:25,06/May/14 22:42,1.0.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"the merged PR for SPARK-1703 made it so that make-distribution.sh will never work with JDK 1.7+

why not have make-distribution.sh perform a check similar to compute-classpath.sh?
https://github.com/apache/spark/pull/627/files",JDK 1.7.0_45-b18,deric,dkanoafry,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390900,,,Tue May 06 22:42:10 UTC 2014,,,,,,,,,,"0|i1vbtz:",391135,,,,,,,,,,,,,,,,,,,,,,,"06/May/14 19:13;pwendell;Even though SPARK-1703 adds logging, once the user sees this logging it's too late, so the idea here was to try and make sure people creating distributions understood the bug.

I'd be fine to add a flag called --allow-java-7 to make-distribution that avoids this check.;;;","06/May/14 19:34;dkanoafry;thanks for the insight & flexibility; i'm not quite sure i understand though, is Spark not expected to work properly/well if the user builds it with JDK-7 and runs with JDK-7 ?;;;","06/May/14 20:04;deric;Is there same issue between JDK-7 and JDK8? At the time when there's a tendency of introducing Java 8 features to Spark I don't see a good reason why limit Spark only to JDK-6 (http://blog.cloudera.com/blog/2014/04/making-apache-spark-easier-to-use-in-java-with-java-8/)

Wouldn't be better if `make-distribution.sh` would store JAVA_VERSION in the distribution and then check it against JAVA_VERSION on slave node?;;;","06/May/14 21:06;pwendell;Yes let's actually make this more tame. We can just log a warning in make-distribution that explains if it's compiled with Java 7+ it won't work on Java 6. Indeed, some people will even want to compile with Java 8.;;;","06/May/14 21:13;pwendell;https://github.com/apache/spark/pull/669/files;;;","06/May/14 21:15;pwendell;The only problem is if Spark is built with Java 7 or 8 and run with Java 6. We just want to make it clear to someone building that they might hit this problem.;;;","06/May/14 22:42;pwendell;Issue resolved by pull request 669
[https://github.com/apache/spark/pull/669];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Make distribution script has missing profiles for special hadoop versions,SPARK-1735,12712491,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andrewor14,andrewor14,andrewor,06/May/14 03:37,05/Nov/14 10:45,14/Jul/23 06:25,06/May/14 05:15,0.9.1,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"SPARK-1556 introduced new profiles for hadoop versions 2.2.x, 2.3.x and 2.4.x, such that without these a java version error will be thrown at run time.",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1556,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390807,,,Tue May 06 05:15:11 UTC 2014,,,,,,,,,,"0|i1vb9z:",391045,,,,,,,,,,,,,,,,,,,,,,,"06/May/14 05:15;pwendell;Issue resolved by pull request 660
[https://github.com/apache/spark/pull/660];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for primitive nulls in SparkSQL,SPARK-1732,12712463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,06/May/14 00:21,06/May/14 06:00,14/Jul/23 06:25,06/May/14 06:00,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"Right now you can't have null values for primitives, we should support Option and primitive classes (Integer).",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390779,,,2014-05-06 00:21:43.0,,,,,,,,,,"0|i1vb4f:",391019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark broadcast values with custom classes won't depickle properly,SPARK-1731,12712460,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bouk,bouk,bouk,06/May/14 00:10,10/May/14 20:02,14/Jul/23 06:25,10/May/14 20:02,,,,,,,,,1.0.0,,,,,PySpark,,,,,0,,,,,,,,bouk,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390776,,,Sat May 10 20:02:41 UTC 2014,,,,,,,,,,"0|i1vb3r:",391016,,,,,,,,,,,,,,,,,,,,,,,"10/May/14 20:02;pwendell;Issue resolved by pull request 656
[https://github.com/apache/spark/pull/656];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaRDDLike.mapPartitionsWithIndex requires ClassTag,SPARK-1728,12712441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sandyr,sandyr,sandyr,05/May/14 22:24,06/May/14 01:26,14/Jul/23 06:25,06/May/14 01:26,,,,,,,,,1.0.0,,,,,Java API,,,,,0,,,,,,We should pass a fake classtag like other JavaRDDLike methods,,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390757,,,Tue May 06 01:26:58 UTC 2014,,,,,,,,,,"0|i1vazj:",390997,,,,,,,,,,,,,,,,,,,,,,,"06/May/14 00:22;sandyr;https://github.com/apache/spark/pull/657;;;","06/May/14 01:26;pwendell;Issue resolved by pull request 657
[https://github.com/apache/spark/pull/657];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Correct small compile errors, typos, and markdown issues in (primarly) MLlib docs",SPARK-1727,12712439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,05/May/14 22:19,15/Jan/15 09:08,14/Jul/23 06:25,07/May/14 03:07,0.9.1,,,,,,,,1.0.0,,,,,Documentation,,,,,0,,,,,,"While play-testing the Scala and Java code examples in the MLlib docs, I noticed a number of small compile errors, and some typos. This led to finding and fixing a few similar items in other docs. 

Then in the course of building the site docs to check the result, I found a few small suggestions for the build instructions. I also found a few more formatting and markdown issues uncovered when I accidentally used maruku instead of kramdown.",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390755,,,Wed May 07 03:07:46 UTC 2014,,,,,,,,,,"0|i1vaz3:",390995,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 03:07;pwendell;Issue resolved by pull request 653
[https://github.com/apache/spark/pull/653];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tasks that fail to serialize remain in active stages forever.,SPARK-1726,12712435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,marmbrus,marmbrus,05/May/14 22:10,25/Jul/14 22:17,14/Jul/23 06:25,25/Jul/14 22:17,1.0.1,,,,,,,,1.1.0,,,,,Web UI,,,,,0,,,,,,"In the spark shell.
{code}
scala> class Adder(x: Int) { def apply(a: Int) = a + x }
defined class Adder
scala> val add = new Adder(10)
scala> sc.parallelize(1 to 10).map(add(_)).collect()
{code}

You get:
{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: $iwC$$iwC$$iwC$$iwC$Adder
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:770)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:713)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1176)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

However, the web ui is messed up.  See attached screen shot.",,andrewor,apachespark,kayousterhout,marmbrus,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2567,,,,,,,,,,,"05/May/14 22:12;marmbrus;ZombieTask.tiff;https://issues.apache.org/jira/secure/attachment/12643439/ZombieTask.tiff",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390751,,,Thu Jul 24 05:05:51 UTC 2014,,,,,,,,,,"0|i1vay7:",390991,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 22:44;marmbrus;[~kayousterhout] reports this is fixed in master.;;;","23/Jul/14 23:13;kayousterhout;[~marmbrus] Sorry I spoke too soon -- this is still broken but much less broken than before.  By default, SparkContext.clean() now checks if tasks are serializable, which typically prevents this problem because the stage never gets submitted.  However, checking if tasks are serializable can be disabled (see https://github.com/apache/spark/pull/143), in which case you can still see this problem.;;;","24/Jul/14 05:05;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/1566;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't use broadcast variables in pyspark on Mesos because pyspark isn't added to PYTHONPATH,SPARK-1725,12712388,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,bouk,bouk,bouk,05/May/14 19:13,09/May/14 03:44,14/Jul/23 06:25,09/May/14 03:44,1.0.0,,,,,,,,1.0.0,,,,,Mesos,PySpark,Spark Core,,,0,,,,,,,,bouk,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390704,,,Fri May 09 03:44:25 UTC 2014,,,,,,,,,,"0|i1vaov:",390949,,,,,,,,,,,,,,,,,,,,,,,"09/May/14 03:44;pwendell;Closed via:
https://github.com/apache/spark/pull/651;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Classloaders not used correctly in Mesos,SPARK-1721,12712352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,bouk,pwendell,pwendell,05/May/14 17:30,05/May/14 18:20,14/Jul/23 06:25,05/May/14 18:20,1.0.0,,,,,,,,1.0.0,,,,,Mesos,Spark Core,,,,0,,,,,,"Reported in https://github.com/apache/spark/pull/620. 

Basically, there are still some cases not caught in SPARK-1480 where we still need to pass the correct classloader when loading classes.

{code}
I0502 17:31:42.672224 14688 exec.cpp:131] Version: 0.18.0
I0502 17:31:42.674959 14707 exec.cpp:205] Executor registered on slave 20140501-182306-16842879-5050-10155-0
14/05/02 17:31:42 INFO MesosExecutorBackend: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
14/05/02 17:31:42 INFO MesosExecutorBackend: Registered with Mesos as executor ID 20140501-182306-16842879-5050-10155-0
14/05/02 17:31:43 INFO SecurityManager: Changing view acls to: vagrant
14/05/02 17:31:43 INFO SecurityManager: SecurityManager, is authentication enabled: false are ui acls enabled: false users with view permissions: Set(vagrant)
14/05/02 17:31:43 INFO Slf4jLogger: Slf4jLogger started
14/05/02 17:31:43 INFO Remoting: Starting remoting
14/05/02 17:31:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@localhost:50843]
14/05/02 17:31:43 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@localhost:50843]
java.lang.ClassNotFoundException: org/apache/spark/serializer/JavaSerializer
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:270)
    at org.apache.spark.SparkEnv$.instantiateClass$1(SparkEnv.scala:165)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:176)
    at org.apache.spark.executor.Executor.<init>(Executor.scala:106)
    at org.apache.spark.executor.MesosExecutorBackend.registered(MesosExecutorBackend.scala:56)
Exception in thread ""Thread-0"" I0502 17:31:43.710039 14707 exec.cpp:412] Deactivating the executor libprocess
{code}",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390668,,,Mon May 05 18:20:50 UTC 2014,,,,,,,,,,"0|i1vah3:",390913,,,,,,,,,,,,,,,,,,,,,,,"05/May/14 18:20;pwendell;Fixed by:
https://github.com/apache/spark/pull/620;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use LD_LIBRARY_PATH instead of -Djava.library.path,SPARK-1720,12712323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gq,tgraves,tgraves,05/May/14 14:43,30/Oct/14 09:58,14/Jul/23 06:25,30/Oct/14 09:58,1.0.0,,,,,,,,1.2.0,,,,,Deploy,,,,,0,,,,,,"I think it would be better to use LD_LIBRARY_PATH rather then -Djava.library.path.  Once  java.library.path is set, it doesn't search LD_LIBRARY_PATH.  In Hadoop we switched to use LD_LIBRARY_PATH instead of java.library.path.  See https://issues.apache.org/jira/browse/MAPREDUCE-4072.",,apachespark,diederik,farrellee,gq,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390639,,,Thu Oct 30 09:58:34 UTC 2014,,,,,,,,,,"0|i1vab3:",390886,,,,,,,,,,,,,1.2.0,,,,,,,,,,"22/Sep/14 22:15;pwendell;Another user reported this issue, so let's try to get it into spark 1.2;;;","08/Oct/14 13:40;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2711;;;","30/Oct/14 09:58;gq;Issue resolved by pull request 2711
[https://github.com/apache/spark/pull/2711]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-ec2.py sometimes doesn't wait long enough for EC2 to stand up,SPARK-1717,12712230,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,msiddalingaiah,msiddalingaiah,05/May/14 03:11,05/May/14 04:59,14/Jul/23 06:25,05/May/14 04:59,0.9.0,1.0.0,,,,,,,1.0.0,,,,,EC2,,,,,0,easyfix,,,,,"spark-ec2.py tries several times to perform operations on EC2. Sometimes EC2 takes longer than expected (I've seen this with spot instances, not sure if it's related) and spark-ec2.py raises an exception even though the instances do eventually come up.",Linux,msiddalingaiah,pwendell,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390549,,,Mon May 05 04:59:46 UTC 2014,,,,,,,,,,"0|i1v9sf:",390802,,,,,,,,,,,,,,,,,,,,,,,"05/May/14 03:25;msiddalingaiah;Increased number of tries from 2 to 5 in two places.;;;","05/May/14 04:59;pwendell;Issue resolved by pull request 641
[https://github.com/apache/spark/pull/641];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EC2 script should exit with non-zero code on UsageError,SPARK-1716,12712224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,douglaz,douglaz,douglaz,05/May/14 01:13,05/May/14 03:37,14/Jul/23 06:25,05/May/14 03:37,,,,,,,,,1.0.0,,,,,EC2,,,,,0,,,,,,"One reason is that some ssh errors are raised as UsageError, preventing an automated usage of the script from detecting the failure.",,douglaz,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390543,,,Mon May 05 03:37:25 UTC 2014,,,,,,,,,,"0|i1v9r3:",390796,,,,,,,,,,,,,,,,,,,,,,,"05/May/14 01:14;douglaz;PR here:
https://github.com/apache/spark/pull/638;;;","05/May/14 03:37;pwendell;Issue resolved by pull request 638
[https://github.com/apache/spark/pull/638];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParallelCollectionRDD operations hanging forever without any error messages ,SPARK-1712,12712205,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,pkolaczk,pkolaczk,04/May/14 19:14,18/Jun/14 14:41,14/Jul/23 06:25,28/May/14 22:58,0.9.0,,,,,,,,0.9.2,1.0.1,,,,Spark Core,,,,,0,,,,,," conf/spark-defaults.conf
{code}
spark.akka.frameSize         5
spark.default.parallelism    1
{code}
{noformat}
scala> val collection = (1 to 1000000).map(i => (""foo"" + i, i)).toVector
collection: Vector[(String, Int)] = Vector((foo1,1), (foo2,2), (foo3,3), (foo4,4), (foo5,5), (foo6,6), (foo7,7), (foo8,8), (foo9,9), (foo10,10), (foo11,11), (foo12,12), (foo13,13), (foo14,14), (foo15,15), (foo16,16), (foo17,17), (foo18,18), (foo19,19), (foo20,20), (foo21,21), (foo22,22), (foo23,23), (foo24,24), (foo25,25), (foo26,26), (foo27,27), (foo28,28), (foo29,29), (foo30,30), (foo31,31), (foo32,32), (foo33,33), (foo34,34), (foo35,35), (foo36,36), (foo37,37), (foo38,38), (foo39,39), (foo40,40), (foo41,41), (foo42,42), (foo43,43), (foo44,44), (foo45,45), (foo46,46), (foo47,47), (foo48,48), (foo49,49), (foo50,50), (foo51,51), (foo52,52), (foo53,53), (foo54,54), (foo55,55), (foo56,56), (foo57,57), (foo58,58), (foo59,59), (foo60,60), (foo61,61), (foo62,62), (foo63,63), (foo64,64), (foo...

scala> val rdd = sc.parallelize(collection)
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> rdd.first
res4: (String, Int) = (foo1,1)

scala> rdd.map(_._2).sum
// nothing happens

{noformat}

CPU and I/O idle. 
Memory usage reported by JVM, after manually triggered GC:
repl: 216 MB / 2 GB
executor: 67 MB / 2 GB
worker: 6 MB / 128 MB
master: 6 MB / 128 MB

No errors found in worker's stderr/stdout. 

It works fine with 700,000 elements and then it takes about 1 second to process the request and calculate the sum. With 700,000 items the spark executor memory doesn't even exceed 300 MB out of 2GB available. It fails with 800,000 items.

Multiple parralelized collections of size 700,000 items at the same time in the same session work fine.","Linux Ubuntu 14.04, a single spark node; standalone mode.",gq,matei,pkolaczk,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2156,,,,,,,,,,,"04/May/14 19:15;pkolaczk;executor.jstack.txt;https://issues.apache.org/jira/secure/attachment/12643290/executor.jstack.txt","04/May/14 19:15;pkolaczk;master.jstack.txt;https://issues.apache.org/jira/secure/attachment/12643288/master.jstack.txt","04/May/14 19:15;pkolaczk;repl.jstack.txt;https://issues.apache.org/jira/secure/attachment/12643289/repl.jstack.txt","04/May/14 19:15;pkolaczk;spark-hang.png;https://issues.apache.org/jira/secure/attachment/12643286/spark-hang.png","04/May/14 19:15;pkolaczk;worker.jstack.txt;https://issues.apache.org/jira/secure/attachment/12643287/worker.jstack.txt",,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390524,,,Wed May 28 23:42:32 UTC 2014,,,,,,,,,,"0|i1v9mv:",390777,,,,,,,,,,,,,,,,,,,,,,,"05/May/14 07:20;pkolaczk;The problem goes away if I increase parallelismLevel. 
{noformat}
val rdd = sc.parallelize(collection, 64)
{noformat}

So I guess this is something related to partition size.;;;","05/May/14 16:58;gq;Unable to reproduce the bug.Can you upload the log?;;;","05/May/14 19:30;pkolaczk;This is log from shell:

{noformat}
scala> val rdd = sc.parallelize(collection)
14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: parse(""       val rdd = sc.parallelize(collection)
"") Some(List(val rdd = sc.parallelize(collection)))
14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter:   11: ValDef
  11: TypeTree
  31: Apply
  20: Select
  17: Ident
  32: Ident

14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: parse(""
class $read extends Serializable {
  class $iwC extends Serializable {
val $VAL2 = $line3.$read.INSTANCE;
import $VAL2.$iw.$iw.`sc`;
class $iwC extends Serializable {
import org.apache.spark.SparkContext._
class $iwC extends Serializable {
class $iwC extends Serializable {
import com.datastax.bdp.spark.CassandraFunctions._
class $iwC extends Serializable {
import com.datastax.bdp.spark.context.CassandraContext
class $iwC extends Serializable {
import com.tuplejump.calliope.Implicits._
class $iwC extends Serializable {
val $VAL3 = $line17.$read.INSTANCE;
import $VAL3.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`collection`;
class $iwC extends Serializable {
       val rdd = sc.parallelize(collection)

      

}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;

}
object $read {
  val INSTANCE = new $read();
}

"") Some(List(class $read extends Serializable {
  def <init>() = {
    super.<init>();
    ()
  };
  class $iwC extends Serializable {
    def <init>() = {
      super.<init>();
      ()
    };
    val $VAL2 = $line3.$read.INSTANCE;
    import $VAL2.$iw.$iw.sc;
    class $iwC extends Serializable {
      def <init>() = {
        super.<init>();
        ()
      };
      import org.apache.spark.SparkContext._;
      class $iwC extends Serializable {
        def <init>() = {
          super.<init>();
          ()
        };
        class $iwC extends Serializable {
          def <init>() = {
            super.<init>();
            ()
          };
          import com.datastax.bdp.spark.CassandraFunctions._;
          class $iwC extends Serializable {
            def <init>() = {
              super.<init>();
              ()
            };
            import com.datastax.bdp.spark.context.CassandraContext;
            class $iwC extends Serializable {
              def <init>() = {
                super.<init>();
                ()
              };
              import com.tuplejump.calliope.Implicits._;
              class $iwC extends Serializable {
                def <init>() = {
                  super.<init>();
                  ()
                };
                val $VAL3 = $line17.$read.INSTANCE;
                import $VAL3.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.collection;
                class $iwC extends Serializable {
                  def <init>() = {
                    super.<init>();
                    ()
                  };
                  val rdd = sc.parallelize(collection)
                };
                val $iw = new $iwC()
              };
              val $iw = new $iwC()
            };
            val $iw = new $iwC()
          };
          val $iw = new $iwC()
        };
        val $iw = new $iwC()
      };
      val $iw = new $iwC()
    };
    val $iw = new $iwC()
  };
  val $iw = new $iwC()
}, object $read extends scala.AnyRef {
  def <init>() = {
    super.<init>();
    ()
  };
  val INSTANCE = new $read()
}))
14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: class $read extends Serializable {
  def <init>() = {
    super.<init>;
    ()
  };
  class $iwC extends Serializable {
    def <init>() = {
      super.<init>;
      ()
    };
    val $VAL2 = $line3.$read.INSTANCE;
    import $VAL2.$iw.$iw.sc;
    class $iwC extends Serializable {
      def <init>() = {
        super.<init>;
        ()
      };
      import org.apache.spark.SparkContext._;
      class $iwC extends Serializable {
        def <init>() = {
          super.<init>;
          ()
        };
        class $iwC extends Serializable {
          def <init>() = {
            super.<init>;
            ()
          };
          import com.datastax.bdp.spark.CassandraFunctions._;
          class $iwC extends Serializable {
            def <init>() = {
              super.<init>;
              ()
            };
            import com.datastax.bdp.spark.context.CassandraContext;
            class $iwC extends Serializable {
              def <init>() = {
                super.<init>;
                ()
              };
              import com.tuplejump.calliope.Implicits._;
              class $iwC extends Serializable {
                def <init>() = {
                  super.<init>;
                  ()
                };
                val $VAL3 = $line17.$read.INSTANCE;
                import $VAL3.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.collection;
                class $iwC extends Serializable {
                  def <init>() = {
                    super.<init>;
                    ()
                  };
                  val rdd = sc parallelize collection
                };
                val $iw = new $iwC.<init>
              };
              val $iw = new $iwC.<init>
            };
            val $iw = new $iwC.<init>
          };
          val $iw = new $iwC.<init>
        };
        val $iw = new $iwC.<init>
      };
      val $iw = new $iwC.<init>
    };
    val $iw = new $iwC.<init>
  };
  val $iw = new $iwC.<init>
}
14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: object $read extends scala.AnyRef {
  def <init>() = {
    super.<init>;
    ()
  };
  val INSTANCE = new $read.<init>
}
14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: Set symbol of rdd to val rdd(): org.apache.spark.rdd.RDD
14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: parse(""
object $eval {
  lazy val $result = $line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`rdd`
  val $print: String =  {
    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw
    (""""
      
 + ""rdd: org.apache.spark.rdd.RDD[(String, Int)] = "" + scala.runtime.ScalaRunTime.replStringOf($line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`rdd`, 1000)

    )
  }
}
      
"") Some(List(object $eval extends scala.AnyRef {
  def <init>() = {
    super.<init>();
    ()
  };
  lazy val $result = $line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd;
  val $print: String = {
    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw;
    """".$plus(""rdd: org.apache.spark.rdd.RDD[(String, Int)] = "").$plus(scala.runtime.ScalaRunTime.replStringOf($line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd, 1000))
  }
}))
14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: object $eval extends scala.AnyRef {
  def <init>() = {
    super.<init>;
    ()
  };
  lazy val $result = $line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd;
  val $print: String = {
    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw;
    """".+(""rdd: org.apache.spark.rdd.RDD[(String, Int)] = "").+(scala.runtime.ScalaRunTime.replStringOf($line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd, 1000))
  }
}
14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: Invoking: public static java.lang.String $line18.$eval.$print()
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:21

scala> rdd.map(_._2).sum
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: parse(""       rdd.map(_._2).sum
"") Some(List(rdd.map(((x$1) => x$1._2)).sum))
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter:   21: Select
  14: Apply
  11: Select
  7: Ident
  17: Function
  15: ValDef
  15: TypeTree
  -1: EmptyTree
  17: Select
  15: Ident

14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: parse(""       val res3 =
              rdd.map(_._2).sum
"") Some(List(val res3 = rdd.map(((x$1) => x$1._2)).sum))
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter:   11: ValDef
  11: TypeTree
  46: Select
  39: Apply
  36: Select
  32: Ident
  42: Function
  40: ValDef
  40: TypeTree
  -1: EmptyTree
  42: Select
  40: Ident

14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: parse(""
class $read extends Serializable {
  class $iwC extends Serializable {
val $VAL4 = $line3.$read.INSTANCE;
import $VAL4.$iw.$iw.`sc`;
class $iwC extends Serializable {
import org.apache.spark.SparkContext._
class $iwC extends Serializable {
class $iwC extends Serializable {
import com.datastax.bdp.spark.CassandraFunctions._
class $iwC extends Serializable {
import com.datastax.bdp.spark.context.CassandraContext
class $iwC extends Serializable {
import com.tuplejump.calliope.Implicits._
class $iwC extends Serializable {
val $VAL5 = $line17.$read.INSTANCE;
import $VAL5.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`collection`;
val $VAL6 = $line18.$read.INSTANCE;
import $VAL6.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`rdd`;
class $iwC extends Serializable {
       val res3 =
              rdd.map(_._2).sum

      

}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;
}
val $iw = new $iwC;

}
object $read {
  val INSTANCE = new $read();
}

"") Some(List(class $read extends Serializable {
  def <init>() = {
    super.<init>();
    ()
  };
  class $iwC extends Serializable {
    def <init>() = {
      super.<init>();
      ()
    };
    val $VAL4 = $line3.$read.INSTANCE;
    import $VAL4.$iw.$iw.sc;
    class $iwC extends Serializable {
      def <init>() = {
        super.<init>();
        ()
      };
      import org.apache.spark.SparkContext._;
      class $iwC extends Serializable {
        def <init>() = {
          super.<init>();
          ()
        };
        class $iwC extends Serializable {
          def <init>() = {
            super.<init>();
            ()
          };
          import com.datastax.bdp.spark.CassandraFunctions._;
          class $iwC extends Serializable {
            def <init>() = {
              super.<init>();
              ()
            };
            import com.datastax.bdp.spark.context.CassandraContext;
            class $iwC extends Serializable {
              def <init>() = {
                super.<init>();
                ()
              };
              import com.tuplejump.calliope.Implicits._;
              class $iwC extends Serializable {
                def <init>() = {
                  super.<init>();
                  ()
                };
                val $VAL5 = $line17.$read.INSTANCE;
                import $VAL5.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.collection;
                val $VAL6 = $line18.$read.INSTANCE;
                import $VAL6.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd;
                class $iwC extends Serializable {
                  def <init>() = {
                    super.<init>();
                    ()
                  };
                  val res3 = rdd.map(((x$1) => x$1._2)).sum
                };
                val $iw = new $iwC()
              };
              val $iw = new $iwC()
            };
            val $iw = new $iwC()
          };
          val $iw = new $iwC()
        };
        val $iw = new $iwC()
      };
      val $iw = new $iwC()
    };
    val $iw = new $iwC()
  };
  val $iw = new $iwC()
}, object $read extends scala.AnyRef {
  def <init>() = {
    super.<init>();
    ()
  };
  val INSTANCE = new $read()
}))
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: class $read extends Serializable {
  def <init>() = {
    super.<init>;
    ()
  };
  class $iwC extends Serializable {
    def <init>() = {
      super.<init>;
      ()
    };
    val $VAL4 = $line3.$read.INSTANCE;
    import $VAL4.$iw.$iw.sc;
    class $iwC extends Serializable {
      def <init>() = {
        super.<init>;
        ()
      };
      import org.apache.spark.SparkContext._;
      class $iwC extends Serializable {
        def <init>() = {
          super.<init>;
          ()
        };
        class $iwC extends Serializable {
          def <init>() = {
            super.<init>;
            ()
          };
          import com.datastax.bdp.spark.CassandraFunctions._;
          class $iwC extends Serializable {
            def <init>() = {
              super.<init>;
              ()
            };
            import com.datastax.bdp.spark.context.CassandraContext;
            class $iwC extends Serializable {
              def <init>() = {
                super.<init>;
                ()
              };
              import com.tuplejump.calliope.Implicits._;
              class $iwC extends Serializable {
                def <init>() = {
                  super.<init>;
                  ()
                };
                val $VAL5 = $line17.$read.INSTANCE;
                import $VAL5.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.collection;
                val $VAL6 = $line18.$read.INSTANCE;
                import $VAL6.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd;
                class $iwC extends Serializable {
                  def <init>() = {
                    super.<init>;
                    ()
                  };
                  val res3 = rdd.map(((x$1) => x$1._2)).sum
                };
                val $iw = new $iwC.<init>
              };
              val $iw = new $iwC.<init>
            };
            val $iw = new $iwC.<init>
          };
          val $iw = new $iwC.<init>
        };
        val $iw = new $iwC.<init>
      };
      val $iw = new $iwC.<init>
    };
    val $iw = new $iwC.<init>
  };
  val $iw = new $iwC.<init>
}
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: object $read extends scala.AnyRef {
  def <init>() = {
    super.<init>;
    ()
  };
  val INSTANCE = new $read.<init>
}
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: Set symbol of res3 to val res3(): Double
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: parse(""
object $eval {
  lazy val $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`res3`
  val $print: String =  {
    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw
    (""""
      
 + ""res3: Double = "" + scala.runtime.ScalaRunTime.replStringOf($line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`res3`, 1000)

    )
  }
}
      
"") Some(List(object $eval extends scala.AnyRef {
  def <init>() = {
    super.<init>();
    ()
  };
  lazy val $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res3;
  val $print: String = {
    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw;
    """".$plus(""res3: Double = "").$plus(scala.runtime.ScalaRunTime.replStringOf($line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res3, 1000))
  }
}))
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: object $eval extends scala.AnyRef {
  def <init>() = {
    super.<init>;
    ()
  };
  lazy val $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res3;
  val $print: String = {
    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw;
    """".+(""res3: Double = "").+(scala.runtime.ScalaRunTime.replStringOf($line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res3, 1000))
  }
}
14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: Invoking: public static java.lang.String $line19.$eval.$print()
14/05/05 21:23:46 INFO SharkContext: Starting job: sum at <console>:24
14/05/05 21:23:46 INFO DAGScheduler: Got job 0 (sum at <console>:24) with 2 output partitions (allowLocal=false)
14/05/05 21:23:46 INFO DAGScheduler: Final stage: Stage 0 (sum at <console>:24)
14/05/05 21:23:46 INFO DAGScheduler: Parents of final stage: List()
14/05/05 21:23:46 INFO DAGScheduler: Missing parents: List()
14/05/05 21:23:46 DEBUG DAGScheduler: submitStage(Stage 0)
14/05/05 21:23:46 DEBUG DAGScheduler: missing: List()
14/05/05 21:23:46 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[2] at numericRDDToDoubleRDDFunctions at <console>:24), which has no missing parents
14/05/05 21:23:46 DEBUG DAGScheduler: submitMissingTasks(Stage 0)
14/05/05 21:23:46 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[2] at numericRDDToDoubleRDDFunctions at <console>:24)
14/05/05 21:23:46 DEBUG DAGScheduler: New pending tasks: Set(ResultTask(0, 0), ResultTask(0, 1))
14/05/05 21:23:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
14/05/05 21:23:46 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0
14/05/05 21:23:46 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: ANY
14/05/05 21:23:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
14/05/05 21:23:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
14/05/05 21:23:46 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor 0: 127.0.0.1 (PROCESS_LOCAL)
14/05/05 21:23:47 INFO TaskSetManager: Serialized task 0.0:0 as 13890654 bytes in 617 ms
14/05/05 21:23:47 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:47 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:48 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:48 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:49 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:49 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:50 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:50 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
14/05/05 21:23:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
{noformat}

How to enable logging from Executor? It doesn't seem to log anything anywhere, but maybe something is misconfigured.;;;","05/May/14 19:36;pkolaczk;There are some logs from SparkWorker and SparkMaster:
{noformat}
INFO 21:32:45,654 SparkMaster: 14/05/05 21:32:45 INFO Slf4jLogger: Slf4jLogger started
 INFO 21:32:45,698 SparkMaster: 14/05/05 21:32:45 INFO Remoting: Starting remoting
 INFO 21:32:45,849 SparkMaster: 14/05/05 21:32:45 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@127.0.0.1:7077]
 INFO 21:32:46,112 SparkMaster: 14/05/05 21:32:46 INFO Master: Starting Spark master at spark://127.0.0.1:7077
 INFO 21:32:46,136 SparkMaster: 14/05/05 21:32:46 INFO Server: jetty-7.6.8.v20121106
 INFO 21:32:46,142 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/master/json,null}
 INFO 21:32:46,142 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/applications/json,null}
 INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}
 INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/app/json,null}
 INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/app,null}
 INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/json,null}
 INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{*,null}
 INFO 21:32:46,152 SparkMaster: 14/05/05 21:32:46 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:7080
 INFO 21:32:46,153 SparkMaster: 14/05/05 21:32:46 INFO MasterWebUI: Started Master web UI at http://m4600.local:7080
 INFO 21:32:46,169 SparkMaster: 14/05/05 21:32:46 INFO Master: I have been elected leader! New state: ALIVE
 INFO 21:32:46,474 Started SparkWork connected to 127.0.0.1:7077
 INFO 21:32:46,994 SparkWorker: 14/05/05 21:32:46 WARN Utils: Your hostname, m4600 resolves to a loopback address: 127.0.0.2; using 192.168.122.1 instead (on interface virbr0)
 INFO 21:32:46,994 SparkWorker: 14/05/05 21:32:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
 INFO 21:32:47,473 SparkWorker: 14/05/05 21:32:47 INFO Slf4jLogger: Slf4jLogger started
 INFO 21:32:47,519 SparkWorker: 14/05/05 21:32:47 INFO Remoting: Starting remoting
 INFO 21:32:47,661 SparkWorker: 14/05/05 21:32:47 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkWorker@127.0.0.1:45858]
 INFO 21:32:47,827 SparkWorker: 14/05/05 21:32:47 INFO Worker: Starting Spark worker 127.0.0.1:45858 with 8 cores, 4.0 GB RAM
 INFO 21:32:47,828 SparkWorker: 14/05/05 21:32:47 INFO Worker: Spark home: /home/pkolaczk/Projekty/datastax/bdp/resources/spark
 INFO 21:32:47,945 SparkWorker: 14/05/05 21:32:47 INFO Server: jetty-7.6.8.v20121106
 INFO 21:32:47,951 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null}
 INFO 21:32:47,951 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}
 INFO 21:32:47,952 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/log,null}
 INFO 21:32:47,952 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/logPage,null}
 INFO 21:32:47,952 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/json,null}
 INFO 21:32:47,952 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{*,null}
 INFO 21:32:47,961 SparkWorker: 14/05/05 21:32:47 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:7081
 INFO 21:32:47,962 SparkWorker: 14/05/05 21:32:47 INFO WorkerWebUI: Started Worker web UI at http://m4600.local:7081
 INFO 21:32:47,963 SparkWorker: 14/05/05 21:32:47 INFO Worker: Connecting to master spark://127.0.0.1:7077...
 INFO 21:32:48,199 SparkMaster: 14/05/05 21:32:48 INFO Master: Registering worker 127.0.0.1:45858 with 8 cores, 4.0 GB RAM
 INFO 21:32:48,225 SparkWorker: 14/05/05 21:32:48 INFO Worker: Successfully registered with master spark://127.0.0.1:7077
 INFO 21:33:00,119 SparkMaster: 14/05/05 21:33:00 INFO Master: Registering app Spark shell
 INFO 21:33:00,124 SparkMaster: 14/05/05 21:33:00 INFO Master: Registered app Spark shell with ID app-20140505213300-0000
 INFO 21:33:00,145 SparkMaster: 14/05/05 21:33:00 INFO Master: Launching executor app-20140505213300-0000/0 on worker worker-20140505213247-127.0.0.1-45858
 INFO 21:33:00,185 SparkWorker: 14/05/05 21:33:00 INFO Worker: Asked to launch executor app-20140505213300-0000/0 for Spark shell
 INFO 21:33:00,812 SparkWorker: 14/05/05 21:33:00 INFO ExecutorRunner: Launch command: ""/opt/jdk/bin/java"" ""-cp"" "":/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/build/maven-ant-tasks-2.1.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/cassandra-driver-core-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/commons-io-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/HdrHistogram-1.0.9.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/java-uuid-generator-3.1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jbcrypt-0.3m.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jline-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jna-3.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/journalio-1.4.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/log4j-1.2.17.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/metrics-core-3.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/netty-3.9.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/netty-all-4.0.13.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-log4j12-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/tools/lib/stress.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-2.7.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-all-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-clientutil-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-thrift-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-cli-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-lang-2.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-lang3-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/compress-lzf-0.8.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/concurrentlinkedhashmap-lru-1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/disruptor-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/elephant-bird-hadoop-compat-4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/hibernate-validator-4.3.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/high-scale-lib-1.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/httpclient-4.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/httpcore-4.2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jamm-0.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jbcrypt-0.3m.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/joda-time-1.6.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/json-simple-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/libthrift-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/log4j-1.2.16.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/lz4-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/metrics-core-2.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/netty-3.6.6.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/reporter-config-2.1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snakeyaml-1.11.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snaptree-0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/stringtemplate-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/super-csv-2.1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/thrift-server-0.3.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/validation-api-1.0.0.GA.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/cassandra-driver-core-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/cassandra-driver-dse-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/metrics-core-3.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/netty-3.9.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/slf4j-api-1.7.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-log4j12-1.7.2.jar:::/home/pkolaczk/Projekty/datastax/bdp/resources/spark/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/activation-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-zeromq_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/algebird-core_2.10-0.1.11.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-commons-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-tree-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/avro-1.7.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/avro-ipc-1.7.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/calliope_2.10-0.9.0-EA.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/chill_2.10-0.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/chill-java-0.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/colt-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-compress-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-el-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-httpclient-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-io-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-lang-2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/compress-lzf-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/concurrent-1.3.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/config-1.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/core-3.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/fastutil-6.4.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/flume-ng-sdk-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/gmetric4j-1.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/guava-14.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/hadoop-client-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/hbase-0.94.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/high-scale-lib-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/httpclient-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/httpcore-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-annotations-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-core-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-databind-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-jaxrs-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-xc-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jamon-runtime-2.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jansi-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jasper-compiler-5.5.23.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jasper-runtime-5.5.23.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/JavaEWAH-0.6.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/javax.servlet-2.5.0.v201103041518.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jaxb-api-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jaxb-impl-2.2.3-1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jblas-1.2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-core-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-json-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-server-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jettison-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-continuation-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-http-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-io-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-server-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-util-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jline-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jnr-constants-0.8.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jruby-complete-1.6.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsp-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsp-api-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsr305-1.3.9.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jul-to-slf4j-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/kafka_2.10-0.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/kryo-2.21.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/libthrift-0.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/lift-json_2.10-2.5.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/mesos-0.13.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-annotation-2.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-core-2.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-core-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-ganglia-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-graphite-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-json-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-jvm-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/minlog-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/mqtt-client-0.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/netty-3.5.9.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/netty-all-4.0.13.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/objenesis-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/oncrpc-1.0.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/paranamer-2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/protobuf-java-2.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/protobuf-java-2.4.1-shaded.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/reflectasm-1.07-shaded.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-compiler-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-library-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-reflect-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/servlet-api-2.5-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-bagel_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-core_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-examples_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-mllib_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-repl_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-flume_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-kafka_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-mqtt_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-twitter_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-zeromq_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/stax-api-1.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/stream-2.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/twitter4j-core-3.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/twitter4j-stream-3.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/uncommons-maths-1.2.2a.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/velocity-1.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/xz-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zeromq-scala-binding_2.10-0.0.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zkclient-0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zookeeper-3.4.5.jar:/home/pkolaczk/.spark/cassandra-context/spark-cassandra-context.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/conf::/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-httpclient-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-io-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-logging-1.1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/guava-14.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/hadoop-client-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/httpclient-4.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/httpcore-4.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/javax.servlet-2.5.0.v201103041518.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/jets3t-0.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/shark_2.10-0.9.0.1-DSP-3062-SNAPSHOT.jar::/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/elephant-bird-hadoop-compat-4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-core-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-examples-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-fairscheduler-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-streaming-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-test-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-tools-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ant-1.6.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/automaton-1.11-8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-el-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-httpclient-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-lang-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-math-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-net-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/core-3.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftplet-api-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftpserver-core-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftpserver-deprecated-1.0.0-M2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/hsqldb-1.8.0.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/httpclient-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/httpcore-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jasper-compiler-5.5.12.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jasper-runtime-5.5.12.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jetty-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jsp-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jsp-api-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/kfs-0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/mina-core-2.0.0-M5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/oro-2.0.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/servlet-api-2.5-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/xmlenc-0.52.jar::/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/conf::/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar::/home/pkolaczk/Projekty/datastax/bdp/resources/hive/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-2.7.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-runtime-3.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/asm-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-ipc-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-mapred-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/bonecp-0.7.1.RELEASE.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-compress-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-io-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-lang-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-lang3-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-logging-api-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-pool-1.5.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-api-jdo-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-core-3.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-rdbms-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/derby-10.4.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-cli-0.12.0.3-20140319.091653-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-common-0.12.0.3-20140319.091659-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-exec-0.12.0.3-20140319.091716-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-hwi-0.12.0.3-20140319.091745-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-jdbc-0.12.0.3-20140319.091754-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-metastore-0.12.0.3-20140319.091801-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-serde-0.12.0.3-20140319.091811-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-service-0.12.0.3-20140319.091817-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-shims-0.12.0.3-20140319.091825-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/httpclient-4.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/httpcore-4.2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/JavaEWAH-0.3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/javolution-5.5.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jdo-api-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/json-20090211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jta-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/libfb303-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/libthrift-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/log4j-1.2.16.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/netty-3.5.9.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/paranamer-2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/protobuf-java-2.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/slf4j-api-1.6.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/snappy-0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/ST4-4.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/stringtemplate-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/velocity-1.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/xz-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/zookeeper-3.4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/conf"" ""-XX:MaxPermSize=256M"" ""-XX:MaxPermSize=256M"" ""-Xms2048M"" ""-Xmx2048M"" ""org.apache.spark.executor.CoarseGrainedExecutorBackend"" ""akka.tcp://spark@m4600.local:45753/user/CoarseGrainedScheduler"" ""0"" ""127.0.0.1"" ""1"" ""akka.tcp://sparkWorker@127.0.0.1:45858/user/Worker"" ""app-20140505213300-0000""
{noformat};;;","06/May/14 02:02;gq;Add conf/log4j.properties file
{code}
# Set everything to be logged to the console
#log4j.rootCategory=INFO,console
log4j.rootCategory=INFO,file
#log4j.rootCategory=DEBUG,file
#- size rotation with log cleanup.
log4j.appender.file=org.apache.log4j.RollingFileAppender
log4j.appender.file.MaxFileSize=100MB
log4j.appender.file.MaxBackupIndex=10

#- File to log to and log format
log4j.appender.file.File=/opt/spark_local/logs/spark.log
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

log4j.logger.DataNucleus=DEBUG
# Ignore messages below warning level from Jetty, because it's a bit verbose
log4j.logger.org.eclipse.jetty=WARN
#log4j.logger.org.eclipse.jetty=DEBUG
{code}
{code}log4j.appender.file.File=/opt/spark_local/logs/spark.log{code} change the directory to fit here;;;","06/May/14 02:13;gq;What is the value of {{spark.akka.frameSize}}? you can find it  in {{http://host:4040/environment/}};;;","06/May/14 07:06;pkolaczk;Hmm, it is not listed in the environment at all. What value does it need to have?
Should I expect parallelized collection partitions larger than akka.frame.size to cause problems?
;;;","06/May/14 07:15;gq;May be caused by the imprope {{spark.akka.frameSize}} value
the value can be changed little, try {{5}} .;;;","06/May/14 07:29;pkolaczk;Setting {{spark.akka.frameSize}} to {{20}} helped. {{5}} or {{10}} wasn't enough.;;;","06/May/14 07:42;gq;According to my not comprehensive test. 20 is too big.
A real example when 25889467 bytes serialized result be sent directly to driver [Executor.scala#L248|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala#L248] will cause a problem;;;","06/May/14 07:45;gq;Hi [~pwendell]  
What are your thoughts on this issue?;;;","06/May/14 08:20;pkolaczk;I modified log4j config and this is what I got in the spark.log:
{noformat}
14/05/06 10:17:29 INFO HttpServer: Starting HTTP Server
14/05/06 10:17:33 WARN Utils: Your hostname, m4600 resolves to a loopback address: 127.0.0.2; using 192.168.122.1 instead (on interface virbr0)
14/05/06 10:17:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
14/05/06 10:17:34 INFO Slf4jLogger: Slf4jLogger started
14/05/06 10:17:34 INFO Remoting: Starting remoting
14/05/06 10:17:34 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@m4600.local:33012]
14/05/06 10:17:34 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@m4600.local:33012]
14/05/06 10:17:34 INFO SparkEnv: Registering BlockManagerMaster
14/05/06 10:17:34 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140506101734-42f3
14/05/06 10:17:34 INFO MemoryStore: MemoryStore started with capacity 1178.1 MB.
14/05/06 10:17:34 INFO ConnectionManager: Bound socket to port 60842 with id = ConnectionManagerId(m4600.local,60842)
14/05/06 10:17:34 INFO BlockManagerMaster: Trying to register BlockManager
14/05/06 10:17:34 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager m4600.local:60842 with 1178.1 MB RAM
14/05/06 10:17:34 INFO BlockManagerMaster: Registered BlockManager
14/05/06 10:17:34 INFO HttpServer: Starting HTTP Server
14/05/06 10:17:34 INFO HttpBroadcast: Broadcast server started at http://192.168.122.1:51030
14/05/06 10:17:35 INFO SparkEnv: Registering MapOutputTracker
14/05/06 10:17:35 INFO HttpFileServer: HTTP File server directory is /tmp/spark-5013023c-f851-4398-a344-5493e62edd26
14/05/06 10:17:35 INFO HttpServer: Starting HTTP Server
14/05/06 10:17:35 INFO SparkUI: Started Spark Web UI at http://m4600.local:4040
14/05/06 10:17:35 INFO SharkContext: Added JAR /home/pkolaczk/.spark/cassandra-context/spark-cassandra-context.jar at http://192.168.122.1:49386/jars/spark-cassandra-context.jar with timestamp 1399364255576
14/05/06 10:17:35 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077...
14/05/06 10:17:35 INFO Master: Registering app Spark shell
14/05/06 10:17:35 INFO Master: Registered app Spark shell with ID app-20140506101735-0001
14/05/06 10:17:35 INFO Master: Launching executor app-20140506101735-0001/0 on worker worker-20140506101633-127.0.0.1-44566
14/05/06 10:17:35 INFO Worker: Asked to launch executor app-20140506101735-0001/0 for Spark shell
14/05/06 10:17:35 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20140506101735-0001
14/05/06 10:17:35 INFO AppClient$ClientActor: Executor added: app-20140506101735-0001/0 on worker-20140506101633-127.0.0.1-44566 (127.0.0.1:44566) with 1 cores
14/05/06 10:17:35 INFO SparkDeploySchedulerBackend: Granted executor ID app-20140506101735-0001/0 on hostPort 127.0.0.1:44566 with 1 cores, 2.0 GB RAM
14/05/06 10:17:35 INFO AppClient$ClientActor: Executor updated: app-20140506101735-0001/0 is now RUNNING
14/05/06 10:17:36 INFO ExecutorRunner: Launch command: ""/opt/jdk/bin/java"" ""-cp"" "":/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/build/maven-ant-tasks-2.1.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/cassandra-driver-core-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/commons-io-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/HdrHistogram-1.0.9.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/java-uuid-generator-3.1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jbcrypt-0.3m.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jline-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jna-3.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/journalio-1.4.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/log4j-1.2.17.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/metrics-core-3.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/netty-3.9.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/netty-all-4.0.13.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-log4j12-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/tools/lib/stress.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-2.7.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-all-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-clientutil-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-thrift-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-cli-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-lang-2.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-lang3-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/compress-lzf-0.8.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/concurrentlinkedhashmap-lru-1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/disruptor-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/elephant-bird-hadoop-compat-4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/hibernate-validator-4.3.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/high-scale-lib-1.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/httpclient-4.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/httpcore-4.2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jamm-0.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jbcrypt-0.3m.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/joda-time-1.6.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/json-simple-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/libthrift-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/log4j-1.2.16.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/lz4-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/metrics-core-2.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/netty-3.6.6.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/reporter-config-2.1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snakeyaml-1.11.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snaptree-0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/stringtemplate-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/super-csv-2.1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/thrift-server-0.3.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/validation-api-1.0.0.GA.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/cassandra-driver-core-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/cassandra-driver-dse-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/metrics-core-3.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/netty-3.9.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/slf4j-api-1.7.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-log4j12-1.7.2.jar:::/home/pkolaczk/Projekty/datastax/bdp/resources/spark/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/activation-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-zeromq_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/algebird-core_2.10-0.1.11.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-commons-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-tree-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/avro-1.7.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/avro-ipc-1.7.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/calliope_2.10-0.9.0-EA.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/chill_2.10-0.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/chill-java-0.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/colt-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-compress-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-el-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-httpclient-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-io-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-lang-2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/compress-lzf-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/concurrent-1.3.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/config-1.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/core-3.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/fastutil-6.4.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/flume-ng-sdk-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/gmetric4j-1.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/guava-14.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/hadoop-client-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/hbase-0.94.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/high-scale-lib-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/httpclient-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/httpcore-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-annotations-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-core-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-databind-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-jaxrs-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-xc-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jamon-runtime-2.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jansi-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jasper-compiler-5.5.23.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jasper-runtime-5.5.23.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/JavaEWAH-0.6.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/javax.servlet-2.5.0.v201103041518.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jaxb-api-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jaxb-impl-2.2.3-1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jblas-1.2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-core-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-json-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-server-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jettison-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-continuation-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-http-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-io-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-server-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-util-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jline-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jnr-constants-0.8.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jruby-complete-1.6.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsp-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsp-api-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsr305-1.3.9.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jul-to-slf4j-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/kafka_2.10-0.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/kryo-2.21.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/libthrift-0.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/lift-json_2.10-2.5.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/mesos-0.13.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-annotation-2.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-core-2.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-core-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-ganglia-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-graphite-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-json-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-jvm-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/minlog-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/mqtt-client-0.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/netty-3.5.9.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/netty-all-4.0.13.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/objenesis-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/oncrpc-1.0.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/paranamer-2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/protobuf-java-2.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/protobuf-java-2.4.1-shaded.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/reflectasm-1.07-shaded.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-compiler-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-library-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-reflect-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/servlet-api-2.5-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-bagel_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-core_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-examples_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-mllib_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-repl_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-flume_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-kafka_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-mqtt_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-twitter_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-zeromq_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/stax-api-1.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/stream-2.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/twitter4j-core-3.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/twitter4j-stream-3.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/uncommons-maths-1.2.2a.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/velocity-1.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/xz-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zeromq-scala-binding_2.10-0.0.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zkclient-0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zookeeper-3.4.5.jar:/home/pkolaczk/.spark/cassandra-context/spark-cassandra-context.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/conf::/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-httpclient-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-io-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-logging-1.1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/guava-14.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/hadoop-client-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/httpclient-4.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/httpcore-4.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/javax.servlet-2.5.0.v201103041518.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/jets3t-0.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/shark_2.10-0.9.0.1-DSP-3062-SNAPSHOT.jar::/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/elephant-bird-hadoop-compat-4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-core-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-examples-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-fairscheduler-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-streaming-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-test-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-tools-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ant-1.6.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/automaton-1.11-8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-el-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-httpclient-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-lang-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-math-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-net-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/core-3.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftplet-api-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftpserver-core-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftpserver-deprecated-1.0.0-M2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/hsqldb-1.8.0.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/httpclient-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/httpcore-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jasper-compiler-5.5.12.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jasper-runtime-5.5.12.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jetty-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jsp-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jsp-api-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/kfs-0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/mina-core-2.0.0-M5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/oro-2.0.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/servlet-api-2.5-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/xmlenc-0.52.jar::/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/conf::/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar::/home/pkolaczk/Projekty/datastax/bdp/resources/hive/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-2.7.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-runtime-3.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/asm-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-ipc-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-mapred-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/bonecp-0.7.1.RELEASE.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-compress-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-io-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-lang-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-lang3-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-logging-api-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-pool-1.5.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-api-jdo-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-core-3.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-rdbms-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/derby-10.4.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-cli-0.12.0.3-20140319.091653-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-common-0.12.0.3-20140319.091659-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-exec-0.12.0.3-20140319.091716-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-hwi-0.12.0.3-20140319.091745-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-jdbc-0.12.0.3-20140319.091754-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-metastore-0.12.0.3-20140319.091801-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-serde-0.12.0.3-20140319.091811-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-service-0.12.0.3-20140319.091817-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-shims-0.12.0.3-20140319.091825-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/httpclient-4.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/httpcore-4.2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/JavaEWAH-0.3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/javolution-5.5.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jdo-api-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/json-20090211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jta-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/libfb303-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/libthrift-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/log4j-1.2.16.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/netty-3.5.9.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/paranamer-2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/protobuf-java-2.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/slf4j-api-1.6.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/snappy-0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/ST4-4.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/stringtemplate-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/velocity-1.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/xz-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/zookeeper-3.4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/conf"" ""-Djava.library.path=:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/native/Linux-amd64-64/lib"" ""-Djava.system.class.loader=com.datastax.bdp.loader.DseClientClassLoader"" ""-XX:MaxPermSize=256M"" ""-Djava.library.path=:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/native/Linux-amd64-64/lib"" ""-Djava.system.class.loader=com.datastax.bdp.loader.DseClientClassLoader"" ""-XX:MaxPermSize=256M"" ""-Xms2048M"" ""-Xmx2048M"" ""org.apache.spark.executor.CoarseGrainedExecutorBackend"" ""akka.tcp://spark@m4600.local:33012/user/CoarseGrainedScheduler"" ""0"" ""127.0.0.1"" ""1"" ""akka.tcp://sparkWorker@127.0.0.1:44566/user/Worker"" ""app-20140506101735-0001""
14/05/06 10:17:37 INFO Slf4jLogger: Slf4jLogger started
14/05/06 10:17:37 INFO Remoting: Starting remoting
14/05/06 10:17:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@127.0.0.1:39919]
14/05/06 10:17:38 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkExecutor@127.0.0.1:39919]
14/05/06 10:17:38 INFO CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://spark@m4600.local:33012/user/CoarseGrainedScheduler
14/05/06 10:17:38 INFO WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@127.0.0.1:44566/user/Worker
14/05/06 10:17:38 INFO WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@127.0.0.1:44566/user/Worker
14/05/06 10:17:38 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@127.0.0.1:39919/user/Executor#-1863004534] with ID 0
14/05/06 10:17:38 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
14/05/06 10:17:38 INFO Executor: Using REPL class URI: http://192.168.122.1:58332
14/05/06 10:17:38 INFO Slf4jLogger: Slf4jLogger started
14/05/06 10:17:38 INFO Remoting: Starting remoting
14/05/06 10:17:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@127.0.0.1:49111]
14/05/06 10:17:38 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@127.0.0.1:49111]
14/05/06 10:17:38 INFO SparkEnv: Connecting to BlockManagerMaster: akka.tcp://spark@m4600.local:33012/user/BlockManagerMaster
14/05/06 10:17:38 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140506101738-4884
14/05/06 10:17:38 INFO MemoryStore: MemoryStore started with capacity 1178.1 MB.
14/05/06 10:17:38 INFO ConnectionManager: Bound socket to port 32898 with id = ConnectionManagerId(127.0.0.1,32898)
14/05/06 10:17:38 INFO BlockManagerMaster: Trying to register BlockManager
14/05/06 10:17:38 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 127.0.0.1:32898 with 1178.1 MB RAM
14/05/06 10:17:38 INFO BlockManagerMaster: Registered BlockManager
14/05/06 10:17:38 INFO SparkEnv: Connecting to MapOutputTracker: akka.tcp://spark@m4600.local:33012/user/MapOutputTracker
14/05/06 10:17:38 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ed849647-1ea9-4446-9798-326ddf33c8da
14/05/06 10:17:38 INFO HttpServer: Starting HTTP Server
14/05/06 10:17:38 WARN Utils: Your hostname, m4600 resolves to a loopback address: 127.0.0.2; using 192.168.122.1 instead (on interface virbr0)
14/05/06 10:17:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
14/05/06 10:17:57 INFO SharkContext: Starting job: sum at <console>:24
14/05/06 10:17:57 INFO DAGScheduler: Got job 0 (sum at <console>:24) with 2 output partitions (allowLocal=false)
14/05/06 10:17:57 INFO DAGScheduler: Final stage: Stage 0 (sum at <console>:24)
14/05/06 10:17:57 INFO DAGScheduler: Parents of final stage: List()
14/05/06 10:17:57 INFO DAGScheduler: Missing parents: List()
14/05/06 10:17:57 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[2] at numericRDDToDoubleRDDFunctions at <console>:24), which has no missing parents
14/05/06 10:17:58 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[2] at numericRDDToDoubleRDDFunctions at <console>:24)
14/05/06 10:17:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
14/05/06 10:17:58 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor 0: 127.0.0.1 (PROCESS_LOCAL)
14/05/06 10:17:59 INFO TaskSetManager: Serialized task 0.0:0 as 13890653 bytes in 615 ms
{noformat}
;;;","06/May/14 08:26;gq;This is just a part of it,  the whole file upload?;;;","06/May/14 09:20;pkolaczk;The whole part from before the moment I started the job.
It ends with {{14/05/06 10:17:59 INFO TaskSetManager: Serialized task 0.0:0 as 13890653 bytes in 615 ms}};;;","06/May/14 10:48;gq;{{TaskDescription}} instance is too big causes the issue. [CoarseGrainedSchedulerBackend.scala#L144|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L144]
[~pwendell] This is a very serious bug;;;","06/May/14 14:58;pkolaczk;Do you need more info / help reproducing this?
I'm worried mostly not by the fact that it failed but because of the manifestation of the bug (no errors anywhere). 
Is it possible we misconfigured something related to logging or setup?;;;","06/May/14 15:49;gq;Thank you, temporarily need not, I roughly to locate.I'm debugging;;;","06/May/14 18:34;pwendell;We should probably add a check that the task is smaller than the akka frame size and throw an error message. Unfortunately akka fails silently here. See a similar check in the MapOutputTracker:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L48;;;","07/May/14 05:25;gq;[The PR 694|https://github.com/apache/spark/pull/694],but I think this solution imperfect.
[Executor.scala#L235|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala#L235] is a good solution.
;;;","28/May/14 23:42;matei;Merged the frame size check into 0.9.2 as well as 1.0.1;;;",,,,,,,,,,,,,
Support EXPLAIN in Spark SQL,SPARK-1704,12712121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ConcreteVitamin,yjplxq,yjplxq,03/May/14 14:51,03/Nov/14 07:22,14/Jul/23 06:25,09/Jun/14 23:48,1.0.0,,,,,,,,1.0.1,1.1.0,,,,SQL,,,,,0,sql,,,,,"14/05/03 22:08:40 INFO ParseDriver: Parsing command: explain select * from src
14/05/03 22:08:40 INFO ParseDriver: Parse Completed
14/05/03 22:08:40 WARN LoggingFilter: EXCEPTION :
java.lang.AssertionError: assertion failed: No plan for ExplainCommand (Project [*])

        at scala.Predef$.assert(Predef.scala:179)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:263)
        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:263)
        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:264)
        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:264)
        at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd$lzycompute(HiveContext.scala:260)
        at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd(HiveContext.scala:248)
        at org.apache.spark.sql.hive.api.java.JavaHiveContext.hql(JavaHiveContext.scala:39)
        at org.apache.spark.examples.TimeServerHandler.messageReceived(TimeServerHandler.java:72)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain$TailFilter.messageReceived(DefaultIoFilterChain.java:690)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain.callNextMessageReceived(DefaultIoFilterChain.java:417)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain.access$1200(DefaultIoFilterChain.java:47)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain$EntryImpl$1.messageReceived(DefaultIoFilterChain.java:765)
        at org.apache.mina.filter.codec.ProtocolCodecFilter$ProtocolDecoderOutputImpl.flush(ProtocolCodecFilter.java:407)
        at org.apache.mina.filter.codec.ProtocolCodecFilter.messageReceived(ProtocolCodecFilter.java:236)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain.callNextMessageReceived(DefaultIoFilterChain.java:417)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain.access$1200(DefaultIoFilterChain.java:47)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain$EntryImpl$1.messageReceived(DefaultIoFilterChain.java:765)
        at org.apache.mina.filter.logging.LoggingFilter.messageReceived(LoggingFilter.java:208)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain.callNextMessageReceived(DefaultIoFilterChain.java:417)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain.access$1200(DefaultIoFilterChain.java:47)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain$EntryImpl$1.messageReceived(DefaultIoFilterChain.java:765)
        at org.apache.mina.core.filterchain.IoFilterAdapter.messageReceived(IoFilterAdapter.java:109)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain.callNextMessageReceived(DefaultIoFilterChain.java:417)
        at org.apache.mina.core.filterchain.DefaultIoFilterChain.fireMessageReceived(DefaultIoFilterChain.java:410)
        at org.apache.mina.core.polling.AbstractPollingIoProcessor.read(AbstractPollingIoProcessor.java:710)
        at org.apache.mina.core.polling.AbstractPollingIoProcessor.process(AbstractPollingIoProcessor.java:664)
        at org.apache.mina.core.polling.AbstractPollingIoProcessor.process(AbstractPollingIoProcessor.java:653)
        at org.apache.mina.core.polling.AbstractPollingIoProcessor.access$600(AbstractPollingIoProcessor.java:67)
        at org.apache.mina.core.polling.AbstractPollingIoProcessor$Processor.run(AbstractPollingIoProcessor.java:1124)
        at org.apache.mina.util.NamePreservingRunnable.run(NamePreservingRunnable.java:64)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:701)",linux,apachespark,ConcreteVitamin,jliwork,marmbrus,rxin,yhuai,yjplxq,,,,,,,,,,,,2203200,2203200,,0%,2203200,2203200,,,,,,,,,,,,,SPARK-1508,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390440,,,Mon Nov 03 07:22:50 UTC 2014,,,,,,,,,,"0|i1v947:",390693,,,,,,,,,,,,,,,,,,,,,,,"06/May/14 00:23;marmbrus;Thanks for reporting this.  Looks like we need to special case explain queries in the various SQL contexts.;;;","06/Jun/14 05:41;rxin;Explain should probably just print out the sql query plan in Spark SQL instead of delegating to Hive ...;;;","06/Jun/14 20:42;ConcreteVitamin;Just so I understand the desirable output for Explain: sql(""EXPLAIN SELECT * FROM src"") should return a SchemaRDD, on which when .collect() is called returns:

== Query Plan ==
HiveTableScan [key#0,value#1], (MetastoreRelation default, src, None), None

where each Row corresponds to one line of the description. Does this sound good?;;;","07/Jun/14 01:33;ConcreteVitamin;Github pull request: https://github.com/apache/spark/pull/1003;;;","03/Nov/14 07:22;apachespark;User 'concretevitamin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1003;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warn users if Spark is run on JRE6 but compiled with JDK7,SPARK-1703,12712100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,03/May/14 06:59,03/Mar/15 14:40,14/Jul/23 06:25,04/May/14 19:22,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Right now since the JRE silently swallows the invalid jar, it will produce really confusing behavior if users hit this. We should check if we are in this situation (either in spark-class or compute-classpath) and fail with an explicit error.

We can do something like:
{code}
$JAVA_HOME/bin/jar -tf lib/spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org/apache/spark/SparkContext
{code}

Which, when a user is running with JRE 6 and a JDK-7-compiled jar will produce:
{code}
java.util.zip.ZipException: invalid CEN header (bad signature)
	at java.util.zip.ZipFile.open(Native Method)
	at java.util.zip.ZipFile.<init>(ZipFile.java:132)
	at java.util.zip.ZipFile.<init>(ZipFile.java:93)
	at sun.tools.jar.Main.list(Main.java:997)
	at sun.tools.jar.Main.run(Main.java:242)
	at sun.tools.jar.Main.main(Main.java:1167)
{code}
",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1911,SPARK-1520,SPARK-1698,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390419,,,Sun May 04 19:22:48 UTC 2014,,,,,,,,,,"0|i1v8zj:",390672,,,,,,,,,,,,,,,,,,,,,,,"04/May/14 19:22;pwendell;Issue resolved by pull request 627
[https://github.com/apache/spark/pull/627];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonRDD leaks socket descriptors during cancellation,SPARK-1700,12712057,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,02/May/14 22:11,11/Dec/14 20:32,14/Jul/23 06:25,11/Dec/14 20:32,0.9.0,1.0.0,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Sockets from Spark to Python workers are not cleaned up over the duration of a job, causing the total number of opened file descriptors to grow to around the number of partitions in the job. Usually these go away if the job is successful, but in the case of cancellation (and possibly exceptions, though I haven't investigated), the socket file descriptors remain indefinitely.",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390376,,,Thu Dec 11 20:32:38 UTC 2014,,,,,,,,,,"0|i1v8q7:",390629,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/14 20:32;srowen;The PR was https://github.com/apache/spark/pull/623 and says it was merged in 1.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowMatrix.dspr is not using parameter alpha for DenseVector,SPARK-1696,12711773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,anishpatel,anishpatel,01/May/14 19:08,15/May/14 00:21,14/Jul/23 06:25,15/May/14 00:21,,,,,,,,,1.0.0,,,,,MLlib,,,,,0,,,,,,"In the master branch, method dspr of RowMatrix takes parameter alpha, but does not use it when given a DenseVector.

This probably slid by because when method computeGramianMatrix calls dspr, it provides an alpha value of 1.0.",,anishpatel,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390094,,,Wed May 14 22:06:56 UTC 2014,,,,,,,,,,"0|i1v6xj:",390331,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 22:06;mengxr;Thanks! I sent a PR: https://github.com/apache/spark/pull/778;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java8-tests compiler error: package com.google.common.collections does not exist,SPARK-1695,12711726,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,01/May/14 15:51,02/May/14 19:41,14/Jul/23 06:25,02/May/14 19:41,,,,,,,,,1.0.0,,,,,Build,Java API,,,,0,,,,,,,,gq,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390047,,,Fri May 02 19:41:00 UTC 2014,,,,,,,,,,"0|i1v6nb:",390285,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 19:41;pwendell;Issue resolved by pull request 611
[https://github.com/apache/spark/pull/611];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dependent on multiple versions of servlet-api jars lead to throw an SecurityException when Spark built for hadoop 2.3.0 , 2.4.0 ",SPARK-1693,12711672,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gq,gq,gq,01/May/14 07:43,02/Mar/17 21:42,14/Jul/23 06:25,05/May/14 00:49,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"{code}mvn test -Pyarn -Dhadoop.version=2.4.0 -Dyarn.version=2.4.0 > log.txt{code}

The log: 
{code}
UnpersistSuite:
- unpersist RDD *** FAILED ***
  java.lang.SecurityException: class ""javax.servlet.FilterRegistration""'s signer information does not match signer information of other classes in the same package
  at java.lang.ClassLoader.checkCerts(ClassLoader.java:952)
  at java.lang.ClassLoader.preDefineClass(ClassLoader.java:666)
  at java.lang.ClassLoader.defineClass(ClassLoader.java:794)
  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
  at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
  at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
{code}",,gq,hudson,mdominguez@cloudera.com,omalley,ottomata,pwendell,qwertymaniac,rxin,Skamandros,terrasect,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-12783,,,,,,,,"01/May/14 07:44;gq;log.txt;https://issues.apache.org/jira/secure/attachment/12642813/log.txt",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389993,,,Thu Mar 02 21:41:54 UTC 2017,,,,,,,,,,"0|i1v6bb:",390231,,,,,,,,,,,,,,,,,,,,,,,"01/May/14 10:04;srowen;I suspect this occurs because two copies of the servlet API jars are included from two sources, and one of those sources includes jar signing information in its manifest. The resulting merged jar has collisions in the signing information and it no longer matches.

If that's right, the fastest way to avoid this is usually to drop signing information that is in the manifest since it is not helpful in the assembly jar. Of course it's ideal to avoid merging two copies of the same dependencies, since only one can be included, and that's why we see some [warn] in the build. In just about all cases it is harmless since they are actually copies of the same version of the same classes.

I will look into what ends up in the manifest.;;;","01/May/14 10:46;srowen;I think this is traceable to a case of jar conflict. I am not sure whether the ultimate cause is signing, but it doesn't matter, since we should simply resolve the conflict. (But I think something like that may be at play, since one of the problem dependencies is from Eclipse's jetty, and there is an Eclipse cert in the manifest at META-INF/ECLIPSEF.RSA...) Anyway.

This is another fun jar hell puzzler, albeit one with a probable solution. The basic issues is that Jetty brings in the Servlet 3.0 API:

{code}
[INFO] |  +- org.eclipse.jetty:jetty-server:jar:8.1.14.v20131031:compile
[INFO] |  |  +- org.eclipse.jetty.orbit:javax.servlet:jar:3.0.0.v201112011016:compile
{code}

... but in Hadoop 2.3.0+, so does Hadoop client:

{code}
[INFO] |  +- org.apache.hadoop:hadoop-client:jar:2.4.0:compile
...
[INFO] |  |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.4.0:compile
[INFO] |  |  |  \- org.apache.hadoop:hadoop-yarn-common:jar:2.4.0:compile
[INFO] |  |  |     +- javax.xml.bind:jaxb-api:jar:2.2.2:compile
...
[INFO] |  |  |     +- javax.servlet:servlet-api:jar:2.5:compile
{code}

Eclipse is naughty for packaging the same API classes in a different artifact, rather than just using javax.servlet:servlet-api 3.0. There may be a reason for that, which is what worries me. In theory Servlet 3.0 is a superset of 2.5, so Hadoop's client should be happy with the 3.0 API on the classpath.

So solution #1 to try is to exclude javax.servlet:servlet-api from the hadoop-client dependency. It won't affect earlier versions at all since they don't have this dependency, and therefore need not even be version-specific.

Hadoop 2.3+ then in theory should happily find Eclipse's Servlet 3.0 API classes and work as normal. I give that about an 90% chance of being true.

Solution #2 is more theoretically sound. We should really exclude Jetty's custom copy of Servlet 3.0, and depend on javax.servlet:servlet-api:jar:3.0 as a runtime dependency. This should transparently override Hadoop 2.3+'s version, and still work for Hadoop. Messing with Jetty increases the chance of another snag. Probability of success: 80%


Li would you be able to try either of those ideas?;;;","01/May/14 10:52;gq;Hi, Sean Owen 
The simplest solution is to remove the dependency of servlet-api
[The related work | https://github.com/witgo/spark/commit/0ed124dc0e453a0a59d3c387651be970859a9a0a];;;","01/May/14 10:57;srowen;Ah didn't even see that! Yeah that's my ""#1"", which you already identified. It is simple. Does it resolve this error? (I assume so.)

The only reason this feels undesirable is that Hadoop's web app then only happens to work because Jetty is also bundled. That kind of accidental working is brittle. (Although, why does the Hadoop client lib need a web app? could be an accident of the dependency graph that totally doesn't matter.)

I had thought that the real fix is to declare that the project's conflicting dependencies must agree, by declaring the dependency they must agree on and then excluding the offenders. That is, does #2 work for you too? It is not much more work and more robust. But I am not 100% sure it works.;;;","01/May/14 11:18;gq;The dependency path is as follows
{code}hadoop-client=> hadoop-mapreduce-client-core => hadoop-yarn-common=>servlet-api{code}
Yarn work in the hadoop cluster,and hadoop already contains servlet-api in {code}${HADOOP_COMMON_HOME}/share/hadoop/yarn/lib/servlet-api-2.5.jar{code}
I guess ""#1"" should be no problem,but there is no test;;;","01/May/14 11:20;srowen;Correct. I thought you had tried option #1 since you had suggested it too. Can you not test it in the same way that you discovered the problem? If you try, I'd suggest the #2 option, because if that works, it is a more robust solution.;;;","01/May/14 11:38;gq;Servlet 3.0, 2.5 together on the classpath,Is that ok?;;;","01/May/14 11:44;srowen;No, that's exactly the problem as far as I can tell. My suggestion is:

- In core, exclude org.eclipse.jetty.orbit:javax.servlet from org.eclipse.jetty:jetty-server
- Declare a runtime-scope dependency in core, on javax.servlet:servlet-api:jar:3.0
- (And that will happen to override Hadoop's javax.servlet:servlet-api:jar:2.5 !)
- Update the SBT build as closely as possible to match;;;","01/May/14 14:04;gq;Hi Sean Owen
You have time to review [the code|https://github.com/witgo/spark/tree/SPARK-1693 ]?;;;","01/May/14 14:26;srowen;Yes looks very close to what I had in mind; I have two suggestions:
- To be ultra-safe, use version 3.0.0 of the Servlet API not 3.0.1
- Maybe drop a comment in the parent pom about why this dependency exists -- even just a reference to this JIRA

Does it work for you then? fingers crossed.;;;","01/May/14 14:35;gq;I didn't find servlet-api  3.0.0 stable version
[http://mvnrepository.com/artifact/javax.servlet]
;;;","01/May/14 14:44;srowen;Huh! you're right. Yes I agree with what you have done then.;;;","01/May/14 15:11;gq;Thanks Sean Owen .
Code has been submitted to [PR 590|https://github.com/apache/spark/pull/590];;;","02/May/14 04:34;pwendell;I'm confused about something [~sowen]: If Hadoop 2.3/2.4 wants version 3 of the servlet-api dependency, why does the Hadoop build declare version 2.5 as a dependency in their POM's?

http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-project/2.3.0/hadoop-project-2.3.0.pom
http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-project/2.4.0/hadoop-project-2.4.0.pom

Also - is there not a risk of bumping this version to 3.X when users are building against older hadoop versions? I noticed the older hadoop versions depend 2.X versions of this... will it break them if 2.X and 3.X are present at runtime?

[~sowen] If you could look over this PR it would be great. It's really scary merging a big change like this to the build at the 11th hour. So the more eyes the better.;;;","02/May/14 04:48;gq;Now, the main problem is to ensure that hadoop 2.3, 2.4 can work in this version.But I don't have Hadoop 2.3, 2.4 clusters.;;;","02/May/14 07:04;srowen;[~pwendell] The Hadoop artifacts want version 2.5, as you see. The Servlet 3.0.x *APIs* should be a superset of 2.5. In theory clients implementing or using 2.5 will be fine with it, which means there's an 80% chance it's actually true. Hence I think a solution is to enforce use of 3.0.1, and exclude all other copies of the servlet API.

Normally that's as simple as declaring a runtime dependency on the latest version of the artifact, which will override any transitive dependencies. The problem here is exactly because that's not true; Eclipse has its own artifact for the servlet 3.0 APIs.

And [~witgo], as you noted, the artifact name actually changed from 2.5 to 3.0.1! So depending on it will *not* remove Hadoop's dependency. I think one more change is needed, to exclude Hadoop's dependency on the old javax.servlet:servlet-api artifact.

Basically, ""mvn ... dependency:tree | grep javax.servlet"" should only show the new 3.0.1 dependency.

This should hopefully solve the client-side, unit test failures at least, which is necessary. [~witgo] I assume you have found it makes the tests pass? I do not think it will affect compatibility with a cluster, as this isn't changing or affecting how the client talks to a cluster. ;;;","02/May/14 20:41;tgraves;Is this only seen in the tests?  When I run on a real hadoop2.4 cluster with spark built for hadoop 2.4 I don't see any exceptions. ;;;","03/May/14 02:19;gq;[~tgraves]
You're right . Different versions of the Servlet APIs are forced to merge into spark-assembly.*hadoop.*.jar,each .class file will only have a presence;;;","03/May/14 08:23;srowen;[~tgraves] I see it in tests. It sounds like it does not affect simple, normal operation, which is good. It would still be good to fix up, for propiety's sake and because of the tests. [~gq] the problem is exactly that several copies of the same class are merged, right?;;;","03/May/14 08:34;gq;[~srowen] You're right,preferably only one version exists. ;;;","05/May/14 00:49;pwendell;Issue resolved by pull request 628
[https://github.com/apache/spark/pull/628];;;","28/May/15 09:14;Skamandros;I think the fix version of this issue is incorrect, I could reproduce the problem with spark-core 1.1.0-cdh5.2.1. With version 1.3.0-cdh5.4.2, the same code works fine in local execution.;;;","03/Sep/15 08:16;hudson;SUCCESS: Integrated in gora-trunk #1610 (See [https://builds.apache.org/job/gora-trunk/1610/])
servlet dependency is excluded from spark, not ant. Without excluding it throws a java security exception. Motivation for it: https://issues.apache.org/jira/browse/SPARK-1693 and (furkankamaci: rev ad4f5601ebd52e182bf79ba4032dcb85d918b925)
* gora-core/pom.xml
* pom.xml
;;;","08/Jan/16 21:03;omalley;Can you explain what the problem is and how to fix it? We are hitting the same problem on the hive on spark work.;;;","08/Jan/16 21:09;srowen;As I recall this happens because the official JavaEE servlet API jar includes MANIFEST.MF entries with a signature. If these classes are repackaged in an assembly jar with these same MANIFEST.MF entries, you get this failure. There's a lot of discussion above, but one quick fix is to manage to not include the signatures from anywhere. ;;;","02/Mar/17 21:41;ottomata;We just upgraded to CDH 5.10, which has Spark 1.6.0, Hadoop 2.6.0, Hive 1.1.0, and Oozie 4.1.0.

We are having trouble running Spark jobs that use HiveContext from Oozie.  They run perfectly fine from the CLI with spark-submit, just not in Oozie.  We aren't certain that HiveContext is related, but we can reproduce regularly with a job that uses HiveContext.

Anyway, I post this here, because the error we are getting is the same that started this issue:

{code}class ""javax.servlet.FilterRegistration""'s signer information does not match signer information of other classes in the same package{code}

I've noticed that the Oozie sharelib includes javax.servlet-3.0.0.v201112011016.jar.  I also see that spark-assembly.jar includes a javax.servlet.FilterRegistration class, although its hard for me to tell which version.  The jetty pom.xml files in spark-assembly.jar seem to say {{javax.servlet.\*;version=""2.6.0""}}, but I'm a little green on how all these dependencies get resolved.  I don't see any javax.servlet .jars in any of /usr/lib/hadoop* (where CDH installs hadoop jars).

Help!  :)  If this is not related to this issue, I'll open a new one.
;;;",,,,,,,
Support quoted arguments inside of spark-submit,SPARK-1691,12711665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,01/May/14 05:20,07/Jul/17 05:15,14/Jul/23 06:25,01/May/14 08:16,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Currently due to the way we send arguments on to spark-class, it doesn't work with quoted strings. For instance:

{code}
./bin/spark-submit --name ""My app"" --spark-driver-extraJavaOptions ""-Dfoo=x -Dbar=y""
{code}",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389986,,,Fri Jul 07 05:15:02 UTC 2017,,,,,,,,,,"0|i1v69r:",390224,,,,,,,,,,,,,,,,,,,,,,,"01/May/14 08:16;pwendell;Issue resolved by pull request 609
[https://github.com/apache/spark/pull/609];;;","07/Jul/17 05:15;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/609;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD.saveAsTextFile throws scala.MatchError if RDD contains empty elements,SPARK-1690,12711640,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kzhang,glennklockwood,glennklockwood,01/May/14 00:08,10/May/14 21:02,14/Jul/23 06:25,10/May/14 21:02,0.9.0,,,,,,,,1.0.0,,,,,PySpark,,,,,0,,,,,,"The following pyspark code fails with a scala.MatchError exception if sample.txt contains any empty lines:

file = sc.textFile('hdfs://gcn-3-45.ibnet0:54310/user/glock/sample.txt')
file.saveAsTextFile('hdfs://gcn-3-45.ibnet0:54310/user/glock/sample.out')

The resulting stack trace:

14/04/30 17:02:46 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)
14/04/30 17:02:46 WARN scheduler.TaskSetManager: Loss was due to scala.MatchError
scala.MatchError: 0 (of class java.lang.Integer)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:129)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:119)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:112)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:732)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
	at org.apache.spark.scheduler.Task.run(Task.scala:53)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

This can be reproduced with a sample.txt containing

""""""
foo

bar
""""""

and disappears if sample.txt is

""""""
foo
bar
""""""","Linux/CentOS6, Spark 0.9.1, standalone mode against HDFS from Hadoop 1.2.1",glennklockwood,kzhang,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389961,,,Sat May 10 21:02:48 UTC 2014,,,,,,,,,,"0|i1v64v:",390202,,,,,,,,,,,,,,,,,,,,,,,"05/May/14 06:39;kzhang;PR: https://github.com/apache/spark/pull/644;;;","10/May/14 21:02;pwendell;Fixed by:
https://github.com/apache/spark/pull/644;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AppClient does not respond correctly to RemoveApplication,SPARK-1689,12711637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,30/Apr/14 23:50,20/May/14 04:02,14/Jul/23 06:25,20/May/14 04:02,0.9.0,1.0.0,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"When the Master removes an application (usually due to too many executor failures), it means no future executors will be assigned to that app. Currently, the AppClient just marks the application as ""disconnected"", which is intended as a transient state during a period of reconnection. Thus, RemoveApplication just causes the application to enter a state where it has no executors and it doesn't die.",,ilikerps,pwendell,rahulkumar-aws,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389958,,,Tue May 20 02:03:57 UTC 2014,,,,,,,,,,"0|i1v647:",390199,,,,,,,,,,,,,,,,,,,,,,,"03/May/14 20:27;pwendell;Issue resolved by pull request 605
[https://github.com/apache/spark/pull/605];;;","20/May/14 02:03;ilikerps;The new behavior correctly informs the scheduler of the failed state, but does not exit though we've been removed.

Created https://github.com/apache/spark/pull/832 to fix this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master switches thread when ElectedLeader,SPARK-1686,12711582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,markhamstra,markhamstra,30/Apr/14 21:02,10/May/14 04:53,14/Jul/23 06:25,10/May/14 04:53,0.9.0,1.0.0,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"In deploy.master.Master, the completeRecovery method is the last thing to be called when a standalone Master is recovering from failure.  It is responsible for resetting some state, relaunching drivers, and eventually resuming its scheduling duties.

There are currently four places in Master.scala where completeRecovery is called.  Three of them are from within the actor's receive method, and aren't problems.  The last starts from within receive when the ElectedLeader message is received, but the actual completeRecovery() call is made from the Akka scheduler.  That means that it will execute on a different scheduler thread, and Master itself will end up running (i.e., schedule() ) from that Akka scheduler thread.  Among other things, that means that uncaught exception handling will be different -- https://issues.apache.org/jira/browse/SPARK-1620 ",,codingcat,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389903,,,2014-04-30 21:02:55.0,,,,,,,,,,"0|i1v5rz:",390144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
retryTimer not canceled on actor restart in Worker and AppClient,SPARK-1685,12711543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,markhamstra,markhamstra,markhamstra,30/Apr/14 18:23,06/May/14 19:56,14/Jul/23 06:25,06/May/14 19:56,0.9.0,0.9.1,1.0.0,,,,,,0.9.2,1.0.0,,,,Spark Core,,,,,0,,,,,,"Both deploy.worker.Worker and deploy.client.AppClient try to registerWithMaster when those Actors start.  The attempt at registration is accomplished by starting a retryTimer via the Akka scheduler that will use the registered timeout interval and retry number to make repeated attempts to register with all known Masters before giving up and either marking as dead or calling System.exit.

The receive methods of these actors can, however, throw exceptions, which will lead to the actor restarting, registerWithMaster being called again on restart, and another retryTimer being scheduled without canceling the already running retryTimer.  Assuming that all of the rest of the restart logic is correct for these actors (which I don't believe is actually a given), having multiple retryTimers running presents at least a condition in which the restarted actor may not be able to make the full number of retry attempts before an earlier retryTimer takes the ""give up"" action.

Canceling the retryTimer in the actor's postStop hook should suffice. ",,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389864,,,2014-04-30 18:23:12.0,,,,,,,,,,"0|i1v5jj:",390105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle hive support correctly in ./make-distribution.sh,SPARK-1681,12711399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,30/Apr/14 06:37,02/Jul/14 09:45,14/Jul/23 06:25,05/May/14 23:29,,,,,,,,,1.0.0,,,,,Build,SQL,,,,0,,,,,,"When Hive support is enabled we should copy the datanucleus jars to the packaged distribution. The simplest way would be to create a lib_managed folder in the final distribution so that the compute-classpath script searches in exactly the same way whether or not it's a release.

A slightly nicer solution is to put the jars inside of `/lib` and have some fancier check for the jar location in the compute-classpath script.

We should also document how to run Spark SQL on YARN when hive support is enabled. In particular how to add the necessary jars to spark-submit.",,gq,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1644,,,,,,,,SPARK-1698,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389720,,,Mon May 05 23:28:55 UTC 2014,,,,,,,,,,"0|i1v4nr:",389962,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 10:42;gq;[The related work|https://github.com/apache/spark/pull/598];;;","05/May/14 23:28;pwendell;Closed via:
https://github.com/apache/spark/pull/610;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In-Memory compression needs to be configurable.,SPARK-1679,12711396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,30/Apr/14 06:20,06/May/14 19:09,14/Jul/23 06:25,06/May/14 19:09,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,Since we are still finding bugs in the compression code I think we should make it configurable in SparkConf and turn it off by default for the 1.0 release.,,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389717,,,Thu May 01 04:59:27 UTC 2014,,,,,,,,,,"0|i1v4n3:",389959,,,,,,,,,,,,,,,,,,,,,,,"01/May/14 04:59;lian cheng;Pull request: https://github.com/apache/spark/pull/608;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression loses repeated values.,SPARK-1678,12711395,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,30/Apr/14 06:17,06/May/14 02:39,14/Jul/23 06:25,06/May/14 02:39,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"Here's a test case:

{code}
  test(""all the same strings"") {
    sparkContext.parallelize(1 to 1000).map(_ => StringData(""test"")).registerAsTable(""test1000"")
    assert(sql(""SELECT * FROM test1000"").count() === 1000)
    cacheTable(""test1000"")
    assert(sql(""SELECT * FROM test1000"").count() === 1000)
  }
{code}

First assert passes, second one fails.",,lian cheng,marmbrus,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389716,,,Tue May 06 02:39:24 UTC 2014,,,,,,,,,,"0|i1v4mv:",389958,,,,,,,,,,,,,,,,,,,,,,,"01/May/14 04:59;lian cheng;Pull request: https://github.com/apache/spark/pull/608;;;","06/May/14 02:39;pwendell;Issue resolved by pull request 608
[https://github.com/apache/spark/pull/608];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDFS FileSystems continually pile up in the FS cache,SPARK-1676,12711378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tgraves,ilikerps,ilikerps,30/Apr/14 04:08,30/Oct/17 17:09,14/Jul/23 06:25,05/May/14 14:52,0.9.1,1.0.0,,,,,,,0.9.2,1.0.0,,,,Spark Core,,,,,0,,,,,,"Due to HDFS-3545, FileSystem.get() always produces (and caches) a new FileSystem when provided with a new UserGroupInformation (UGI), even if the UGI represents the same user as another UGI. This causes a buildup of FileSystem objects at an alarming rate, often one per task for something like sc.textFile(). The bug is especially hard-hitting for NativeS3FileSystem, which also maintains an open connection to S3, clogging up the system file handles.

The bug was introduced in https://github.com/apache/spark/pull/29, where doAs was made the default behavior.

A fix is not forthcoming for the general case, as UGIs do not cache well, but this problem can lead to spark clusters entering into a failed state and requiring executors be restarted.",,ilikerps,sandyr,sarutak,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22374,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389699,,,Mon May 05 14:50:57 UTC 2014,,,,,,,,,,"0|i1v4j3:",389941,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 23:02;ilikerps;Discussion for this issue lies mostly in https://github.com/apache/spark/pull/607

The current master branch PR by Tom Graves: https://github.com/apache/spark/pull/621;;;","05/May/14 14:50;tgraves;https://github.com/apache/spark/pull/621;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interrupted system call error in pyspark's RDD.pipe,SPARK-1674,12711344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,30/Apr/14 00:13,30/Apr/14 01:07,14/Jul/23 06:25,30/Apr/14 01:07,1.0.0,,,,,,,,1.0.0,,,,,PySpark,,,,,0,,,,,,RDD.pipe's doctest throws interrupted system call exception on Mac. It can be fixed by wrapping pipe.stdout.readline in an iterator.,,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389665,,,Wed Apr 30 00:17:57 UTC 2014,,,,,,,,,,"0|i1v4bj:",389907,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 00:17;mengxr;PR: https://github.com/apache/spark/pull/594;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark Fails to Create SparkContext Due To Debugging Options in conf/java-opts,SPARK-1670,12711329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cheffpj,cheffpj,29/Apr/14 23:20,17/Feb/15 01:03,14/Jul/23 06:25,17/Feb/15 01:03,1.0.0,,,,,,,,1.3.0,,,,,PySpark,,,,,0,,,,,,"When JVM debugging options are in conf/java-opts, it causes pyspark to fail when creating the SparkContext. The java-opts file looks like the following:
{code}-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005
{code}
Here's the error:
{code}---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/Library/Python/2.7/site-packages/IPython/utils/py3compat.pyc in execfile(fname, *where)
    202             else:
    203                 filename = fname
--> 204             __builtin__.execfile(filename, *where)

/Users/pat/Projects/spark/python/pyspark/shell.py in <module>()
     41     SparkContext.setSystemProperty(""spark.executor.uri"", os.environ[""SPARK_EXECUTOR_URI""])
     42 
---> 43 sc = SparkContext(os.environ.get(""MASTER"", ""local[*]""), ""PySparkShell"", pyFiles=add_files)
     44 
     45 print(""""""Welcome to

/Users/pat/Projects/spark/python/pyspark/context.pyc in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway)
     92             tempNamedTuple = namedtuple(""Callsite"", ""function file linenum"")
     93             self._callsite = tempNamedTuple(function=None, file=None, linenum=None)
---> 94         SparkContext._ensure_initialized(self, gateway=gateway)
     95 
     96         self.environment = environment or {}

/Users/pat/Projects/spark/python/pyspark/context.pyc in _ensure_initialized(cls, instance, gateway)
    172         with SparkContext._lock:
    173             if not SparkContext._gateway:
--> 174                 SparkContext._gateway = gateway or launch_gateway()
    175                 SparkContext._jvm = SparkContext._gateway.jvm
    176                 SparkContext._writeToFile = SparkContext._jvm.PythonRDD.writeToFile

/Users/pat/Projects/spark/python/pyspark/java_gateway.pyc in launch_gateway()
     44         proc = Popen(command, stdout=PIPE, stdin=PIPE)
     45     # Determine which ephemeral port the server started on:
---> 46     port = int(proc.stdout.readline())
     47     # Create a thread to echo output from the GatewayServer, which is required
     48     # for Java log output to show up:

ValueError: invalid literal for int() with base 10: 'Listening for transport dt_socket at address: 5005\n'
{code}

Note that when you use JVM debugging, the very first line of output (e.g. when running spark-shell) looks like this:
{code}Listening for transport dt_socket at address: 5005{code}","pats-air:spark pat$ IPYTHON=1 bin/pyspark
Python 2.7.5 (default, Aug 25 2013, 00:04:04) 
...
IPython 1.1.0
...
Spark version 1.0.0-SNAPSHOT

Using Python version 2.7.5 (default, Aug 25 2013 00:04:04)",cheffpj,farrellee,joshrosen,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2313,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389650,,,Tue Feb 17 01:03:18 UTC 2015,,,,,,,,,,"0|i1v487:",389892,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 23:22;cheffpj;FYI [~ahirreddy] [~matei], here's the pyspark issue I was talking to you guys about;;;","17/Jul/14 15:54;farrellee;SPARK-2313 is the root cause of this. a workaround for this would be complex because the extra text on stdout is coming from the same jvm that should produce the py4j port.;;;","17/Feb/15 01:03;joshrosen;The root cause, SPARK-2313, was fixed for 1.3, so I'm going to mark this as Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobs never finish successfully once bucket file missing occurred,SPARK-1667,12711250,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sarutak,sarutak,29/Apr/14 18:18,17/May/20 18:31,14/Jul/23 06:25,07/Sep/14 08:14,1.0.0,,,,,,,,,,,,,Shuffle,Spark Core,,,,0,,,,,,"If jobs execute shuffle, bucket files are created in a temporary directory (named like spark-local-*).
When the bucket files are missing cased by disk failure or any reasons, jobs cannot execute shuffle which has same shuffle id for the bucket files.",,aash,joshrosen,rxin,sarutak,sinisa_lyh,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2670,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389571,,,Sun Sep 07 08:14:31 UTC 2014,,,,,,,,,,"0|i1v3qn:",389813,,,,,,,,,,,,,1.0.3,1.1.0,,,,,,,,,"12/Jul/14 02:49;sarutak;PR: https://github.com/apache/spark/pull/1383;;;","15/Jul/14 22:29;sarutak;Can anyone review the patch or have opinions?;;;","15/Jul/14 22:38;rxin;Thanks. Just took a look and left some comments.;;;","17/Aug/14 00:04;joshrosen;Is this still an issue?  Was it resolved in a different PR?;;;","07/Sep/14 08:09;aash;Hi [~sarutak] it looks like you sent in a better fix for this problem in SPARK-2670.  Are we good to close this ticket now?;;;","07/Sep/14 08:13;sarutak;[~Andrew Ash] Oh yeah, I close this ticket.;;;","07/Sep/14 08:14;sarutak;This ticket is resolved by SPARK-2670.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Streaming docs code has several small errors,SPARK-1663,12711204,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,29/Apr/14 16:22,15/Jan/15 09:08,14/Jul/23 06:25,03/May/14 19:32,0.9.1,,,,,,,,1.0.0,,,,,Documentation,,,,,0,streaming,,,,,"The changes are easiest to elaborate in the PR, which I will open shortly.

Those changes raised a few little questions about the API too.",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389525,,,Sat May 03 19:32:17 UTC 2014,,,,,,,,,,"0|i1v3gf:",389767,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 16:31;srowen;PR: https://github.com/apache/spark/pull/589;;;","03/May/14 19:32;pwendell;Issue resolved by pull request 589
[https://github.com/apache/spark/pull/589];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly identify if maven is installed and working,SPARK-1658,12710960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,rahulsinghal.iitd,rahulsinghal.iitd,28/Apr/14 16:34,04/May/14 18:09,14/Jul/23 06:25,04/May/14 18:09,1.0.0,,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"The current test in make-distribution.sh to identify if maven is installed is incorrect since the exit code is being checked for ""tail"" rather than ""mvn""",,pwendell,rahulsinghal.iitd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389281,,,Sun May 04 18:09:05 UTC 2014,,,,,,,,,,"0|i1v1z3:",389527,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/14 16:41;rahulsinghal.iitd;PR: https://github.com/apache/spark/pull/580;;;","04/May/14 18:09;pwendell;Issue resolved by pull request 580
[https://github.com/apache/spark/pull/580];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Potential resource leak in HttpBroadcast, SparkSubmitArguments, FileSystemPersistenceEngine and DiskStore",SPARK-1656,12710848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,28/Apr/14 04:02,08/Oct/14 02:38,14/Jul/23 06:25,08/Oct/14 02:38,,,,,,,,,1.1.1,1.2.0,,,,Spark Core,,,,,0,easyfix,,,,,Again... I'm trying to review all `close` statements to find such issues.,,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389169,,,Wed Oct 08 02:38:53 UTC 2014,,,,,,,,,,"0|i1v1a7:",389415,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/14 04:07;zsxwing;PR: https://github.com/apache/spark/pull/577;;;","08/Oct/14 02:38;zsxwing;Already merged into master and branch-1.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes and improvements for spark-submit/configs,SPARK-1652,12710837,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,28/Apr/14 02:31,27/Dec/14 07:40,14/Jul/23 06:25,27/Dec/14 07:40,,,,,,,,,1.2.0,,,,,Spark Core,YARN,,,,0,,,,,,These are almost all a result of my config patch. Unfortunately the changes were difficult to unit-test and there several edge cases reported.,,dklassen,donnchadh,eronwright,gq,pwendell,Sephiroth-Lin,tgraves,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1905,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389158,,,Fri Jun 20 23:57:29 UTC 2014,,,,,,,,,,"0|i1v17r:",389404,,,,,,,,,,,,,,,,,,,,,,,"05/May/14 14:42;tgraves;I added the spark.executor.extraLibraryPath here but I don't necessarily consider it a blocker since there is a workaround of using SPARK_YARN_USER_ENV and setting LD_LIBRARY_PATH;;;","13/May/14 00:51;pwendell;The remaining issues here all have work-arounds in 1.0. So I'm bumping this to 1.1;;;","20/Jun/14 23:57;nchammas;[~pwendell], is there currently a work-around for running Python driver programs on the cluster?

Trying this in 1.0.0 currently yields a succinct: 
{code}
Error: Cannot currently run Python driver programs on cluster
{code}

I'm not sure if there is a separate issue to track this, or if this is the issue I should watch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete existing (tar) deployment directory in distribution script,SPARK-1651,12710811,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,rahulsinghal.iitd,rahulsinghal.iitd,rahulsinghal.iitd,27/Apr/14 20:08,27/Apr/14 22:51,14/Jul/23 06:25,27/Apr/14 22:51,1.0.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,,"If for some reason a directory with the same name as the default tar distribution directory exists then the ""spark contents"" are copied incorrectly.

E.g. scenario: the developer extracts the output tgz from last run",pwendell,rahulsinghal.iitd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389132,,,Sun Apr 27 22:51:16 UTC 2014,,,,,,,,,,"0|i1v11z:",389378,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/14 20:27;rahulsinghal.iitd;PR: https://github.com/apache/spark/pull/573;;;","27/Apr/14 22:51;pwendell;Issue resolved by pull request 573
[https://github.com/apache/spark/pull/573];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly identify maven project version in distribution script,SPARK-1650,12710810,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rahulsinghal.iitd,rahulsinghal.iitd,rahulsinghal.iitd,27/Apr/14 20:05,27/Apr/14 22:20,14/Jul/23 06:25,27/Apr/14 22:19,1.0.0,,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"make-distribution.sh uses ""mvn help:evaluate -Dexpression=project.version"" to extract the project version but this command also has a side-effect of downloading any dependencies from maven servers. So sometimes wrong version string is used.",,rahulsinghal.iitd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389131,,,Sun Apr 27 20:27:02 UTC 2014,,,,,,,,,,"0|i1v11r:",389377,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/14 20:27;rahulsinghal.iitd;PR: https://github.com/apache/spark/pull/572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The org.datanucleus:*  should not be packaged into spark-assembly-*.jar,SPARK-1644,12710722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,witgo,witgo,26/Apr/14 16:10,10/May/14 17:15,14/Jul/23 06:25,10/May/14 17:15,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"cat conf/hive-site.xml
{code:xml}
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:postgresql://bj-java-hugedata1:7432/hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>passwd</value>
  </property>
  <property>
    <name>hive.metastore.local</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>hdfs://host:8020/user/hive/warehouse</value>
  </property>
</configuration>
{code}",,gq,pwendell,vladimir-lu,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1681,,SPARK-1643,,,,,,,,,,,,,,,,,,,"26/Apr/14 16:12;witgo;spark.log;https://issues.apache.org/jira/secure/attachment/12642085/spark.log",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389043,,,Sat May 10 17:15:27 UTC 2014,,,,,,,,,,"0|i1v0i7:",389289,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/14 15:18;witgo;The org.datanucleus:*  can't be packaged into spark-assembly-*.jar 
{code}
Caused by: org.datanucleus.exceptions.NucleusUserException: Persistence process has been specified to use a ClassLoaderResolver of name ""datanucleus"" yet this has not been found by the DataNucleus plugin mechanism. Please check your CLASSPATH and plugin specification.
{code}

The plugin.xml,MANIFEST.MF is damaged;;;","29/Apr/14 08:28;gq;{code}
# When Hive support is needed, Datanucleus jars must be included on the classpath.
# Datanucleus jars do not work if only included in the  uber jar as plugin.xml metadata is lost.
# Both sbt and maven will populate ""lib_managed/jars/"" with the datanucleus jars when Spark is
# built with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark
# assembly is built for Hive, before actually populating the CLASSPATH with the jars.
# Note that this check order is faster (by up to half a second) in the case where Hive is not used.
num_datanucleus_jars=$(ls ""$FWDIR""/lib_managed/jars/ 2>/dev/null | grep ""datanucleus-.*\\.jar"" | wc -l)
if [ $num_datanucleus_jars -gt 0 ]; then
  AN_ASSEMBLY_JAR=${ASSEMBLY_JAR:-$DEPS_ASSEMBLY_JAR}
  num_hive_files=$(jar tvf ""$AN_ASSEMBLY_JAR"" org/apache/hadoop/hive/ql/exec 2>/dev/null | wc -l)
  if [ $num_hive_files -gt 0 ]; then
    echo ""Spark assembly has been built with Hive, including Datanucleus jars on classpath"" 1>&2
    DATANUCLEUSJARS=$(echo ""$FWDIR/lib_managed/jars""/datanucleus-*.jar | tr "" "" :)
    CLASSPATH=$CLASSPATH:$DATANUCLEUSJARS
  fi
fi
{code} 
only add /lib_managed/jars of files to the CLASSPATH. In the current directory is dist is unable to work
 
;;;","10/May/14 03:48;gq;Why {{assignee to me}} button can not find? Adjust the permissions? :);;;","10/May/14 17:15;pwendell;Issue resolved by pull request 688
[https://github.com/apache/spark/pull/688];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid boxing in ExternalAppendOnlyMap compares,SPARK-1632,12710633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandyr,sandyr,25/Apr/14 20:05,26/Apr/14 00:55,14/Jul/23 06:25,26/Apr/14 00:55,0.9.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Hitting an OOME in ExternalAppendOnlyMap.KCComparator while boxing an int.  I don't know if this is the root cause, but the boxing is also avoidable.

Code:
{code}
    def compare(kc1: (K, C), kc2: (K, C)): Int = {
      kc1._1.hashCode().compareTo(kc2._1.hashCode())
    }
{code}

Error:
{code}
java.lang.OutOfMemoryError: GC overhead limit exceeded
     at java.lang.Integer.valueOf(Integer.java:642)
     at scala.Predef$.int2Integer(Predef.scala:370)
     at org.apache.spark.util.collection.ExternalAppendOnlyMap$KCComparator.compare(ExternalAppendOnlyMap.scala:432)
     at org.apache.spark.util.collection.ExternalAppendOnlyMap$KCComparator.compare(ExternalAppendOnlyMap.scala:430)
     at org.apache.spark.util.collection.AppendOnlyMap$$anon$3.compare(AppendOnlyMap.scala:271)
     at java.util.TimSort.mergeLo(TimSort.java:687)
     at java.util.TimSort.mergeAt(TimSort.java:483)
     at java.util.TimSort.mergeCollapse(TimSort.java:410)
     at java.util.TimSort.sort(TimSort.java:214)
     at java.util.Arrays.sort(Arrays.java:727)
     at org.apache.spark.util.collection.AppendOnlyMap.destructiveSortedIterator(AppendOnlyMap.scala:274)
     at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:188)
     at org.apache.spark.util.collection.ExternalAppendOnlyMap.insert(ExternalAppendOnlyMap.scala:141)
     at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:59)
     at org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:96)
     at org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:95)
     at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:471)
     at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:471)
     at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)
     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
     at org.apache.spark.scheduler.Task.run(Task.scala:53)
     at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
     at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
     at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
     at java.security.AccessController.doPrivileged(Native Method)
     at javax.security.auth.Subject.doAs(Subject.java:415)
     at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
     at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
{code}",,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388954,,,2014-04-25 20:05:44.0,,,,,,,,,,"0|i1uzyf:",389200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
App name set in SparkConf (not in JVM properties) not respected by Yarn backend,SPARK-1631,12710607,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,vanzin,vanzin,25/Apr/14 18:07,09/May/14 03:46,14/Jul/23 06:25,09/May/14 03:46,1.0.0,,,,,,,,1.0.0,,,,,YARN,,,,,0,,,,,,"When you submit an application that sets its name using a SparkContext constructor or SparkConf.setAppName(), the Yarn app name is not set and the app shows up as ""Spark"" in the RM UI.

That's because YarnClientSchedulerBackend only looks at the system properties to look for the app name, instead of looking at the app's config.

e.g., app initializes like this:

{code}
    val sc = new SparkContext(new SparkConf().setAppName(""Blah""));
{code}

Start app like this:

{noformat}
  ./bin/spark-submit --master yarn --deploy-mode client blah blah blah
{noformat}

And app name in RM UI does not reflect the code.",,pwendell,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1755,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388928,,,Fri May 09 03:46:28 UTC 2014,,,,,,,,,,"0|i1uzt3:",389174,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 18:08;vanzin;PR: https://github.com/apache/spark/pull/539;;;","09/May/14 03:46;pwendell;Issue resolved by pull request 539
[https://github.com/apache/spark/pull/539];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark should inline use of commons-lang `SystemUtils.IS_OS_WINDOWS` ,SPARK-1629,12710525,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,witgo,witgo,25/Apr/14 10:54,30/Apr/14 16:54,14/Jul/23 06:25,30/Apr/14 16:54,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,Right now we use this but don't depend on it explicitly (which is wrong). We should probably just inline this function and remove the need to add a dependency.,,gq,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388846,,,Wed Apr 30 16:54:23 UTC 2014,,,,,,,,,,"0|i1uzav:",389092,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 11:00;srowen;I don't see any usage of Commons Lang in the whole project?
Tachyon uses commons-lang3 but it also brings it in as a dependency.;;;","25/Apr/14 11:06;witgo;Hi Sean Owen,see [Utils.scala|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L33];;;","25/Apr/14 11:20;srowen;Oh I see, that was added yesterday and I hadn't updated my fork. Yes that's right. It only works since commons-lang happens to be imported by another dependency. I don't think it's worth bringing it all in just for one method.

How about we just 'inline' this with a utility method like:

{code:scala}
def isWindows(): Boolean = {
  try {
    val osName = System.getProperty(""os.name"")
    osName != null && osName.startsWith(""Windows"")
  } catch {
    case e: SecurityException => (log a warning and return false)
  }
}
{code}
;;;","30/Apr/14 10:23;gq;[The PR 569|https://github.com/apache/spark/pull/569];;;","30/Apr/14 16:54;pwendell;Thanks for this fix;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing hashCode methods in Partitioner subclasses,SPARK-1628,12710505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,25/Apr/14 07:46,08/Jun/14 21:19,14/Jul/23 06:25,08/Jun/14 21:19,,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,easyfix,,,,,"`hashCode` is not override in HashPartitioner, RangePartitioner, PythonPartitioner and PageRankUtils.CustomPartitioner. Should override hashcode() if overriding equals().",,rxin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388826,,,Sun Jun 08 21:19:19 UTC 2014,,,,,,,,,,"0|i1uz6f:",389072,,,,,,,,,,,,,1.1.0,,,,,,,,,,"08/Jun/14 21:19;rxin;PR merged: https://github.com/apache/spark/pull/549;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK-1623. Broadcast cleaner should use getCanonicalPath when deleting files by name,SPARK-1623,12710484,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nirajsuthar,pwendell,pwendell,25/Apr/14 03:10,25/Nov/14 11:58,14/Jul/23 06:25,25/Nov/14 11:58,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,,,apachespark,nirajsuthar,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1103,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388805,,,Tue Nov 25 11:58:41 UTC 2014,,,,,,,,,,"0|i1uz27:",389053,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 00:28;pwendell;I'm not sure if this solves the issue reported, but we did merge this patch:
https://github.com/apache/spark/pull/749;;;","19/Jul/14 23:53;apachespark;User 'nsuthar' has created a pull request for this issue:
[https://github.com/apache/spark/pull/546|https://github.com/apache/spark/pull/546];;;","25/Nov/14 11:58;srowen;Yes, the original PR was clearly superseded by https://github.com/apache/spark/pull/749 , which was merged. It resolves the same issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught exception from Akka scheduler,SPARK-1620,12710449,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,markhamstra,markhamstra,markhamstra,24/Apr/14 22:49,14/May/14 17:07,14/Jul/23 06:25,14/May/14 17:07,0.9.0,1.0.0,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"I've been looking at this one in the context of a BlockManagerMaster that OOMs and doesn't respond to heartBeat(), but I suspect that there may be problems elsewhere where we use Akka's scheduler.

The basic nature of the problem is that we are expecting exceptions thrown from a scheduled function to be caught in the thread where _ActorSystem_.scheduler.schedule() or scheduleOnce() has been called.  In fact, the scheduled function runs on its own thread, so any exceptions that it throws are not caught in the thread that called schedule() -- e.g., unanswered BlockManager heartBeats (scheduled in BlockManager#initialize) that end up throwing exceptions in BlockManagerMaster#askDriverWithReply do not cause those exceptions to be handled by the Executor thread's UncaughtExceptionHandler. ",,codingcat,markhamstra,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388770,,,Wed May 14 17:07:56 UTC 2014,,,,,,,,,,"0|i1uyuv:",389019,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 23:58;markhamstra;I'm going to close this one for now, since I think something different is happening than I previously thought.;;;","30/Apr/14 20:59;markhamstra;On further investigation, it looks like there really is a problem with exceptions thrown by scheduled functions not being caught by any uncaught exception handler.;;;","30/Apr/14 22:34;markhamstra;Another two instances of the problem that actually aren't a problem at the moment: In deploy.worker.Worker and deploy.client.AppClient, tryRegisterAllMasters() can throw exceptions (e.g., from Master.toAkkaUrl(masterUrl)), and those exception would go unhandled in the calls from within the Akka scheduler -- i.e. within an invocation of registerWithMaster, all but the first call to tryRegisterAllMasters.  Right now, any later call to tryRegisterAllMasters() that would throw an exception should already have thrown in the first call that occurs outside the scheduled thread, so we should never get to the problem case.  If in the future, however, that behavior would change so that tryRegisterAllMasters() could succeed on the first call but throw within the later, scheduled calls (or if code added within the scheduled retryTimer could throw an exception) then the exception thrown from the scheduler thread will not be caught. ;;;","30/Apr/14 22:41;markhamstra;And one last instance: In scheduler.TaskSchedulerImpl#start(), checkSpeculatableTasks() is scheduled to be called every SPECULATION_INTERVAL.  If checkSpeculatableTasks() throws an exception, that exception will not be caught and no more invocations of checkSpeculatableTasks() will occur. ;;;","14/May/14 17:07;pwendell;Issue resolved by pull request 622
[https://github.com/apache/spark/pull/622];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Socket receiver not restarting properly when connection is refused,SPARK-1618,12710441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,24/Apr/14 21:58,25/Apr/14 04:37,14/Jul/23 06:25,25/Apr/14 04:37,,,,,,,,,1.0.0,,,,,DStreams,,,,,0,,,,,,"If the socket receiver cannot connect in the first attempt, it should try to restart after a delay. That was broken, as the thread that restarts (hence, stops) the receiver waited on Thread.join on itself!",,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388762,,,Fri Apr 25 04:36:24 UTC 2014,,,,,,,,,,"0|i1uyt3:",389011,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 04:36;tdas;https://github.com/apache/spark/pull/540/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Very subtle race condition in SparkListenerSuite,SPARK-1615,12710374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andrewor14,andrewor14,andrewor,24/Apr/14 17:33,05/Nov/14 10:45,14/Jul/23 06:25,25/Apr/14 03:19,0.9.1,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Much of SparkListenerSuite relies on LiveListenerBus's waitUntilEmpty() method. As the name suggests, this waits until the event queue is empty. However, the following race condition could happen:

(1) We dequeue the event
(2) The queue is empty, we return true
(3) The test asserts something assuming that all listeners have finished executing (and fails)
(4) The listeners receive the event

This has been a possible race condition for a long time, but for some reason we've never run into it.",,andrewor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388696,,,2014-04-24 17:33:03.0,,,,,,,,,,"0|i1uyf3:",388947,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential resource leaks in Utils.copyStream and Utils.offsetBytes,SPARK-1612,12710316,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,24/Apr/14 13:57,01/Aug/14 20:25,14/Jul/23 06:25,01/Aug/14 20:25,,,,,,,,,1.1.0,,,,,Spark Core,,,,,0,easyfix,,,,,"Should move the ""close"" statements into a ""finally"" block.",,pwendell,smartnut007,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388638,,,Sat Jul 19 23:15:17 UTC 2014,,,,,,,,,,"0|i1uy27:",388889,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 14:02;zsxwing;PR: https://github.com/apache/spark/pull/535;;;","19/Jul/14 23:15;pwendell;A pull request has been posted for this issue:
Author: zsxwing
URL: [https://github.com/apache/spark/pull/535|https://github.com/apache/spark/pull/535];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect initialization order in AppendOnlyMap,SPARK-1611,12710302,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,24/Apr/14 12:51,25/Apr/14 06:45,14/Jul/23 06:25,25/Apr/14 06:45,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,easyfix,,,,,"The initialization order of ""growThreshold"" and ""LOAD_FACTOR"" is incorrect. ""growThreshold"" will be initialized to 0.",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388624,,,Thu Apr 24 12:54:00 UTC 2014,,,,,,,,,,"0|i1uxz3:",388875,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 12:54;zsxwing;PR: https://github.com/apache/spark/pull/534;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cast from BooleanType to NumericType should use exact type value.,SPARK-1610,12710294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,24/Apr/14 11:56,29/Apr/14 22:35,14/Jul/23 06:25,29/Apr/14 22:35,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"Cast from BooleanType to NumericType are all using Int value.
But it causes ClassCastException when the casted value is used by the following evaluation like the code below:

{quote}
scala> import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst._

scala> import types._
import types._

scala> import expressions._
import expressions._

scala> Add(Cast(Literal(true), ShortType), Literal(1.toShort)).eval()
java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Short
	at scala.runtime.BoxesRunTime.unboxToShort(BoxesRunTime.java:102)
	at scala.math.Numeric$ShortIsIntegral$.plus(Numeric.scala:72)
	at org.apache.spark.sql.catalyst.expressions.Add$$anonfun$eval$2.apply(arithmetic.scala:58)
	at org.apache.spark.sql.catalyst.expressions.Add$$anonfun$eval$2.apply(arithmetic.scala:58)
	at org.apache.spark.sql.catalyst.expressions.Expression.n2(Expression.scala:114)
	at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:58)
	at .<init>(<console>:17)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:734)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:983)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:604)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:568)
	at scala.tools.nsc.interpreter.ILoop.reallyInterpret$1(ILoop.scala:760)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:805)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:717)
	at scala.tools.nsc.interpreter.ILoop.processLine$1(ILoop.scala:581)
	at scala.tools.nsc.interpreter.ILoop.innerLoop$1(ILoop.scala:588)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:591)
	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:882)
	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:837)
	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:837)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:837)
	at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:83)
	at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:96)
	at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:105)
	at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala)
{quote}
",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388616,,,Thu Apr 24 12:03:46 UTC 2014,,,,,,,,,,"0|i1uxxb:",388867,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 12:03;ueshin;Pull-requested: https://github.com/apache/spark/pull/533;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cast.nullable should be true when cast from StringType to NumericType/TimestampType,SPARK-1608,12710283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,24/Apr/14 10:29,29/Apr/14 22:36,14/Jul/23 06:25,29/Apr/14 22:36,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"Cast.nullable should be true when cast from StringType to NumericType or TimestampType.
Because if StringType expression has an illegal number string or illegal timestamp string, the casted value becomes null.",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388605,,,Thu Apr 24 10:40:20 UTC 2014,,,,,,,,,,"0|i1uxuv:",388856,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 10:40;ueshin;Pull-requested: https://github.com/apache/spark/pull/532;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancelled jobs can lead to corrupted cached partitions,SPARK-1602,12710210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,ilikerps,ilikerps,24/Apr/14 02:55,29/Apr/14 00:35,14/Jul/23 06:25,29/Apr/14 00:35,0.9.1,1.0.0,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"When jobs are cancelled, the InterruptibleIterator simply returns hasNext = false, which can confuse the CacheManager into thinking that the RDD was fully computed, causing it to store the incomplete result into the BlockManager.

This unfortunately will lead to incorrect results being returned on all future operations containing this RDD while it's still cached.",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388532,,,2014-04-24 02:55:53.0,,,,,,,,,,"0|i1uxen:",388783,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CacheManager#getOrCompute() does not return an InterruptibleIterator,SPARK-1601,12710209,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,ilikerps,ilikerps,24/Apr/14 02:52,15/May/14 18:27,14/Jul/23 06:25,15/May/14 18:26,0.9.1,1.0.0,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"When getOrCompute goes down the ""compute"" path for an RDD that should be stored in memory, it returns an iterator over an array, which is not interruptible. This mainly means that any consumers of that iterator, which may consume slowly, will not be interrupted in a timely manner.",,ilikerps,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388531,,,Thu May 15 18:26:44 UTC 2014,,,,,,,,,,"0|i1uxef:",388782,,,,,,,,,,,,,,,,,,,,,,,"15/May/14 18:26;pwendell;This was fixed in:
https://github.com/apache/spark/pull/521;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flaky ""recovery with file input stream"" test in streaming.CheckpointSuite",SPARK-1600,12710184,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,codingcat,codingcat,23/Apr/14 23:29,16/Feb/15 23:42,14/Jul/23 06:25,06/Jan/15 08:35,0.9.0,0.9.1,1.0.0,1.2.0,,,,,1.2.2,1.3.0,,,,DStreams,,,,,1,flaky-test,,,,,"the case ""recovery with file input stream.recovery with file input stream  "" sometimes fails when the Jenkins is very busy with an unrelated change 

I have met it for 3 times, I also saw it in other places, 

the latest example is in 

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/14397/

where the modification is just in YARN related files....

I once reported in dev mail list: http://apache-spark-developers-list.1001551.n3.nabble.com/a-weird-test-case-in-Streaming-td6116.html
",,apachespark,codingcat,joshrosen,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388506,,,Mon Feb 16 23:42:20 UTC 2015,,,,,,,,,,"0|i1ux9b:",388759,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,"25/Apr/14 20:27;tdas;Will try to address this post 1.0 release;;;","08/Dec/14 19:13;joshrosen;This is still flaky; here's the test result from a recent failure:

{code}
Error Message

List() was empty
Stacktrace

sbt.ForkMain$ForkError: List() was empty
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.streaming.CheckpointSuite$$anonfun$12.apply$mcV$sp(CheckpointSuite.scala:336)
	at org.apache.spark.streaming.CheckpointSuite$$anonfun$12.apply(CheckpointSuite.scala:282)
	at org.apache.spark.streaming.CheckpointSuite$$anonfun$12.apply(CheckpointSuite.scala:282)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.streaming.CheckpointSuite.org$scalatest$BeforeAndAfter$$super$runTest(CheckpointSuite.scala:43)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.streaming.CheckpointSuite.runTest(CheckpointSuite.scala:43)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.streaming.CheckpointSuite.org$scalatest$BeforeAndAfter$$super$run(CheckpointSuite.scala:43)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.streaming.CheckpointSuite.run(CheckpointSuite.scala:43)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code};;;","16/Dec/14 07:46;joshrosen;I'm planning to address this as part of https://github.com/apache/spark/pull/3687;;;","25/Dec/14 10:23;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3801;;;","16/Feb/15 21:46;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4633;;;","16/Feb/15 23:42;joshrosen;I've also merged this into branch-1.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Old streaming input blocks not removed automatically from the BlockManagers,SPARK-1592,12710119,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,23/Apr/14 19:17,29/Oct/16 18:34,14/Jul/23 06:25,25/Apr/14 04:37,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"The raw input data is stored as blocks in BlockManagers. Earlier they were cleared by cleaner ttl. Now since streaming does not require cleaner TTL to be set, the block would not get cleared. This increases up the Spark's memory usage, which is not even accounted and shown in the Spark storage UI. It may cause the data blocks to spill over to disk, which eventually slows down the receiving of data (persisting to memory become bottlenecked by writing to disk).

",,maropu,tdas,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5001,,,,,,,,,,SPARK-16160,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388441,,,Fri Apr 25 04:37:56 UTC 2014,,,,,,,,,,"0|i1uwvj:",388696,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 04:37;tdas;https://github.com/apache/spark/pull/512;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compare Option[Partitioner] and Partitioner directly,SPARK-1589,12710052,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,23/Apr/14 14:58,24/Apr/14 12:54,14/Jul/23 06:25,24/Apr/14 12:54,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"PairRDDFunctions.partitionBy compares Option[Partitioner] and Partitioner directly.

Code: https://github.com/apache/spark/blob/39f85e0322cfecefbc30e7d5a30356cfab1e9640/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L294",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388374,,,Wed Apr 23 15:01:54 UTC 2014,,,,,,,,,,"0|i1uwgn:",388629,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 15:01;zsxwing;PR: https://github.com/apache/spark/pull/508;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix thread leak in spark,SPARK-1587,12709993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mridulm80,mridulm80,mridulm80,23/Apr/14 09:51,25/Apr/14 17:54,14/Jul/23 06:25,25/Apr/14 17:54,,,,,,,,,,,,,,,,,,,0,,,,,,"SparkContext.stop does not cause all threads to exit.
When running tests via scalatest (which keeps reusing the same vm), over time, this causes too many threads to be created causing tests to fail due to inability to create more threads.
",,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388315,,,Fri Apr 25 17:54:24 UTC 2014,,,,,,,,,,"0|i1uw3j:",388570,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 17:54;mridulm80;Fixed, https://github.com/apache/spark/pull/504;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix issues with spark development under windows,SPARK-1586,12709992,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mridulm80,mridulm80,mridulm80,23/Apr/14 09:49,25/Apr/14 17:55,14/Jul/23 06:25,25/Apr/14 03:50,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,,,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388314,,,Fri Apr 25 17:55:16 UTC 2014,,,,,,,,,,"0|i1uw3b:",388569,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 17:55;mridulm80;Immediate issues fixed though there are more hive tests failing due to path related issues. pr : https://github.com/apache/spark/pull/505;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not robust Lasso causes Infinity on weights and losses,SPARK-1585,12709990,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yinxusen,yinxusen,yinxusen,23/Apr/14 09:45,19/May/14 06:00,14/Jul/23 06:25,19/May/14 06:00,0.9.1,,,,,,,,1.1.0,,,,,MLlib,,,,26/Apr/14 00:00,0,,,,,,"Lasso uses LeastSquaresGradient and L1Updater, but 

diff = brzWeights.dot(brzData) - label

in LeastSquaresGradient would cause too big diff, then will affect the L1Updater, which increases weights exponentially. Small shrinkage value cannot lasso weights back to zero then. Finally, the weights and losses reach Infinity.

For example, data = (0.5 repeats 10k times), weights = (0.6 repeats 10k times), then data.dot(weights) approximates 300+, the diff will be 300. Then L1Updater sets weights to approximate 300. In the next iteration, the weights will be set to approximate 30000, and so on.",,mengxr,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1859,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388312,,,Mon May 19 06:00:59 UTC 2014,,,,,,,,,,"0|i1uw2v:",388567,,,,,,,,,,,,,,,,,,,,,,,"15/May/14 19:07;mengxr;I think the gradient should pull the weights back. If I'm wrong, could you create an example code to demonstrate the problem? -Xiangrui;;;","19/May/14 05:07;mengxr;All relates to the step size.;;;","19/May/14 05:58;yinxusen;I see. I close it now.;;;","19/May/14 06:00;yinxusen;Parameter tuning is vital for LASSO, especially the step size. Large step size causes large updating value, then infinity occurs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use java.util.HashMap.remove by mistake in BlockManagerMasterActor.removeBlockManager,SPARK-1583,12709945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,23/Apr/14 06:26,24/Apr/14 12:54,14/Jul/23 06:25,24/Apr/14 12:54,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,easyfix,,,,,"The following code in BlockManagerMasterActor.removeBlockManager uses a value to remove an entry from java.util.HashMap.

      if (locations.size == 0) {
        blockLocations.remove(locations)
      }

Should change to ""blockLocations.remove(blockId)"".",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388267,,,Wed Apr 23 06:37:26 UTC 2014,,,,,,,,,,"0|i1uvsv:",388522,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 06:37;zsxwing;PR: https://github.com/apache/spark/pull/500;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job cancellation does not interrupt threads,SPARK-1582,12709938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,23/Apr/14 04:46,25/Apr/14 06:25,14/Jul/23 06:25,25/Apr/14 06:25,0.9.1,1.0.0,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Cancelling Spark jobs is limited because executors that are blocked are not interrupted. In effect, the cancellation will succeed and the job will no longer be ""running"", but executor threads may still be tied up with the cancelled job and unable to do further work until complete. This is particularly problematic in the case of deadlock or unlimited/long timeouts.

It would be useful if cancelling a job would call Thread.interrupt() in order to interrupt blocking in most situations, such as Object monitors or IO. The one caveat is [HDFS-1208|https://issues.apache.org/jira/browse/HDFS-1208], where HDFS's DFSClient will not only swallow InterruptedException but may reinterpret them as IOException, causing HDFS to mark a node as permanently failed. Thus, this feature must be optional and probably off by default.",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388260,,,2014-04-23 04:46:07.0,,,,,,,,,,"0|i1uvrb:",388515,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not require setting of cleaner TTL when creating StreamingContext,SPARK-1578,12709916,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,23/Apr/14 00:25,23/Apr/14 02:36,14/Jul/23 06:25,23/Apr/14 02:36,,,,,,,,,1.0.0,,,,,DStreams,,,,,0,,,,,,Since shuffles and RDDs that are out of context are automatically cleaned by Spark core (using ContextCleaner) there is no need for setting the cleaner TTL in StreamingContext.,,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388238,,,2014-04-23 00:25:24.0,,,,,,,,,,"0|i1uvmf:",388493,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphX mapVertices with KryoSerialization,SPARK-1577,12709915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,jegonzal,jegonzal,23/Apr/14 00:02,26/May/14 20:20,14/Jul/23 06:25,26/May/14 20:20,,,,,,,,,1.0.0,,,,,GraphX,,,,,0,,,,,,"If Kryo is enabled by setting:

{code}
SPARK_JAVA_OPTS+=""-Dspark.serializer=org.apache.spark.serializer.KryoSerializer ""
SPARK_JAVA_OPTS+=""-Dspark.kryo.registrator=org.apache.spark.graphx.GraphKryoRegistrator  ""
{code}

in conf/spark_env.conf and running the following block of code in the shell:

{code}
import org.apache.spark.graphx._
import org.apache.spark.graphx.lib._
import org.apache.spark.rdd.RDD

val vertexArray = Array(
  (1L, (""Alice"", 28)),
  (2L, (""Bob"", 27)),
  (3L, (""Charlie"", 65)),
  (4L, (""David"", 42)),
  (5L, (""Ed"", 55)),
  (6L, (""Fran"", 50))
  )
val edgeArray = Array(
  Edge(2L, 1L, 7),
  Edge(2L, 4L, 2),
  Edge(3L, 2L, 4),
  Edge(3L, 6L, 3),
  Edge(4L, 1L, 1),
  Edge(5L, 2L, 2),
  Edge(5L, 3L, 8),
  Edge(5L, 6L, 3)
  )

val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray)
val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)

val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)

// Define a class to more clearly model the user property
case class User(name: String, age: Int, inDeg: Int, outDeg: Int)

// Transform the graph
val userGraph = graph.mapVertices{ case (id, (name, age)) => User(name, age, 0, 0) }
{code}

The following block of code works:

{code}
userGraph.vertices.count
{code}

and the following block of code generates a Kryo error:

{code}
userGraph.vertices.collect
{code}

There error:

{code}
java.lang.StackOverflowError
	at sun.reflect.UnsafeFieldAccessorImpl.ensureObj(UnsafeFieldAccessorImpl.java:54)
	at sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.get(UnsafeQualifiedObjectFieldAccessorImpl.java:38)
	at java.lang.reflect.Field.get(Field.java:379)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:552)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
{code}

 ",,ankurd,jegonzal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388237,,,Mon May 26 18:17:47 UTC 2014,,,,,,,,,,"0|i1uvm7:",388492,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 05:53;jegonzal;I have narrowed the issued down to (line 46 in GraphKryoRegistrator):

{code} 
kryo.setReferences(false)
{code}

This creates an issue in the Spark REPL which leads to cyclic references.  Removing this line addresses the issue.  I will submit a pull request with the fix. 

In fact, I can reproduce the bug with the following much simpler block of code:

{code}
class A(a: String) extends Serializable
val x = sc.parallelize(Array.fill(10)(new A(""hello"")))
x.collect
{code}

tl;dr

Disabling reference tracking in Kryo will break the Spark Shell.  
;;;","26/May/14 18:17;ankurd;Resolved by re-enabling Kryo reference tracking in #742: https://github.com/apache/spark/pull/742;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught IO exceptions in Pyspark kill Executor,SPARK-1572,12709864,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,22/Apr/14 20:29,23/Apr/14 21:47,14/Jul/23 06:25,23/Apr/14 21:47,0.9.1,1.0.0,,,,,,,1.0.0,,,,,PySpark,,,,,0,,,,,,"If an exception is thrown in the Python ""stdin writer"" thread during this line:

{code}
PythonRDD.writeIteratorToStream(parent.iterator(split, context), dataOut)
{code}

(e.g., while reading from an HDFS source) then the exception will be handled by the default ThreadUncaughtExceptionHandler, which is set in Executor. The default behavior is, unfortunately, to call System.exit().

Ideally, normal exceptions while running a task should not bring down all the executors of a Spark cluster.",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388186,,,2014-04-22 20:29:55.0,,,,,,,,,,"0|i1uvbj:",388443,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnresolvedException when running JavaSparkSQL example,SPARK-1571,12709856,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,kzhang,kzhang,22/Apr/14 19:53,24/Apr/14 18:09,14/Jul/23 06:25,23/Apr/14 22:07,1.0.0,,,,,,,,,,,,,SQL,,,,,0,,,,,,"When running JavaSparkSQL example using spark-submit in local mode (this happens after fixing the class loading issue in SPARK-1570).

14/04/22 12:46:47 ERROR Executor: Exception in task ID 0
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'age
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:49)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:47)
	at org.apache.spark.sql.catalyst.expressions.Expression.c2(Expression.scala:203)
	at org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual.eval(predicates.scala:142)
	at org.apache.spark.sql.catalyst.expressions.And.eval(predicates.scala:84)
	at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:43)
	at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:43)
",,kzhang,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388178,,,Thu Apr 24 18:09:52 UTC 2014,,,,,,,,,,"0|i1uv9r:",388435,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 22:07;marmbrus;I think this is fixed by https://github.com/apache/spark/commit/39f85e0322cfecefbc30e7d5a30356cfab1e9640

Kan, please let me know if you have any further problems!;;;","24/Apr/14 18:09;kzhang;Thanks, it worked.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class loading issue when using Spark SQL Java API,SPARK-1570,12709853,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kzhang,kzhang,kzhang,22/Apr/14 19:35,22/Apr/14 23:19,14/Jul/23 06:25,22/Apr/14 23:19,1.0.0,,,,,,,,1.0.0,,,,,Java API,SQL,,,,0,,,,,,"ClassNotFoundException in Executor when running JavaSparkSQL example using spark-submit in local mode.

14/04/22 12:26:20 ERROR Executor: Exception in task ID 0
java.lang.ClassNotFoundException: org.apache.spark.examples.sql.JavaSparkSQL.Person
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.apache.spark.sql.api.java.JavaSQLContext$$anonfun$1.apply(JavaSQLContext.scala:90)
	at org.apache.spark.sql.api.java.JavaSQLContext$$anonfun$1.apply(JavaSQLContext.scala:88)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:512)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:512)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
",,kzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388175,,,Tue Apr 22 19:46:08 UTC 2014,,,,,,,,,,"0|i1uv93:",388432,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 19:46;kzhang;PR: https://github.com/apache/spark/pull/484;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 0.9.0 hangs reading s3,SPARK-1568,12709843,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sams,sams,22/Apr/14 18:59,14/Nov/14 15:23,14/Jul/23 06:25,14/Nov/14 15:23,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"I've tried several jobs now and many of the tasks complete, then it get stuck and just hangs.  The exact same jobs function perfectly fine if I distcp to hdfs first and read from hdfs.

Many thanks",,aash,sams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388165,,,Fri Nov 14 13:54:27 UTC 2014,,,,,,,,,,"0|i1uv6v:",388422,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/14 12:51;srowen;Sam, did the other recent changes to S3 deps resolve this, do you think?;;;","21/Jun/14 14:46;sams;When we upgrade to 1.0.0 I'll test this.

This particular problem was from quite a while back when our cluster was quite different from it is now.  At the moment we get the jets3 thing, which is supposed to go away in 1.0.0.;;;","14/Nov/14 11:22;aash;[~sams] did you see an improvement when you upgraded to 1.0.0?

I've noticed myself that reading from s3 can be slow, particularly in the scenario where there are many small files.  I think the hadoop S3 adapter makes many more API calls than is necessary, and that scales on the number of files you have.;;;","14/Nov/14 13:54;sams;Sorry to have not updated. Yes no problems reading s3 with 1.0.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark SQL depends on Java 7 only jars,SPARK-1560,12709665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ahirreddy,marmbrus,marmbrus,22/Apr/14 02:00,22/Apr/14 16:45,14/Jul/23 06:25,22/Apr/14 16:45,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"We need to republish the pickler built with java 7. Details below:

{code}
14/04/19 12:31:29 INFO rdd.HadoopRDD: Input split: file:/Users/ceteri/opt/spark-branch-1.0/examples/src/main/resources/people.txt:0+16
Exception in thread ""Local computation of job 1"" java.lang.UnsupportedClassVersionError: net/razorvine/pickle/Unpickler : Unsupported major.minor version 51.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:621)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
	at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at org.apache.spark.api.python.PythonRDD$$anonfun$pythonToJavaMap$1.apply(PythonRDD.scala:295)
	at org.apache.spark.api.python.PythonRDD$$anonfun$pythonToJavaMap$1.apply(PythonRDD.scala:294)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:518)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:518)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:243)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:234)
	at org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:700)
	at org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:685)
{code}",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387987,,,2014-04-22 02:00:09.0,,,,,,,,,,"0|i1uu3z:",388246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jets3t dep doesn't update properly with newer Hadoop versions,SPARK-1556,12709611,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,codingcat,codingcat,21/Apr/14 19:52,27/Apr/15 06:47,14/Jul/23 06:25,05/May/14 17:34,0.8.1,0.9.0,1.0.0,,,,,,1.0.0,,,,,Spark Core,,,,,1,,,,,,"In Hadoop 2.2.x or newer, Jet3st 0.9.0 which defines S3ServiceException/ServiceException is introduced, however, Spark still relies on Jet3st 0.7.x which has no definition of these classes

What I met is that 

[code]

14/04/21 19:30:53 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
14/04/21 19:30:53 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
14/04/21 19:30:53 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
14/04/21 19:30:53 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
14/04/21 19:30:53 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(NativeS3FileSystem.java:280)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:270)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2316)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:90)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2350)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2332)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:369)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:221)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:270)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:140)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:205)
	at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:205)
	at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:205)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:891)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:741)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:692)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:574)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:900)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:15)
	at $iwC$$iwC$$iwC.<init>(<console>:20)
	at $iwC$$iwC.<init>(<console>:22)
	at $iwC.<init>(<console>:24)
	at <init>(<console>:26)
	at .<init>(<console>:30)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:772)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1040)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:609)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:640)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:604)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:793)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:838)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:750)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:598)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:605)
	at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:608)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:931)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:881)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:881)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:881)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:973)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
Caused by: java.lang.ClassNotFoundException: org.jets3t.service.S3ServiceException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 63 more

[/code]

",,apachespark,codingcat,darose,mengxr,mkanchwala,pwendell,rafal.kwasny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2471,,,SPARK-1735,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387933,,,Mon Apr 27 06:47:51 UTC 2015,,,,,,,,,,"0|i1utsf:",388193,,,,,,,,,,,,,,,,,,,,,,,"05/May/14 17:34;pwendell;Issue resolved by pull request 629
[https://github.com/apache/spark/pull/629];;;","20/May/14 16:25;rafal.kwasny;If you're running spark on cdh5 there is a dirty hotfix:
{code}
cd /usr/lib/spark/assembly/lib/ && ln -s /usr/lib/hadoop/lib/jets3t-0.9.0.jar
{code}

You have to do this on all nodes, this will put jets3t-0.9.0 before spark_assembly .jar on a classpath
Fortunately ""j"" is before ""s"" in the alphabet :)
;;;","20/May/14 16:28;darose;Good tip!!! Many thanks! Will give that a shot.;;;","24/Jun/14 16:37;darose;Thanks again for the tip.  I didn't seem to have the /usr/lib/spark/assembly/lib directory in my installation, but adding the symlink in /usr/lib/shark/lib did the trick as well.;;;","14/Jul/14 08:41;mengxr;[~srowen] I saw you set jets3t's scope to runtime. Any particular reason for that setting? Now sbt reads deps info from pom. The assembly jar won't include jets3t if its scope is runtime only. ;;;","14/Jul/14 09:12;srowen;Yeah see PR comments -- runtime is correct because Spark code should not compile against it. Ideally sbt assembly should include runtime dependencies since they are needed exactly at, well, runtime!;;;","27/Apr/15 06:47;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/468;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
GraphX performs type comparison incorrectly,SPARK-1552,12709524,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,21/Apr/14 08:41,06/Jun/14 06:33,14/Jul/23 06:25,06/Jun/14 06:33,,,,,,,,,1.1.0,,,,,GraphX,,,,,0,,,,,,"In GraphImpl, mapVertices and outerJoinVertices use a more efficient implementation when the map function preserves vertex attribute types. This is implemented by comparing the ClassTags of the old and new vertex attribute types. However, ClassTags store _erased_ types, so the comparison will return a false positive for types with different type parameters, such as Option[Int] and Option[Double].

Thanks to Pierre-Alexandre Fonta for reporting this bug on the [mailing list|http://apache-spark-user-list.1001560.n3.nabble.com/GraphX-Cast-error-when-comparing-a-vertex-attribute-after-its-type-has-changed-td4119.html].

Demo in the Scala shell:

scala> import scala.reflect.{classTag, ClassTag}
scala> def typesEqual[A: ClassTag, B: ClassTag](a: A, b: B): Boolean = classTag[A] equals classTag[B]
scala> typesEqual(Some(1), Some(2.0)) // should return false
res2: Boolean = true

We can require richer TypeTags for these methods, or just take a flag from the caller specifying whether the types are equal.",,ankurd,npanj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387846,,,Wed Jun 04 03:39:07 UTC 2014,,,,,,,,,,"0|i1ut9b:",388107,,,,,,,,,,,,,,,,,,,,,,,"28/May/14 06:45;npanj;Does it make sense to get rid of this optimization until richer TypeTags are available? ;;;","28/May/14 07:17;ankurd;Alternatively, we could introduce type-preserving variants of these methods, maybe called mapVerticesSameType and outerJoinVerticesSameType.

We can't make it into 1.0.0, but it would be good to get a fix into 1.0.1.;;;","04/Jun/14 03:39;ankurd;Proposed fix: https://github.com/apache/spark/pull/967;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDDPage.scala contains RddPage,SPARK-1539,12709418,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,19/Apr/14 18:39,21/Apr/14 21:28,14/Jul/23 06:25,21/Apr/14 21:28,1.0.0,,,,,,,,1.0.0,,,,,Web UI,,,,,0,,,,,,"SPARK-1386 changed RDDPage to RddPage but didn't change the filename. I tried sbt/sbt publish-local. Inside the spark-core jar, the unit name is RDDPage.class and hence I got the following error:

{code}
[error] (run-main) java.lang.NoClassDefFoundError: org/apache/spark/ui/storage/RddPage
java.lang.NoClassDefFoundError: org/apache/spark/ui/storage/RddPage
	at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:59)
	at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:52)
	at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:42)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:215)
	at MovieLensALS$.main(MovieLensALS.scala:38)
	at MovieLensALS.main(MovieLensALS.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.ui.storage.RddPage
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:59)
	at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:52)
	at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:42)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:215)
	at MovieLensALS$.main(MovieLensALS.scala:38)
	at MovieLensALS.main(MovieLensALS.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
{code}

This can be fixed after renaming RddPage to RDDPage.",,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387740,,,2014-04-19 18:39:23.0,,,,,,,,,,"0|i1uslr:",388001,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkUI forgets about all persisted RDD's not directly associated with the Stage,SPARK-1538,12709384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor,19/Apr/14 00:49,05/Nov/14 10:45,14/Jul/23 06:25,23/Apr/14 02:24,0.9.1,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"The following command creates two RDDs in one Stage:

sc.parallelize(1 to 1000, 4).persist.map(_ + 1).count

More specifically, parallelize creates one, and map creates another. If we persist only the first one, it does not actually show up on the StorageTab of the SparkUI.

This is because StageInfo only keeps around information for the last RDD associated with the stage, but forgets about all of its parents. The proposal here is to have StageInfo climb the RDD dependency ladder to keep a list of all associated RDDInfos.",,andrewor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387706,,,2014-04-19 00:49:07.0,,,,,,,,,,"0|i1use7:",387967,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming UI test can hang indefinitely,SPARK-1530,12709178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,pwendell,pwendell,17/Apr/14 23:50,08/Nov/14 11:53,14/Jul/23 06:25,08/Nov/14 11:53,,,,,,,,,,,,,,,,,,,0,,,,,,"This has been causing Jenkins to hang recently:

{code}
""pool-1-thread-1"" prio=10 tid=0x00007f4b9449f000 nid=0x6c37 runnable [0x00007f4b8a26c000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:152)
        at java.net.SocketInputStream.read(SocketInputStream.java:122)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        - locked <0x00000007cad700d0> (a java.io.BufferedInputStream)
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
        - locked <0x00000007cad662b8> (a sun.net.www.protocol.http.HttpURLConnection)
        at java.net.URL.openStream(URL.java:1037)
        at scala.io.Source$.fromURL(Source.scala:140)
        at scala.io.Source$.fromURL(Source.scala:130)
        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2$$anonfun$apply$2.apply$mcV$sp(UISuite.scala:57)
        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2$$anonfun$apply$2.apply(UISuite.scala:56)
        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2$$anonfun$apply$2.apply(UISuite.scala:56)
        at org.scalatest.concurrent.Eventually$class.makeAValiantAttempt$1(Eventually.scala:394)
        at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:408)
        at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:437)
        at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:477)
        at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:307)
        at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:477)
        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(UISuite.scala:56)
        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(UISuite.scala:54)
        at org.apache.spark.LocalSparkContext$.withSpark(LocalSparkContext.scala:60)
        at org.apache.spark.ui.UISuite$$anonfun$2.apply$mcV$sp(UISuite.scala:54)
        at org.apache.spark.ui.UISuite$$anonfun$2.apply(UISuite.scala:54)
        at org.apache.spark.ui.UISuite$$anonfun$2.apply(UISuite.scala:54)
        at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
        at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
        at org.apache.spark.ui.UISuite.withFixture(UISuite.scala:37)
        at org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
        at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
        at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
        at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
        at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
        at org.apache.spark.ui.UISuite.runTest(UISuite.scala:37)
        at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
        at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
        at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
        at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
        at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
        at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
        at org.apache.spark.ui.UISuite.runTests(UISuite.scala:37)
        at org.scalatest.Suite$class.run(Suite.scala:2303)
        at org.apache.spark.ui.UISuite.org$scalatest$FunSuite$$super$run(UISuite.scala:37)
        at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
        at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
        at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
        at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
        at org.apache.spark.ui.UISuite.run(UISuite.scala:37)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
        at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:223)
        at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:236)
        at sbt.ForkMain$Run$2.call(ForkMain.java:294)
        at sbt.ForkMain$Run$2.call(ForkMain.java:284)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

{code}",,pwendell,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387501,,,Sat Nov 08 11:53:37 UTC 2014,,,,,,,,,,"0|i1ur47:",387763,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/14 02:59;tdas;I wonder what about Jenkins environment is causing this. 
And how frequently does this happen?
;;;","08/Nov/14 11:53;srowen;I assume this got fixed along the way, either by [~tdas]'s changes or [~shaneknapp]'s changes to Jenkins?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"rootDirs in DiskBlockManagerSuite doesn't get full path from rootDir0, rootDir1",SPARK-1527,12709075,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nirajsuthar,advancedxy,advancedxy,17/Apr/14 15:21,14/May/14 02:04,14/Jul/23 06:25,14/May/14 02:04,0.9.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,starter,,,,,"In core/src/test/scala/org/apache/storage/DiskBlockManagerSuite.scala

  val rootDir0 = Files.createTempDir()
  rootDir0.deleteOnExit()
  val rootDir1 = Files.createTempDir()
  rootDir1.deleteOnExit()
  val rootDirs = rootDir0.getName + "","" + rootDir1.getName

rootDir0 and rootDir1 are in system's temporary directory. 
rootDir0.getName will not get the full path of the directory but the last component of the directory. When passing to DiskBlockManage constructor, the DiskBlockerManger creates directories in pwd not the temporary directory.

rootDir0.toString will fix this issue.",,advancedxy,nirajsuthar,pwendell,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387398,,,Wed May 14 02:04:45 UTC 2014,,,,,,,,,,"0|i1uqhb:",387660,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 15:26;srowen;{{toString()}} returns {{getPath()}} which may still be relative. {{getAbsolutePath()}} is better, but even {{getCanonicalPath()}} may be better still.;;;","17/Apr/14 15:50;advancedxy;Yes. You are right. toString() may give relative path. And since it's determined by java.io.tmpdir system property. see https://code.google.com/p/guava-libraries/source/browse/guava/src/com/google/common/io/Files.java line 591. It's possible that the DiskBlockManager will create different directories than the original temp dir when java.io.tmpdir is a relative path. 

so use getAbsolutePath since I use this method in my last pr?

But, I saw toString() was called other places! Should we do something about that?;;;","17/Apr/14 15:54;srowen;If the paths are only used locally, then an absolute path never hurts (except to be a bit longer). I assume that since these are references to a temp directory that is by definition only valid locally, that absolute path is the right thing to use.

In other cases, similar logic may apply. I could imagine in some cases the right thing to do is transmit a relative path. ;;;","17/Apr/14 16:06;advancedxy;Yes, of course, sometimes we want absolute path, sometimes we want to transmit a relative path. It depends on logic. 
But I think maybe we should review these usages so that we can make sure absolute paths or relative paths are used appropriately.

I may have time to review it after I finish another JIRA issue. If you want to take it over, please!

Anyway, thanks for your comments and help.
;;;","17/Apr/14 17:09;srowen;There are a number of other uses of File.getName(), but a quick glance suggests all the others are appropriate.

There are a number of other uses of File.toString(), almost all in tests. I suspect the Files in question already have absolute paths, and that even relative paths happen to work fine in a test since the working dir doesn't change. So those could change, but are probably not a concern.

The only one that gave me pause was the use in HttpBroadcast.scala, though I suspect it turns out to work fine for similar reasons.

If reviewers are interested in changing the toString()s I'll test and submit a PR for that.;;;","24/Apr/14 05:35;advancedxy;hi, [~nirajsuthar]
This is my pr. https://github.com/apache/spark/pull/436. As [~srowen] said, if someone interested in changing the toString()s, please leave a comment.

[~rxin] what do you think?;;;","24/Apr/14 20:03;nirajsuthar;Sure Ye Xianjin,

I am more thn happy to do so. after reading the comments I looked at the HttpBroadcast.scala and will update it appropriately.

if you guys have any suggestions here..please let me know. 

Thank you,
Niraj;;;","14/May/14 02:04;pwendell;Issue resolved by pull request 436
[https://github.com/apache/spark/pull/436];;;",,,,,,,,,,,,,,,,,,,,,,,,,
TaskSchedulerImpl should decrease availableCpus by spark.task.cpus not 1,SPARK-1525,12709047,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,yantangzhai,yantangzhai,17/Apr/14 13:07,02/Jul/14 06:25,14/Jul/23 06:25,02/Jul/14 06:25,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"TaskSchedulerImpl decreases availableCpus by 1 in resourceOffers process always even though spark.task.cpus is more than 1, which will schedule more tasks to some node when spark.task.cpus is more than 1.",,codingcat,witgo,yantangzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387370,,,Tue Jul 01 16:44:18 UTC 2014,,,,,,,,,,"0|i1uqb3:",387632,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 13:55;witgo;The latest code already fix the bug.;;;","01/Jul/14 16:44;codingcat;this is fixed by https://github.com/apache/spark/commit/f8111eaeb0e35f6aa9b1e3ec1173fff207174155;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN ClientBase will throw a NPE if there is no YARN application specific classpath.,SPARK-1522,12709012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,berngp,berngp,berngp,17/Apr/14 09:04,09/Jun/14 21:18,14/Jul/23 06:25,09/Jun/14 21:18,0.9.0,0.9.1,1.0.0,,,,,,1.1.0,,,,,YARN,,,,,0,YARN,,,,,"The current implementation of ClientBase.getDefaultYarnApplicationClasspath inspects the MRJobConfig class for the field DEFAULT_YARN_APPLICATION_CLASSPATH when it should be really looking into YarnConfiguration.

If the Application Configuration has no yarn.application.classpath defined a NPE exception will be thrown.",,berngp,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387335,,,Thu Apr 17 10:00:29 UTC 2014,,,,,,,,,,"0|i1uq3b:",387597,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 10:00;berngp;https://github.com/apache/spark/pull/433;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark master doesn't compile against hadoop-common trunk,SPARK-1518,12708902,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cmccabe,vanzin,vanzin,16/Apr/14 20:23,04/Jun/14 22:56,14/Jul/23 06:25,04/Jun/14 22:56,,,,,,,,,1.0.1,1.1.0,,,,Spark Core,,,,,0,,,,,,"FSDataOutputStream::sync() has disappeared from trunk in Hadoop; FileLogger.scala is calling it.

I've changed it locally to hsync() so I can compile the code, but haven't checked yet whether those are equivalent. hsync() seems to have been there forever, so it hopefully works with all versions Spark cares about.",,cmccabe,matei,pwendell,sandyr,tgraves,vanzin,witgo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387225,,,Wed Jun 04 22:56:55 UTC 2014,,,,,,,,,,"0|i1upfb:",387488,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"17/Apr/14 15:46;witgo;As the hadoop API changes, some methods have been removed.
The hadoop related in spark core Independence to new modules. As in the case of yarn.;;;","17/May/14 02:06;cmccabe;{{FSDataOutputStream::sync()}} has been deprecated for a while, and finally got removed in HADOOP-8124.  It was a synonym for {{hflush}}, so replacing it with that function would probably be more appropriate.  You usually don't want {{hsync}}, since it forces an {{fsync}} on each datanode that contains the file.

It looks like there's some other YARN-related stuff that still fails to build once I make this fix, though...;;;","19/May/14 22:11;cmccabe;[~pwendell], I can take a look at this.  Can you assign it to me?;;;","26/May/14 23:26;pwendell;Hey [~cmccabe], what is the oldest version of hadooop that contains hflush? /cc [~andrewor] who IIRC looked into this a bunch when writing the logger.;;;","27/May/14 18:34;cmccabe;bq. Hey Colin Patrick McCabe, what is the oldest version of hadooop that contains hflush? /cc Andrew Or who IIRC looked into this a bunch when writing the logger.

The oldest Apache release I know about with hflush is Hadoop 0.21.;;;","27/May/14 18:51;pwendell;Ah okay. I'm not sure what the oldest version Hadoop that spark Spark compiles against pre 0.21, but it's worth knowing whether this change would cause us to drop support for some of the older versions.;;;","27/May/14 21:17;cmccabe;I think it's very, very, very unlikely that anyone will want to run Hadoop 0.20 against Spark.  Why don't we:
* fix the compile against Hadoop trunk
* wait for someone to show up who wants compatibility with hadoop 0.20 before we work on it?

It seems like if there is interest in Spark on Hadoop 0.20, there will quickly be a patch submitted to get it compiling there.  If there is no such interest, then we'll be done here without doing a lot of work up front.

If you agree then I'll create a pull req.  I have verified that fixing the flush thing un-breaks the compile on master.;;;","27/May/14 21:23;srowen;0.20.x stopped in early 2010. It is ancient.;;;","27/May/14 21:27;cmccabe;I wonder if we should have a discussion on the mailing list about the oldest version of Hadoop we should support.  I would argue that it should be 0.23.  Yahoo! is still using that version.  Perhaps other people have more information than I do, though.

If we decide to support 0.20, I will create a patch that does this using reflection.  But I'd rather get your guys' opinion on whether that make sense first.;;;","27/May/14 21:33;vanzin;Hmm, may I suggest a different approach?

Andrew, who wrote the code, might have more info. But from my understanding, the flushes were needed because the history server might read logs from applications that were not yet finished. So the flush was a best-effort to avoid having the HS read files that contained partial JSON objects (and fail to parse them).

But since then the HS was changed to only read logs from finished applications. I think it's safe to assume that finished applications are not writing to the event log anymore, so the above scenario doesn't exist.

So could we just get rid of the explicit flush instead?;;;","27/May/14 21:39;srowen;RE: Hadoop versions, in my reckoning of the twisted world of Hadoop versions, the 0.23.x branch is still active and so is kind of later than 1.0.x. It may be easier to retain 0.23 compatibility than 1.0.x for example.;;;","27/May/14 21:47;pwendell;Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That's a question we should ask of all Hadoop patches. I just tested Spark and it doesn't compile against 0.20.X, so this is a no-op in terms of compatibility anyways.

It would be good to list the oldest upstream Hadoop version we support. Many people still run Spark against Hadoop 1.X/CDH3 variants. We get high download rates for those pre-built packages. I think this is a little different than e.g. CDH where people upgrade all components at the same time... people download newer versions of Spark and run it with old filesystems very often.;;;","27/May/14 21:56;cmccabe;bq. It would be good to list the oldest upstream Hadoop version we support. Many people still run Spark against Hadoop 1.X/CDH3 variants. We get high download rates for those pre-built packages. I think this is a little different than e.g. CDH where people upgrade all components at the same time... people download newer versions of Spark and run it with old filesystems very often.

Thanks, Patrick.  This is useful info... I didn't realize there was still interest in running Spark against CDH3.  Certainly we'll never support it directly, since CDH3 was end-of-lifed last year.  So we don't really support doing anything on CDH3... except upgrading it to CDH4 (or hopefully, 5 which has Spark. :)

bq. Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That's a question we should ask of all Hadoop patches. I just tested Spark and it doesn't compile against 0.20.X, so this is a no-op in terms of compatibility anyways.

It sounds like we're good to go on replacing the {{flush}} with {{hsync}} then?  I notice you marked this as ""critical"" recently; do you think it's important to 1.0?;;;","27/May/14 22:13;pwendell;bq. Thanks, Patrick. This is useful info... I didn't realize there was still interest in running Spark against CDH3. Certainly we'll never support it directly, since CDH3 was end-of-lifed last year. So we don't really support doing anything on CDH3... except upgrading it to CDH4 (or hopefully, 5 which has Spark. 

Yeah, the upstream project tries pretty hard to maintain compatibility with as many Hadoop versions as possible since we see many people running even fairly old ones in the wild. Of course, I'm sure Cloudera will commercially support only the most recent ones.

bq. Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That's a question we should ask of all Hadoop patches. I just tested Spark and it doesn't compile against 0.20.X, so this is a no-op in terms of compatibility anyways.

It won't make the 1.0.0 window but we should get it into a 1.0.X release. My concern is that once newer Hadoop versions come out, I want people to be able to compile Spark against them. Again, since Spark is distributed independently from HDFS, this is something that happens a lot, people try to compile older Spark releases against newer Hadoop releases.;;;","28/May/14 00:29;cmccabe;Sounds good.  https://github.com/apache/spark/pull/898;;;","28/May/14 08:55;srowen;Re: versioning one more time, really supporting a bunch of versions may get costly. It's already tricky to manage two builds times YARN-or-not, Hive-or-not, times 4 flavors of Hadoop. I doubt the assemblies are yet problem-free in all cases. 

In practice it look like one generic Hadoop 1, Hadoop 2, and CDH 4 release is produced, and 1 set of Maven artifact. (PS again I am not sure Spark should contain a CDH-specific distribution? realizing it's really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain) That means right now you can't build a Spark app for anything but Hadoop 1.x with Maven, without installing it yourself, and there's not an official distro for anything but two major Hadoop versions. Support for niche versions isn't really there or promised anyway, and fleshing out ""support"" may make doing so pretty burdensome. 

There is no suggested action here; if anything I suggest that the right thing is to add Maven artifacts with classifiers, add a few binary artifacts, subtract a few vendor artifacts, but this is a different action.;;;","28/May/14 17:32;cmccabe;bq. Re: versioning one more time, really supporting a bunch of versions may get costly. It's already tricky to manage two builds times YARN-or-not, Hive-or-not, times 4 flavors of Hadoop. I doubt the assemblies are yet problem-free in all cases.

I think in this particular case, we can use reflection to support both Hadoop 1.X and newer stuff.

bq. I am not sure Spark should contain a CDH-specific distribution? realizing it's really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain)

I agree 100%.  We should keep vendor stuff out of the Apache release.  Vendors can create their own build setups (that's what they get paid to do, after all.)

bq. There is no suggested action here; if anything I suggest that the right thing is to add Maven artifacts with classifiers, add a few binary artifacts, subtract a few vendor artifacts, but this is a different action.

If you have some ideas for how to improve the Maven build, it could be worth creating a JIRA.  I think you're right that we need to make it more flexible so that people can build against more versions without editing the pom.  It might be helpful to look at how HBase handles this in its {{pom.xml}} files.;;;","28/May/14 23:59;pwendell;bq. In practice it look like one generic Hadoop 1, Hadoop 2, and CDH 4 release is produced, and 1 set of Maven artifact. (PS again I am not sure Spark should contain a CDH-specific distribution? realizing it's really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain) That means right now you can't build a Spark app for anything but Hadoop 1.x with Maven, without installing it yourself, and there's not an official distro for anything but two major Hadoop versions. Support for niche versions isn't really there or promised anyway, and fleshing out ""support"" may make doing so pretty burdensome.

We need to update the list of binary builds for Spark... some are getting outdated. The workflow for people building Spark apps is that they write their app against the Spark API's in Maven central (they can do this no matter which cluster they want to run on). To run the app, If they just want to run it locally they can spark-submit from any compiled package of Spark, or they can use their build tool to just run it. If they want to submit it to a cluster, users need to have a Spark package compiled for the Hadoop version on the cluster. Because of this we distribute pre-compiled builds to allow people to avoid ever having to compile Spark.

In terms of vendor-specific builds, we've done this because users asked for it. It's useful if, e.g. a user wants to submit a Spark job to a CDH or MapR cluster. Or run spark-shell locally and read data from a CDH HDFS cluster. That's the main use case we want to support.

I don't know what it means that you ""can't build a Spark app"" for Hadoop 2.X. Building a Spark app is intentionally decoupled from the process of submitting an app to a cluster. We want users to be able to build Spark apps that they can run on e.g. different versions of Hadoop.;;;","29/May/14 00:42;srowen;""they write their app against the Spark API's in Maven central (they can do this no matter which cluster they want to run on)"" 

Yeah this is the issue. OK, if I compile against Spark artifacts as a runtime dependency and submit an app to the cluster, it should be OK no matter what build of Spark is running. The binding from Spark to Hadoop is hidden from the app.

I am thinking of the case where I want to build an app that is a client of Spark -- embedding it. Then I am including the client of Hadoop for example. I have to match my cluster than and there is no Hadoop 2 Spark artifact.

Am I missing something big here? that's my premise about why there would ever be a need for different artifacts. It's the same use case as in Sandy's blog: http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/;;;","29/May/14 01:31;matei;Sean, the model for linking to Hadoop has been that users also add a dependency on hadoop-client if they want to access HDFS for the past few releases. See http://spark.apache.org/docs/latest/scala-programming-guide.html#linking-with-spark for example. This model is there because Hadoop itself has decided to create the hadoop-client Maven artifact as a way to get apps to link to it. It works for all the recent versions of Hadoop as far as I know -- users don't have to link against a custom-built Spark for their distro.

Regarding binary builds on apache.org, we want users to be able to start using Spark as conveniently as possible on any distribution. It is the goal of the Apache project to have people use Apache Spark as easily as possible.;;;","29/May/14 11:55;srowen;Yes Matei that's what I'm getting at. Spark is a client of Hadoop, so if I use Spark, and Spark uses Hadoop, then I have to match the Hadoop that Spark uses to the cluster. It's not just if my app uses HDFS directly. I can manually override hadoop-client, although, I'd have to reproduce a lot of the dependency-graph manipulation in Spark's build to make it work.

In Sandy's blog post example he's just running the code on the cluster and pointing at the matched Spark/Hadoop jars already there. That's also a solution that will work for a lot of use cases. I accept that the use case I have in mind, which is adding Spark to a larger stand-alone app, is not everyone's use case, although it's not crazy. It doesn't work out if instead the Spark/Hadoop jars are packaged together into an assembly and run that way.

I agree overriding the Hadoop dependency is a solution, and accept that Spark shouldn't necessarily bend over backwards for these Hadoop issues, but this does go back to your point about accessibility. Right now I think anyone that wants to do what I'm doing for any Hadoop 2 app, and doesn't want to make a custom build or manually override dependencies, will just point at Cloudera's ""0.9.0-cdh5.0.1"" even if not using CDH. That felt funny.

Apologies if I have somehow totally missed something. I've talked too much, thanks for hearing out the use case. Maybe best to see if this is actually an issue anyone shares.;;;","29/May/14 16:50;matei;Sorry, I'm still not sure I understand what you're asking for -- maybe I missed it above. Are you worried that the Spark assembly on the cluster has to be pre-built against Hadoop? We could perhaps make it find stuff out of HADOOP_HOME, but then it wouldn't work for users that don't have a Hadoop installation, which is a lot of users. For client apps, it's really enough to add that hadoop-client dependency. No other manipulation is needed.

If you want to build a client app that automatically works with multiple versions of Hadoop, you can also package it with Spark and hadoop-client marked as ""provided"" and use spark-submit to put the Spark assembly on your cluster in the classpath. Then it will work with whatever version that was built against. But you need to specify hadoop-client when you run without spark-submit if you want to talk to the version of HDFS in your cluster (e.g. you're testing the app on your laptop and trying to make it read from HDFS).;;;","29/May/14 17:11;srowen;Heh, I think the essence is: at least one more separate Maven artifact, under a different classifier, for Hadoop 2.x builds. If you package that, you get Spark and everything it needs to work against a Hadoop 2 cluster. Yeah I see that you're suggesting various ways to push the app to the cluster, where it can bind to the right version of things, and that may be the right-est way to think about this. I had envisioned running a stand-alone app on a machine that is not part of the cluster, that is a client of it, and this means packaging in the right Hadoop client dependencies, and Spark already declares how it wants to include these various Hadoop client versions -- it's more than just including hadoop-client -- so wanted to leverage that. Let's see if this actually turns out to be a broader request though.;;;","29/May/14 18:15;matei;Okay, got it. But this only applies to you running the job on your laptop, right? Because otherwise you'll get the right Hadoop via the installation on the cluster.

For this use case I still think it's fine to require use of hadoop-client. It's been like that for the past 2 releases and nobody has asked questions about it. It's just one more entry to add to your pom.xml.

The concrete problem is that Hadoop has been extremely fickle with compatibility even within a major release series (1.x or 2.x). HDFS protocol versions change and you can't access the cluster, YARN versions change, etc. I don't think there's a single release I'd call ""Hadoop 2"", and it would be confusing to users to link to the ""Hadoop 2"" artifact and not have it run on their cluster.;;;","29/May/14 18:17;matei;BTW one other thing is that in 1.0, you can also use spark-submit in local mode to get your locally installed Spark. So people will be able to yum install spark-from-their-vendor, build their app with just spark-core, and then run it with the spark-submit on their PATH.;;;","29/May/14 21:00;cmccabe;bq. The concrete problem is that Hadoop has been extremely fickle with compatibility even within a major release series (1.x or 2.x). HDFS protocol versions change and you can't access the cluster, YARN versions change, etc. I don't think there's a single release I'd call ""Hadoop 2"", and it would be confusing to users to link to the ""Hadoop 2"" artifact and not have it run on their cluster.

I know that there was an RPC compatibility break between {{2.1.1-beta}} and {{2.1.0-beta}}.  Around the 2.3 time-frame, Hadoop decided to freeze the RPC format at version 9, and try to maintain compatibility going forward.  You are right that bundling the appropriate version of the Hadoop client is the usual approach that projects which depend on Hadoop take, exactly to avoid these kinds of worries.;;;","30/May/14 07:26;sandyr;bq. I don't think there's a single release I'd call ""Hadoop 2"", and it would be confusing to users to link to the ""Hadoop 2"" artifact and not have it run on their cluster.

While Hadoop releases have not historically been amazing at maintaining compatibility, I think this a bit of an overstatement.  There is a definitive Hadoop 2, which became GA starting at 2.2.  It has a set of public/stable APIs that have not been broken since then, a promise not to break them for the remainder of 2.x, and a comprehensive compatibility guide that describes exactly what ""break"" means - http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/Compatibility.html.  All the major distributions (CDH, Pivotal, HDP, I think MapR?) support these APIs.  The Hadoop 2 releases that preceded 2.2 were labeled alpha and beta, and did not come with these same guarantees.

While, with my Cloudera hat on, I'd love for the CDH5 Spark artifacts to become the canonical Spark Hadoop 2 artifacts, with my Apache hat on, I do see some value in publishing Spark Hadoop 2 artifacts.  Though, as Matei and Patrick pointed out, these only matter when bundling Spark inside your own application.  In most cases, it's better to point to Spark jars installed on one's laptop or cluster.
;;;","30/May/14 12:17;srowen;Sorry for one more message here to reply to Matei -- yes it's the ""laptop"" use case except I'd describe that as a not-uncommon production deployment! it's the embedded-client scenario. It is more than adding one hadoop-client dependency, because you need to emulate the excludes, etc that Spark has to. (But yeah then it works.) I agree supporting a bunch of Hadoop versions gets painful, as a result. This was why I was suggesting way up top that supporting old versions may become more trouble than its worht.;;;","31/May/14 06:08;matei;Got it, the excludes have indeed gotten more painful, and I can see that being a problem. Maybe the solution would be to publish some kind of ""spark-core-hadoopX"" for each version of Hadoop, which depends on hadoop-client and spark-core. But then we'll need a list of supported versions to publish for. By the way AFAIK Maven classifiers do not solve this issue, as versions of an artifact with different classifiers must have the same dependency tree (they can differ in other things, e.g. maybe they're compiled on different Java versions).

BTW, in terms of the Hadoop 2 thing, I just meant that there was a lot of variability before the community decided to go GA, and unfortunately a lot of users are on older versions. (Ironically sometimes because of this volatility). I definitely appreciate the move to GA and the compatibility policies. When I said Hadoop 2, I meant that, for example, do you consider 0.23 to be Hadoop 2? It's YARN-based and it was (and still is AFAIK) widely used at Yahoo. What about 2.0.x? Some users are on that too. What about CDH4? Its version number is 2.0.0-something. In any case we will be sensible about old versions, but my philosophy is always to support the broadest range possible, and from everything I've seen it's paid off -- users appreciate when you do not force their hand to upgrade. This is why Yahoo for example continues to be our biggest contributor on YARN support, even though their YARN is pretty different.;;;","01/Jun/14 05:31;cmccabe;It seems reasonable to have a list of supported versions in the Maven build.  That wouldn't exclude people from building against other versions, of course, but they might have to supply a maven definition via {{\-D}} or something.

bq. For example, do you consider 0.23 to be Hadoop 2? It's YARN-based and it was (and still is AFAIK) widely used at Yahoo

0.23 is not Hadoop 2.  It's a branch that Yahoo! uses internally.  Everyone else has moved on to branch-2 (Hortonworks, Cloudera, WANDisco, Intel, etc. etc.)  Yahoo! also has some clusters running on branch-2, and that is their future too.  More info here: http://osdir.com/ml/general-hadoop-apache/2012-04/msg00000.html

bq. What about CDH4? Its version number is 2.0.0-something

Technically CDH4 is ""Cloudera's distribution of Hadoop including Apache Hadoop 2.0.0.""  Its evolution didn't stop with 2.0.0, though.  We still are going to make another release in the cdh4 line where we backport some things.  CDH5 is where the focus is now, though.

bq. In any case we will be sensible about old versions, but my philosophy is always to support the broadest range possible, and from everything I've seen it's paid off – users appreciate when you do not force their hand to upgrade. This is why Yahoo for example continues to be our biggest contributor on YARN support, even though their YARN is pretty different.

Agree.;;;","04/Jun/14 22:56;pwendell;Issue resolved by pull request 898
[https://github.com/apache/spark/pull/898];;;",,
Standardize process for creating Spark packages,SPARK-1514,12708833,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,16/Apr/14 16:50,21/May/14 23:17,14/Jul/23 06:25,19/May/14 18:51,,,,,,,,,1.1.0,,,,,Build,,,,,0,,,,,,"Over time we've got (a) make-distribution.sh (b) maven distribution targets (c) create-release.sh script in /dev. This is pretty confusing for downstream packagers.

We should have a single way to package releases, probably using a modified maven distribution.",,elee,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387156,,,Wed May 21 23:16:27 UTC 2014,,,,,,,,,,"0|i1uozz:",387419,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 19:12;tgraves;What was the resolution here?  is there a pull request that went in?;;;","21/May/14 18:51;elee;+1 [~tgraves] - what occurred in the branch-1.0 branch to move this to Resolved?

Working with make-distribution.sh now and wondering if there will be a unification at some point in the future of building Spark{,-Mesos} artifacts for distribution;;;","21/May/14 23:16;pwendell;The create-release script and the make-distribution script have been consolidated (the former now calls the latter) which was the main point of this JIRA. This was fixed in master and 1.0 a while back. Here is an example:

https://github.com/apache/spark/blob/master/dev/create-release/create-release.sh#L98

So now make-distribution is the official way of making binary distributions. This is what we used for all of the 1.0 release candidates and has been extensively tested.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update TestUtils.createCompiledClass() API to work with creating class file on different filesystem,SPARK-1511,12708758,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,advancedxy,advancedxy,16/Apr/14 13:21,17/Apr/14 14:07,14/Jul/23 06:25,17/Apr/14 14:07,0.8.1,0.9.0,1.0.0,,,,,,1.0.0,,,,,Spark Core,,,,,0,starter,,,,,"The createCompliedClass method uses java File.renameTo method to rename source file to destination file, which will fail if source and destination files are on different disks (or partitions).

see http://apache-spark-developers-list.1001551.n3.nabble.com/Tests-failed-after-assembling-the-latest-code-from-github-td6315.html for more details.

Use com.google.common.io.Files.move instead of renameTo will solve this issue.","Mac OS X, two disks. ",advancedxy,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387081,,,Thu Apr 17 14:06:42 UTC 2014,,,,,,,,,,"0|i1uojb:",387344,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 14:06;advancedxy;Close this issue.
pr https://github.com/apache/spark/pull/427 solves it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark can hang if pyspark tasks fail,SPARK-1498,12708446,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kayousterhout,kayousterhout,15/Apr/14 05:10,06/Nov/14 07:59,14/Jul/23 06:25,06/Nov/14 07:59,0.9.0,0.9.1,0.9.2,,,,,,1.0.0,,,,,PySpark,,,,,0,,,,,,"In pyspark, when some kinds of jobs fail, Spark hangs rather than returning an error.  This is partially a scheduler problem -- the scheduler sometimes thinks failed tasks succeed, even though they have a stack trace and exception.

You can reproduce this problem with:
ardd = sc.parallelize([(1,2,3), (4,5,6)])
brdd = sc.parallelize([(1,2,6), (4,5,9)])
ardd.join(brdd).count()

The last line will run forever (the problem in this code is that the RDD entries have 3 values instead of the expected 2).  I haven't verified if this is a problem for 1.0 as well as 0.9.

Thanks to Shivaram for helping diagnose this issue!",,codingcat,davies,joshrosen,kayousterhout,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386769,,,Thu Nov 06 07:59:28 UTC 2014,,,,,,,,,,"0|i1ummf:",387033,,,,,,,,,,,,,0.9.3,,,,,,,,,,"15/Apr/14 05:13;shivaram;I see this problem on the master branch. To add more details I get a log like 

14/04/14 22:07:18 INFO TaskSetManager: Starting task 1.0:4 using 1 cores as TID 4 on executor localhost: localhost (PROCESS_LOCAL)
14/04/14 22:07:18 INFO TaskSetManager: Serialized task 1.0:4 as 6651 bytes in 1 ms
14/04/14 22:07:18 INFO Executor: Running task ID 4
14/04/14 22:07:18 INFO DAGScheduler: Completed ShuffleMapTask(1, 2)
14/04/14 22:07:18 INFO TaskSetManager: Finished TID 2 in 132 ms on localhost (progress: 1/8)
14/04/14 22:07:18 INFO TaskSetManager: Starting task 1.0:5 using 1 cores as TID 5 on executor localhost: localhost (PROCESS_LOCAL)
14/04/14 22:07:18 INFO TaskSetManager: Serialized task 1.0:5 as 6673 bytes in 1 ms
14/04/14 22:07:18 INFO Executor: Running task ID 5
14/04/14 22:07:18 INFO DAGScheduler: Completed ShuffleMapTask(1, 0)
14/04/14 22:07:18 INFO TaskSetManager: Finished TID 0 in 150 ms on localhost (progress: 2/8)
14/04/14 22:07:18 INFO PythonRDD: Times: total = 2, boot = 1, init = 1, finish = 0
14/04/14 22:07:18 INFO PythonRDD: Times: total = 6, boot = 1, init = 5, finish = 0
14/04/14 22:07:18 INFO Executor: Serialized size of result for 4 is 847
14/04/14 22:07:18 INFO Executor: Sending result for 4 directly to driver
14/04/14 22:07:18 INFO Executor: Finished task ID 4
14/04/14 22:07:18 INFO TaskSetManager: Starting task 1.0:6 using 1 cores as TID 6 on executor localhost: localhost (PROCESS_LOCAL)
14/04/14 22:07:18 INFO TaskSetManager: Serialized task 1.0:6 as 6651 bytes in 1 ms
14/04/14 22:07:18 INFO Executor: Running task ID 6
14/04/14 22:07:18 INFO DAGScheduler: Completed ShuffleMapTask(1, 4)
14/04/14 22:07:18 INFO TaskSetManager: Finished TID 4 in 23 ms on localhost (progress: 3/8)
PySpark worker failed with exception:
Traceback (most recent call last):
  File ""/home/shivaram/projects/spark/python/pyspark/worker.py"", line 77, in main
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/home/shivaram/projects/spark/python/pyspark/serializers.py"", line 191, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/home/shivaram/projects/spark/python/pyspark/serializers.py"", line 123, in dump_stream
    for obj in iterator:
  File ""/home/shivaram/projects/spark/python/pyspark/serializers.py"", line 180, in _batched
    for item in iterator:
  File ""/home/shivaram/projects/spark/python/pyspark/join.py"", line 38, in <lambda>
    ws = other.map(lambda (k, v): (k, (2, v)))
ValueError: too many values to unpack

But the web ui shows 4 out of 8 tasks succeeded and 4 tasks keep running forever;;;","25/Jul/14 23:49;joshrosen;This is still a problem as of 0.9.2 but it's fixed in 1.0+.;;;","06/Nov/14 07:59;kayousterhout;I closed this since 0.9 seems pretty ancient now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Dependencies being checked by MIMA,SPARK-1494,12708413,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,marmbrus,ahirreddy,ahirreddy,15/Apr/14 00:00,09/May/14 21:01,14/Jul/23 06:25,09/May/14 21:01,1.0.0,,,,,,,,1.0.0,,,,,Project Infra,SQL,,,,0,,,,,,"It looks like code in companion objects is being invoked by the MIMA checker, as it uses Scala reflection to check all of the interfaces. As a result it's starting a Spark context and eventually out of memory errors. As a temporary fix all classes that contain ""hive"" or ""Hive"" are excluded from the check.",,ahirreddy,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386736,,,Fri May 09 21:01:29 UTC 2014,,,,,,,,,,"0|i1umf3:",387000,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 01:22;marmbrus;We could change the object to contain a lazy val instead of being a TestHiveContext itself.  That should fix the issues.;;;","09/May/14 21:01;marmbrus;I think this is fixed now that MIMA loads classes passing false to ""initialize"".;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache RAT excludes don't work with file path (instead of file name),SPARK-1493,12708391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pwendell,pwendell,14/Apr/14 22:42,29/Sep/15 16:58,14/Jul/23 06:25,11/Nov/14 13:02,,,,,,,,,,,,,,Project Infra,,,,,0,starter,,,,,"Right now the way we do RAT checks, it doesn't work if you try to exclude:

/path/to/file.ext

you have to just exclude

file.ext",,eje,pwendell,sandhya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-746,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386714,,,Tue Sep 29 16:58:52 UTC 2015,,,,,,,,,,"0|i1uma7:",386978,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 23:03;eje;RAT itself appears to preclude exclusion using a ""/path/to/file.ext"" regex because it traverses the directory tree and applies its exclusion filter only to individual file names.  The filter never sees an entire path ""path/to/file.ext"", only ""path"", ""to"", and ""file.ext""

https://github.com/apache/rat/blob/incubator-site-import/rat/rat-core/src/main/java/org/apache/rat/DirectoryWalker.java#L127

Either RAT needs a new filtering feature that can see an entire path, or the report it generates has to be filtered post-hoc.

Filed an RFE against RAT:  RAT-161;;;","10/Jun/14 00:03;pwendell;Thanks for looking into this Erik. It seems like maybe there isn't a good way to do unless we want to implement filtering post-hoc (and it might be tricky to support e.g. globbing in that case).;;;","20/Jun/14 15:36;eje;I submitted a proposal patch for RAT-161, which allows one to request path-spanning patterns by including a leading '/'

If '--dir' argument is /path/to/repo, and contents of '-E' file includes:
/subpath/to/.*ext

then the pattern induced is:
/path/to/repo + /subpath/to/.*ex t --> /path/to/repo/subpath/to/.*ext
;;;","11/Nov/14 13:02;srowen;Looks like this was fixed? There are no paths of this form in .rat-excludes now.;;;","29/Sep/15 16:58;sandhya;Erik's proposal would certainly help us in the Trafodion project. We have a high level directory sturcture on which we'd like to run RAT. We have several subdirectories and want to include only a subset of files in each of those directories. Using filename patterns without /path would make the exclusions imprecise. There may be the same file it 2 different subdirectories and both would get excluded which we don't really intend to do.We want to exclude specific files in specific directories with a regexp. Unfortunately  it isn't working.  
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
maven hadoop-provided profile fails to build,SPARK-1491,12708319,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,tgraves,tgraves,14/Apr/14 17:51,29/Apr/14 06:28,14/Jul/23 06:25,29/Apr/14 06:28,1.0.0,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"I tried to use the -Phadoop-profile with maven to build the assembly without the yarn/hadoop pieces that are already on the cluster, but it fails with:

[ERROR]   The project org.apache.spark:spark-parent:1.0.0-SNAPSHOT (/home/tgraves/tgravescs-spark/pom.xml) has 2 errors
[ERROR]     'dependencies.dependency.version' for org.apache.avro:avro-ipc:jar is missing. @ line 884, column 21
[ERROR]     'dependencies.dependency.version' for org.apache.zookeeper:zookeeper:jar is missing. @ line 889, column 21
[ERROR] 


This means avro-ipc and zookeeper don't have versions in the dependencyManagement section.
I tried on both hadoop 0.23 and hadoop 2.3.0.  build command like:

 mvn  -Dyarn.version=0.23.9 -Dhadoop.version=0.23.9  -Pyarn-alpha -Phadoop-provided package -DskipTests",,pwendell,sandyr,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386642,,,Tue Apr 29 06:28:01 UTC 2014,,,,,,,,,,"0|i1ulu7:",386906,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 06:28;pwendell;Fixed by :
https://github.com/apache/spark/pull/480

I tested the build with this exact configuration and it worked.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the HistoryServer view acls,SPARK-1489,12708289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,14/Apr/14 15:38,25/Apr/14 01:39,14/Jul/23 06:25,25/Apr/14 01:39,1.0.0,,,,,,,,1.0.0,,,,,Web UI,,,,,0,,,,,,"If you are running the historyServer with view acls enabled (and a filter added to do auth), the application acls don't work properly.  It is looking at the user running the history server and not the user who ran the actual application, so basically no one other then the user running the history server can see anything.

We also need a way to allow all users to see the front page of the history server and only do authorization on the particular applications.",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386612,,,Wed Apr 23 15:55:20 UTC 2014,,,,,,,,,,"0|i1ulnj:",386876,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 15:55;tgraves;https://github.com/apache/spark/pull/509;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential resource leaks in saveAsHadoopDataset and saveAsNewAPIHadoopDataset,SPARK-1482,12708132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,13/Apr/14 10:32,19/Apr/14 00:52,14/Jul/23 06:25,19/Apr/14 00:52,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,easyfix,,,,,"""writer.close"" should be put in the ""finally"" block to avoid potential resource leaks.",,matei,tgraves,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386455,,,Sat Apr 19 00:52:04 UTC 2014,,,,,,,,,,"0|i1ukon:",386719,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/14 10:34;zsxwing;PR: https://github.com/apache/spark/pull/400;;;","19/Apr/14 00:52;matei;https://github.com/apache/spark/pull/400;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drain event logging queue before stopping event logger,SPARK-1475,12708043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kzhang,kzhang,kzhang,11/Apr/14 23:00,22/Sep/14 17:27,14/Jul/23 06:25,11/Apr/14 23:22,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"When stopping SparkListenerBus, its event queue needs to be drained. And this needs to happen before event logger is stopped. Otherwise, any event still waiting to be processed in the queue may be lost and consequently event log file may be incomplete. ",,kzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1407,SPARK-1132,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386366,,,Thu Apr 17 02:08:04 UTC 2014,,,,,,,,,,"0|i1uk53:",386631,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 23:19;kzhang;When event queue is not drained, users may observe similar issues as those reported in SPARK-1407 (when sc.stop() is not called). 

https://github.com/apache/spark/pull/366

The above PR fixes this issue. It does require applications to call sc.stop() to properly stop SparkListenerBus and event logger.;;;","17/Apr/14 02:08;kzhang;A second PR that fixes the unit test introduced above.

https://github.com/apache/spark/pull/401;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on yarn assembly doesn't include org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,SPARK-1474,12708037,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,xgong,xgong,11/Apr/14 22:19,06/May/14 19:00,14/Jul/23 06:25,06/May/14 19:00,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"part of the error logs 
{code}
14/04/11 15:08:06 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
14/04/11 15:08:06 WARN Holder: 
java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at org.eclipse.jetty.util.Loader.loadClass(Loader.java:100)
	at org.eclipse.jetty.util.Loader.loadClass(Loader.java:79)
	at org.eclipse.jetty.servlet.Holder.doStart(Holder.java:107)
	at org.eclipse.jetty.servlet.FilterHolder.doStart(FilterHolder.java:90)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:768)
	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:265)
	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)
	at org.eclipse.jetty.server.Server.doStart(Server.java:282)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:189)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:189)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:189)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:188)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:201)
	at org.apache.spark.ui.SparkUI.bind(SparkUI.scala:101)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:215)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:110)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:147)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:394)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:178)
14/04/11 15:08:06 WARN AbstractLifeCycle: FAILED org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-364e50ee: javax.servlet.UnavailableException: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
javax.servlet.UnavailableException: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	at org.eclipse.jetty.servlet.Holder.doStart(Holder.java:114)
	at org.eclipse.jetty.servlet.FilterHolder.doStart(FilterHolder.java:90)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:768)
	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:265)
	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)
	at org.eclipse.jetty.server.Server.doStart(Server.java:282)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:189)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:189)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:189)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:188)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:201)
	at org.apache.spark.ui.SparkUI.bind(SparkUI.scala:101)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:215)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:110)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:147)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:394)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:178)
{code}",,jianhe,pwendell,tgraves,xgong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386360,,,Tue May 06 19:00:35 UTC 2014,,,,,,,,,,"0|i1uk3r:",386625,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 22:21;xgong;use mvn to Compile the spark code from spark master branch with hadoop-2.2.0:
{code}
mvn -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean package
{code}

And launch spark application with yarn-standalone mode 
{code}
SPARK_JAR=assembly/target/scala-2.10/spark-assembly_2.10-1.0.0-SNAPSHOT-hadoop2.2.0.jar ./bin/spark-class org.apache.spark.deploy.yarn.Client --jar examples/target/scala-2.10/spark-examples_2.10-assembly-1.0.0-SNAPSHOT.jar --class org.apache.spark.examples.SparkPi --arg yarn-standalone --num-executors 3 --driver-memory 512m --executor-memory 512m --executor-cores 1
{code}
;;;","11/Apr/14 22:23;xgong;Verified that spark application *works* in yarn-client mode.
{code}
SPARK_JAR=assembly/target/scala-2.10/spark-assembly_2.10-1.0.0-SNAPSHOT-hadoop2.2.0.jar SPARK_YARN_APP=examples/target/scala-2.10/spark-examples_2.10-assembly-1.0.0-SNAPSHOT.jar ./bin/run-example org.apache.spark.examples.SparkPi yarn-client
{code};;;","14/Apr/14 15:56;tgraves;are you including the yarn jars in the classpath when launching on yarn?  

The error is ClassNotFoundException: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.

The yarn-client mode doesn't hook into the yarn UI filter since its running on the client and not on the cluster which is why it works there.

We may want to change it so the assembly includes that class also.;;;","14/Apr/14 17:32;xgong;[~tgraves]
I think so, I did add YARN_HOME into class path
{code}
YARN_HOME=""/Users/xuan/dep/hadoop-yarn-project-2.4.1-SNAPSHOT/share/hadoop/yarn""
{code} which will includes all the yarn jars;;;","14/Apr/14 17:43;tgraves;do you have a /* at the end to pick up the jars?  Where are you setting that?

YARN has the default classpath set for all applications by the config: yarn.application.classpath.  That generally default to including all the yarn/mapreduce/hdfs stuff so I would expect this to work by default.  

Note that it is a bug that we don't package it in the assembly so fixing that, but you should be able to work around just by adding it to the classpath.;;;","14/Apr/14 17:52;xgong;Thanks, [~tgraves]. 
I messed up the configurations....;;;","14/Apr/14 21:39;tgraves;https://github.com/apache/spark/pull/406;;;","06/May/14 19:00;pwendell;Issue resolved by pull request 406
[https://github.com/apache/spark/pull/406];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Worker not  recognize Driver state at standalone mode,SPARK-1471,12707863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,shenhong,shenhong,11/Apr/14 03:25,18/Jun/14 12:20,14/Jul/23 06:25,17/Jun/14 16:35,0.9.0,,,,,,,,1.1.0,,,,,Deploy,,,,,0,,,,,,"When I run a spark job in standalone,
./bin/spark-class org.apache.spark.deploy.Client  launch spark://v125050024.bja:7077 file:///home/yuling.sh/spark-0.9.0-incubating/examples/target/spark-examples_2.10-0.9.0-incubating.jar org.apache.spark.examples.SparkPi

 Here is the Worker log.
14/04/11 11:15:04 ERROR OneForOneStrategy: FAILED (of class scala.Enumeration$Val)
scala.MatchError: FAILED (of class scala.Enumeration$Val)
        at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.applyOrElse(Worker.scala:277)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


",standalone,codingcat,fedragon,shenhong,tavoaqp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386186,,,Tue Jun 17 16:25:25 UTC 2014,,,,,,,,,,"0|i1uj13:",386451,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 03:34;shenhong;The reason is when Worker receive DriverStateChanged, it only match  ERROR, FINISHED and KILLED. it should add a case like :case _ =>
;;;","16/Apr/14 20:28;codingcat;I see, the new FAILED status is introduced in https://github.com/apache/spark/commit/0f9d2ace6baefeacb1abf9d51a457644b67f2f8d, it should be a pretty easy fix;;;","17/Jun/14 15:47;fedragon;Hello,
I'm facing the same issue in version 1.0.0 (built from the sources distribution using {{make-distribution.sh --hadoop 2.0.0-cdh4.7.0}}).

I'm running a job using the new {{bin/spark-submit}} script. When the job fails, one of the worker dies with the following error:

{code}
2014-06-17 17:00:04,675 [sparkWorker-akka.actor.default-dispatcher-3] ERROR akka.actor.OneForOneStrategy - FAILED (of class scala.Enumeration$Val)
scala.MatchError: FAILED (of class scala.Enumeration$Val)
        at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.applyOrElse(Worker.scala:317)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code};;;","17/Jun/14 16:21;codingcat;I will fix it right now;;;","17/Jun/14 16:25;codingcat;this has been fixed by https://github.com/apache/spark/commit/95e4c9c6fb153b7f0aa4c442c4bdb6552d326640;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler mode should accept lower-case definitions and have nicer error messages,SPARK-1469,12707829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,techaddict,pwendell,pwendell,10/Apr/14 23:26,16/Apr/14 16:59,14/Jul/23 06:25,16/Apr/14 16:59,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,starter,,,,,"I tried setting spark.scheduler.mode=fair and I got the following nasty exception:

{code}
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:313)
	at scala.None$.get(Option.scala:311)
	at scala.Enumeration.withName(Enumeration.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.<init>(TaskSchedulerImpl.scala:101)
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:1338)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:230)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:956)
	at $iwC$$iwC.<init>(<console>:8)
	at $iwC.<init>(<console>:14)
	at <init>(<console>:16)
	at .<init>(<console>:20)
	at .<clinit>(<console>)
{code}

We should do two improvements:
1. We should make the built in ones case insensitive (fair/FAIR, fifo/FIFO).
2. If an invalid mode is given we should print a better error message.",,pwendell,techaddict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386152,,,Fri Apr 11 04:40:23 UTC 2014,,,,,,,,,,"0|i1uitj:",386417,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 04:40;techaddict;https://github.com/apache/spark/pull/388;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The hash method used by partitionBy in Pyspark doesn't deal with None correctly.,SPARK-1468,12707825,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tyro89,tyro89,tyro89,10/Apr/14 22:57,03/Jun/14 20:35,14/Jul/23 06:25,03/Jun/14 20:33,0.9.0,1.0.0,,,,,,,0.9.2,1.0.1,,,,PySpark,,,,,0,,,,,,"In python the default hash method uses the memory address of objects. Since None is an object None will get partitioned into different partitions depending on which python process it is run in. This causes some really odd results when None key's are used in the partitionBy.

I've created a fix using a consistent hashing method that sends None to 0. That pr lives at https://github.com/apache/spark/pull/371",,tyro89,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386148,,,2014-04-10 22:57:28.0,,,,,,,,,,"0|i1uisn:",386413,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make StorageLevel.apply() factory methods developer API's,SPARK-1467,12707820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,techaddict,matei,matei,10/Apr/14 22:20,27/Apr/14 02:05,14/Jul/23 06:25,27/Apr/14 02:05,,,,,,,,,1.0.0,,,,,Documentation,,,,,0,,,,,,"We may want to evolve these in the future to add things like SSDs, so let's mark them as experimental for now. Long-term the right solution might be some kind of builder. The stable API should be the existing StorageLevel constants.",,matei,techaddict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386143,,,Fri Apr 25 13:52:39 UTC 2014,,,,,,,,,,"0|i1uirj:",386408,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 13:52;techaddict;https://github.com/apache/spark/pull/551;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark doesn't check if gateway process launches correctly,SPARK-1466,12707798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kayousterhout,kayousterhout,kayousterhout,10/Apr/14 21:17,18/Jun/14 20:23,14/Jul/23 06:25,18/Jun/14 20:21,0.9.0,0.9.1,,,,,,,1.1.0,,,,,PySpark,,,,,0,,,,,,"If the gateway process fails to start correctly (e.g., because JAVA_HOME isn't set correctly, there's no Spark jar, etc.), right now pyspark fails because of a very difficult-to-understand error, where we try to parse stdout to get the port where Spark started and there's nothing there.  We should properly catch the error, print it to the user, and exit.",,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1688,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386121,,,Wed Jun 18 20:23:22 UTC 2014,,,,,,,,,,"0|i1uimn:",386386,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 21:19;kayousterhout;https://github.com/apache/spark/pull/383;;;","18/Jun/14 20:23;kayousterhout;[~pwendell] I didn't merge this into the 1.0 branch since it's pretty minor and I don't know if anyone besides the students in the course where we used iPython notebook have noticed this. I know it was marked as a blocker but I think that was an inflated priority -- anyway let me know if you think it should go in for 1.0.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark compilation is broken with the latest hadoop-2.4.0 release,SPARK-1465,12707796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,xgong,xgong,10/Apr/14 21:10,16/Apr/14 19:46,14/Jul/23 06:25,16/Apr/14 19:46,,,,,,,,,1.0.0,,,,,,,,,,1,,,,,,"Building spark with the latest 2.4.0 version of yarn, appears to be broken
{code}
[ERROR]       Apps.addToEnvironment(env, Environment.CLASSPATH.name, Environment.PWD.$() +
[ERROR]                            ^
[ERROR] /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala:441: not enough arguments for method addToEnvironment: (x$1: java.util.Map[String,String], x$2: String, x$3: String, x$4: String)Unit.
Unspecified value parameter x$4.
[ERROR]     Apps.addToEnvironment(env, Environment.CLASSPATH.name, Environment.PWD.$() +
[ERROR]                          ^
[ERROR] /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala:446: not enough arguments for method addToEnvironment: (x$1: java.util.Map[String,String], x$2: String, x$3: String, x$4: String)Unit.
Unspecified value parameter x$4.
[ERROR]       Apps.addToEnvironment(env, Environment.CLASSPATH.name, Environment.PWD.$() +
[ERROR]                            ^
[ERROR] /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala:449: not enough arguments for method addToEnvironment: (x$1: java.util.Map[String,String], x$2: String, x$3: String, x$4: String)Unit.
Unspecified value parameter x$4.
[ERROR]     Apps.addToEnvironment(env, Environment.CLASSPATH.name, Environment.PWD.$() +
[ERROR]                          ^
[ERROR] /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnableUtil.scala:170: not enough arguments for method setEnvFromInputString: (x$1: java.util.Map[String,String], x$2: String, x$3: String)Unit.
Unspecified value parameter x$3.
[ERROR]     Apps.setEnvFromInputString(env, System.getenv(""SPARK_YARN_USER_ENV""))
{code}",,berngp,hitesh,tgraves,xgong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386119,,,Fri Apr 11 22:04:51 UTC 2014,,,,,,,,,,"0|i1uim7:",386384,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 21:11;xgong;YARN-1824 changes the APIs in Apps, which causes the spark build to break if built against a version 2.4.0.
;;;","10/Apr/14 21:11;xgong;I am starting to fix this.;;;","11/Apr/14 14:17;tgraves;You can't just update spark to the new api as it will break compiling against 2.3 and other 2.x versions.    I will file a bug against Hadoop for this and it should be fixed in a 2.4.1 release.  Unfortunately that doesn't help people using 2.4.  

For Spark I think we are better off just to make our own functions that do that functionality.;;;","11/Apr/14 14:32;tgraves;https://issues.apache.org/jira/browse/YARN-1931 filed for this breakage in api.;;;","11/Apr/14 18:02;xgong;[~tgraves] 
bq. For Spark I think we are better off just to make our own functions that do that functionality.

Yes, that is what I am doing right now...;;;","11/Apr/14 22:04;xgong;https://github.com/apache/spark/pull/396;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"EventLoggingListener does not work with ""file://"" target dir",SPARK-1459,12707591,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,09/Apr/14 23:03,22/Apr/14 06:12,14/Jul/23 06:25,22/Apr/14 06:12,1.0.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Bug is simple; FileLogger tries to pass a URL to FileOutputStream's constructor, and that fails. I'll upload a PR soon.",,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385914,,,Fri Apr 18 18:06:10 UTC 2014,,,,,,,,,,"0|i1uhcv:",386178,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 23:06;vanzin;PR #375 (https://github.com/apache/spark/pull/375);;;","18/Apr/14 18:01;vanzin;This was commit 69047506. (If someone with permissions could set me as the assignee that would be great.);;;","18/Apr/14 18:06;vanzin;Sorry, got confused. This PR is still pending.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark examples should not do a System.exit,SPARK-1446,12707260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,techaddict,tgraves,tgraves,08/Apr/14 21:18,10/Apr/14 07:37,14/Jul/23 06:25,10/Apr/14 07:37,1.0.0,,,,,,,,,,,,,Examples,,,,,0,,,,,,The spark examples should exit nice (sparkcontext.stop()) rather then doing a System.exit. The System.exit can cause issues like in SPARK-1407.  SparkHdfsLR and JavaWordCount both do the System.exit. We should look through all the examples.,,techaddict,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385583,,,Wed Apr 09 17:05:55 UTC 2014,,,,,,,,,,"0|i1ufbr:",385849,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 04:37;techaddict;I want to work on this.
Calling sparkcontext.stop() before System.exit(0) will resolve this, or should we remove System.exit(0) and use just sparkcontext.stop()?;;;","09/Apr/14 17:05;techaddict;https://github.com/apache/spark/pull/370;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compute-classpath.sh should not print error if lib_managed not found,SPARK-1445,12707230,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ilikerps,ilikerps,ilikerps,08/Apr/14 18:23,08/Apr/14 21:40,14/Jul/23 06:25,08/Apr/14 21:40,1.0.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,,,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385553,,,2014-04-08 18:23:30.0,,,,,,,,,,"0|i1uf53:",385819,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compile Spark Core error with Hadoop 0.23.x when not using YARN,SPARK-1441,12707100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,witgo,witgo,08/Apr/14 04:01,29/Apr/14 06:18,14/Jul/23 06:25,29/Apr/14 06:18,1.0.0,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,,,tgraves,witgo,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1121,,SPARK-1121,,,,,,,,,,,,,,,,,,,"08/Apr/14 04:05;witgo;mvn.log;https://issues.apache.org/jira/secure/attachment/12639131/mvn.log","08/Apr/14 04:05;witgo;sbt.log;https://issues.apache.org/jira/secure/attachment/12639130/sbt.log",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385423,,,Tue Apr 08 14:41:30 UTC 2014,,,,,,,,,,"0|i1uecn:",385690,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 04:05;witgo;{code}
./make-distribution.sh --hadoop 0.23.9  > sbt.log
mvn -Dhadoop.version=0.23.9 -DskipTests package -X > mvn.log
{code};;;","08/Apr/14 07:05;srowen;You're not enabling the yarn-alpha build profile here, and Hadoop 0.23.x is what it's for. Similarity for SBT although the ""profile"" is enabled by the SPARK_YARN env variable. Your change just forced the config intended for yarn-alpha into the rest of the build, which seems incorrect.;;;","08/Apr/14 07:33;witgo;If someone compile spark without yarn support,like this
 {code}./make-distribution.sh --hadoop 0.23.9{code}
How do we ensure compile?;;;","08/Apr/14 14:06;tgraves;It sounds like the avro changes we put into the poms for yarn-alpha needs to be tied to another independent profile then so that someone building hadoop 0.23 without yarn can also pick them up.  (like -Pinclude-avro)  This is what was originally proposed but we changed it to tie to yarn-alpha profile because we didn't realize it affected this build. ;;;","08/Apr/14 14:12;srowen;I see, I had taken the ""yarn-alpha"" profile to be the slightly-misnamed profile you would use when building the whole project with 0.23, and building this distro means building everything. At least, that's a fairly fine solution, no? you should set yarn.version to 0.23.9 too.;;;","08/Apr/14 14:20;tgraves;The yarn-alpha profile is tied to meaning included the spark on yarn support for hadoop 0.23.   If you don't need the spark on yarn support you should be able to just compile spark for hadoop 0.23 so that you can talk to HDFS on a hadoop 0.23 cluster, but you are running spark separately (standalone cluster or mesos).  I think the latter is what is trying to be done here.      It doesn't hurt anything to included the yarn pieces (other then slightly larger assembly jar) so that is atleast a work around for now. ;;;","08/Apr/14 14:25;srowen;Got it. Maybe the profile can be switched around to activate based on hadoop.version and/or yarn.version being 0.23.x. This sort of thing is possible in Maven, although I'm not 100% sure you can express a range constraint on a user property. ;;;","08/Apr/14 14:41;witgo;How can use multiple property to activate profile?
{code}
<activation>
   <property><name>hadoop.version</name><value>0.23.9</value></property>
</activation>
{code} 
This configuration is only activated when hadoop.version = 0.23.9;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Compression code broke in-memory store,SPARK-1436,12707009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,rxin,rxin,07/Apr/14 19:11,15/May/14 03:30,14/Jul/23 06:25,14/Apr/14 22:24,1.0.0,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"Try run the following code:

{code}

package org.apache.spark.sql

import org.apache.spark.sql.test.TestSQLContext._
import org.apache.spark.sql.catalyst.util._

case class Data(a: Int, b: Long)

object AggregationBenchmark {
  def main(args: Array[String]): Unit = {
    val rdd =
      sparkContext.parallelize(1 to 20).flatMap(_ => (1 to 500000).map(i => Data(i % 100, i)))
    rdd.registerAsTable(""data"")
    cacheTable(""data"")

    (1 to 10).foreach { i =>
      println(s""=== ITERATION $i ==="")

      benchmark { println(""SELECT COUNT() FROM data:"" + sql(""SELECT COUNT(*) FROM data"").collect().head) }

      println(""SELECT a, SUM(b) FROM data GROUP BY a"")
      benchmark { sql(""SELECT a, SUM(b) FROM data GROUP BY a"").count() }

      println(""SELECT SUM(b) FROM data"")
      benchmark { sql(""SELECT SUM(b) FROM data"").count() }
    }
  }
}
{code}

The following exception is thrown:
{code}
java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:498)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)
	at org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:103)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1.<init>(InMemoryColumnarTableScan.scala:61)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:60)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:56)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.Task.run(Task.scala:52)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/04/07 12:07:38 WARN TaskSetManager: Lost TID 3 (task 4.0:0)
14/04/07 12:07:38 WARN TaskSetManager: Loss was due to java.nio.BufferUnderflowException
java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:498)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)
	at org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:103)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1.<init>(InMemoryColumnarTableScan.scala:61)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:60)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:56)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.Task.run(Task.scala:52)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

{code}",,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385332,,,Tue Apr 08 05:47:15 UTC 2014,,,,,,,,,,"0|i1udsf:",385599,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 19:11;rxin;Try run the following code:

{code}

package org.apache.spark.sql

import org.apache.spark.sql.test.TestSQLContext._
import org.apache.spark.sql.catalyst.util._

case class Data(a: Int, b: Long)

object AggregationBenchmark {
  def main(args: Array[String]): Unit = {
    val rdd =
      sparkContext.parallelize(1 to 20).flatMap(_ => (1 to 500000).map(i => Data(i % 100, i)))
    rdd.registerAsTable(""data"")
    cacheTable(""data"")

    (1 to 10).foreach { i =>
      println(s""=== ITERATION $i ==="")

      benchmark { println(""SELECT COUNT() FROM data:"" + sql(""SELECT COUNT(*) FROM data"").collect().head) }

      println(""SELECT a, SUM(b) FROM data GROUP BY a"")
      benchmark { sql(""SELECT a, SUM(b) FROM data GROUP BY a"").count() }

      println(""SELECT SUM(b) FROM data"")
      benchmark { sql(""SELECT SUM(b) FROM data"").count() }
    }
  }
}
{code}

The following exception is thrown:
{code}
java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:498)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)
	at org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:103)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1.<init>(InMemoryColumnarTableScan.scala:61)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:60)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:56)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.Task.run(Task.scala:52)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/04/07 12:07:38 WARN TaskSetManager: Lost TID 3 (task 4.0:0)
14/04/07 12:07:38 WARN TaskSetManager: Loss was due to java.nio.BufferUnderflowException
java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:498)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)
	at org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:103)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1.<init>(InMemoryColumnarTableScan.scala:61)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:60)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:56)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)
	at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.Task.run(Task.scala:52)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

{code};;;","08/Apr/14 02:00;lian cheng;Sorry, forgot to duplicate the in-memory column byte buffer when creating new {{ColumnAccessor}}'s, so that when the column byte buffer is accessed multiple times, the position is not reset to 0. Will fix this in PR [#330|https://github.com/apache/spark/pull/330] with regression test.;;;","08/Apr/14 05:47;lian cheng;Fixed in [this commit|https://github.com/liancheng/spark/commit/1d037b83191099da961c247a57ef686cb508c447] of PR [#330|https://github.com/apache/spark/pull/330];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential memory leak in stageIdToExecutorSummaries in JobProgressTracker,SPARK-1432,12706976,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dgshep,dgshep,dgshep,07/Apr/14 16:01,07/Apr/14 17:07,14/Jul/23 06:25,07/Apr/14 17:07,0.9.0,,,,,,,,0.9.2,1.0.0,,,,Web UI,,,,,0,,,,,,"JobProgressTracker continuously cleans up old metadata as per the spark.ui.retainedStages configuration parameter. It seems however that not all metadata maps are being cleaned, in particular stageIdToExecutorSummaries could grow in an unbounded manner in a long running application.",,dgshep,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385299,,,Mon Apr 07 17:07:16 UTC 2014,,,,,,,,,,"0|i1udl3:",385566,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 17:07;pwendell;https://github.com/apache/spark/pull/338;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HQL Examples Don't Work,SPARK-1427,12706828,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,pwendell,pwendell,06/Apr/14 05:49,07/Apr/14 20:55,14/Jul/23 06:25,07/Apr/14 20:55,1.0.0,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"{code}
scala> hql(""CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"")
14/04/05 22:40:29 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)
14/04/05 22:40:30 INFO ParseDriver: Parse Completed
14/04/05 22:40:30 INFO Driver: <PERFLOG method=Driver.run>
14/04/05 22:40:30 INFO Driver: <PERFLOG method=TimeToSubmit>
14/04/05 22:40:30 INFO Driver: <PERFLOG method=compile>
14/04/05 22:40:30 INFO Driver: <PERFLOG method=parse>
14/04/05 22:40:30 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)
14/04/05 22:40:30 INFO ParseDriver: Parse Completed
14/04/05 22:40:30 INFO Driver: </PERFLOG method=parse start=1396762830162 end=1396762830163 duration=1>
14/04/05 22:40:30 INFO Driver: <PERFLOG method=semanticAnalyze>
14/04/05 22:40:30 INFO SemanticAnalyzer: Starting Semantic Analysis
14/04/05 22:40:30 INFO SemanticAnalyzer: Creating table src position=27
14/04/05 22:40:30 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
14/04/05 22:40:30 INFO ObjectStore: ObjectStore, initialize called
14/04/05 22:40:30 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
14/04/05 22:40:30 WARN BoneCPConfig: Max Connections < 1. Setting to 20
14/04/05 22:40:32 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""
14/04/05 22:40:32 INFO ObjectStore: Initialized ObjectStore
14/04/05 22:40:33 WARN BoneCPConfig: Max Connections < 1. Setting to 20
14/04/05 22:40:33 INFO HiveMetaStore: 0: get_table : db=default tbl=src
14/04/05 22:40:33 INFO audit: ugi=patrick	ip=unknown-ip-addr	cmd=get_table : db=default tbl=src	
14/04/05 22:40:33 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
14/04/05 22:40:33 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
14/04/05 22:40:34 INFO Driver: Semantic Analysis Completed
14/04/05 22:40:34 INFO Driver: </PERFLOG method=semanticAnalyze start=1396762830163 end=1396762834001 duration=3838>
14/04/05 22:40:34 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
14/04/05 22:40:34 INFO Driver: </PERFLOG method=compile start=1396762830146 end=1396762834006 duration=3860>
14/04/05 22:40:34 INFO Driver: <PERFLOG method=Driver.execute>
14/04/05 22:40:34 INFO Driver: Starting command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)
14/04/05 22:40:34 INFO Driver: </PERFLOG method=TimeToSubmit start=1396762830146 end=1396762834016 duration=3870>
14/04/05 22:40:34 INFO Driver: <PERFLOG method=runTasks>
14/04/05 22:40:34 INFO Driver: </PERFLOG method=runTasks start=1396762834016 end=1396762834016 duration=0>
14/04/05 22:40:34 INFO Driver: </PERFLOG method=Driver.execute start=1396762834006 end=1396762834017 duration=11>
14/04/05 22:40:34 INFO Driver: OK
14/04/05 22:40:34 INFO Driver: <PERFLOG method=releaseLocks>
14/04/05 22:40:34 INFO Driver: </PERFLOG method=releaseLocks start=1396762834019 end=1396762834019 duration=0>
14/04/05 22:40:34 INFO Driver: </PERFLOG method=Driver.run start=1396762830146 end=1396762834019 duration=3873>
14/04/05 22:40:34 INFO Driver: <PERFLOG method=releaseLocks>
14/04/05 22:40:34 INFO Driver: </PERFLOG method=releaseLocks start=1396762834019 end=1396762834020 duration=1>
java.lang.AssertionError: assertion failed: No plan for NativeCommand CREATE TABLE IF NOT EXISTS src (key INT, value STRING)

	at scala.Predef$.assert(Predef.scala:179)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:218)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:218)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:219)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:219)
	at org.apache.spark.sql.SchemaRDDLike$class.toString(SchemaRDDLike.scala:44)
	at org.apache.spark.sql.SchemaRDD.toString(SchemaRDD.scala:93)
	at java.lang.String.valueOf(String.java:2854)
	at scala.runtime.ScalaRunTime$.stringOf(ScalaRunTime.scala:331)
	at scala.runtime.ScalaRunTime$.replStringOf(ScalaRunTime.scala:337)
	at .<init>(<console>:10)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
{code}

{code}
scala> hql(""select count(*) from src"")
14/04/05 22:47:13 INFO ParseDriver: Parsing command: select count(*) from src
14/04/05 22:47:13 INFO ParseDriver: Parse Completed
14/04/05 22:47:13 INFO HiveMetaStore: 0: get_table : db=default tbl=src
14/04/05 22:47:13 INFO audit: ugi=patrick	ip=unknown-ip-addr	cmd=get_table : db=default tbl=src	
14/04/05 22:47:13 INFO MemoryStore: ensureFreeSpace(147107) called with curMem=0, maxMem=308713881
14/04/05 22:47:13 INFO MemoryStore: Block broadcast_0 stored as values to memory (estimated size 143.7 KB, free 294.3 MB)
14/04/05 22:47:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/04/05 22:47:13 WARN LoadSnappy: Snappy native library not loaded
14/04/05 22:47:13 INFO FileInputFormat: Total input paths to process : 1
14/04/05 22:47:13 INFO SparkContext: Starting job: count at aggregates.scala:107
14/04/05 22:47:13 INFO DAGScheduler: Got job 0 (count at aggregates.scala:107) with 2 output partitions (allowLocal=false)
14/04/05 22:47:13 INFO DAGScheduler: Final stage: Stage 0 (count at aggregates.scala:107)
14/04/05 22:47:13 INFO DAGScheduler: Parents of final stage: List()
14/04/05 22:47:13 INFO DAGScheduler: Missing parents: List()
14/04/05 22:47:13 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[9] at map at aggregates.scala:94), which has no missing parents
14/04/05 22:47:13 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[9] at map at aggregates.scala:94)
14/04/05 22:47:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
14/04/05 22:47:13 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)
14/04/05 22:47:14 INFO TaskSetManager: Serialized task 0.0:0 as 3919 bytes in 323 ms
14/04/05 22:47:14 INFO Executor: Running task ID 0
Exception in thread ""Executor task launch worker-0"" java.lang.OutOfMemoryError: PermGen space
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:271)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{code}",,marmbrus,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385151,,,Mon Apr 07 20:55:15 UTC 2014,,,,,,,,,,"0|i1uco7:",385418,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 20:55;marmbrus;Fixed the toString issue here: https://github.com/apache/spark/pull/343

Could not recreate the permgen problem, but I did run the examples by hand successfully.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make MLlib work on Python 2.6,SPARK-1421,12706808,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,matei,05/Apr/14 22:46,06/Apr/14 03:53,14/Jul/23 06:25,06/Apr/14 03:53,0.9.0,0.9.1,,,,,,,0.9.2,1.0.0,,,,MLlib,PySpark,,,,0,,,,,,"Currently it requires Python 2.7 because it uses some new APIs, but they should not be essential for running our code.",,matei,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385131,,,2014-04-05 22:46:15.0,,,,,,,,,,"0|i1ucjr:",385398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The maven build error for Spark Catalyst,SPARK-1420,12706793,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,witgo,witgo,05/Apr/14 15:24,25/May/14 20:20,14/Jul/23 06:25,07/Apr/14 15:25,,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,,,witgo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385116,,,Sat Apr 05 15:27:26 UTC 2014,,,,,,,,,,"0|i1ucgf:",385383,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/14 15:27;witgo;{code}
mvn -Pyarn -Dhadoop.version=2.3.0 -Dyarn.version=2.3.0 -DskipTests install
{code} 
=>
{code} 
[ERROR] /Users/witgo/work/code/java/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala:31: object runtime is not a member of package reflect
[ERROR]   import scala.reflect.runtime.universe._
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on Yarn - spark UI link from resourcemanager is broken,SPARK-1417,12706695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,tgraves,04/Apr/14 20:02,11/Apr/14 15:28,14/Jul/23 06:25,11/Apr/14 15:28,1.0.0,,,,,,,,1.0.0,,,,,YARN,,,,,0,,,,,,"When running spark on yarn in yarn-cluster mode, spark registers a url with the Yarn ResourceManager to point to the spark UI.  This link is now broken. 

The link should be something like: < resourcemanager >/proxy/< applicationId >

instead its coming back as < resourcemanager >/< host of am:port >",,berngp,hsaputra,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1132,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385018,,,Mon Apr 07 14:04:56 UTC 2014,,,,,,,,,,"0|i1ubun:",385285,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 14:04;tgraves;https://github.com/apache/spark/pull/344;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a minSplits parameter to wholeTextFiles,SPARK-1415,12706684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yinxusen,matei,matei,04/Apr/14 18:47,13/Apr/14 20:19,14/Jul/23 06:25,13/Apr/14 20:19,,,,,,,,,1.0.0,,,,,,,,,,0,Starter,,,,,This probably requires adding one to newAPIHadoopFile too.,,matei,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385007,,,Tue Apr 08 05:05:47 UTC 2014,,,,,,,,,,"0|i1ubs7:",385274,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/14 01:58;yinxusen;Hi Matei, I just looked around in those Hadoop APIs. I find that the new Hadoop API deprecates the minSplit, instead of minSplit, they prefer minSplitSize and maxSplitSize to control the split. minSplit is negative correlated with maxSplitSize, so I think we have 2 ways to fix the issue:

1. We just provide a new API with maxSplitSize, say, wholeTextFiles(path: String, maxSplitSize: Long);

2. We write a delegation to compute the maxSplitSize using minSplit (easy to write, taking old Hadoop API as an example), and provide the API wholeTextFile(path: String, minSplit: Int);

I also think we can provide the two APIs simultaneously. What do you think?;;;","08/Apr/14 05:05;matei;Hey Xusen, that makes sense. I think that for consistency with our other API methods, we should add minSplits here, and we can compute maxSplitSize from it. Later on we can have versions of the methods that take a maxSplitSize. But on the old Hadoop API for example we can't easily change this, and it seems that a maxSplitSize is always possible to compute from minSplits.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python API for SparkContext.wholeTextFiles,SPARK-1414,12706683,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,matei,04/Apr/14 18:46,05/Apr/14 00:30,14/Jul/23 06:25,05/Apr/14 00:30,,,,,,,,,1.0.0,,,,,PySpark,,,,,0,,,,,,,,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385006,,,2014-04-04 18:46:02.0,,,,,,,,,,"0|i1ubrz:",385273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet messes up stdout and stdin when used in Spark REPL,SPARK-1413,12706559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,matei,matei,04/Apr/14 03:23,10/Apr/14 17:37,14/Jul/23 06:25,10/Apr/14 17:37,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"I have a simple Parquet file in ""foos.parquet"", but after I type this code, it freezes the shell, to the point where I can't read or write stuff:

scala> val qc = new org.apache.spark.sql.SQLContext(sc); import qc._
qc: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@1c0c8826
import qc._

scala> qc.parquetFile(""foos.parquet"").saveAsTextFile(""bar"")

The job itself completes successfully, and ""bar"" contains the right text, but I can no longer see commands I type in, or further log output.",,matei,witgo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384882,,,Fri Apr 04 09:05:39 UTC 2014,,,,,,,,,,"0|i1ub0f:",385149,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 09:05;witgo;You can try this  [PR|https://github.com/apache/spark/pull/325]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-exported spark-env.sh variables are no longer present in spark-shell,SPARK-1404,12706353,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,03/Apr/14 03:10,04/Apr/14 16:50,14/Jul/23 06:25,04/Apr/14 16:50,1.0.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"spark-env.sh is now only loaded at most once. This means that if users had variable settings like

{{code}}
FOO=1
export BAR=2
{{code}}

then a user who runs spark-shell may source spark-env.sh in ""spark-shell"" to have the non-exported variables disappear from scope when ""spark-class"" is called. In effect, BAR is visible within Spark, but not FOO.",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384676,,,2014-04-03 03:10:05.0,,,,,,,,,,"0|i1u9qv:",384943,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on Mesos does not set Thread's context class loader,SPARK-1403,12706352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,manku.timma,manku.timma,03/Apr/14 02:54,14/Jul/15 02:59,14/Jul/23 06:25,14/Jul/15 02:59,1.0.0,1.3.0,1.4.0,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"I can run spark 0.9.0 on mesos but not spark 1.0.0. This is because the spark executor on mesos slave throws a  java.lang.ClassNotFoundException for org.apache.spark.serializer.JavaSerializer.

The lengthy discussion is here: http://apache-spark-user-list.1001560.n3.nabble.com/java-lang-ClassNotFoundException-spark-on-mesos-td3510.html#a3513
",ubuntu 12.04 on vagrant,cnstar9988,mandoskippy,manku.timma,pwendell,rkannan82,yanakad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1480,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384675,,,Tue Jul 14 02:59:49 UTC 2015,,,,,,,,,,"0|i1u9qn:",384942,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 19:00;manku.timma;In 0.9.0 I see that the classLoader is org.apache.spark.repl.ExecutorClassLoader (in SparkEnv.scala)
In 1.0.0 I see that the classLoader is null. My guess is that this should not be null.;;;","07/Apr/14 17:24;pwendell;The underlying issue here is that we've made assumptions in various parts of the codebase that the context classloader is set on a thread. In general, we should relax these assumptions and just fallback to the classloader that loaded Spark. As a workaround this patch:

https://github.com/apache/spark/pull/322/files

just manually sets the classloader to the system class loader.;;;","23/Mar/15 08:43;cnstar9988;I want to reopen this bug, because I can reproduce it at spark 1.3.0 + mesos 0.21.1 with ""run-example SparkPi"" 
;;;","08/Jun/15 17:58;rkannan82;A similar problem has been reported while running Spark on Mesos using MapR distribution. The current thread's context class loader is NULL inside the executor process causing NPE in MapR code.

Refer to this discussion.
http://answers.mapr.com/questions/163353/spark-from-apache-downloads-site-for-mapr.html#answer-163484;;;","11/Jun/15 13:23;yanakad;Multiple users reporting this is occuring again in 1.3;;;","11/Jun/15 13:43;mandoskippy;Per Kannan: Seeing this in Spark 1.2.2, 1.3.0, an 1.3.1.  1.2.0 does not produce the error. The case I have is a NPE caused by what appears to be this bug. 

Tasks fail with the stack trace below.  Per a discussion on the relavent code in the MapR Shim, ] it is trying to get a root classloader to use for loading a bunch of classes. It uses the thread's context class loader (TCCL) and keeps going up the parent chain. The Null being sent causes the NPE.  The concern here is not so much (in this case) the MapR component not handling the NPE (Althought that should be addressed too) but that Spark is switching between versions.  Happy to provide any other details. 

Full Error on 1.3.1 on Mesos:
15/05/19 09:31:26 INFO MemoryStore: MemoryStore started with capacity 1060.3 MB java.lang.NullPointerException at com.mapr.fs.ShimLoader.getRootClassLoader(ShimLoader.java:96) at com.mapr.fs.ShimLoader.injectNativeLoader(ShimLoader.java:232) at com.mapr.fs.ShimLoader.load(ShimLoader.java:194) at org.apache.hadoop.conf.CoreDefaultProperties.(CoreDefaultProperties.java:60) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:274) at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1847) at org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2062) at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2272) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2224) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2141) at org.apache.hadoop.conf.Configuration.set(Configuration.java:992) at org.apache.hadoop.conf.Configuration.set(Configuration.java:966) at org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:98) at org.apache.spark.deploy.SparkHadoopUtil.(SparkHadoopUtil.scala:43) at org.apache.spark.deploy.SparkHadoopUtil$.(SparkHadoopUtil.scala:220) at org.apache.spark.deploy.SparkHadoopUtil$.(SparkHadoopUtil.scala) at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1959) at org.apache.spark.storage.BlockManager.(BlockManager.scala:104) at org.apache.spark.storage.BlockManager.(BlockManager.scala:179) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:310) at org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:186) at org.apache.spark.executor.MesosExecutorBackend.registered(MesosExecutorBackend.scala:70) java.lang.RuntimeException: Failure loading MapRClient. at com.mapr.fs.ShimLoader.injectNativeLoader(ShimLoader.java:283) at com.mapr.fs.ShimLoader.load(ShimLoader.java:194) at org.apache.hadoop.conf.CoreDefaultProperties.(CoreDefaultProperties.java:60) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:274) at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1847) at org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2062) at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2272) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2224) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2141) at org.apache.hadoop.conf.Configuration.set(Configuration.java:992) at org.apache.hadoop.conf.Configuration.set(Configuration.java:966) at org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:98) at org.apache.spark.deploy.SparkHadoopUtil.(SparkHadoopUtil.scala:43) at org.apache.spark.deploy.SparkHadoopUtil$.(SparkHadoopUtil.scala:220) at org.apache.spark.deploy.SparkHadoopUtil$.(SparkHadoopUtil.scala) at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1959) at org.apache.spark.storage.BlockManager.(BlockManager.scala:104) at org.apache.spark.storage.BlockManager.(BlockManager.scala:179) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:310) at org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:186) at org.apache.spark.executor.MesosExecutorBackend.registered(MesosExecutorBackend.scala:70);;;","17/Jun/15 12:58;mandoskippy;This is occurring still in 1.4.0

I've attempted to look through the code to determine what may have changed but the Class Loading code has shifted around quite a bit, and I could not pinpoint when the change, or which update changed the code to break it again.  If there is anything I can do to help troubleshoot, please advise. 

;;;","19/Jun/15 18:11;yanakad;[~pwendell] at your convenience can you please triage this bug. It was originally opened as a blocker. I reopened but am not sure if it's a release blocker. I set the target release to 1.5 just so it shows up in triage queries since it's a reopened bug...;;;","14/Jul/15 02:59;pwendell;Hey All,

This issue should remain fixed. [~mandoskippy] I think you are just running into a different issue that is also in some way related to classloading.

Can you open a new JIRA for your issue, paste in the stack trace and give as much information as possible about the environment? Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,
Reason for Stage Failure should be shown in UI,SPARK-1399,12706330,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,kayousterhout,kayousterhout,03/Apr/14 00:15,21/Apr/14 21:28,14/Jul/23 06:25,21/Apr/14 21:28,0.9.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"Right now, we don't show why a stage failed in the UI.  We have this information, and it would be useful for users to see (e.g., to see that a stage was killed because the job was cancelled).",,codingcat,kayousterhout,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384653,,,Wed Apr 16 20:32:55 UTC 2014,,,,,,,,,,"0|i1u9m7:",384921,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 23:31;kayousterhout;FYI this outstanding pull request changes this behavior: https://github.com/apache/spark/pull/309, so probably don't make sense to work on this until that gets resolved.;;;","16/Apr/14 02:15;codingcat;made the PR: https://github.com/apache/spark/pull/421;;;","16/Apr/14 13:54;lianhuiwang;i think the user defined accumulators of every task need to shown in ui ;;;","16/Apr/14 20:32;kayousterhout;Lianhui, I think that issue is unrelated (although valid) -- can you file a separate JIRA for that?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkListeners should be notified when stages are cancelled,SPARK-1397,12706294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kayousterhout,kayousterhout,kayousterhout,02/Apr/14 21:27,08/Apr/14 23:08,14/Jul/23 06:25,08/Apr/14 23:08,0.9.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"Right now, we don't explicitly tell SparkListeners when stages get cancelled.  This leads to weird behavior in the UI, where when jobs are cancelled, some stages never move out of the ""Running Stages"" table.",,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384617,,,2014-04-02 21:27:51.0,,,,,,,,,,"0|i1u9e7:",384885,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler has a memory leak for cancelled jobs,SPARK-1396,12706288,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,kayousterhout,kayousterhout,02/Apr/14 21:16,08/Apr/14 08:29,14/Jul/23 06:25,08/Apr/14 08:29,0.9.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"When a job is cancelled, the DAGScheduler doesn't clean up all of the relevant state, leading to a memory leak.",,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384611,,,Wed Apr 02 21:20:18 UTC 2014,,,,,,,,,,"0|i1u9cv:",384879,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 21:20;kayousterhout;https://github.com/apache/spark/pull/305;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot launch jobs on Yarn cluster with ""local:"" scheme in SPARK_JAR",SPARK-1395,12706275,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,02/Apr/14 20:23,23/Jun/14 14:14,14/Jul/23 06:25,23/Jun/14 14:14,1.0.0,,,,,,,,1.1.0,,,,,YARN,,,,,0,,,,,,"If you define SPARK_JAR and friends to use ""local:"" URIs, you cannot submit a job on a Yarn cluster. e.g., I have:

SPARK_JAR=local:/tmp/spark-assembly-1.0.0-SNAPSHOT-hadoop2.3.0-cdh5.0.0.jar
SPARK_YARN_APP_JAR=local:/tmp/spark-examples-assembly-1.0.0-SNAPSHOT.jar

And running SparkPi using bin/run-example yields this:

14/04/02 13:23:33 INFO yarn.Client: Preparing Local resources
Exception in thread ""main"" java.io.IOException: No FileSystem for scheme: local
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2385)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
        at org.apache.spark.deploy.yarn.ClientBase$class.org$apache$spark$deploy$yarn$ClientBase$$copyRemoteFile(ClientBase.scala:156)
        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$3.apply(ClientBase.scala:217)
        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$3.apply(ClientBase.scala:212)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:212)
        at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:41)
        at org.apache.spark.deploy.yarn.Client.runApp(Client.scala:76)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:81)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:129)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:226)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:96)
        at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)
        at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
",,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384598,,,Mon Jun 23 14:14:41 UTC 2014,,,,,,,,,,"0|i1u99z:",384866,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 20:56;vanzin;https://github.com/apache/spark/pull/303;;;","25/Apr/14 20:05;vanzin;This is broken again.;;;","23/Jun/14 14:14;tgraves;https://github.com/apache/spark/pull/560;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
calling system.platform on worker raises IOError,SPARK-1394,12706259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,idanzalz,idanzalz,02/Apr/14 18:29,25/Jul/14 21:36,14/Jul/23 06:25,25/Jul/14 21:36,0.9.0,,,,,,,,1.0.1,,,,,PySpark,,,,,2,pyspark,,,,,"A simple program that calls system.platform() on the worker fails most of the time (it works some times but very rarely).
This is critical since many libraries call that method (e.g. boto).

Here is the trace of the attempt to call that method:



$ /usr/local/spark/bin/pyspark
Python 2.7.3 (default, Feb 27 2014, 20:00:17)
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
14/04/02 18:18:37 INFO Utils: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
14/04/02 18:18:37 WARN Utils: Your hostname, qlika-dev resolves to a loopback address: 127.0.1.1; using 10.33.102.46 instead (on interface eth1)
14/04/02 18:18:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
14/04/02 18:18:38 INFO Slf4jLogger: Slf4jLogger started
14/04/02 18:18:38 INFO Remoting: Starting remoting
14/04/02 18:18:39 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@10.33.102.46:36640]
14/04/02 18:18:39 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@10.33.102.46:36640]
14/04/02 18:18:39 INFO SparkEnv: Registering BlockManagerMaster
14/04/02 18:18:39 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140402181839-919f
14/04/02 18:18:39 INFO MemoryStore: MemoryStore started with capacity 294.6 MB.
14/04/02 18:18:39 INFO ConnectionManager: Bound socket to port 43357 with id = ConnectionManagerId(10.33.102.46,43357)
14/04/02 18:18:39 INFO BlockManagerMaster: Trying to register BlockManager
14/04/02 18:18:39 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 10.33.102.46:43357 with 294.6 MB RAM
14/04/02 18:18:39 INFO BlockManagerMaster: Registered BlockManager
14/04/02 18:18:39 INFO HttpServer: Starting HTTP Server
14/04/02 18:18:39 INFO HttpBroadcast: Broadcast server started at http://10.33.102.46:51803
14/04/02 18:18:39 INFO SparkEnv: Registering MapOutputTracker
14/04/02 18:18:39 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9b38acb0-7b01-4463-b0a6-602bfed05a2b
14/04/02 18:18:39 INFO HttpServer: Starting HTTP Server
14/04/02 18:18:40 INFO SparkUI: Started Spark Web UI at http://10.33.102.46:4040
14/04/02 18:18:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 0.9.0
      /_/

Using Python version 2.7.3 (default, Feb 27 2014 20:00:17)
Spark context available as sc.
>>> import platform
>>> sc.parallelize([1]).map(lambda x : platform.system()).collect()
14/04/02 18:19:17 INFO SparkContext: Starting job: collect at <stdin>:1
14/04/02 18:19:17 INFO DAGScheduler: Got job 0 (collect at <stdin>:1) with 1 output partitions (allowLocal=false)
14/04/02 18:19:17 INFO DAGScheduler: Final stage: Stage 0 (collect at <stdin>:1)
14/04/02 18:19:17 INFO DAGScheduler: Parents of final stage: List()
14/04/02 18:19:17 INFO DAGScheduler: Missing parents: List()
14/04/02 18:19:17 INFO DAGScheduler: Submitting Stage 0 (PythonRDD[1] at collect at <stdin>:1), which has no missing parents
14/04/02 18:19:17 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (PythonRDD[1] at collect at <stdin>:1)
14/04/02 18:19:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
14/04/02 18:19:17 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)
14/04/02 18:19:17 INFO TaskSetManager: Serialized task 0.0:0 as 2152 bytes in 12 ms
14/04/02 18:19:17 INFO Executor: Running task ID 0
PySpark worker failed with exception:
Traceback (most recent call last):
  File ""/usr/local/spark/python/pyspark/worker.py"", line 77, in main
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/usr/local/spark/python/pyspark/serializers.py"", line 182, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/usr/local/spark/python/pyspark/serializers.py"", line 117, in dump_stream
    for obj in iterator:
  File ""/usr/local/spark/python/pyspark/serializers.py"", line 171, in _batched
    for item in iterator:
  File ""<stdin>"", line 1, in <lambda>
  File ""/usr/lib/python2.7/platform.py"", line 1306, in system
    return uname()[0]
  File ""/usr/lib/python2.7/platform.py"", line 1273, in uname
    processor = _syscmd_uname('-p','')
  File ""/usr/lib/python2.7/platform.py"", line 1030, in _syscmd_uname
    rc = f.close()
IOError: [Errno 10] No child processes

14/04/02 18:19:17 ERROR Executor: Exception in task ID 0
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/usr/local/spark/python/pyspark/worker.py"", line 77, in main
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/usr/local/spark/python/pyspark/serializers.py"", line 182, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/usr/local/spark/python/pyspark/serializers.py"", line 117, in dump_stream
    for obj in iterator:
  File ""/usr/local/spark/python/pyspark/serializers.py"", line 171, in _batched
    for item in iterator:
  File ""<stdin>"", line 1, in <lambda>
  File ""/usr/lib/python2.7/platform.py"", line 1306, in system
    return uname()[0]
  File ""/usr/lib/python2.7/platform.py"", line 1273, in uname
    processor = _syscmd_uname('-p','')
  File ""/usr/lib/python2.7/platform.py"", line 1030, in _syscmd_uname
    rc = f.close()
IOError: [Errno 10] No child processes

        at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:131)
        at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:153)
        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
        at org.apache.spark.scheduler.Task.run(Task.scala:53)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
","Tested on Ubuntu and Linux, local and remote master, python 2.7.*",farrellee,frol,idanzalz,matei,mrocklin,sandyr,sgottipa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384582,,,Sun Jun 29 09:34:22 UTC 2014,,,,,,,,,,"0|i1u96f:",384850,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 07:51;idanzalz;This seems to be related to the way the handle_sigchld method in daemon.py works.
In order to kill the zombie processes the worker calls os.waitpid on SIGCHLD. however. since using Popen also tries to do that eventually, you get a closed handle.

Since platform.py is a native library, I would guess we should find a solution in pyspark (i.e. change the way handle_sigchld works, or maybe limit the processes it waits on);;;","29/Apr/14 06:23;frol;I have the same issue with Spark 0.9.1. Is there any workaround?;;;","29/Apr/14 06:32;idanzalz;If you have an __init__.py  you are sure to go through, you can add the following code to your file:

{code}
# PySpark adds a SIGCHLD signal handler, but that breaks other packages, so we remove it
try:
    import signal
    signal.signal(signal.SIGCHLD, signal.SIG_DFL)
except: pass
{code}

It's a work around, it would be better to have a ""smart"" signal handler that only handles the processes that are direct descendants of the daemon. I might try to get something like that out. ;;;","30/Apr/14 00:31;frol;[~idanzalz] unfortunately, it had helped to avoid only one exception, so I commented signal binding in PySpark and these crashes went away. I hope it will be fixed somehow in next Spark release.;;;","08/May/14 16:57;sgottipa;We also bumping into the same issue. My I know, how and where can we comment the signal binding in pyspark?;;;","09/May/14 02:53;frol;[~sgottipa] spark/python/pyspark/daemon.py:75     #signal.signal(SIGCHLD, handle_sigchld);;;","20/May/14 21:54;mrocklin;For what it's worth the platform library is unfortunately called by a number of numerical libraries associated to machine learning.  In particular

Theano calls on platform
NumExpr calls on platform
Pandas calls on NumExpr

These libraries are very popular in the numerics / machine learning space.;;;","20/May/14 22:06;frol;[~mrocklin] What? What is your reply for?;;;","20/May/14 22:09;mrocklin;Just trying to lend weight to this issue by adding context.  
Let me know if this is an inappropriate use of this forum.;;;","27/Jun/14 18:06;farrellee;i'm taking a look at this;;;","27/Jun/14 19:15;farrellee;fyi, this certainly looks like the waitpid(0,...) cleanup handler is cleaning up more than it should be

also, fyi, if you comment it out you'll start accumulating defunct worker processes, which is not good;;;","27/Jun/14 20:14;farrellee;the python daemon does two levels of forking. the manager forks workers and each worker forks sub-workers, who then handle connections.

the master has a sigchld handler to cleanup workers. the workers also have sigchld handlers to clean up sub-workers.

the problem here is the worker's handler is also installed on the sub-worker, which interferes with calls like platform.system() (but not os.uname, btw!)

i assert that the sub-workers, who do not intentionally fork, should not be responsible for cleaning up unexpected sub-processes, and as such should not have a sigchld handler.

if it's desired to have tighter control of process cleanup, namespaces should be used and it should be the manager's responsibility.

pull request - https://github.com/apache/spark/pull/1247;;;","29/Jun/14 09:34;farrellee;this should be resolved, @aarondav;;;",,,,,,,,,,,,,,,,,,,,
spark-shell on yarn on spark 0.9 branch doesn't always work with secure hdfs,SPARK-1384,12706007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,01/Apr/14 17:23,10/Jun/14 06:08,14/Jul/23 06:25,10/Jun/14 06:08,0.9.0,0.9.1,,,,,,,0.9.2,,,,,YARN,,,,,0,,,,,," I've found an issue with the spark-shell in yarn-client mode in the 0.9.1 rc3 release.  It doesn't work with secure HDFS unless you 
export SPARK_YARN_MODE=true before starting the shell, or if you happen to do something immediately with HDFS.  If you wait for the connection to the namenode to timeout it will fail. 
 
The fix actually went in to master branch  with the authentication changes I made in master but I never realized that change needed to apply to 0.9. 

https://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220db8b7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0
See the SparkILoop diff.
",,berngp,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384330,,,Wed Apr 02 13:41:35 UTC 2014,,,,,,,,,,"0|i1u7mv:",384598,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 13:41;tgraves;https://github.com/apache/spark/pull/287;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when calling DStream.slice() before StreamingContext.start(),SPARK-1382,12705958,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,aecc,aecc,01/Apr/14 14:12,27/Apr/14 14:07,14/Jul/23 06:25,27/Apr/14 14:07,0.9.0,,,,,,,,1.0.0,,,,,DStreams,,,,,0,,,,,,"If we call the DStream.slice() before StreamingContext.start() has been called, then zeroTime is still null, and it will throw a null pointer exception. Ideally, it should throw something like a ""ContextNotInitlalized"" exception.",,aecc,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384281,,,Sun Apr 27 14:06:55 UTC 2014,,,,,,,,,,"0|i1u7bz:",384549,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 03:05;zsxwing;PR: https://github.com/apache/spark/pull/365;;;","27/Apr/14 14:06;zsxwing;PR: https://github.com/apache/spark/pull/562;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the yarn-cluster submitter, rename ""args"" option to ""arg""",SPARK-1376,12705778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandyr,sandyr,31/Mar/14 20:18,23/Apr/14 02:39,14/Jul/23 06:25,01/Apr/14 05:53,0.9.0,,,,,,,,,,,,,YARN,,,,,0,,,,,,"This was discussed in the SPARK-1126 PR.  ""args"" will be kept around for backwards compatibility.",,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384102,,,Wed Apr 23 02:39:20 UTC 2014,,,,,,,,,,"0|i1u68f:",384370,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 20:23;sandyr;https://github.com/apache/spark/pull/279;;;","23/Apr/14 02:39;pwendell;There was follow up work on this by [~mengxr] in https://github.com/apache/spark/pull/485;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-submit script additional cleanup,SPARK-1375,12705776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandyr,sandyr,31/Mar/14 20:07,04/Apr/14 20:29,14/Jul/23 06:25,04/Apr/14 20:29,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"A couple small things:
* accept ""yarn-cluster"" where ""yarn-standalone"" is accepted
* As discussed on the SPARK-1126 PR, call the first argument ""app jar"" instead of ""jar"" or ""primary binary""",,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384100,,,Mon Mar 31 20:16:51 UTC 2014,,,,,,,,,,"0|i1u67z:",384368,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 20:16;sandyr;https://github.com/apache/spark/pull/278;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashAggregate should stream tuples and avoid doing an extra count,SPARK-1371,12705733,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,31/Mar/14 18:17,07/Apr/14 07:32,14/Jul/23 06:25,07/Apr/14 07:32,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384067,,,Wed Apr 02 05:42:49 UTC 2014,,,,,,,,,,"0|i1u60n:",384335,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 05:42;marmbrus;https://github.com/apache/spark/pull/295;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashJoin should stream one relation,SPARK-1370,12705732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,31/Mar/14 18:16,31/Mar/14 22:27,14/Jul/23 06:25,31/Mar/14 22:27,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"Right now we materialize all of the results for a partition in memory.

This has been started here: https://github.com/apache/spark/pull/250",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384066,,,2014-03-31 18:16:39.0,,,,,,,,,,"0|i1u60f:",384334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableScan is slow,SPARK-1368,12705726,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,31/Mar/14 17:53,29/May/14 22:27,14/Jul/23 06:25,29/May/14 22:27,,,,,,,,,1.1.0,,,,,SQL,,,,,0,,,,,,"The major issues here are the use of functional programming (.map, .foreach) and the creation of a new Row object for each output tuple. We should switch to while loops in the critical path and a single MutableRow per partition.",,lian cheng,marmbrus,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384060,,,Thu May 15 09:26:05 UTC 2014,,,,,,,,,,"0|i1u5z3:",384328,,,,,,,,,,,,,,,,,,,,,,,"15/May/14 09:26;lian cheng;Corresponding PR: https://github.com/apache/spark/pull/758;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when joining Parquet Relations,SPARK-1367,12705723,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,schumach,marmbrus,marmbrus,31/Mar/14 17:51,03/Apr/14 22:33,14/Jul/23 06:25,03/Apr/14 22:33,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"{code}
  test(""self-join parquet files"") {
    val x = ParquetTestData.testData.subquery('x)
    val y = ParquetTestData.testData.newInstance.subquery('y)
    val query = x.join(y).where(""x.myint"".attr === ""y.myint"".attr)
    query.collect()
  }
{code}",,marmbrus,schumach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384057,,,Wed Apr 02 09:00:41 UTC 2014,,,,,,,,,,"0|i1u5yf:",384325,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 15:05;schumach;I believe this issue can be closed due to
https://github.com/apache/spark/commit/2861b07bb030f72769f5b757b4a7d4a635807140
?;;;","01/Apr/14 17:16;marmbrus;No, in that commit there is a TODO as the testcase still NPEs.  We still need to remove the @transient from ParquetTableScan.  If you don't have time to do this I can.;;;","02/Apr/14 09:00;schumach;That should be now fixed when https://github.com/apache/spark/pull/195 is merged. Sorry, since the test you had added did not actually failed (as I understood the TODO) I thought your patch had solved the problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The sql function should be consistent between different types of SQLContext,SPARK-1366,12705721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,31/Mar/14 17:47,05/Apr/14 20:09,14/Jul/23 06:25,05/Apr/14 20:09,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"Right now calling `context.sql` will cause things to be parsed with different parsers, which is kinda confusing. Instead HiveContext should have a specialized `hiveql` method that uses the HiveQL parser.

Also need to update the documentation.",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384055,,,Fri Apr 04 19:04:33 UTC 2014,,,,,,,,,,"0|i1u5xz:",384323,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 19:04;marmbrus;https://github.com/apache/spark/pull/319;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix RateLimitedOutputStream Test,SPARK-1365,12705717,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,31/Mar/14 17:40,31/Mar/14 23:26,14/Jul/23 06:25,31/Mar/14 23:26,,,,,,,,,1.0.0,,,,,DStreams,,,,,0,,,,,,"This test currently assumes that calling Thread.sleep(X) will sleep exactly X milliseconds. In fact, it can sleep X or more milliseconds so sometimes the test fails.",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384051,,,2014-03-31 17:40:52.0,,,,,,,,,,"0|i1u5x3:",384319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataTypes missing from ScalaReflection,SPARK-1364,12705690,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,31/Mar/14 16:06,03/Apr/14 01:15,14/Jul/23 06:25,03/Apr/14 01:15,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"BigDecimal, possibly others.",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384024,,,Wed Apr 02 05:43:15 UTC 2014,,,,,,,,,,"0|i1u5r3:",384292,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 05:43;marmbrus;https://github.com/apache/spark/pull/293;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler throws NullPointerException occasionally,SPARK-1361,12705606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,31/Mar/14 09:10,04/May/14 04:17,14/Jul/23 06:25,04/May/14 04:17,0.9.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"DAGScheduler throws this NullPointerException below occasionally when running spark apps.
java.lang.NullPointerException
at org.apache.spark.scheduler.DAGScheduler.executorAdded(DAGScheduler.scala:186)
at org.apache.spark.scheduler.TaskSchedulerImpl.executorAdded(TaskSchedulerImpl.scala:409)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1.apply(TaskSchedulerImpl.scala:210)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1.apply(TaskSchedulerImpl.scala:206)
at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:73)
at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:206)
at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor.makeOffers(CoarseGrainedSchedulerBackend.scala:130)
at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1.applyOrEls",,sarutak,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383940,,,Sun May 04 04:08:50 UTC 2014,,,,,,,,,,"0|i1u58f:",384208,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 09:17;scwf;this is an order issue of construction and starting of the taskScheduler and the dagScheduler in SparkContext.  now taskScheduler start before  dagScheduler, so executor backend may register executor even before dagScheduler(or the eventProcessActor) has not initialized. in this situation it will throw NPE;;;","30/Apr/14 22:30;sarutak;Hi [~scwf], are there any update for this issue?
Do you mind my taking over addressing this issue?;;;","04/May/14 04:08;scwf;Hi Kousuke Saruta, this issue has already resolved. 
https://github.com/apache/spark/pull/186;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to resolve attribute when query with table name as a qualifer in SQLContext,SPARK-1354,12705527,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jerryshao,jerryshao,30/Mar/14 06:58,30/Mar/14 17:05,14/Jul/23 06:25,30/Mar/14 17:05,1.0.0,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"For SQLContext with SimpleCatelog, table name does not register into attribute as a qualifier, so query like ""SELECT * FROM records JOIN records1 ON records.key = records1,key"" will be failed. The logical plan cannot resolve ""records.key"" because of missing qualifier ""records"". The physical plan shows as below

    Project [*]
     Filter ('records.key = 'records1.key)
      CartesianProduct
       ExistingRdd [key#0,value#1], MappedRDD[2] at map at basicOperators.scala:124
       ParquetTableScan [key#2,value#3], (ParquetRelation ParquetFile, pair.parquet), None)

And the exception shows:

org.apache.spark.sql.catalyst.errors.package$TreeNodeException: No function to evaluate expression. type: UnresolvedAttribute, tree: 'records.key
        at org.apache.spark.sql.catalyst.expressions.Expression.apply(Expression.scala:54)
        at org.apache.spark.sql.catalyst.expressions.Equals.apply(predicates.scala:112)
        at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:43)
        at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:43)
        at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:643)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:643)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:936)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:936)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        at org.apache.spark.scheduler.Task.run(Task.scala:52)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)

",,githubbot,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382866,,,Sun Mar 30 07:15:26 UTC 2014,,,,,,,,,,"0|i1tylr:",383134,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/14 07:15;githubbot;GitHub user jerryshao opened a pull request:

    https://github.com/apache/spark/pull/272

    [SPARK-1354][SQL] Add tableName as a qualifier for SimpleCatelogy

    Fix attribute unresolved when query with table name as a qualifier in SQLContext with SimplCatelog, details please see [SPARK-1354](https://issues.apache.org/jira/browse/SPARK-1354?jql=project%20%3D%20SPARK).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/apache-spark qualifier-fix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/272.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #272
    
----
commit 795017045a671d0f3827a97b7d57db2980e8c1fd
Author: jerryshao <saisai.shao@intel.com>
Date:   2014-03-30T02:58:21Z

    Add tableName as a qualifier for SimpleCatelogy

----
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN ContainerLaunchContext should use cluster's JAVA_HOME,SPARK-1350,12704628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandyr,,28/Mar/14 16:22,09/Jan/15 04:19,14/Jul/23 06:25,04/Apr/14 13:56,0.9.0,,,,,,,,1.0.0,,,,,YARN,,,,,0,,,,,,"{code}
    var javaCommand = ""java""
    val javaHome = System.getenv(""JAVA_HOME"")
    if ((javaHome != null && !javaHome.isEmpty()) || env.isDefinedAt(""JAVA_HOME"")) {
      javaCommand = Environment.JAVA_HOME.$() + ""/bin/java""
    }
{code}

Currently, if JAVA_HOME is specified on the client, it will be used instead of the value given on the cluster.  This makes it so that Java must be installed in the same place on the client as on the cluster.

This is a possibly incompatible change that we should get in before 1.0.",,aniket,mridulm80,sandy,sandyr,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5164,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382859,,,Tue Dec 09 06:50:38 UTC 2014,,,,,,,,,,"0|i1tyk7:",383127,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 14:25;tgraves;I assume you are referring to the one in ClientBase?

It actually does not use the same java as specified by JAVA_HOME on the client.  If JAVA_HOME is set on the client then it just adds $JAVA_HOME/bin to the java command to launch the AM. Yarn can be configured (and is the default config) to put JAVA_HOME into the container launch script, so the launch command will use the JAVA_HOME specified by yarn.   I agree its a bit hacky and could be improved though.  On MapReduce it just always uses ""$JAVA_HOME/bin/java""

The main idea is that I want $JAVA_HOME to be included in the launch command so that its not relying on the default install of java on the system.  I want it to use the JAVA_HOME specified by yarn and I also want to allow it to be overridden so that a user could choose to run either 32 bit or 64 bit jdk.  

export SPARK_YARN_USER_ENV=""JAVA_HOME=/myyarinstall/jdk64/current""

I was told this goes against how spark is in general setup where it relies on whatever is installed on the system so this was the compromise.

;;;","31/Mar/14 17:39;sandyr;bq. It actually does not use the same java as specified by JAVA_HOME on the client.
Oops, you're right.  I was misunderstanding the code.  This is less of a big deal than I thought.

bq. The main idea is that I want $JAVA_HOME to be included in the launch command so that its not relying on the default install of java on the system.
I 100% agree that this is the right thing to do.  What still seems a little weird to me is that the use of JAVA_HOME on the cluster depends on whether the client JAVA_HOME is set.

bq. I was told this goes against how spark is in general setup where it relies on whatever is installed on the system so this was the compromise.
So this has already been discussed and always using JAVA_HOME in the launch command (never just ""java"") was rejected for this reason?  If all YARN apps accessed java in the same way, the way that MapReduce does, I think it would make things a lot simpler operationally.;;;","31/Mar/14 17:52;tgraves;I agree it is weird, lets bring this back up and see if we can just always use $JAVA_HOME.  This is yarn specific code so shouldn't affect other parts of spark.  I don't know if anyone would run YARN without this being set.

Patrick, Mridul,  any objections to changing this?;;;","31/Mar/14 21:42;sandyr;+ [~pwendell] and [~mridulm@yahoo-inc.com];;;","01/Apr/14 01:17;mridulm80;
We will need a way to configure JAVA_HOME (like the 32bit vs 64bit case Tom mentioned about : which is used quite often)

Tom, is there a way to do this within context of yarn ? Ability to override the JAVA_HOME used via some config ?

- If yes, we should leverage that and always use $JAVA_HOME/bin/java in our code.

If not, let us continue with what we currently have : it is the best we can currently do.;;;","01/Apr/14 01:25;sandyr;bq. Tom, is there a way to do this within context of yarn ? Ability to override the JAVA_HOME used via some config ?
The submitter can set a different JAVA_HOME in SPARK_YARN_USER_ENV.  Any properties placed there will override versions of these properties in the YARN NodeManager's environment.;;;","01/Apr/14 01:39;mridulm80;You mistook my question; I meant in context of yarn - not in context of spark over yarn.;;;","01/Apr/14 03:33;sandyr;Ah, sorry.  YARN doesn't have any configs for this.  The container environment is the environment of the NodeManager process, a few additional variables YARN adds such as the container ID and user name, and anything specified by the application in the ContainerLaunchContext.;;;","04/Apr/14 13:56;tgraves;https://github.com/apache/spark/pull/313;;;","08/Dec/14 06:54;aniket;[~sandyr] Using Environment.JAVA_HOME.$() causes issues while submitting spark applications from a windows box into a Yarn cluster running on Linux (with spark master set as yarn-client). This is because Environment.JAVA_HOME.$() resolves to %JAVA_HOME% which results in not a valid executable on Linux. Is this a Spark issue or a YARN issue?;;;","08/Dec/14 14:44;tgraves; which version of hadoop are you using?  Spark is using Environment.JAVA_HOME.$() but Environment is a hadoop class and its supposed to handle windows:

   public String $() {
      if (Shell.WINDOWS) {
        return ""%"" + variable + ""%"";
      } else {
        return ""$"" + variable;
      }
    };;;","09/Dec/14 06:50;aniket;I am using hadoop 2.5.0 (CDH). Agreed that it handles for windows. But the use case I am talking about is when SparkContext is created programmatically on a windows machine and is used to submit jobs on a yarn cluster running on Linux. As per above code, %JAVA_HOME%/bin/java will be generated as one of the commands by ClientBase and submitted to YARN cluster. This will obviously fail while YARN tries to execute the container as % is treated differently in linux.;;;",,,,,,,,,,,,,,,,,,,,,
Spark UI's do not bind to localhost interface anymore,SPARK-1348,12704517,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kzhang,pwendell,,28/Mar/14 14:05,16/Jan/16 21:29,14/Jul/23 06:25,08/Apr/14 21:33,,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"When running the shell or standalone master, it no longer binds to localhost. I think this may have been caused by the security patch. We should figure out what caused it and revert to the old behavior. Maybe we want to always bind to `localhost` or just to bind to all interfaces.",,kzhang,patrick,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12827,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382851,,,Tue Apr 08 21:31:46 UTC 2014,,,,,,,,,,"0|i1tyif:",383119,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 00:15;kzhang;JettyUtils.startJettyServer() used to bind to all interfaces, however, SPARK-1060 change that to only bind to a specific interface (preferably a non-loopback address).

If you want to revert to previous behavior, here's the patch.

https://github.com/apache/spark/pull/318;;;","08/Apr/14 21:31;pwendell;Okay we fixed this for 1.0. The decision was just to revert to the old behavior of binding to all interfaces. Note that this means SPARK_LOCAL_IP is not relevant for the UI.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport SPARK-1210 into 0.9 branch,SPARK-1346,12704515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,pwendell,,28/Mar/14 10:25,14/Mar/15 08:51,14/Jul/23 06:25,14/Mar/15 08:51,,,,,,,,,0.9.3,,,,,Spark Core,,,,,0,,,,,,We should backport this after the 0.9.1 release happens.,,patrick,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1210,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382819,,,Sat Mar 14 08:51:25 UTC 2015,,,,,,,,,,"0|i1tybb:",383087,,,,,,,,,,,,,0.9.3,,,,,,,,,,"01/Mar/15 09:44;srowen;Q: is anything being backported to 0.9 at this point? would there ever be another release that needs this, or is it done?;;;","14/Mar/15 08:51;srowen;Although it may be unlikely an 0.9.3 release happens, I just made this back-port anyway. It's a simple change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark on yarn 0.23 using maven doesn't build,SPARK-1345,12704479,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,,28/Mar/14 07:51,31/Mar/14 14:30,14/Jul/23 06:25,31/Mar/14 14:30,1.0.0,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"The spark on yarn hadoop 0.23 build on maven fails:

ERROR] /home/tgraves/tgravescs-spark/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:62: org.apache.hadoop.fs.Path does not have a constructor
[ERROR]       .readMetaData(new Path(path))
[ERROR]                     ^
[ERROR] /home/tgraves/tgravescs-spark/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:116: org.apache.hadoop.fs.Path does not have a constructor
[ERROR]     val origPath = new Path(pathStr)
[ERROR]                    ^
",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382876,,,Fri Mar 28 08:41:27 UTC 2014,,,,,,,,,,"0|i1tynz:",383144,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 08:23;tgraves;I think the more relevant part of the error is:

[WARNING] Class org.apache.avro.reflect.Stringable not found - continuing with a stub.
[WARNING] Caught: java.lang.NullPointerException while parsing annotations in /home/hadoopqa/.m2/repository/org/apache/hadoop/hadoop-common/0.23.9/hadoop-common-0.23.9.jar(org/apache/hadoop/io/Text.class)
[ERROR] error while loading Text, class file '/home/hadoopqa/.m2/repository/org/apache/hadoop/hadoop-common/0.23.9/hadoop-common-0.23.9.jar(org/apache/hadoop/io/Text.class)' is broken

Basically this is happening because the pom file doesn't include the avro dependency that hadoop 0.23 needs.;;;","28/Mar/14 08:41;tgraves;https://github.com/apache/spark/pull/263;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala API docs for top methods,SPARK-1344,12704470,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,dcarroll@cloudera.com,,28/Mar/14 07:36,10/Nov/14 01:42,14/Jul/23 06:25,10/Nov/14 01:42,0.9.0,,,,,,,,1.2.0,,,,,Documentation,,,,,0,,,,,,"The RDD.top() methods are documented as follows:
bq. Returns the top *K* elements from this RDD using the natural ordering for *T*.
bq. Returns the top *K* elements from this RDD as defined by the specified Comparator[[T]].

I believe those should read
bq. Returns the top *num* elements from this RDD using the natural ordering for *K*.
bq. Returns the top *num* elements from this RDD as defined by the specified Comparator[[K]].
",,apachespark,dcarroll@cloudera.com,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382872,,,Sat Nov 08 10:46:39 UTC 2014,,,,,,,,,,"0|i1tyn3:",383140,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 09:57;markhamstra;No, the original is correct.  K is the number of elements; T is the type of the elements. ;;;","28/Mar/14 10:30;dcarroll@cloudera.com;This doesn't make sense to me.  The signature of the method is 
top(num: Int): List[(K, V)]

So there's a parameter ""num"" but the description doesn't reference it.

There's no T in the signature, and the description *does* reference it.  (Perhaps it is assumed that all uses of ""T"" anywhere refer to the type of something?)

The only K in the signature is in the return value (the key of the returned tuples).  This doesn't make sense with ""...top K references...""

I don't see how this description helps a user understand how to use the function.

;;;","28/Mar/14 10:47;markhamstra;Which docs/comments are you looking at?  When dealing with an RDD\[(K, V)\] there is room for confusion since the way data scientists refer to getting the top-ranked results is Top-K, where K is the number of ranked items fetched, whereas our generic way of referring to an RDD of key-value pairs is RDD\[(K, V)\], where K is the type of the keys.  On top of that, top() doesn't require a PairRDD, so it is available on a simple RDD\[T\] as {{def top(num: Int)(implicit ord: Ordering\[T\]): Array\[T\]}} -- and that's where the original wording comes from.  ;;;","28/Mar/14 10:50;dcarroll@cloudera.com;Oh, I think I figured out the misunderstanding between your comment and mine:
There are multiple ""top"" functions.

I am specifically looking at JavaPairRDD.top:
http://spark.incubator.apache.org/docs/latest/api/core/index.html#org.apache.spark.api.java.JavaPairRDD
{quote}
{{def top(num: Int): List[[(K, V)]]}}
Returns the top K elements from this RDD using the natural ordering for T.
num - the number of top elements to return
returns - an array of top elements
{quote}

This one I see is different than, say, RDD.top:
http://spark.incubator.apache.org/docs/latest/api/core/index.html#org.apache.spark.rdd.RDD
{quote}
{{def top(num: Int)(implicit ord: Ordering[[T]]): Array[[T]]}}
Returns the top K elements from this RDD as defined by the specified implicit Ordering[[T]].
num - the number of top elements to return
ord - the implicit ordering for T
returns- an array of top elements
{quote}




;;;","28/Mar/14 10:54;dcarroll@cloudera.com;Our comments crossed paths.

At any rate, regardless of the origin of the phrasing, I found it confusing to see a reference to a non-existent parameter ""K"" in the description.  I'm not a data scientist, just a programmer, trying to use the API to understand how to call a function, and I believe that my proposed phrasing is both correct and the least confusing.  ""Room for confusion"" is not a plus in docs.
;;;","28/Mar/14 11:04;markhamstra;The problem with your re-wording, though, is that it is not correct.  Since top() for RDD\[(K, V)\] is inherited from RDD\[T\], the Ordering is parameterized on (K, V), not just K.  Typically for PairRDD, this will be a lexicographical ordering of the pairs, but it could be any user-defined Ordering\[(K, V)\]. ;;;","28/Mar/14 11:23;dcarroll@cloudera.com;Fair enough; what I meant was that I prefer my wording for ""num"" over the non-existent ""K"" for how many items to return.

So how about:
{{def top(num: Int): List[[(K, V)]]}}
Returns the top num elements from this RDD using the natural ordering for the elements of the RDD.
and
{{def top(num: Int, comp: Comparator[[(K, V)]]): List[[(K, V)]]}}
Returns the top num elements from this RDD as defined by the specified Comparator[[(K,V)]].;;;","28/Mar/14 11:31;markhamstra;For the specific case of JavaPairRDD, that's fine; but you won't be able to make equivalent changes for Scala RDDs that inherit from org.apache.spark.rdd.RDD;;;","08/Nov/14 10:46;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3168;;;",,,,,,,,,,,,,,,,,,,,,,,,
PySpark OOMs without caching,SPARK-1343,12704449,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,matei,,27/Mar/14 22:49,28/Jul/14 22:22,14/Jul/23 06:25,28/Jul/14 22:22,0.9.0,,,,,,,,0.9.0,1.0.0,,,,PySpark,,,,,0,,,,,,"There have been several reports on the list of PySpark 0.9 OOMing even if it does simple maps and counts, whereas 0.9 didn't. This may be due to either the batching added to serialization, or due to invalid serialized data which makes the Java side allocate an overly large array. Needs investigating for 1.0.",,davies,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382880,,,Mon Jul 28 22:22:03 UTC 2014,,,,,,,,,,"0|i1tyov:",383148,,,,,,,,,,,,,1.1.0,,,,,,,,,,"28/Jul/14 22:21;davies;Maybe it's related to partitionBy() with small number of partitions, the data in one partition will send to JVM as several huge bytearray, they will cost huge memory before writing into disks, because default spark.serializer.objectStreamReset is too large.

Hopefully, PR-1568 and PR-1460 will fix these issues.

Close this now, will re-open it if it happens again.;;;","28/Jul/14 22:22;davies;https://github.com/apache/spark/pull/1460

https://github.com/apache/spark/pull/1568;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some Spark Streaming receivers are not restarted when worker fails,SPARK-1340,12704429,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,,27/Mar/14 15:24,15/May/14 20:25,14/Jul/23 06:25,15/May/14 20:25,0.9.0,,,,,,,,1.0.0,,,,,DStreams,,,,,0,,,,,,"For some streams like Kafka stream, the receiver do not get restarted if the worker running the receiver fails. ",,anandriyer,eshishki,huitseeker,kdang,mcchang,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382883,,,Thu May 15 20:25:44 UTC 2014,,,,,,,,,,"0|i1typj:",383151,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 06:29;kdang;Any udpate on when this fix would be available?;;;","25/Apr/14 01:17;tdas;I havent explicitly tested this, but this should be fixed after after a whole refactoring in the receiver API done in https://github.com/apache/spark/pull/300

To elaborate further, the new refactored receiver ensures that the task that launches the receiver does not complete until the receiver is explicitly shutdown. So if the receiver fails with an exception it should get relaunched. Well, ideally. This still needs to be tested.
;;;","15/May/14 20:25;tdas;Resolved with 
https://issues.apache.org/jira/browse/SPARK-1332;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application web UI garbage collects newest stages instead old ones,SPARK-1337,12704909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,dmitrybugaychenko,,27/Mar/14 11:12,04/Apr/14 05:35,14/Jul/23 06:25,04/Apr/14 05:35,0.9.0,,,,,,,,0.9.2,1.0.0,,,,Web UI,,,,,1,,,,,,When running task with many stages (eg. streaming task) and spark.ui.retainedStages set to small value (100) application UI removes newest stages keeping 90 old ones... ,,dmitrybugaychenko,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1411,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382888,,,Fri Apr 04 01:51:27 UTC 2014,,,,,,,,,,"0|i1tyqn:",383156,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 01:51;pwendell;https://github.com/apache/spark/pull/320

This is a simple fix if people want to pull this in and build Spark on their own.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Also increase perm gen / code cache for scalatest when invoked via Maven build,SPARK-1335,12704904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,,27/Mar/14 03:09,15/Jan/15 09:08,14/Jul/23 06:25,27/Mar/14 11:57,0.9.0,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"I am observing build failures when the Maven build reaches tests in the new SQL components. (I'm on Java 7 / OSX 10.9). The failure is the usual complaint from scala, that it's out of permgen space, or that JIT out of code cache space.

I see that various build scripts increase these both for SBT. This change simply adds these settings to scalatest's arguments. Works for me and seems a bit more consistent.

(In the PR I'm going to tack on some other little changes too -- see PR.)",,gq,patrick,sowen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382886,,,Sun Aug 03 16:36:10 UTC 2014,,,,,,,,,,"0|i1tyq7:",383154,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/14 09:57;patrick;This problem is also messing up our jenkins maven builds, so thanks a bunch for digging around.;;;","03/Aug/14 16:36;gq;The problem also appeared in branch 1.1. The following command fails. 
{{mvn  -Pyarn-alpha -Phive -Dhadoop.version=2.0.0-cdh4.5.0 -DskipTests  package}} .
 I'm on Java 6 / OSX 10.9.4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compute_classpath.sh has extra echo which prevents spark-class from working,SPARK-1330,12704895,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,,26/Mar/14 13:29,27/Mar/14 09:56,14/Jul/23 06:25,27/Mar/14 09:56,1.0.0,,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"if I just use spark-class to try to run an example on yarn it errors out because of the echo put in compute_classpath.sh under the hive assembly check:

 echo ""Hive assembly found, including hive support.  If this isn't desired run sbt hive/clean.""

This causes the classpath to look like:

exec /home/y/share/yjava_jdk/java//bin/java -cp Hive assembly found, including hive support.  If this isn't desired run sbt hive/clean.
/home/user1/user1cs-spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop0.23.10.jar:/home/user1/user1cs-spark/conf:/home/user1/user1cs-spark/lib_managed/jars/datanucleus-api-jdo-3.2.1.jar:/home/user1/user1cs-spark/lib_managed/jars/datanucleus-core-3.2.2.jar:/home/user1/user1cs-spark/lib_managed/jars/datanucleus-rdbms-3.2.1.jar:/home/user1/user1cs-spark/sql/hive/target/scala-2.10//spark-hive-assembly-1.0.0-SNAPSHOT-hadoop0.23.10.jar:/home/user1/yarn_install//conf -Djava.library.path= -Xms512m -Xmx512m org.apache.spark.deploy.yarn.Client --jar examples/target/scala-2.10/spark-examples-assembly-1.0.0-SNAPSHOT.jar --class org.apache.spark.examples.SparkPi --args yarn-cluster",,sandy,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1251,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382891,,,Thu Mar 27 09:56:40 UTC 2014,,,,,,,,,,"0|i1tyrb:",383159,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 13:33;tgraves;echo was introduced by SPARK-1251;;;","26/Mar/14 13:36;tgraves;The comment in compute_classpath.sh also seems to be wrong.  It says hive assembly isn't included unless you specifically build it with sbt hive/assembly.  I did not do this.  I built it with SPARK_HADOOP_VERSION=0.23.10 SPARK_YARN=true sbt/sbt assembly and it built it.;;;","26/Mar/14 13:46;tgraves;https://github.com/apache/spark/pull/241;;;","27/Mar/14 09:56;tgraves;I merged this in so that things start working again. Note that there are more fixes related to this going in with https://github.com/apache/spark/pull/237,SPARK-1314;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException if graphx.Graph has more edge partitions than node partitions,SPARK-1329,12704632,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,darabos,,26/Mar/14 10:25,15/Feb/15 11:53,14/Jul/23 06:25,26/May/14 20:19,0.9.0,,,,,,,,1.0.0,,,,,GraphX,,,,,0,,,,,,"To reproduce, let's look at a graph with two nodes in one partition, and two edges between them split across two partitions:
scala> val vs = sc.makeRDD(Seq(1L->null, 2L->null), 1)
scala> val es = sc.makeRDD(Seq(graphx.Edge(1, 2, null), graphx.Edge(2, 1, null)), 2)
scala> val g = graphx.Graph(vs, es)

Everything seems fine, until GraphX needs to join the two RDDs:
scala> g.triplets.collect
[...]
java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.spark.graphx.impl.RoutingTable$$anonfun$2$$anonfun$apply$3.apply(RoutingTable.scala:76)
	at org.apache.spark.graphx.impl.RoutingTable$$anonfun$2$$anonfun$apply$3.apply(RoutingTable.scala:75)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.graphx.impl.RoutingTable$$anonfun$2.apply(RoutingTable.scala:75)
	at org.apache.spark.graphx.impl.RoutingTable$$anonfun$2.apply(RoutingTable.scala:73)
	at org.apache.spark.rdd.RDD$$anonfun$1.apply(RDD.scala:450)
	at org.apache.spark.rdd.RDD$$anonfun$1.apply(RDD.scala:450)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:71)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:85)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.Task.run(Task.scala:53)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

The bug is fairly obvious in RoutingTable.createPid2Vid() -- it creates an array of length vertices.partitions.size, and then looks up partition IDs from the edges.partitionsRDD in it.

A graph usually has more edges than nodes. So it is natural to have more edge partitions than node partitions.",,ankurd,darabos,glenn.strycker@gmail.com,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5480,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382871,,,Mon May 26 18:19:07 UTC 2014,,,,,,,,,,"0|i1tymv:",383139,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 10:45;rxin;Thanks - [~darabos]. Do you mind submitting a pull request to fix this?;;;","28/Mar/14 13:01;darabos;The bug being obvious does not mean the fix is obvious too :). But I'll give it a try!;;;","28/Mar/14 13:03;rxin;Thanks - really appreciate it. Let me know if you run into problems. ;;;","26/May/14 11:38;darabos;Sorry, I haven't looked into fixing this. We ended up not using GraphX, so we are no longer affected by the bug.;;;","26/May/14 18:19;ankurd;This was resolved by #368: https://github.com/apache/spark/pull/368;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
GLM needs to check addIntercept for intercept and weights,SPARK-1327,12704874,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,,25/Mar/14 23:10,31/Mar/14 00:41,14/Jul/23 06:25,31/Mar/14 00:41,0.9.0,,,,,,,,0.9.1,1.0.0,,,,MLlib,,,,,0,,,,,,GLM needs to check addIntercept for intercept and weights.,,mengxr,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382894,,,Mon Mar 31 00:41:52 UTC 2014,,,,,,,,,,"0|i1tyrz:",383162,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 23:24;mengxr;PR: https://github.com/apache/spark/pull/236;;;","27/Mar/14 01:20;tdas;Partial fix for 0.9.1 where adding intercepts will fail fast. ;;;","31/Mar/14 00:41;mengxr;Merged into master and backported to 0.9.x.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make-distribution.sh's Tachyon support relies on GNU sed,SPARK-1326,12704871,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,matei,,25/Mar/14 22:35,08/May/14 09:16,14/Jul/23 06:25,08/May/14 09:16,,,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"It fails on Mac OS X, with {{sed: 1: ""/Users/matei/ ..."": invalid command code m}}

",,matei,techaddict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382881,,,Thu May 08 09:15:45 UTC 2014,,,,,,,,,,"0|i1typ3:",383149,,,,,,,,,,,,,,,,,,,,,,,"08/May/14 09:15;techaddict;fixed by PR: https://github.com/apache/spark/pull/264;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI Should Not Try to Bind to SPARK_PUBLIC_DNS,SPARK-1324,12704890,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,patrick,pwendell,,25/Mar/14 16:37,30/Mar/14 04:14,14/Jul/23 06:25,26/Mar/14 18:22,0.9.0,,,,,,,,1.0.0,,,,,Web UI,,,,,0,,,,,,"Right now the Spark UI will try to bind to the SPARK_PUBLIC_DNS. This is not correct - the SPARK_PUBLIC_DNS name denotes how Spark components should advertise to other services/components, not the actual bind address.",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382824,,,Wed Mar 26 18:23:02 UTC 2014,,,,,,,,,,"0|i1tycf:",383092,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 18:23;patrick;Might be worth back porting this if users want it in 0.9. Not doing it yet, we'll see if anyone requests it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job hangs with java.io.UTFDataFormatException when reading strings > 65536 bytes. ,SPARK-1323,12704883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,karthikk,,25/Mar/14 15:18,30/Mar/14 05:43,14/Jul/23 06:25,30/Mar/14 05:43,0.9.0,,,,,,,,0.9.1,1.0.0,,,,PySpark,,,,,0,pyspark,,,,,"Steps to reproduce in Python
{code:borderStyle=solid}
st = ''.join(['1' for i in range(65537)])
sc.parallelize([st]).saveAsTextFile(""testfile"")
sc.textFile('testfile').count()
{code}
The last line never completes..  Looking at the logs (with DEBUG enabled) reveals the exception, here is the stack trace
{code:borderStyle=solid}
14/03/25 15:03:34 INFO PythonRDD: stdin writer to Python finished early
14/03/25 15:03:34 DEBUG PythonRDD: stdin writer to Python finished early
java.io.UTFDataFormatException: encoded string too long: 65537 bytes
        at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364)
        at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$2.apply(PythonRDD.scala:222)
        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$2.apply(PythonRDD.scala:221)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:221)
        at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:81)
{code}
",,jblomo,karthikk,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382857,,,Sun Mar 30 05:43:25 UTC 2014,,,,,,,,,,"0|i1tyjr:",383125,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/14 05:34;jblomo;I believe this was fixed in 1.0.0-SNAPSHOT with https://github.com/apache/incubator-spark/commit/1381fc72f7a34f690a98ab72cec8ffb61e0e564d#diff-0a67bc4d171abe4df8eb305b0f4123a2;;;","30/Mar/14 05:43;pwendell;Yes good call. I think this is fixed in 0.9.1 (upcoming) and 1.0.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ordering of top() in Python,SPARK-1322,12704887,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,prashant,pwendell,,25/Mar/14 15:00,07/Feb/20 17:26,14/Jul/23 06:25,26/Mar/14 09:18,,,,,,,,,0.9.1,1.0.0,,,,PySpark,,,,,0,,,,,,Right now top() returns values in ascending order. It should be consistent with Scala and return them in descending order. /cc [~rxin],,apachespark,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382827,,,Thu Dec 10 15:06:02 UTC 2015,,,,,,,,,,"0|i1tyd3:",383095,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/15 15:06;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/235;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The current code effectively ignores spark.task.cpus,SPARK-1319,12704901,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,kayousterhout,,25/Mar/14 12:33,25/Mar/14 13:06,14/Jul/23 06:25,25/Mar/14 13:06,0.9.0,,,,,,,,1.0.0,,,,,Mesos,,,,,0,,,,,,"Currently, if spark.task.cpus is set to > 1, we still treat the task as if it only needs 1 core (for both standalone mode and with Mesos).  So, we're effectively ignoring the parameter. 

The parameter only gets used in TaskSchedulerImpl.resourceOffers() as it's assigning tasks, but it is then ignored upstream when CoarseGrainedSchedulerBackend and MesosSchedulerBackend account for the number of free cores.",,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382908,,,Tue Mar 25 13:06:40 UTC 2014,,,,,,,,,,"0|i1tyv3:",383176,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 12:33;kayousterhout;PR here: https://github.com/apache/spark/pull/219;;;","25/Mar/14 13:06;kayousterhout;Fixed by https://github.com/apache/spark/pull/219;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark on yarn-alpha with mvn on master branch won't build,SPARK-1315,12704852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,tgraves,,25/Mar/14 07:10,15/Jan/15 09:08,14/Jul/23 06:25,26/Mar/14 18:35,,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"I try to build off master branch using maven to build yarn-alpha but get the following errors.

mvn  -Dyarn.version=0.23.10 -Dhadoop.version=0.23.10  -Pyarn-alpha  clean package -DskipTests 

-----
[ERROR] /home/tgraves/y-spark-git/tools/src/main/scala/org/apache/spark/tools/GenerateMIMAIgnore.scala:25: object runtime i
s not a member of package reflect [ERROR] import scala.reflect.runtime.universe.runtimeMirror
[ERROR]                      ^
[ERROR] /home/tgraves/y-spark-git/tools/src/main/scala/org/apache/spark/tools/GenerateMIMAIgnore.scala:40: not found: value runtimeMirror
[ERROR]   private val mirror = runtimeMirror(classLoader)
[ERROR]                        ^
[ERROR] /home/tgraves/y-spark-git/tools/src/main/scala/org/apache/spark/tools/GenerateMIMAIgnore.scala:92: object tools is not a member of package scala
[ERROR]     scala.tools.nsc.io.File("".mima-excludes"").
[ERROR]           ^
[ERROR] three errors found
",,patrick,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1325,,,,,,,,,SPARK-1094,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382896,,,Wed Mar 26 18:35:12 UTC 2014,,,,,,,,,,"0|i1tysf:",383164,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 06:00;tgraves;I believe this was broken by SPARK-1094  - https://github.com/apache/spark/pull/20;;;","26/Mar/14 11:34;patrick;Sorry this was my bad - I didn't realize that scala reflection wasn't included by default:

https://github.com/apache/spark/pull/234/files;;;","26/Mar/14 18:35;patrick;Sorry for breaking this guys. Thanks for the quick fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sbt assembly builds hive jar,SPARK-1314,12704819,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,,25/Mar/14 01:19,07/Apr/14 00:50,14/Jul/23 06:25,07/Apr/14 00:50,1.0.0,,,,,,,,1.0.0,,,,,Build,SQL,,,,0,,,,,,"Running ""sbt/sbt assembly"" will create a spark-hive-assembly jar which causes all subsequent operations using spark-class to use the Hive jar instead of the Spark one. For people who use this workflow, this negates most of the use of extracting out Hive to its own assembly.

One way to get around this would be to create an environment variable which controls whether or not we include the Hive dependencies into the normal spark assembly jar. This would be in line with the behavior of similarly ""optional"" components such as Yarn and Ganglia.",,ilikerps,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1309,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382914,,,2014-03-25 01:19:42.0,,,,,,,,,,"0|i1tywf:",383182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch should read based on the batch interval provided in the StreamingContext,SPARK-1312,12704818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,sanjay_awat,,24/Mar/14 22:06,01/Jun/15 21:16,14/Jul/23 06:25,01/Jun/15 21:16,0.9.0,,,,,,,,,,,,,DStreams,,,,,2,sliding,streaming,window,,,"This problem primarily affects sliding window operations in spark streaming.

Consider the following scenario:
- a DStream is created from any source. (I've checked with file and socket)
- No actions are applied to this DStream
- Sliding Window operation is applied to this DStream and an action is applied to the sliding window.
In this case, Spark will not even read the input stream in the batch in which the sliding interval isn't a multiple of batch interval. Put another way, it won't read the input when it doesn't have to apply the window function. This is happening because all transformations in Spark are lazy.

How to fix this or workaround it (see line#3):
JavaStreamingContext stcObj = new JavaStreamingContext(confObj, new Duration(1 * 60 * 1000));
JavaDStream<String> inputStream = stcObj.textFileStream(""/Input"");
inputStream.print(); // This is the workaround
JavaDStream<String> objWindow = inputStream.window(new Duration(windowLen*60*1000), new Duration(slideInt*60*1000));
objWindow.dstream().saveAsTextFiles(""/Output"", """");

The ""Window operations"" example on the streaming guide implies that Spark will read the stream in every batch, which is not happening because of the lazy transformations.
Wherever sliding window would be used, in most of the cases, no actions will be taken on the pre-window batch, hence my gut feeling was that Streaming would read every batch if any actions are being taken in the windowed stream.
As per Tathagata,
""Ideally every batch should read based on the batch interval provided in the StreamingContext.""

Refer the original thread on http://apache-spark-user-list.1001560.n3.nabble.com/Sliding-Window-operations-do-not-work-as-documented-tp2999.html for more details, including Tathagata's conclusion.",,anandriyer,cfregly,liqusha,mikeoz,mubarak.seyed,sanjay_awat,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382916,,,Mon Jun 01 21:16:19 UTC 2015,,,,,,,,,,"0|i1tywv:",383184,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/14 21:27;tdas;This has probably been solved in Spark 1.2.0 with changes in how blocks are assigned to batches.;;;","24/Dec/14 21:29;tdas;I will try to add a unit test to make sure this bug has been solved. ;;;","01/Jun/15 21:16;tdas;I have verified this in current Spark and I am closing this JIRa. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use map side distinct in collect vertex ids from edges graphx,SPARK-1311,12704796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ankurd,holdenk_amp,,24/Mar/14 17:14,19/Jul/14 22:58,14/Jul/23 06:25,26/May/14 20:20,,,,,,,,,1.0.0,,,,,GraphX,,,,,0,,,,,,See GRAPH-1,,ankurd,holdenk_amp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382919,,,Mon May 26 18:22:51 UTC 2014,,,,,,,,,,"0|i1tyxj:",383187,,,,,,,,,,,,,,,,,,,,,,,"26/May/14 18:22;ankurd;This was resolved by PR #497, which removed collectVertexIds and instead performed the operation as a side effect of constructing the routing tables: https://github.com/apache/spark/pull/497/files#diff-8ea535724b3f014cfef17284b3e783feR397;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for  cross validation to MLLibb,SPARK-1310,12704793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holdenkarau,holdenk_amp,,24/Mar/14 17:13,18/Aug/14 05:54,14/Jul/23 06:25,16/Apr/14 16:36,,,,,,,,,1.0.0,,,,,MLlib,,,,,0,,,,,,See MLI-2,,bing,holdenk_amp,mengxr,ryanleitaiwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382920,,,Mon Aug 18 05:54:45 UTC 2014,,,,,,,,,,"0|i1tyxr:",383188,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/14 05:54;bing;can you provide a link to your solution?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
don't use term 'standalone' to refer to a Spark Application,SPARK-1307,12704769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,dcarroll@cloudera.com,,24/Mar/14 12:13,15/Oct/14 04:39,14/Jul/23 06:25,15/Oct/14 04:39,0.9.0,,,,,,,,1.2.0,,,,,Documentation,,,,,0,,,,,,"In the ""Quick Start Guide"" for Scala, there are three sections entitled ""A Standalone App in Scala"", ""A Standalone App in Java"" and ""A Standalone App in Python.""  

In these sections, the word ""standalone"" is meant to refer to a Spark application that is run outside of the Spark Shell. This nomenclature is quite confusing, because the cluster management framework included in Spark is called ""Spark Standalone""...this overlap of terms has resulted in at least one person (me) think that a ""standalone app"" was somehow related to a ""standalone cluster""...and that in order to run my app on a Standalone Spark cluster, I had to write follow the instructions to write a Standalone app.

Fortunately, the only place I can find this usage of ""standalone"" to refer to an application is on the Quick Start page.   I think those three sections should instead be retitled as
Writing a Spark Application in Scala
Writing a Spark Application in Java
Writing a Spark Application in Python
and rephrased to remove the use of the term ""standalone"".",,apachespark,dcarroll@cloudera.com,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382922,,,Wed Oct 15 04:39:50 UTC 2014,,,,,,,,,,"0|i1tyy7:",383190,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/14 19:45;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2787;;;","14/Oct/14 06:49;pwendell;Yeah this is a good idea - there is no reason to overload that name, it's just confusing.;;;","15/Oct/14 04:39;mengxr;Issue resolved by pull request 2787
[https://github.com/apache/spark/pull/2787];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no instructions provided for sbt assembly with Hadoop 2.2,SPARK-1306,12704758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dcarroll@cloudera.com,,24/Mar/14 11:55,13/Oct/14 17:56,14/Jul/23 06:25,13/Oct/14 17:56,0.9.0,,,,,,,,,,,,,Documentation,,,,,0,,,,,,"on the running-on-yarn.html page, in the section ""Building a YARN-Enabled Assembly JAR"", only the instructions for building for ""old"" Hadoop (2.0.5) are provided.  There's a comment that ""The build process now also supports new YARN versions (2.2.x). See below.""

However, the only mention below is a single sentence which says ""See Building Spark with Maven for instructions on how to build Spark using the Maven process.""  There are no instructions for building with sbt. This is different than in prior versions of the docs, in which a whole paragraph was provided.

I'd like to see the command line to build for Hadoop 2.2 included right at the top of the page. Also remove the bit about how it is ""now"" supported.   Hadoop 2.2 is now the ""norm"", no longer an exception, as I see it. 

Unfortunately I'm not sure exactly what the command should be.  I tried this, but got errors:
SPARK_HADOOP_VERSION=2.2.0 SPARK_YARN=true sbt/sbt assembly",,dcarroll@cloudera.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382923,,,Mon Oct 13 17:56:52 UTC 2014,,,,,,,,,,"0|i1tyyf:",383191,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/14 17:56;srowen;I think this was obviated by subsequent changes to this documentation. SBT is no longer the focus, but, building-spark.md now has more comprehensive documentation on building with YARN, including these recent versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
httpd doesn't start in spark-ec2 (cc2.8xlarge),SPARK-1302,12704660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivaram,shivaram,,23/Mar/14 23:56,04/Jun/15 11:17,14/Jul/23 06:25,11/Feb/15 22:07,0.9.0,,,,,,,,1.3.0,,,,,EC2,,,,,2,,,,,,"In a cc2.8xlarge EC2 cluster launched from master branch, httpd won't start (i.e ganglia doesn't work). The reason seems to be httpd.conf is wrong (newer httpd version ?).  The config file contains a bunch of non-existent modules and this happens because we overwrite the default conf with our config file from spark-ec2. We could explore using patch or something like that to just apply the diff we need ",,douglaz,grzegorz-dubicki,kluyg,nchammas,sandorw,shivaram,soid,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2649,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382928,,,Thu Jun 04 11:17:57 UTC 2015,,,,,,,,,,"0|i1tyzj:",383196,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/14 00:00;sandorw;I seem to be hitting this issue with all hvm amis, but not pvm amis. This means that ganglia does not work out of the box for many of the newer instance types. Has anyone found a way to fix httpd for ganglia?;;;","10/Jan/15 17:22;nchammas;There is an open PR by [~fredcons] here to solve this problem: https://github.com/mesos/spark-ec2/pull/76;;;","04/Feb/15 20:21;kluyg;The PR fixed the issue for me, also the PR doesn't affect a lot of files, should be safe to merge, I hope somebody from the Spark team will find time to merge it into master, despite the fact that it has ""Minor"" priority.;;;","11/Feb/15 02:28;soid;I'm getting this httpd error on t2.medium instance. Maybe you can describe recommended instance types in the documentation?

{code}
Starting httpd: httpd: Syntax error on line 153 of /etc/httpd/conf/httpd.conf: Cannot load modules/mod_authn_alias.so into server: /etc/httpd/modules/mod_authn_alias.so: cannot open shared object file: No such file or directory
{code};;;","11/Feb/15 08:13;srowen;It looks like this was basically resolved recently given the discussion at https://github.com/mesos/spark-ec2/pull/76  I wonder if [~soid] is able to access the latest change there? or if this is evidence that something wasn't quite right about the change?;;;","11/Feb/15 17:06;shivaram;[~soid] Could you let us which Spark version you were using to launch the cluster. The fix for spark-ec2 was merged into `branch-1.3` (and the master branch);;;","11/Feb/15 22:05;soid;Indeed, I used 1.2.0. Sorry for the false alarm. ;;;","04/Jun/15 11:17;grzegorz-dubicki;I am having this problem again in Spark 1.3.1. This patch helped https://github.com/grzegorz-dubicki/spark-ec2/commit/56332c0879bd8d15af123b64cc0923dc80f7fdd1. Can anyone confirm this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Progress values on Spark UI progress bar wander off-screen,SPARK-1295,12704592,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,ilikerps,,21/Mar/14 17:27,21/Mar/14 18:07,14/Jul/23 06:25,21/Mar/14 18:07,0.9.0,,,,,,,,0.9.1,1.0.0,,,,,,,,,0,,,,,,"Current:
!https://f.cloud.github.com/assets/563652/2489083/8c127e80-b153-11e3-807c-048ebd45104b.png!

Expected:
!https://f.cloud.github.com/assets/563652/2489084/8c12cf5c-b153-11e3-8747-9d93ff6fceb4.png!",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382936,,,Fri Mar 21 17:27:48 UTC 2014,,,,,,,,,,"0|i1tz1b:",383204,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 17:27;ilikerps;https://github.com/apache/spark/pull/201 has been created to fix this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Case insensitive resolution in HiveContext breaks the ability to access fields with upper case letters,SPARK-1294,12704801,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,,21/Mar/14 16:55,24/Mar/14 19:27,14/Jul/23 06:25,24/Mar/14 19:27,,,,,,,,,1.0.0,,,,,SQL,,,,,0,,,,,,"Here's a test case:

{code}
case class Data(a: Int, B: Int, n: Nested)
case class Nested(a: Int, B: Int)

  test(""case insensitivity with scala reflection"") {
    // Test resolution with Scala Reflection
    TestHive.sparkContext.parallelize(Data(1, 2, Nested(1,2)) :: Nil)
      .registerAsTable(""caseSensitivityTest"")

    sql(""SELECT a, b, A, B, n.a, n.b, n.A, n.B FROM caseSensitivityTest"")
  }
{code}",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382918,,,2014-03-21 16:55:50.0,,,,,,,,,,"0|i1tyxb:",383186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn stable finishApplicationMaster incomplete,SPARK-1288,12704529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,,20/Mar/14 09:44,17/Apr/14 21:39,14/Jul/23 06:25,17/Apr/14 21:39,0.9.0,1.0.0,,,,,,,1.0.0,,,,,YARN,,,,,0,,,,,,The yarn stable version of ApplicationMaster.finishApplicationMaster is incomplete.  It doesn't set the diagnostic message or app trcking url.,,berngp,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382956,,,Wed Apr 09 00:29:52 UTC 2014,,,,,,,,,,"0|i1tz5r:",383224,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 00:29;tgraves;https://github.com/apache/spark/pull/362;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make usage of spark-env.sh idempotent,SPARK-1286,12704631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ilikerps,ilikerps,,19/Mar/14 18:28,28/Mar/14 14:38,14/Jul/23 06:25,28/Mar/14 14:38,0.9.0,1.0.0,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"Various spark scripts load spark-env.sh. This can cause growth of any variables that may be appended to (SPARK_CLASSPATH, SPARK_REPL_OPTS) and it makes the precedence order for options specified in spark-env.sh less clear.

One use-case for the latter is that we want to set options from the command-line of spark-shell, but these options will be overridden by subsequent loading of spark-env.sh. If we were to load the spark-env.sh first and then set our command-line options, we could guarantee correct precedence order.",,ash211,ilikerps,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382870,,,Wed Mar 19 22:07:15 UTC 2014,,,,,,,,,,"0|i1tymn:",383138,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 22:07;patrick;Relevant: https://github.com/apache/incubator-spark/pull/326;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark hangs after IOError on Executor,SPARK-1284,12704557,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,jblomo,,19/Mar/14 16:16,02/Oct/14 21:23,14/Jul/23 06:25,02/Oct/14 21:23,,,,,,,,,1.1.0,,,,,PySpark,,,,,0,,,,,,"When running a reduceByKey over a cached RDD, Python fails with an exception, but the failure is not detected by the task runner.  Spark and the pyspark shell hang waiting for the task to finish.

The error is:
{code}
PySpark worker failed with exception:
Traceback (most recent call last):
  File ""/home/hadoop/spark/python/pyspark/worker.py"", line 77, in main
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/home/hadoop/spark/python/pyspark/serializers.py"", line 182, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/home/hadoop/spark/python/pyspark/serializers.py"", line 118, in dump_stream
    self._write_with_length(obj, stream)
  File ""/home/hadoop/spark/python/pyspark/serializers.py"", line 130, in _write_with_length
    stream.write(serialized)
IOError: [Errno 104] Connection reset by peer

14/03/19 22:48:15 INFO scheduler.TaskSetManager: Serialized task 4.0:0 as 4257 bytes in 47 ms
Traceback (most recent call last):
  File ""/home/hadoop/spark/python/pyspark/daemon.py"", line 117, in launch_worker
    worker(listen_sock)
  File ""/home/hadoop/spark/python/pyspark/daemon.py"", line 107, in worker
    outfile.flush()
IOError: [Errno 32] Broken pipe
{code}

I can reproduce the error by running take(10) on the cached RDD before running reduceByKey (which looks at the whole input file).

Affects Version 1.0.0-SNAPSHOT (4d88030486)",,davies,farrellee,jblomo,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382961,,,Thu Oct 02 21:23:03 UTC 2014,,,,,,,,,,"0|i1tz6v:",383229,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/14 14:11;farrellee;[~jblomo] -

will you add a reproducer script to this issue?

i did a simple test based on what you suggested w/ the tip of master and could not reproduce -

{code}
$ ./dist/bin/pyspark
Python 2.7.5 (default, Feb 19 2014, 13:47:28) 
[GCC 4.8.2 20131212 (Red Hat 4.8.2-7)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.0.0-SNAPSHOT
      /_/

Using Python version 2.7.5 (default, Feb 19 2014 13:47:28)
SparkContext available as sc.
>>> data = sc.textFile('/etc/passwd')
14/07/02 07:03:59 INFO MemoryStore: ensureFreeSpace(32816) called with curMem=0, maxMem=308910489
14/07/02 07:03:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 294.6 MB)
>>> data.cache()
/etc/passwd MappedRDD[1] at textFile at NativeMethodAccessorImpl.java:-2
>>> data.take(10)
...[expected output]...
>>> data.flatMap(lambda line: line.split(':')).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).collect()
...[expected output, no hang]...
{code};;;","11/Aug/14 18:43;davies;[~jblomo], could you reproduce this on master or 1.1 branch?

Maybe the pyspark did not hange after this error message, the take() had finished successfully before the error message pop up. The noisy error messages had been fixed in PR https://github.com/apache/spark/pull/1625 ;;;","11/Aug/14 20:00;jblomo;I will try to reproduce on the 1.1 branch later this week, thanks for the update!;;;","14/Aug/14 22:26;jblomo;Hi, having trouble compiling either master or branch-1.1, I sent a request to the mailing list for help.  Are there any compiled snapshots?;;;","26/Aug/14 18:35;farrellee;[~jblomo] master should be buildable again, please give it another shot;;;","02/Oct/14 21:23;davies;I think this is an logging issue ,should be fixed by https://github.com/apache/spark/pull/1625, so close it.

If anyone meet this again, we can reopen it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Stage.name return ""apply at Option.scala:120""",SPARK-1280,12704880,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ilikerps,gq,,19/Mar/14 07:35,27/Sep/14 18:53,14/Jul/23 06:25,25/Mar/14 13:29,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,,,gq,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3539,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382907,,,Mon Mar 24 12:52:13 UTC 2014,,,,,,,,,,"0|i1tyuv:",383175,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/14 12:52;ilikerps;Here's what the stack trace looks like:

{code}
java.lang.RuntimeException
    at org.apache.spark.util.Utils$.getCallSiteInfo(Utils.scala:687)
    at org.apache.spark.util.Utils$.formatCallSiteInfo$default$1(Utils.scala:723)
    at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880)
    at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.SparkContext.getCallSite(SparkContext.scala:880)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:898)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:920)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:934)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:948)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:657)
        ...
    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:981)
    at org.apache.spark.repl.Main$.main(Main.scala:31)
    at org.apache.spark.repl.Main.main(Main.scala)
{code}

It appears this was accidentally introduced when SparkContext#getCallSite was changed to use the Option pattern.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improper use of SimpleDateFormat,SPARK-1278,12704873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,,19/Mar/14 06:44,21/Mar/14 16:41,14/Jul/23 06:25,21/Mar/14 16:40,,,,,,,,,0.9.1,1.0.0,,,,Web UI,,,,,0,,,,,,SimpleDateFormat is not thread-safe. Some places use the same SimpleDateFormat object without safeguard in the multiple threads. It will cause that the Web UI displays improper date.,,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382937,,,Thu Mar 20 20:29:41 UTC 2014,,,,,,,,,,"0|i1tz1j:",383205,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 20:29;zsxwing;PR: https://github.com/apache/spark/pull/179;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Made SPARK_HOME/dev/tests executable,SPARK-1275,12704832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,,18/Mar/14 22:45,19/Mar/14 16:42,14/Jul/23 06:25,19/Mar/14 16:42,,,,,,,,,0.9.1,,,,,Project Infra,,,,,0,,,,,,dev/run-tests was causing Jenkins tests to fail.,,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382959,,,2014-03-18 22:45:46.0,,,,,,,,,,"0|i1tz6f:",383227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix 404 not found error in UI introduced in Jetty 9.0 upgrade,SPARK-1265,12704810,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,,17/Mar/14 19:50,19/Mar/14 10:57,14/Jul/23 06:25,19/Mar/14 10:54,,,,,,,,,0.9.1,,,,,Spark Core,,,,,0,,,,,,"We recently upgraded Jetty from v7.6.8 to v9.1.3. This introduced a 404 not found HTTP error when accessing the root of any UI.

The problem is that the existing code attaches two handlers to the root, one of which is a handler for static resources. In Jetty 9.1.3, we are no longer allowed to do this. Instead, we are supposed to use ResourceHandler rather than ServletContextHandler for serving static resources: http://stackoverflow.com/questions/10284584/serving-static-files-w-embedded-jetty.",,andrewor14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1256,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382965,,,Wed Mar 19 10:54:44 UTC 2014,,,,,,,,,,"0|i1tz7r:",383233,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 10:54;andrewor14;Fixed in 1256, or PR #150.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs don't explain how to run Python examples,SPARK-1261,12704776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dcarroll@cloudera.com,dcarroll@cloudera.com,,17/Mar/14 12:02,17/Mar/14 17:37,14/Jul/23 06:25,17/Mar/14 17:37,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Documentation,,,,,0,,,,,,"The main Spark docs overview page has a section called Running the Examples and Shell, which explains how to start both the Scala and Python spark shells, but only how to run the Scala examples, not the python examples.",,dcarroll@cloudera.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382989,,,2014-03-17 12:02:47.0,,,,,,,,,,"0|i1tzd3:",383257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endless running task when using pyspark with input file containing a long line,SPARK-1257,12704664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,Hanchen,,15/Mar/14 04:36,25/Jul/14 21:53,14/Jul/23 06:25,25/Jul/14 21:53,0.9.0,,,,,,,,0.9.1,,,,,PySpark,,,,,0,,,,,,"When launching any pyspark applications with an input file containing a very long line(about 70000 characters), the job will be hanging and never stops. The application UI shows that there is a task running endlessly.

There will be no problem using the scala version with the same input.",,farrellee,Hanchen,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383002,,,Wed Jul 02 14:00:49 UTC 2014,,,,,,,,,,"0|i1tzfz:",383270,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/14 11:21;joshrosen;I suspect that this was caused by SPARK-1043, which has been fixed for Spark 0.9.1.;;;","02/Jul/14 14:00;farrellee;recommend close as resolved w/ option for filer to reopen if the issue reproduces in 1.0 /cc: [~pwendell] [~joshrosen];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master web UI and Worker web UI returns a 404 error,SPARK-1256,12704719,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,,15/Mar/14 03:58,19/Mar/14 10:57,14/Jul/23 06:25,18/Mar/14 21:58,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,,,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1265,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382964,,,2014-03-15 03:58:22.0,,,,,,,,,,"0|i1tz7j:",383232,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"On YARN, use container-log4j.properties for executors",SPARK-1252,12704702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sandyr,sandyr,,14/Mar/14 14:09,07/Apr/14 18:30,14/Jul/23 06:25,07/Apr/14 18:30,0.9.0,,,,,,,,1.0.0,,,,,YARN,,,,,0,,,,,,YARN provides a log4j.properties file that's distinct from the NodeManager log4j.properties.  Containers are supposed to use this so that they don't try to write to the NodeManager log file.,,berngp,sandy,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383011,,,Mon Apr 07 18:29:58 UTC 2014,,,,,,,,,,"0|i1tzhz:",383279,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 18:29;tgraves;https://github.com/apache/spark/pull/148;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading comments in Spark startup scripts,SPARK-1250,12704714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sumedh,sumedh,,14/Mar/14 11:13,25/May/14 20:57,14/Jul/23 06:25,25/May/14 20:57,1.0.0,,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"A couple of the scripts in bin/ (run-example, spark-class) have the following comment in the code:

{noformat}
# Figure out where the Scala framework is installed
{noformat}

This suggests that the Scala framework is required to be installed before running Spark, which is misleading for newcomers. Instead, the comment should say:

{noformat}
# Figure out where Spark is installed
{noformat}

Will submit a pull request for this.",,sumedh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383007,,,Tue May 20 22:33:07 UTC 2014,,,,,,,,,,"0|i1tzh3:",383275,,,,,,,,,,,,,,,,,,,,,,,"20/May/14 22:33;sumedh;Submitted pull request: https://github.com/apache/spark/pull/843;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark build error with Apache Hadoop(Cloudera CDH4),SPARK-1248,12704687,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,gq,,14/Mar/14 05:03,15/Jan/15 09:08,14/Jul/23 06:25,15/Mar/14 16:46,,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"{code}
SPARK_HADOOP_VERSION=2.0.0-cdh4.5.0 SPARK_YARN=true sbt/sbt assembly -d > error.log
{code}",,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383006,,,2014-03-14 05:03:46.0,,,,,,,,,,"0|i1tzgv:",383274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log an exception if map output status message exceeds frame size,SPARK-1244,12704659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,pwendell,,13/Mar/14 21:56,30/Mar/14 04:15,14/Jul/23 06:25,17/Mar/14 14:30,,,,,,,,,0.9.1,1.0.0,,,,Spark Core,,,,,0,,,,,,"This causes a silent failure if not set correctly.
The associated PR - https://github.com/apache/spark/pull/147",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382812,,,2014-03-13 21:56:03.0,,,,,,,,,,"0|i1ty9r:",383080,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark compilation error,SPARK-1243,12704647,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Qiuzhuang,,13/Mar/14 20:33,13/Oct/14 17:55,14/Jul/23 06:25,13/Oct/14 17:55,,,,,,,,,,,,,,Build,,,,,0,,,,,,"After issuing git pull from git master, spark could not compile any longer

Here is the error message, it seems that it is related to jetty upgrade.@rxin

> 
> 
> compile
[info] Compiling 301 Scala sources and 19 Java sources to E:\projects\amplab\spark\core\target\scala-2.10\classes...
[warn] Class java.nio.channels.ReadPendingException not found - continuing with a stub.
[error] 
[error]      while compiling: E:\projects\amplab\spark\core\src\main\scala\org\apache\spark\HttpServer.scala
[error]         during phase: erasure
[error]      library version: version 2.10.3
[error]     compiler version: version 2.10.3
[error]   reconstructed args: -Xmax-classfile-name 120 -deprecation -bootclasspath C:\Java\jdk1.6.0_27\jre\lib\resources.jar;C:\Java\jdk1.6.0_27\jre\lib\rt.jar;C:\Java\jdk1.6.0_27\jre\lib\sunrsasign.jar;C:\Java\jdk1.6.0_27\jre\lib\jsse.jar;C:\Java\jdk1.6.0_27\jre\lib\jce.jar;C:\Java\jdk1.6.0_27\jre\lib\charsets.jar;C:\Java\jdk1.6.0_27\jre\lib\modules\jdk.boot.jar;C:\Java\jdk1.6.0_27\jre\classes;C:\Users\Kand\.sbt\boot\scala-2.10.3\lib\scala-library.jar -unchecked -classpath E:\projects\amplab\spark\core\target\scala-2.10\classes;E:\projects\amplab\spark\lib_managed\jars\netty-all-4.0.17.Final.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-server-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\javax.servlet-api-3.1.0.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-http-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-util-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-io-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-plus-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-webapp-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-xml-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-servlet-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-security-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\jetty-jndi-9.1.3.v20140225.jar;E:\projects\amplab\spark\lib_managed\jars\javax.servlet-2.5.0.v201103041518.jar;E:\projects\amplab\spark\lib_managed\bundles\guava-14.0.1.jar;E:\projects\amplab\spark\lib_managed\jars\jsr305-1.3.9.jar;E:\projects\amplab\spark\lib_managed\bundles\log4j-1.2.17.jar;E:\projects\amplab\spark\lib_managed\jars\slf4j-api-1.7.5.jar;E:\projects\amplab\spark\lib_managed\jars\slf4j-log4j12-1.7.5.jar;E:\projects\amplab\spark\lib_managed\jars\jul-to-slf4j-1.7.5.jar;E:\projects\amplab\spark\lib_managed\jars\jcl-over-slf4j-1.7.5.jar;E:\projects\amplab\spark\lib_managed\jars\commons-daemon-1.0.10.jar;E:\projects\amplab\spark\lib_managed\bundles\compress-lzf-1.0.0.jar;E:\projects\amplab\spark\lib_managed\bundles\snappy-java-1.0.5.jar;E:\projects\amplab\spark\lib_managed\bundles\akka-remote_2.10-2.2.3-shaded-protobuf.jar;E:\projects\amplab\spark\lib_managed\jars\akka-actor_2.10-2.2.3-shaded-protobuf.jar;E:\projects\amplab\spark\lib_managed\bundles\config-1.0.2.jar;E:\projects\amplab\spark\lib_managed\bundles\netty-3.6.6.Final.jar;E:\projects\amplab\spark\lib_managed\jars\protobuf-java-2.4.1-shaded.jar;E:\projects\amplab\spark\lib_managed\jars\uncommons-maths-1.2.2a.jar;E:\projects\amplab\spark\lib_managed\bundles\akka-slf4j_2.10-2.2.3-shaded-protobuf.jar;E:\projects\amplab\spark\lib_managed\jars\json4s-jackson_2.10-3.2.6.jar;E:\projects\amplab\spark\lib_managed\jars\json4s-core_2.10-3.2.6.jar;E:\projects\amplab\spark\lib_managed\jars\json4s-ast_2.10-3.2.6.jar;E:\projects\amplab\spark\lib_managed\jars\paranamer-2.6.jar;E:\projects\amplab\spark\lib_managed\jars\scalap-2.10.0.jar;E:\projects\amplab\spark\lib_managed\jars\scala-compiler-2.10.0.jar;E:\projects\amplab\spark\lib_managed\jars\scala-reflect-2.10.0.jar;E:\projects\amplab\spark\lib_managed\bundles\jackson-databind-2.3.0.jar;E:\projects\amplab\spark\lib_managed\bundles\jackson-annotations-2.3.0.jar;E:\projects\amplab\spark\lib_managed\bundles\jackson-core-2.3.0.jar;E:\projects\amplab\spark\lib_managed\jars\colt-1.2.0.jar;E:\projects\amplab\spark\lib_managed\jars\concurrent-1.3.4.jar;E:\projects\amplab\spark\lib_managed\jars\mesos-0.13.0.jar;E:\projects\amplab\spark\lib_managed\jars\protobuf-java-2.4.1.jar;E:\projects\amplab\spark\lib_managed\jars\commons-net-2.2.jar;E:\projects\amplab\spark\lib_managed\jars\jets3t-0.7.1.jar;E:\projects\amplab\spark\lib_managed\jars\commons-httpclient-3.1.jar;E:\projects\amplab\spark\lib_managed\jars\hadoop-client-1.0.4.jar;E:\projects\amplab\spark\lib_managed\jars\hadoop-core-1.0.4.jar;E:\projects\amplab\spark\lib_managed\jars\xmlenc-0.52.jar;E:\projects\amplab\spark\lib_managed\jars\commons-codec-1.4.jar;E:\projects\amplab\spark\lib_managed\jars\commons-math-2.1.jar;E:\projects\amplab\spark\lib_managed\jars\commons-configuration-1.6.jar;E:\projects\amplab\spark\lib_managed\jars\commons-collections-3.2.1.jar;E:\projects\amplab\spark\lib_managed\jars\commons-lang-2.4.jar;E:\projects\amplab\spark\lib_managed\jars\commons-digester-1.8.jar;E:\projects\amplab\spark\lib_managed\jars\commons-beanutils-1.7.0.jar;E:\projects\amplab\spark\lib_managed\jars\commons-beanutils-core-1.8.0.jar;E:\projects\amplab\spark\lib_managed\jars\commons-el-1.0.jar;E:\projects\amplab\spark\lib_managed\jars\hsqldb-1.8.0.10.jar;E:\projects\amplab\spark\lib_managed\jars\oro-2.0.8.jar;E:\projects\amplab\spark\lib_managed\jars\jackson-mapper-asl-1.0.1.jar;E:\projects\amplab\spark\lib_managed\jars\jackson-core-asl-1.0.1.jar;E:\projects\amplab\spark\lib_managed\bundles\curator-recipes-2.4.0.jar;E:\projects\amplab\spark\lib_managed\bundles\curator-framework-2.4.0.jar;E:\projects\amplab\spark\lib_managed\bundles\curator-client-2.4.0.jar;E:\projects\amplab\spark\lib_managed\jars\zookeeper-3.4.5.jar;E:\projects\amplab\spark\lib_managed\jars\jline-0.9.94.jar;E:\projects\amplab\spark\lib_managed\bundles\metrics-core-3.0.0.jar;E:\projects\amplab\spark\lib_managed\bundles\metrics-jvm-3.0.0.jar;E:\projects\amplab\spark\lib_managed\bundles\metrics-json-3.0.0.jar;E:\projects\amplab\spark\lib_managed\bundles\metrics-graphite-3.0.0.jar;E:\projects\amplab\spark\lib_managed\jars\chill_2.10-0.3.1.jar;E:\projects\amplab\spark\lib_managed\jars\chill-java-0.3.1.jar;E:\projects\amplab\spark\lib_managed\bundles\kryo-2.21.jar;E:\projects\amplab\spark\lib_managed\jars\reflectasm-1.07-shaded.jar;E:\projects\amplab\spark\lib_managed\jars\minlog-1.2.jar;E:\projects\amplab\spark\lib_managed\jars\objenesis-1.2.jar;E:\projects\amplab\spark\lib_managed\jars\stream-2.5.1.jar;E:\projects\amplab\spark\lib_managed\jars\fastutil-6.5.7.jar
[error] 
[error]   last tree to typer: TypeTree(class Logging$class)
[error]               symbol: class Logging$class in package spark (flags: abstract <trait> <implclass>)
[error]    symbol definition: abstract class Logging$class extends Logging
[error]                  tpe: org.apache.spark.Logging$class
[error]        symbol owners: class Logging$class -> package spark
[error]       context owners: method start -> class HttpServer -> package spark
[error] 
[error] == Enclosing template or block ==
[error] 
[error] Apply( // def setIdleTimeout(x$1: Long): Unit in class AbstractConnector
[error]   ""connector"".""setIdleTimeout"" // def setIdleTimeout(x$1: Long): Unit in class AbstractConnector
[error]   60000L
[error] )
[error] 
[error] == Expanded type of tree ==
[error] 
[error] TypeRef(TypeSymbol(abstract class Logging$class extends Logging))
[error] 
[error] uncaught exception during compilation: java.lang.AssertionError
[trace] Stack trace suppressed: run 'last core/compile:compile' for the full output.
[error] (core/compile:compile) java.lang.AssertionError: assertion failed: java.nio.channels.ReadPendingException
[error] Total time: 73 s, completed Mar 14, 2014 11:24:14 AM
",,koert,Qiuzhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382987,,,Mon Oct 13 17:55:17 UTC 2014,,,,,,,,,,"0|i1tzcn:",383255,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 14:41;koert;i see same thing with java 6, but not with java 7;;;","17/Mar/14 20:16;Qiuzhuang;Yes, it compiles successfully with JDK 7. Currently, Spark build scripts marks to use JDK  6. Should we update them to reflect to use JDK 7 instead?
;;;","13/Oct/14 17:55;srowen;This appears to be long since resolved by something else, perhaps a subsequent change to Jetty deps. I have never seen this personally, and Jenkins builds are fine.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
takeSample called on empty RDD never ends,SPARK-1240,12704622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,shinigami,,13/Mar/14 03:04,16/Mar/14 22:38,14/Jul/23 06:25,16/Mar/14 22:18,0.9.0,,,,,,,,0.9.1,1.0.0,,,,,,,,,0,,,,,,"It seems, that method takeSample ends in infinite loop if called on empty RDD, trying to collect enough samples.

val list = List\[String\](""aaa"")

val rdd = sc.parallelize(list)
rdd.takeSample(true, 1, System.nanoTime.toInt)

val empty = rdd.filter(_ => false)
empty.takeSample(true, 1, System.nanoTime.toInt)


",,codingcat,shinigami,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382998,,,Thu Mar 13 11:23:11 UTC 2014,,,,,,,,,,"0|i1tzf3:",383266,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 11:23;codingcat;Made a PR: 

https://github.com/apache/spark/pull/135;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler ignores exceptions thrown in handleTaskCompletion,SPARK-1235,12704553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,codingcat,kayousterhout,,12/Mar/14 13:05,25/Apr/14 23:06,14/Jul/23 06:25,25/Apr/14 23:06,0.9.0,0.9.1,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"If an exception gets thrown in the handleTaskCompletion method, the method exits, but the exception is caught somewhere (not clear where) and the DAGScheduler keeps running.  Jobs hang as a result -- because not all of the task completion code gets run.

This was first reported by Brad Miller on the mailing list: http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-pyspark-crash-on-mesos-td2256.html and this behavior seems to have changed since 0.8 (when, based on Brad's description, it sounds like an exception in handleTaskCompletion would cause the DAGScheduler to crash), suggesting that this may be related to the Scala 2.10.3.

To reproduce this problem, add ""throw new Exception(""foo"")"" anywhere in handleTaskCompletion and run any job locally.  The job will hang and you can see the exception get printed in the logs.",,codingcat,darabos,githubbot,kayousterhout,markhamstra,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382762,,,Fri Apr 25 23:06:08 UTC 2014,,,,,,,,,,"0|i1txyn:",383030,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 14:52;markhamstra;A big, relevant difference post-0.8 is that the DAGScheduler event loop was replaced with the eventProcessActor.  That means that an uncaught exception thrown during that actor's processing of a message will kill that actor and restart a new one (while preserving the existing message queue without the message that resulted in the prior actor's death.)  You can see some of the logging of that happening in Brad's 0.9 stack trace that shows ""ERROR OneForOneStrategy"" -- see http://doc.akka.io/docs/akka/2.0.5/general/supervision.html.

At least in current versions of Akka, the handling of the actor's death and any necessary cleanup before restarting a fresh eventProcessActor would need to be done in the preRestart and postRestart hooks -- but I'm not 100% certain that the techniques are the same when using the Akka versions that Spark does.  ;;;","19/Mar/14 18:51;codingcat;Hi, I will work on this issue

Actually the problem is more about handleTaskCompletion, if any exception happens during the event processing of DAGScheduler, the system will hang, 

we need the fault-tolerance strategy when implement the akka actor;;;","20/Mar/14 05:42;codingcat;PR: https://github.com/apache/spark/pull/186;;;","29/Mar/14 13:57;githubbot;Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/186#issuecomment-38996202
  
     Merged build triggered. Build is starting -or- tests failed to complete.
;;;","29/Mar/14 13:57;githubbot;Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/186#issuecomment-38996211
  
    Merged build started. Build is starting -or- tests failed to complete.
;;;","29/Mar/14 14:46;githubbot;Github user CodingCat commented on the pull request:

    https://github.com/apache/spark/pull/186#issuecomment-38997523
  
    It seems that when Jenkins is very busy, some weird thing can happen, in the last test, DAGScheduler even failed to create eventProcessingActor...
    
    I'm retesting it
;;;","29/Mar/14 14:50;githubbot;Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/186#issuecomment-38997636
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13572/
;;;","29/Mar/14 14:50;githubbot;Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/186#issuecomment-38997635
  
    Merged build finished. All automated tests passed.
;;;","25/Apr/14 23:06;matei;Resolved in https://github.com/apache/spark/pull/186.;;;",,,,,,,,,,,,,,,,,,,,,,,,
spark on hadoop 0.23 yarn fails to run: java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH,SPARK-1233,12704563,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,,12/Mar/14 09:05,20/Mar/14 12:17,14/Jul/23 06:25,12/Mar/14 14:57,1.0.0,,,,,,,,1.0.0,,,,,YARN,,,,,0,,,,,,"Trying to run on Yarn (hadoop 0.23) I get the following exception:

Exception in thread ""main"" java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH
        at java.lang.Class.getField(Class.java:1569)
        at org.apache.spark.deploy.yarn.ClientBase$.getDefaultMRApplicationClasspath(ClientBase.scala:417)
        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$6.apply(ClientBase.scala:393)
        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$6.apply(ClientBase.scala:393)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.deploy.yarn.ClientBase$.populateHadoopClasspath(ClientBase.scala:392)
        at org.apache.spark.deploy.yarn.ClientBase$.populateClasspath(ClientBase.scala:444)
        at org.apache.spark.deploy.yarn.ClientBase$class.setupLaunchEnv(ClientBase.scala:274)
        at org.apache.spark.deploy.yarn.Client.setupLaunchEnv(Client.scala:37)
        at org.apache.spark.deploy.yarn.Client.runApp(Client.scala:67)
        at org.apache.spark.deploy.yarn.Client.run(Client.scala:84)
        at org.apache.spark.deploy.yarn.Client$.main(Client.scala:177)
        at org.apache.spark.deploy.yarn.Client.main(Client.scala)
",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1064,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382954,,,Thu Mar 20 12:17:04 UTC 2014,,,,,,,,,,"0|i1tz5b:",383222,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 12:17;tgraves;https://github.com/apache/spark/pull/129

broken by https://github.com/apache/spark/pull/102 -> SPARK-1064;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
maven hadoop 0.23 yarn-alpha build broken,SPARK-1232,12704441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,,12/Mar/14 08:17,31/Mar/14 14:29,14/Jul/23 06:25,31/Mar/14 14:29,1.0.0,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"The maven hadoop 0.23 yarn build was broken.  It looks like the avro dependency got removed by:

https://github.com/apache/spark/pull/91/",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1193,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382952,,,Thu Mar 20 12:18:32 UTC 2014,,,,,,,,,,"0|i1tz4v:",383220,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 12:18;tgraves;broken by SPARK-1193;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add proximal gradient updater.,SPARK-1217,12705042,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ameet,,25/Aug/13 18:21,07/Apr/14 17:46,14/Jul/23 06:24,07/Apr/14 17:46,,,,,,,,,0.9.0,,,,,MLlib,,,,,0,,,,,,"Add proximal gradient updater, in particular for L1 regularization.",,ameet,jaggi,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383037,,,Mon Apr 07 16:48:27 UTC 2014,,,,,,,,,,"0|i1tznr:",383305,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 16:48;jaggi;The L1 updater is already proximal, as in the current code. Since it has no effect for L2, we could mark the issue as resolved for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clustering: Index out of bounds error,SPARK-1215,12704858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,dewshick,,17/Jan/14 05:24,27/Aug/14 08:57,14/Jul/23 06:25,17/Jul/14 22:05,,,,,,,,,1.1.0,,,,,MLlib,,,,,0,,,,,,"code:
import org.apache.spark.mllib.clustering._

val test = sc.makeRDD(Array(4,4,4,4,4).map(e => Array(e.toDouble)))
val kmeans = new KMeans().setK(4)
kmeans.run(test) evals with java.lang.ArrayIndexOutOfBoundsException

error:
14/01/17 12:35:54 INFO scheduler.DAGScheduler: Stage 25 (collectAsMap at KMeans.scala:243) finished in 0.047 s
14/01/17 12:35:54 INFO spark.SparkContext: Job finished: collectAsMap at KMeans.scala:243, took 16.389537116 s
Exception in thread ""main"" java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.simontuffs.onejar.Boot.run(Boot.java:340)
	at com.simontuffs.onejar.Boot.main(Boot.java:166)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.spark.mllib.clustering.LocalKMeans$.kMeansPlusPlus(LocalKMeans.scala:47)
	at org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:247)
	at org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.immutable.Range.foreach(Range.scala:81)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.immutable.Range.map(Range.scala:46)
	at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:244)
	at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:124)
	at Clustering$$anonfun$1.apply$mcDI$sp(Clustering.scala:21)
	at Clustering$$anonfun$1.apply(Clustering.scala:19)
	at Clustering$$anonfun$1.apply(Clustering.scala:19)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.immutable.Range.foreach(Range.scala:78)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.immutable.Range.map(Range.scala:46)
	at Clustering$.main(Clustering.scala:19)
	at Clustering.main(Clustering.scala)
	... 6 more",,dewshick,dmaverick,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2355,,,,,,SPARK-3218,,,,,,,,,,,"23/May/14 12:28;dmaverick;test.csv;https://issues.apache.org/jira/secure/attachment/12646502/test.csv",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383031,,,Thu Jul 17 22:05:30 UTC 2014,,,,,,,,,,"0|i1tzmf:",383299,,,,,,,,,,,,,1.1.0,,,,,,,,,,"15/Apr/14 16:46;mengxr;The error was due to small number of points and large k. The k-means|| initialization doesn't collect more than k candidates. This is very unlikely to appear in practice because k is much smaller than number of points. I will re-visit this issue once we implement better weighted sampling algorithms.;;;","23/May/14 12:26;dmaverick;I don't think that the problem is about size of dataset. I've faced with similar issue on dataset  with about 900 items. As a workaround we've decided to fallback with random init mode.

;;;","23/May/14 12:28;dmaverick;attach test dataset
MLLib failed to find 4 centers with k-means|| init mode on this data
;;;","14/Jul/14 19:35;josephkb;Submitted fix as PR 1407: https://github.com/apache/spark/pull/1407

Made default behavior to return k clusters still, with some duplicated;;;","17/Jul/14 22:05;mengxr;Issue resolved by pull request 1468
[https://github.com/apache/spark/pull/1468];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
0-1 labels ,SPARK-1214,12704581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,ameet,,25/Aug/13 18:25,07/Apr/14 17:45,14/Jul/23 06:25,07/Apr/14 17:45,,,,,,,,,0.9.0,,,,,MLlib,,,,,0,,,,,,"Use \{0,1\} labels for binary classification instead of {-1,1}. Advantages include:
(+) Consistency across algorithms
(+) Naturally extends to multi-class classification",,ameet,jaggi,mengxr,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383036,,,Mon Apr 07 17:45:23 UTC 2014,,,,,,,,,,"0|i1tznj:",383304,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 17:45;mengxr;Fixed in 0.9.0 or an earlier version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
loss function error of logistic loss,SPARK-1213,12704857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,xusen,,05/Jan/14 03:02,01/Apr/14 06:34,14/Jul/23 06:25,01/Apr/14 06:34,,,,,,,,,0.9.0,,,,,MLlib,,,,,0,"MLLib,",,,,,"There might be a error in incubator-spark/mllib/src/main/scala/org/apache/spark/mllib/optimizatio/Gradient.scala

The loss function of class LogisticGradient might be wrong.

The original one is :
val loss =
  if (margin > 0) {
      math.log(1 + math.exp(0 - margin))
   } else {
      math.log(1 + math.exp(margin)) - margin
   }

But when we use this kind of loss function, we will find that the loss is increasing when optimizing, such as LogisticRegressionWithSGD.

I think it should be something like this:

val loss =
      if (label > 0) {
        math.log(1 + math.exp(margin))
      } else {
        math.log(1 + math.exp(margin)) - margin
      }

I tested the loss function. It works well.
",,crazyjvm,mengxr,xusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383032,,,2014-01-05 03:02:00.0,,,,,,,,,,"0|i1tzmn:",383300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In ApplicationMaster, set spark.master system property to ""yarn-cluster""",SPARK-1211,12704830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandy,,10/Mar/14 16:18,04/Apr/14 20:50,14/Jul/23 06:25,10/Mar/14 18:41,0.9.0,,,,,,,,,,,,,YARN,,,,,0,,,,,,This would make it so that users don't need to pass it in to their SparkConf.  It won't break anything for apps that already pass it in.,,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383028,,,Mon Mar 10 16:30:01 UTC 2014,,,,,,,,,,"0|i1tzlr:",383296,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 16:30;sandy;https://github.com/apache/spark/pull/118;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent ContextClassLoader of Actor from becoming ClassLoader of Executor,SPARK-1210,12704906,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ueshin,ueshin,,10/Mar/14 01:34,28/Mar/14 10:32,14/Jul/23 06:25,27/Mar/14 22:18,0.9.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,0,,,,,,"Constructor of {{org.apache.spark.executor.Executor}} should not set context class loader of current thread, which is backend Actor's thread.

Run the following code in local-mode REPL.

{quote}
scala> case class Foo(i: Int)
scala> val ret = sc.parallelize((1 to 100).map(Foo), 10).collect
{quote}

This causes errors as follows:

{quote}
ERROR actor.OneForOneStrategy: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo;
java.lang.ArrayStoreException: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo;
     at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)
     at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870)
     at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870)
     at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)
     at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859)
     at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616)
     at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
     at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
     at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
     at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
     at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{quote}

This is because the class loaders to deserialize result {{Foo}} instances might be different from backend Actor's, and the Actor's class loader should be the same as Driver's.

See PR: https://github.com/apache/spark/pull/15",,patrick,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1152,,,,,,SPARK-1346,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382874,,,Fri Mar 28 10:32:05 UTC 2014,,,,,,,,,,"0|i1tynj:",383142,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 10:28;patrick;\[edited from an earlier comment\]

Just to clarify there are two different issues going on here. One is that the classloader used in the executors is not set-up to correctly delegate to the active classloader in the thread that created it. Instead it delegates to the class loader of the class that defines Executor.scala. This is incorrect delegation and results in a problem where, when running in local mode, the Executor things there isn't an existing definition of a case class defined in the repl (e.g. Foo) and it goes and gets one over the HTTP served and ends up with a technically different definition of Foo. When that ends up back in the driver code it thinks the types are different and freaks out.

A second issue is that there is code that, when the executor is created, sets the ClassLoader of  the thread in which the Executor is created. This is not actually needed because user classes can only be created/referenced inside of the run() method of a TaskRunner. Also in some cases, such as when an Executor is created inside of an Actor with multiple threads, this leads to an inconstent state amongst threads in which executor code is executed. This can have bad consequences when the Executor and the DAGScheduler are sharing an ActorSystem because the DAGScheduler's classloader can get redefined. The code was added here:
https://github.com/apache/spark/commit/b864c36a#diff-40f975518e47d34cc8ecd7a49440228dR50;;;","27/Mar/14 22:19;patrick;I merged this into master. This might be worth back porting into 0.9 at some point.;;;","28/Mar/14 10:32;patrick;I've created SPARK-1346 to track back-porting this into 0.9.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SparkHadoop{MapRed,MapReduce}Util should not use package org.apache.hadoop",SPARK-1209,12704846,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,sandyr,,09/Mar/14 22:19,15/Jan/15 09:08,14/Jul/23 06:25,30/Oct/14 22:57,0.9.0,,,,,,,,1.2.0,,,,,Spark Core,,,,,1,,,,,,"It's private, so the change won't break compatibility",,apachespark,mgrover,pwendell,sandy,sandyr,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383047,,,Sat Nov 01 11:44:55 UTC 2014,,,,,,,,,,"0|i1tzpz:",383315,,,,,,,,,,,,,1.2.0,,,,,,,,,,"09/Mar/14 22:24;mgrover;I did notice the same thing and did start some work on it:
https://github.com/markgrover/spark/compare/master...package_fix

I haven't tried compiling it yet but I will do so soonish. If you don't mind, I'd love to get this JIRA assigned to me.
Thanks for filing it!;;;","09/Mar/14 22:29;mgrover;Thanks Sandy!;;;","19/Jun/14 01:33;sandyr;It doesn't look like this was actually fixed.;;;","20/Jun/14 03:41;mgrover;ok, I will take over. Thanks Sandy.;;;","13/Oct/14 19:50;srowen;Yes, I wonder too, does SparkHadoopMapRedUtil and SparkHadoopMapReduceUtil need to live in {{org.apache.hadoop}} anymore? I assume they may have in the past to access some package-private Hadoop code. But I've tried moving them under {{org.apache.spark}} and compiling versus a few Hadoop versions and it all seems fine.

Am I missing something or is this worth changing? it's private to Spark (well, org.apache right now by necessity) so think it's fair game to move.

See https://github.com/srowen/spark/tree/SPARK-1209;;;","13/Oct/14 19:57;sandyr;Definitely worth changing, in my opinion.  This has been bothering me for a while;;;","14/Oct/14 00:36;pwendell;It's possible this previously used package-private code that it doesn't need any more. I agree it's better not to pollute the Hadoop namespace, but I also think many people use this and I don't think it would be justified to break this API for the purpose of cleanliness of our own code. IIRC we tried to make this private[spark] earlier but people asked to open it up.

I think it would be okay to deprecate the old one and add forwarder methods to a new thing inside of the spark package. But wholesale moving it IMO isn't justified for the purpose of cleanliness alone.;;;","14/Oct/14 00:38;pwendell;Actually a full set of forwarders might be overkill anyways since there's not much code.;;;","14/Oct/14 07:17;srowen;Hm, it's {{private[apache]}} though. Couldn't this only be used by people writing in the {{org.apache}} namespace? naturally a project might do just this to access this code, but I hadn't though this was promised as an stable API. People that pull this trick can I suppose declare their hack in {{org.apache.spark}}, although that's a source change.

I can set up a forwarder and deprecate to see how that looks but wanted to check if it's really these classes in question that are being used outside Spark.;;;","15/Oct/14 15:37;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2814;;;","15/Oct/14 15:50;pwendell;Hey Sean the class I thought this pertained to was SparkHadoopUtil:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala;;;","15/Oct/14 16:44;srowen;... and why wouldn't you, that's the title of the JIRA, oops. It's not that class that moves or even changes actually, and yes it should not move. Let me fix the title and fix my PR too. Maybe that's a more palatable change.;;;","16/Oct/14 03:17;pwendell;Got it! Yeah so anything that is internal, of course would be great to move it to the Spark namespace if it's not necessary to be in Hadoop.;;;","01/Nov/14 11:44;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3048;;;",,,,,,,,,,,,,,,,,,,
after some hours of working the :4040 monitoring UI stops working.,SPARK-1208,12704826,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sliwo,,08/Mar/14 23:28,29/Sep/14 07:52,14/Jul/23 06:25,29/Sep/14 07:52,0.9.0,,,,,,,,,,,,,Web UI,,,,,0,,,,,,"
This issue is inconsistent, but it did not exist in prior versions.
The Driver app otherwise works normally.

The log file below is from the driver.


2014-03-09 07:24:55,837 WARN  [qtp1187052686-17453] AbstractHttpConnection  - /stages/
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:313)
        at scala.None$.get(Option.scala:311)
        at org.apache.spark.ui.jobs.StageTable.org$apache$spark$ui$jobs$StageTable$$stageRow(StageTable.scala:114)
        at org.apache.spark.ui.jobs.StageTable$$anonfun$toNodeSeq$1.apply(StageTable.scala:39)
        at org.apache.spark.ui.jobs.StageTable$$anonfun$toNodeSeq$1.apply(StageTable.scala:39)
        at org.apache.spark.ui.jobs.StageTable$$anonfun$stageTable$1.apply(StageTable.scala:57)
        at org.apache.spark.ui.jobs.StageTable$$anonfun$stageTable$1.apply(StageTable.scala:57)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.ui.jobs.StageTable.stageTable(StageTable.scala:57)
        at org.apache.spark.ui.jobs.StageTable.toNodeSeq(StageTable.scala:39)
        at org.apache.spark.ui.jobs.IndexPage.render(IndexPage.scala:81)
        at org.apache.spark.ui.jobs.JobProgressUI$$anonfun$getHandlers$3.apply(JobProgressUI.scala:59)
        at org.apache.spark.ui.jobs.JobProgressUI$$anonfun$getHandlers$3.apply(JobProgressUI.scala:59)
        at org.apache.spark.ui.JettyUtils$$anon$1.handle(JettyUtils.scala:61)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1040)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:976)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:363)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:628)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:662)
",,codingcat,patrick,sliwo,tgraves,tsliwowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2643,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383046,,,Mon Sep 29 07:52:45 UTC 2014,,,,,,,,,,"0|i1tzpr:",383314,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/14 17:33;patrick;Hey just have a few questions to help narrow this down:

- Are you using the fair scheduler here?
- Does this happen when a large number of stages is present (e.g. 1000+)?
- Do you get an exception one time, every time, or intermittantly when you access the jobs page?

Looking at the code it looks like it can't correctly lookup the pool name. In theory the deletion of the stage itself and the entry in the lookup table corresponding to the stage should be atomic, but maybe one is somehow happening without the other.

;;;","10/Mar/14 00:57;sliwo;Hi,
1. Yes - the FAIR scheduler, although I am almost positive this happened in FIFO (the default) too. If important, we can switch to FIFO and see.
2. Yes (10s of thousands, our job runs repeatedly on the same context), although this is issue is inconsistent. In most cases, even with a very large number of stages, this does not happen.
3. The issue is not easily reproducible. It does not occur always (I do not have a scenario to make it start happening), but once it starts happening, it is totally consistent in the sense that it happens always.;;;","16/Apr/14 09:01;tsliwowicz;This issue does not happen with the FIFO scheduler. ;;;","29/Sep/14 07:52;srowen;This appears to be a similar, if not the same issue, as in SPARK-2643. The discussion in the PR indicates this was resolved by a subsequent change: https://github.com/apache/spark/pull/1854#issuecomment-55061571;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell on yarn-client race in properly getting hdfs delegation tokens,SPARK-1203,12704809,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,,07/Mar/14 11:31,09/Apr/15 12:05,14/Jul/23 06:25,19/Mar/14 08:07,1.0.0,,,,,,,,0.9.1,1.0.0,,,,YARN,,,,,0,,,,,,"There seems to be a race when using the spark-shell on yarn (yarn-client mode) as to when it gets the hdfs delegation tokens.  

It appears to be that if  you do an action that causes it to get a delegation token before the workers are launched things work fine, if you want until after the workers launch you get an error about not having a delegation token.

",,bcwalrus,gu chi,sandy,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382962,,,Thu Apr 09 12:05:50 UTC 2015,,,,,,,,,,"0|i1tz73:",383230,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 11:57;tgraves;So this appears to have been something environment related.  Or perhaps the KDC was having issues.  I can't reproduce it now.;;;","14/Mar/14 18:20;bcwalrus;I just hit something like this, an intermittent failure with HdfsTest in yarn-client mode. Tom, does your error look like this:

{noformat}
14/03/14 17:57:15 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
         appMasterRpcPort: 0
         appStartTime: 1394845025101
         yarnAppState: RUNNING

14/03/14 17:57:17 INFO cluster.YarnClientClusterScheduler: YarnClientClusterScheduler.postStartHook done
14/03/14 17:57:17 INFO storage.MemoryStore: ensureFreeSpace(215427) called with curMem=0, maxMem=1383491174
14/03/14 17:57:17 INFO storage.MemoryStore: Block broadcast_0 stored as values to memory (estimated size 210.4 KB, free 1319.2 MB)
Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6211)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:461)
        ...
        at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:920)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1336)
        at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:527)
        at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:505)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202)
{noformat};;;","17/Mar/14 06:12;tgraves;I'll give it a try again today to see if I can reproduce also.;;;","18/Mar/14 08:17;tgraves;Ok I'm able to reproduce it again.  Debbugging it.;;;","18/Mar/14 12:03;tgraves;So I was able to reproduce this issue when doing a saveAsTextFile on the client.  If you wait for a while before doing anything I was hitting this.  If you do operations immediately it doesn't happen.    It looks like we aren't propogating the credentials properly. 

I have not been able to reproduce it running HdfsTest.  That is a different case since your reading the file.  That should be propogating the credentials in SparkContext.hadoopRDD.  if you have more details please let me know.   You are sure you were kinit'd?;;;","18/Mar/14 12:41;tgraves;Note the reason this works if you do it quickly is because the original connection to the namenode is up and it reuses it.;;;","18/Mar/14 12:44;tgraves;https://github.com/apache/spark/pull/173;;;","19/Mar/14 13:38;bcwalrus;I found out why I was having problem with HdfsTest. I was integrating on 0.9 and missed the changes that set YARN_CLIENT_MODE in a couple of places. I got that working. And I also verified the fix for this bug. Thanks a lot, Tom!;;;","09/Apr/15 12:05;gu chi;Seems I meet with the similar issue using saveAsNewAPIHadoopDataset
I tried to reproduce with create SC and then sleep for 10mins, then do saveAsNewAPIHadoopDataset, still not able to reproduce, can u give some advice on how to reproduce, thx;;;",,,,,,,,,,,,,,,,,,,,,,,,
Type mismatch in Spark shell when using case class defined in shell,SPARK-1199,12704888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,prashant,andrewkerr,,06/Mar/14 12:51,15/Jun/16 18:08,14/Jul/23 06:25,04/Jul/14 07:06,0.9.0,,,,,,,,1.1.0,,,,,Spark Core,,,,,1,,,,,,"*NOTE: This issue was fixed in 1.0.1, but the fix was reverted in Spark 1.0.2 pending further testing. The final fix will be in Spark 1.1.0.*

Define a class in the shell:
{code}
case class TestClass(a:String)
{code}

and an RDD
{code}
val data = sc.parallelize(Seq(""a"")).map(TestClass(_))
{code}

define a function on it and map over the RDD
{code}
def itemFunc(a:TestClass):TestClass = a
data.map(itemFunc)
{code}

Error:
{code}
<console>:19: error: type mismatch;
 found   : TestClass => TestClass
 required: TestClass => ?
              data.map(itemFunc)
{code}

Similarly with a mapPartitions:
{code}
def partitionFunc(a:Iterator[TestClass]):Iterator[TestClass] = a
data.mapPartitions(partitionFunc)
{code}

{code}
<console>:19: error: type mismatch;
 found   : Iterator[TestClass] => Iterator[TestClass]
 required: Iterator[TestClass] => Iterator[?]
Error occurred in an application involving default arguments.
              data.mapPartitions(partitionFunc)
{code}

The behavior is the same whether in local mode or on a cluster.

This isn't specific to RDDs. A Scala collection in the Spark shell has the same problem.

{code}
scala> Seq(TestClass(""foo"")).map(itemFunc)
<console>:15: error: type mismatch;
 found   : TestClass => TestClass
 required: TestClass => ?
              Seq(TestClass(""foo"")).map(itemFunc)
                                        ^
{code}

When run in the Scala console (not the Spark shell) there are no type mismatch errors.",,andrea.ferretti,andrewkerr,apachespark,berngp,fe2s,glenn.strycker@gmail.com,jkleckner,joshrosen,marmbrus,michaelmalak,pkolaczk,prashant,pwendell,yhuai,,,,,,,,,,,,,,,,,SPARK-1836,SPARK-2330,,,,,SPARK-15964,,,SPARK-2452,,,SPARK-2576,,SPARK-5149,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382941,,,Wed Feb 24 13:41:02 UTC 2016,,,,,,,,,,"0|i1tz2f:",383209,,,,,,,,,,,,,1.0.1,1.1.0,,,,,,,,,"06/Mar/14 13:13;joshrosen;This looks similar to the problem reported in https://groups.google.com/d/msg/spark-users/bwAmbUgxWrA/HwP4Nv4adfEJ;;;","13/Mar/14 04:26;andrewkerr;The workaround from the above link doesn't help.

{code}
object TestClasses {
case class TestClass(a:String)
}
import TestClasses._
def itemFunc(a:TestClass):TestClass = a
Seq(TestClass(""foo"")).map(itemFunc)
{code}

{code}
<console>:22: error: type mismatch;
 found   : TestClasses.TestClass => TestClasses.TestClass
 required: TestClasses.TestClass => ?
              data.map(itemFunc)
                       ^
{code}

From the Spark shell both locally and on a cluster.;;;","21/Mar/14 14:42;marmbrus;I think I may be hitting a related issue when trying to created nested classes for use in SparkSQL.

{code}
scala> case class A(a: String)
defined class A
scala> case class B(b: A)
defined class B
scala> B(A(""a""))
<console>:22: error: type mismatch;
 found   : A
 required: A
              B(A(""a""))
                 ^
{code}

In this case, the described workaround does work, but is kind of annoying.  Mostly I want to bump this thread since Matei had talked about maybe fixing this using macros?  Is that something we would like to consider for a future release.;;;","25/Apr/14 07:26;pkolaczk;+1 to fixing this. We're affected as well. Classes defined in Shell are inner classes, and therefore cannot be easily instantiated by reflection. They need additional reference to the outer object, which is non-trivial to obtain (is it obtainable at all without modifying Spark?). 

{noformat}
scala> class Test
defined class Test

scala> new Test
res5: Test = $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test@4f755864

// good, so there is a default constructor and we can call it through reflection?
// not so fast...
scala> classOf[Test].getConstructor()
java.lang.NoSuchMethodException: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test.<init>()
...

scala> classOf[Test].getConstructors()(0)
res7: java.lang.reflect.Constructor[_] = public $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test($iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)
       
{noformat}

The workaround does not work for us.
;;;","25/Apr/14 16:01;andrewkerr;I have something of a workaround:

{code}
object MyTypes {
  case class TestClass(a:Int)
}

object MyLogic {
  import MyClasses._
  def fn(b:TestClass) = TestClass(b.a * 2)
  val result = Seq(TestClass(1)).map(fn)
}

MyLogic.result
// Seq[MyTypes.TestClass] = List(TestClass(2))
{code}

Still can't access TestClass outside an object.;;;","28/May/14 19:39;michaelmalak;See also additional test cases in https://issues.apache.org/jira/browse/SPARK-1836 which has now been marked as a duplicate.;;;","17/Jun/14 17:34;pwendell;Prashant said he could look into this - so I'm assigning it to him.;;;","24/Jun/14 06:59;prashant;One work around is to use `:paste` command of repl to work with these kind of scenarios. So if you use :paste and put the whole thing at once it will work nicely. I am just mentioning it because I found it, we also have a slightly better fix on github PR. ;;;","03/Jul/14 09:59;andrea.ferretti;More examples on https://issues.apache.org/jira/browse/SPARK-2330 which should also be a duplicate;;;","04/Jul/14 07:06;pwendell;Resolved via:
https://github.com/apache/spark/pull/1179;;;","21/Jul/14 18:53;pwendell;Just a note, I've reverted this fix in branch-1.0 the fix here caused other issues that were worse than the original bug (SPARK-2452). This will be fixed in 1.1.;;;","10/Dec/15 15:06;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/1179;;;","10/Dec/15 15:06;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/1176;;;","23/Feb/16 18:07;fe2s;I have problems with declaring case classes in shell, Spark 1.6

This doesn't work for me:

{code}
scala> case class ABCD()
defined class ABCD

scala> new ABCD()
res33: ABCD = ABCD()

scala> classOf[ABCD].getConstructor()
java.lang.NoSuchMethodException: $iwC$$iwC$ABCD.<init>()
 at java.lang.Class.getConstructor0(Class.java:3074)
 at java.lang.Class.getConstructor(Class.java:1817)

scala> classOf[ABCD].getConstructors()
res31: Array[java.lang.reflect.Constructor[_]] = Array(public $iwC$$iwC$ABCD($iwC$$iwC))
{code};;;","23/Feb/16 18:19;marmbrus;All classes defined in the REPL are inner classes due to the way compilation works.  Therefore there is not going to be a no-arg constructor.  This is expected behavior.;;;","23/Feb/16 19:01;fe2s; Michael Armbrust,
in my use case I have a library that relies on having a default constructor and I want to use this library in the REPL. Any workaround for that?;;;","23/Feb/16 19:02;marmbrus;You will have to define your case classes in a jar instead of the REPL.;;;","23/Feb/16 19:09;fe2s;Thanks, any other options? I want to be able to define classes in the REPL.;;;","23/Feb/16 19:14;marmbrus;Not that I know of.  Also, please use the spark-user list instead of JIRA for tech support questions :);;;","24/Feb/16 03:59;prashant;Did you try the :paste option ?;;;","24/Feb/16 13:41;fe2s;Yes, I did. It doesn't help, the inner class still doesn't have a no-arg constructor visible with a reflection.;;;",,,,,,,,,,,,
Rename yarn-standalone and fix up docs for running on YARN,SPARK-1197,12704772,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandy,,06/Mar/14 12:31,04/Apr/14 20:50,14/Jul/23 06:25,06/Mar/14 17:26,0.9.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"yarn-standalone is a confusing name because the use of ""standalone"" is different than the use in the sense of Spark standalone cluster manager.  It would also be nice to fix up some typos in the YARN docs and add a section on how to view container logs.",,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383063,,,Thu Mar 06 12:36:54 UTC 2014,,,,,,,,,,"0|i1tztj:",383331,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 12:36;sandy;https://github.com/apache/spark/pull/95;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The same-RDD rule for cache replacement is not properly implemented,SPARK-1194,12704760,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liancheng,liancheng,,06/Mar/14 07:45,07/Mar/14 23:28,14/Jul/23 06:25,07/Mar/14 23:28,0.9.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,2,,,,,,"The same-RDD rule for cache replacement described in the original RDD paper prevents cycling partitions from the same RDD in and out. [Commit 6098f7e|https://github.com/apache/spark/commit/6098f7e87a88d0b847c402b95510cb07352db643#diff-f82f9761c5f006f4b2a7efb57ccb7699R182] meant to implement this, but I believe it introduced some problems.

In the current implementation, when selecting candidate blocks to be swapped out, once we find a block from the same RDD that the block to be stored belongs to, [cache eviction fails  and aborts|https://github.com/apache/spark/commit/6098f7e87a88d0b847c402b95510cb07352db643#diff-f82f9761c5f006f4b2a7efb57ccb7699R185]. -Also, LRU eviction (as described in the paper) is not employed.-

A possible cache eviction strategy can be: keep selecting blocks _not_ from the RDD that the block to be stored belongs to until either enough free space can be ensured (cache eviction succeeds) or all such blocks are checked (cache eviction fails).

-LRU should also be employed, but not necessarily in this issue.-

Any thoughts? Especially, did I miss any apparent facts behind current implementation?

*Update*: LRU is implemented with {{LinkedHashMap}} by setting constructor argument {{accessOrder}} to {{true}}.",,codingcat,crazyjvm,liancheng,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383057,,,Thu Mar 06 11:27:53 UTC 2014,,,,,,,,,,"0|i1tzs7:",383325,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 11:27;codingcat;Is it possible to make the replacement policy as pluggable?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent indendation between pom.xmls,SPARK-1193,12704722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandy,,05/Mar/14 18:01,04/Apr/14 20:50,14/Jul/23 06:25,08/Mar/14 03:22,0.9.0,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,e.g. core/pom.xml uses 4 spaces and graphx/pom.xml uses 2,,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1232,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382951,,,Wed Mar 05 20:59:40 UTC 2014,,,,,,,,,,"0|i1tz4n:",383219,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 20:59;sandy;https://github.com/apache/spark/pull/91;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not initialize log4j if slf4j log4j backend is not being used,SPARK-1190,12704823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,,05/Mar/14 11:09,06/Oct/14 04:29,14/Jul/23 06:25,08/Mar/14 16:03,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Spark Core,,,,,0,,,,,,"I already have a patch here just need to test it and commit. IIRC there were some issues with the maven build.

https://github.com/apache/incubator-spark/pull/573
https://github.com/pwendell/incubator-spark/commit/66594e88e5be50fca073a7ef38fa62db4082b3c8

initialization with: java.lang.StackOverflowError
	at java.lang.ThreadLocal.access$400(ThreadLocal.java:72)
	at java.lang.ThreadLocal$ThreadLocalMap.getEntry(ThreadLocal.java:376)
	at java.lang.ThreadLocal$ThreadLocalMap.access$000(ThreadLocal.java:261)
	at java.lang.ThreadLocal.get(ThreadLocal.java:146)
	at java.lang.StringCoding.deref(StringCoding.java:63)
	at java.lang.StringCoding.encode(StringCoding.java:330)
	at java.lang.String.getBytes(String.java:916)
	at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
	at java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:242)
	at java.io.File.exists(File.java:813)
	at sun.misc.URLClassPath$FileLoader.getResource(URLClassPath.java:1080)
	at sun.misc.URLClassPath$FileLoader.findResource(URLClassPath.java:1047)
	at sun.misc.URLClassPath.findResource(URLClassPath.java:176)
	at java.net.URLClassLoader$2.run(URLClassLoader.java:551)
	at java.net.URLClassLoader$2.run(URLClassLoader.java:549)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findResource(URLClassLoader.java:548)
	at java.lang.ClassLoader.getResource(ClassLoader.java:1147)
	at org.apache.spark.Logging$class.initializeLogging(Logging.scala:109)
	at org.apache.spark.Logging$class.initializeIfNecessary(Logging.scala:97)
	at org.apache.spark.Logging$class.log(Logging.scala:36)
	at org.apache.spark.util.Utils$.log(Utils.scala:47)",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1067,,,,,,,,,SPARK-3782,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382830,,,2014-03-05 11:09:16.0,,,,,,,,,,"0|i1tydr:",383098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphX triplets not working properly,SPARK-1188,12704872,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,darabos,k0alak0der,,05/Mar/14 00:13,29/May/14 08:20,14/Jul/23 06:25,23/Apr/14 18:41,0.9.0,,,,,,,,0.9.2,1.0.0,,,,GraphX,,,,,0,,,,,,"I followed the GraphX tutorial at http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html 

on a local stand-alone cluster (Spark version 0.9.0) with two workers. Somehow, the graph.triplets is not returning what it should -- only Eds and Frans.

```
scala> graph.edges.toArray
14/03/04 16:15:57 INFO SparkContext: Starting job: collect at EdgeRDD.scala:51
14/03/04 16:15:57 INFO DAGScheduler: Got job 5 (collect at EdgeRDD.scala:51) with 1 output partitions (allowLocal=false)
14/03/04 16:15:57 INFO DAGScheduler: Final stage: Stage 27 (collect at EdgeRDD.scala:51)
14/03/04 16:15:57 INFO DAGScheduler: Parents of final stage: List()
14/03/04 16:15:57 INFO DAGScheduler: Missing parents: List()
14/03/04 16:15:57 INFO DAGScheduler: Submitting Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51), which has no missing parents
14/03/04 16:15:57 INFO DAGScheduler: Submitting 1 missing tasks from Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51)
14/03/04 16:15:57 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
14/03/04 16:15:57 INFO TaskSetManager: Starting task 27.0:0 as TID 11 on executor localhost: localhost (PROCESS_LOCAL)
14/03/04 16:15:57 INFO TaskSetManager: Serialized task 27.0:0 as 2068 bytes in 1 ms
14/03/04 16:15:57 INFO Executor: Running task ID 11
14/03/04 16:15:57 INFO BlockManager: Found block rdd_2_0 locally
14/03/04 16:15:57 INFO Executor: Serialized size of result for 11 is 936
14/03/04 16:15:57 INFO Executor: Sending result for 11 directly to driver
14/03/04 16:15:57 INFO Executor: Finished task ID 11
14/03/04 16:15:57 INFO TaskSetManager: Finished TID 11 in 13 ms on localhost (progress: 0/1)
14/03/04 16:15:57 INFO DAGScheduler: Completed ResultTask(27, 0)
14/03/04 16:15:57 INFO TaskSchedulerImpl: Remove TaskSet 27.0 from pool
14/03/04 16:15:57 INFO DAGScheduler: Stage 27 (collect at EdgeRDD.scala:51) finished in 0.015 s
14/03/04 16:15:57 INFO SparkContext: Job finished: collect at EdgeRDD.scala:51, took 0.023602266 s
res7: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,7), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3))


scala> graph.vertices.toArray
14/03/04 16:16:18 INFO SparkContext: Starting job: toArray at <console>:27
14/03/04 16:16:18 INFO DAGScheduler: Got job 6 (toArray at <console>:27) with 1 output partitions (allowLocal=false)
14/03/04 16:16:18 INFO DAGScheduler: Final stage: Stage 28 (toArray at <console>:27)
14/03/04 16:16:18 INFO DAGScheduler: Parents of final stage: List(Stage 32, Stage 29)
14/03/04 16:16:18 INFO DAGScheduler: Missing parents: List()
14/03/04 16:16:18 INFO DAGScheduler: Submitting Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52), which has no missing parents
14/03/04 16:16:18 INFO DAGScheduler: Submitting 1 missing tasks from Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52)
14/03/04 16:16:18 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
14/03/04 16:16:18 INFO TaskSetManager: Starting task 28.0:0 as TID 12 on executor localhost: localhost (PROCESS_LOCAL)
14/03/04 16:16:18 INFO TaskSetManager: Serialized task 28.0:0 as 2426 bytes in 0 ms
14/03/04 16:16:18 INFO Executor: Running task ID 12
14/03/04 16:16:18 INFO BlockManager: Found block rdd_14_0 locally
14/03/04 16:16:18 INFO Executor: Serialized size of result for 12 is 947
14/03/04 16:16:18 INFO Executor: Sending result for 12 directly to driver
14/03/04 16:16:18 INFO Executor: Finished task ID 12
14/03/04 16:16:18 INFO TaskSetManager: Finished TID 12 in 13 ms on localhost (progress: 0/1)
14/03/04 16:16:18 INFO DAGScheduler: Completed ResultTask(28, 0)
14/03/04 16:16:18 INFO TaskSchedulerImpl: Remove TaskSet 28.0 from pool
14/03/04 16:16:18 INFO DAGScheduler: Stage 28 (toArray at <console>:27) finished in 0.015 s
14/03/04 16:16:18 INFO SparkContext: Job finished: toArray at <console>:27, took 0.027839851 s
res9: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((4,(David,42)), (2,(Bob,27)), (6,(Fran,50)), (5,(Ed,55)), (3,(Charlie,65)), (1,(Alice,28)))


scala> graph.triplets.toArray
14/03/04 16:16:30 INFO SparkContext: Starting job: toArray at <console>:27
14/03/04 16:16:30 INFO DAGScheduler: Got job 7 (toArray at <console>:27) with 1 output partitions (allowLocal=false)
14/03/04 16:16:31 INFO DAGScheduler: Final stage: Stage 33 (toArray at <console>:27)
14/03/04 16:16:31 INFO DAGScheduler: Parents of final stage: List(Stage 34)
14/03/04 16:16:31 INFO DAGScheduler: Missing parents: List()
14/03/04 16:16:31 INFO DAGScheduler: Submitting Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60), which has no missing parents
14/03/04 16:16:31 INFO DAGScheduler: Submitting 1 missing tasks from Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60)
14/03/04 16:16:31 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
14/03/04 16:16:31 INFO TaskSetManager: Starting task 33.0:0 as TID 13 on executor localhost: localhost (PROCESS_LOCAL)
14/03/04 16:16:31 INFO TaskSetManager: Serialized task 33.0:0 as 3322 bytes in 1 ms
14/03/04 16:16:31 INFO Executor: Running task ID 13
14/03/04 16:16:31 INFO BlockManager: Found block rdd_2_0 locally
14/03/04 16:16:31 INFO BlockManager: Found block rdd_31_0 locally
14/03/04 16:16:31 INFO Executor: Serialized size of result for 13 is 931
14/03/04 16:16:31 INFO Executor: Sending result for 13 directly to driver
14/03/04 16:16:31 INFO Executor: Finished task ID 13
14/03/04 16:16:31 INFO TaskSetManager: Finished TID 13 in 17 ms on localhost (progress: 0/1)
14/03/04 16:16:31 INFO DAGScheduler: Completed ResultTask(33, 0)
14/03/04 16:16:31 INFO TaskSchedulerImpl: Remove TaskSet 33.0 from pool
14/03/04 16:16:31 INFO DAGScheduler: Stage 33 (toArray at <console>:27) finished in 0.019 s
14/03/04 16:16:31 INFO SparkContext: Job finished: toArray at <console>:27, took 0.037909394 s
res10: Array[org.apache.spark.graphx.EdgeTriplet[(String, Int),Int]] = Array(((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3))
```
",,darabos,k0alak0der,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1883,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382877,,,Mon May 19 20:28:45 UTC 2014,,,,,,,,,,"0|i1tyo7:",383145,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 00:25;rxin;The problem is that we are reusing an EdgeTriplet object. Try force a copy before you do the collect.

e.g.

triplets.map(_.copy).collect();;;","13/Mar/14 17:42;darabos;This has bitten us as well. I am no expert, but my impression is that re-using an object in these iterators was a terrible idea.

Consider this example:

scala> val g = graphx.util.GraphGenerators.starGraph(sc, 5)
scala> g.edges.collect
Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1))
scala> g.edges.map(x => x).collect
Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1))

It can and does go very wrong in practice too. Consider this:

scala> g.edges.saveAsObjectFile(""edges"")
scala> sc.objectFile[graphx.Edge[Int]](""edges"").collect
Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1))

Indeed map(_.copy) is a good workaround. But seems like this is a nasty trap laid out for your users. Did you measure the performance gain? If not, or if it's insignificant, I would suggest not re-using the object in the iterators. It would simplify the GraphX source too. However, if it is significant, one idea is to offer separate mapFast() methods that do re-use, while the more commonly used map() would offer the more commonly expected (copying) semantics.

Also I wish GraphX offered graph save/load functionality. If it did, I would only have discovered this bug a few weeks from now :).;;;","19/Mar/14 06:32;darabos;Sorry, I forgot to test your workaround before commenting. It does not work.

scala> g.triplets.map(_.copy).collect
<console>:16: error: missing arguments for method copy in class Edge;
follow this method with `_' if you want to treat it as a partially applied function
              g.triplets.map(_.copy).collect
                               ^
You can of course write a copy function yourself.

I'd be happy to work on this. Can I just get rid of the re-use, or would you prefer another approach?;;;","28/Mar/14 08:19;darabos;Okay, it was just missing the parentheses:

scala> g.triplets.map(_.copy()).collect
res1: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1))

(Scala novice here :).);;;","31/Mar/14 11:13;darabos;I've sent a pull request to eliminate the re-use (https://github.com/apache/spark/pull/276). The change has no performance impact and simplifies the code.;;;","23/Apr/14 09:29;darabos;The changes are in the master branch now. I can't figure out how to close a JIRA ticket :).;;;","23/Apr/14 18:41;rxin;I added you to contributor list so you should be able to edit in the future. Cheers.;;;","19/May/14 20:28;rxin;Adding a link to the commit: https://github.com/apache/spark/commit/78236334e4ca7518b6d7d9b38464dbbda854a777#diff-a2b19aac11cb2fbe9962b5d2290ea77e;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"In Spark Programming Guide, ""Master URLs"" should mention yarn-client",SPARK-1185,12704545,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandy,sandy,,04/Mar/14 14:42,06/Nov/14 17:42,14/Jul/23 06:25,06/Nov/14 17:42,0.9.0,,,,,,,,,,,,,Documentation,,,,,0,,,,,,It would also be helpful to mention that the reason a host:post isn't required for YARN mode is that it comes from the Hadoop configuration.,,berngp,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383078,,,2014-03-04 14:42:07.0,,,,,,,,,,"0|i1tzwv:",383346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the distribution tar.gz to include spark-assembly jar,SPARK-1184,12704524,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgrover,mgrover,,04/Mar/14 14:30,18/Apr/14 21:07,14/Jul/23 06:25,18/Apr/14 21:07,0.9.0,,,,,,,,0.9.0,,,,,Build,,,,,0,,,,,,"This JIRA tracks 2 things:
1. There seems to be something going on in our assembly generation logic because of which are two assembly jars.
Something like:
{code}spark-assembly_2.10-1.0.0-SNAPSHOT.jar{code}
and 
{code}spark-assembly_2.10-1.0.0-SNAPSHOT-hadoop2.0.5-alpha.jar{code}

The former is pretty bogus and doesn't contain any class files and should be gotten rid of. The latter contains all the good stuff. It essentially is the uber jar generated by the maven-shade-plugin

2. The current bigtop-dist profile that builds the maven assembly (a .tar.gz file) using the maven-assembly-plugin includes the bogus jar and not the legit spark-assembly jar. We should get rid of the first one from this assembly (which would happen when we fix #1) and put the legit uber jar in it.

3. Also, the bigtop-dist profile is meant to exclude the hadoop related jars from the distribution. It does a good job of doing so for org.apache.hadoop jars but misses the avro and zookeeper jars that are also provided by hadoop land.",,gkesavan,mgrover,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383079,,,Fri Apr 18 21:07:01 UTC 2014,,,,,,,,,,"0|i1tzx3:",383347,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/14 21:07;mgrover;Committed quite a while ago:
https://github.com/apache/spark/commit/cda381f88cc03340fdf7b2d681699babbae2a56e

Resolving;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Inconsistent meaning of ""worker"" in docs",SPARK-1183,12704536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandy,,04/Mar/14 12:13,04/Apr/14 20:50,14/Jul/23 06:25,13/Mar/14 12:42,,,,,,,,,1.0.0,,,,,Documentation,,,,,0,,,,,,"In the Spark Standalone Mode docs, ""worker"" refers to the long-running slave processes that start executor processes.  In most other docs, ""worker"" refers to the executor processes themselves.",,sandy,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383016,,,Tue Mar 04 12:19:28 UTC 2014,,,,,,,,,,"0|i1tzj3:",383284,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/14 12:19;sandy;The best thing to me would be to remove the word worker wherever possible.  I.e. refer to ""executor"" or ""slave"".  If master/slave isn't PC, we could use lord/vassal instead?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"on shutting down a long running job, the cluster does not accept new jobs and gets hung",SPARK-1175,12704724,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,sliwo,,03/Mar/14 22:27,04/May/14 08:12,14/Jul/23 06:25,04/May/14 08:12,0.8.1,0.9.0,,,,,,,1.0.0,,,,,Spark Core,,,,,0,shutdown,worker,,,,"When shutting down a long processing job (24+ hours) that runs periodically on the same context and generates a lot of shuffles (many hundreds of GB) the spark workers get hung for a long while and the cluster does not accept new jobs. The only way to proceed is to kill -9 the workers.
This is a big problem because when multiple contexts run on the same cluster, one mast stop them all for a simple restart.
The context is stopped using sc.stop()
This happens both in standalone mode and under mesos.

We suspect this is caused by the ""delete Spark local dirs"" thread. Attached a thread dump of the worker. Also, the relevant part may be:

""SIGTERM handler"" - Thread t@41040
   java.lang.Thread.State: BLOCKED
	at java.lang.Shutdown.exit(Shutdown.java:168)
	- waiting to lock <69eab6a3> (a java.lang.Class) owned by ""SIGTERM handler"" t@41038
	at java.lang.Terminator$1.handle(Terminator.java:35)
	at sun.misc.Signal$1.run(Signal.java:195)
	at java.lang.Thread.run(Thread.java:662)

   Locked ownable synchronizers:
	- None

""delete Spark local dirs"" - Thread t@40
   java.lang.Thread.State: RUNNABLE
	at java.io.UnixFileSystem.delete0(Native Method)
	at java.io.UnixFileSystem.delete(UnixFileSystem.java:251)
	at java.io.File.delete(File.java:904)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:482)
	at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:479)
	at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:478)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:478)
	at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:479)
	at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:478)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:478)
	at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(DiskBlockManager.scala:141)
	at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(DiskBlockManager.scala:139)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.storage.DiskBlockManager$$anon$1.run(DiskBlockManager.scala:139)

   Locked ownable synchronizers:
	- None

""SIGTERM handler"" - Thread t@41038
   java.lang.Thread.State: WAITING
	at java.lang.Object.wait(Native Method)
	- waiting on <355c6c8d> (a org.apache.spark.storage.DiskBlockManager$$anon$1)
	at java.lang.Thread.join(Thread.java:1186)
	at java.lang.Thread.join(Thread.java:1239)
	at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:79)
	at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:24)
	at java.lang.Shutdown.runHooks(Shutdown.java:79)
	at java.lang.Shutdown.sequence(Shutdown.java:123)
	at java.lang.Shutdown.exit(Shutdown.java:168)
	- locked <69eab6a3> (a java.lang.Class)
	at java.lang.Terminator$1.handle(Terminator.java:35)
	at sun.misc.Signal$1.run(Signal.java:195)
	at java.lang.Thread.run(Thread.java:662)

   Locked ownable synchronizers:
	- None

",,codingcat,glenn.strycker@gmail.com,sliwo,tsliwowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383072,,,Sun May 04 08:11:43 UTC 2014,,,,,,,,,,"0|i1tzvj:",383340,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/14 04:58;codingcat;it is related to https://spark-project.atlassian.net/browse/SPARK-1104?

;;;","04/Mar/14 05:07;sliwo;Seems related, but not sure - symptoms are the same though.;;;","04/Mar/14 16:42;codingcat;I spent some time to look at the code, here is my basic analysis, I think these two issues are closely related

BlockManager and many other components are started for each Executor, while the Executor object is created by CoarseExecuterBackend. Many shutdown hooks are registered for all of them...

CoarseExecutorBackend is actually started in ExecutorRunner, where   a workerThread is created to start CoarseExecutorBackend process through command line (specified in appDesc) (the process is handled by a process object inside ExecutorRunner)

However, once the process is killed, i.e. process.destroy() is called, a lot of cleanup work is triggered. The current implementation calls ExecutorRunner.kill() in Worker thread, causing the blocking of worker on shutdown of some application. (what confuses me is since the shutdownhooks are started in separate thread, why the worker is still blocked? release buffer in stdout and stderr takes time?)
;;;","04/Mar/14 16:51;codingcat;or, process.destroy() is a blocking method, it only returns after shutdown thread returns.....

In my PR, I moved calling of that method to the workerThread, I think it should resolve the issue

;;;","04/Mar/14 22:36;sliwo;Great!
Can you point me to the PR? Is it - https://github.com/apache/spark/pull/35 ?;;;","05/Mar/14 05:24;codingcat;yes, that's it;;;","16/Apr/14 09:02;tsliwowicz;Do you think it will be included in 1.0?;;;","16/Apr/14 10:17;codingcat;It has been there for a long time...I'm not sure...and obviously, I need to refresh it......;;;","16/Apr/14 10:20;tsliwowicz;This prevents us from having a real automated solution for fail over when the driver fails. We cannot automatically start a new driver because spark is stuck on cleanup.;;;","16/Apr/14 10:22;codingcat;BTW, did you see workers as DEAD in the UI?;;;","16/Apr/14 15:04;tsliwowicz;Yes;;;","04/May/14 08:11;codingcat;this should have been fixed in https://github.com/apache/spark/commit/f99af8529b6969986f0c3e03f6ff9b7bb9d53ece;;;",,,,,,,,,,,,,,,,,,,,,
"when executor is removed, we should reduce totalCores instead of just freeCores on that executor ",SPARK-1171,12704430,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,,02/Mar/14 20:32,05/Mar/14 14:03,14/Jul/23 06:25,05/Mar/14 14:01,0.9.0,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"When the executor is removed, the current implementation will only reduce the freeCores on that executor. Actually we should reduce the totalCores...

The impact of this bug is that the default parallelism of a job may be set incorrectly after an executor is removed (so an RDD may get split into more partitions than the amount of parallelism in the cluster).  In other words, the bug leads to performance degredation but no correctness problem.",,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383069,,,Sun Mar 02 20:40:42 UTC 2014,,,,,,,,,,"0|i1tzuv:",383337,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 20:40;codingcat;proposed a PR: https://github.com/apache/spark/pull/63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove metrics-ganglia from default build due to LGPL issue,SPARK-1167,12704759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,patrick,pwendell,,02/Mar/14 20:16,30/Mar/14 04:13,14/Jul/23 06:25,11/Mar/14 11:55,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Spark Core,,,,,0,,,,,,"The metrics ganglia code depends on an LGPL library which we can't distribute with Spark. More information can be found here:
https://groups.google.com/forum/#!searchin/metrics-user/lgpl/metrics-user/1tQd_qZHQNE/TqAfXYwh7OUJ

We should isolate this code in a separate module inside of an `/extras` folder and have a build flag in Maven/SBT (e.g. `-Pganglia`) that will pull this in if desired. That way users can still use it if they do a special build.",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382831,,,2014-03-02 20:16:28.0,,,,,,,,,,"0|i1tydz:",383099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky RateLimitedOutputStreamSuite,SPARK-1158,12704736,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,,02/Mar/14 16:25,19/Dec/14 16:16,14/Jul/23 06:25,03/Mar/14 21:25,,,,,,,,,1.0.0,,,,,,,,,,0,flaky-test,,,,,,,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383086,,,2014-03-02 16:25:45.0,,,,,,,,,,"0|i1tzyn:",383354,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow spark-ec2 to login to a cluster with 0 slaves,SPARK-1156,12704667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,codingcat,nchammas,,01/Mar/14 21:14,07/Mar/14 07:23,14/Jul/23 06:25,07/Mar/14 07:23,0.9.0,,,,,,,,,,,,,EC2,,,,,0,,,,,,"{{spark-ec2}} allows you to launch a cluster with no slaves. 

However, if you try to login to such a cluster, you get the following error:

{code}
Searching for existing cluster <cluster-name>...
Found 1 master(s), 0 slaves
ERROR: Could not find slaves in group <cluster-name>-slaves
{code}

{{spark-ec2}} should allow you to connect to a cluster with no slaves.",,codingcat,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383062,,,Sun Mar 02 18:31:45 UTC 2014,,,,,,,,,,"0|i1tztb:",383330,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 18:31;codingcat;PR: https://github.com/apache/spark/pull/58;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dev merge_spark_pr.py still references incubator-spark,SPARK-1151,12704670,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,,28/Feb/14 09:04,28/Feb/14 18:28,14/Jul/23 06:25,28/Feb/14 18:28,1.0.0,,,,,,,,1.0.0,,,,,Project Infra,,,,,0,,,,,,"We need to update the script to use spark.git instead of incubator-spark.git
",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383105,,,2014-02-28 09:04:11.0,,,,,,,,,,"0|i1u02v:",383373,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repo location in create_release script out of date,SPARK-1150,12704692,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,codingcat,codingcat,,28/Feb/14 07:56,01/Mar/14 17:27,14/Jul/23 06:25,01/Mar/14 17:27,,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,repo location in create_release script out of date,,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383103,,,Fri Feb 28 09:44:05 UTC 2014,,,,,,,,,,"0|i1u02f:",383371,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 09:44;codingcat;https://github.com/apache/spark/pull/48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad partitioners can cause Spark to hang,SPARK-1149,12704710,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,xoltar,,27/Feb/14 21:00,13/Oct/14 18:05,14/Jul/23 06:25,13/Oct/14 18:05,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"While implementing a unit test for lookup, I accidentally created a situation where a partitioner returned a partition number that was outside its range. It should have returned 0 or 1, but in the last case, it returned a -1. 

Rather than reporting the problem via an exception, Spark simply hangs during the unit test run.

We should catch this bad behavior by partitioners and throw an exception.

test(""lookup with bad partitioner"") {
    val pairs = sc.parallelize(Array((1,2), (3,4), (5,6), (5,7)))

    val p = new Partitioner {
      def numPartitions: Int = 2

      def getPartition(key: Any): Int = key.hashCode() % 2
    }
    val shuffled = pairs.partitionBy(p)

    assert(shuffled.partitioner === Some(p))
    assert(shuffled.lookup(1) === Seq(2))
    assert(shuffled.lookup(5) === Seq(6,7))
    assert(shuffled.lookup(-1) === Seq())
  }",,gq,xoltar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383109,,,Mon Oct 13 18:05:14 UTC 2014,,,,,,,,,,"0|i1u03r:",383377,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 01:18;gq;I opened a pull request:[#44|https://github.com/apache/spark/pull/44] . Spark should not to hang.;;;","13/Oct/14 18:05;srowen;Looks like Patrick merged this into master in March. It might have been fixed for ... 1.0?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory mapping with many small blocks can cause JVM allocation failures,SPARK-1145,12704679,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,,27/Feb/14 00:20,11/Dec/17 12:59,14/Jul/23 06:25,28/Apr/14 00:41,0.9.0,,,,,,,,0.9.2,1.0.0,,,,Spark Core,,,,,0,,,,,,"During a shuffle each block or block segment is memory mapped to a file. When the segments are very small and there are a large number of them, the memory maps can start failing and eventually the JVM will terminate. It's not clear exactly what's happening but it appears that when the JVM terminates about 265MB of virtual address space is used by memory mapped files. This doesn't seem affected at all by `-XXmaxdirectmemorysize` - AFAIK that option is just to give the JVM its own self imposed limit rather than allow it to run into OS limits. 

At the time of JVM failure it appears the overall OS memory becomes scarce, so it's possible there are overheads for each memory mapped file that are adding up here. One overhead is that the memory mapping occurs at the granularity of pages, so if blocks are really small there is natural overhead required to pad to the page boundary.

In the particular case where I saw this, the JVM was running 4 reducers, each of which was trying to access about 30,000 blocks for a total of 120,000 concurrent reads. At about 65,000 open files it crapped out. In this case each file was about 1000 bytes.

User should really be coalescing or using fewer reducers if they have 1000 byte shuffle files, but I expect this to happen nonetheless. My proposal was that if the file is smaller than a few pages, we should just read it into a java buffer and not bother to memory map it. Memory mapping huge numbers of small files in the JVM is neither recommended or good for performance, AFAIK.

Below is the stack trace:
{code}
14/02/27 08:32:35 ERROR storage.BlockManagerWorker: Exception handling buffer message
java.io.IOException: Map failed
  at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)
  at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:89)
  at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:285)
  at org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:90)
  at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:69)
  at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)
  at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
  at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
  at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28)
  at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:44)
  at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)
  at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)
  at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:512)
  at org.apache.spark.network.ConnectionManager$$anon$8.run(ConnectionManager.scala:478)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
{code}

And the JVM error log had a bunch of entries like this:

{code}
7f4b48f89000-7f4b48f8a000 r--s 00000000 ca:30 1622077901                 /mnt4/spark/spark-local-20140227020022-227c/26/shuffle_0_22312_38
7f4b48f8a000-7f4b48f8b000 r--s 00000000 ca:20 545892715                  /mnt3/spark/spark-local-20140227020022-5ef5/3a/shuffle_0_26808_20
7f4b48f8b000-7f4b48f8c000 r--s 00000000 ca:50 1622480741                 /mnt2/spark/spark-local-20140227020022-315b/1c/shuffle_0_29013_19
7f4b48f8c000-7f4b48f8d000 r--s 00000000 ca:30 10082610                   /mnt4/spark/spark-local-20140227020022-227c/3b/shuffle_0_28002_9
7f4b48f8d000-7f4b48f8e000 r--s 00000000 ca:50 1622268539                 /mnt2/spark/spark-local-20140227020022-315b/3e/shuffle_0_23983_17
7f4b48f8e000-7f4b48f8f000 r--s 00000000 ca:50 1083068239                 /mnt2/spark/spark-local-20140227020022-315b/37/shuffle_0_25505_22
7f4b48f8f000-7f4b48f90000 r--s 00000000 ca:30 9921006                    /mnt4/spark/spark-local-20140227020022-227c/31/shuffle_0_24072_95
7f4b48f90000-7f4b48f91000 r--s 00000000 ca:50 10441349                   /mnt2/spark/spark-local-20140227020022-315b/20/shuffle_0_27409_47
7f4b48f91000-7f4b48f92000 r--s 00000000 ca:50 10406042                   /mnt2/spark/spark-local-20140227020022-315b/0e/shuffle_0_26481_84
7f4b48f92000-7f4b48f93000 r--s 00000000 ca:50 1622268192                 /mnt2/spark/spark-local-20140227020022-315b/14/shuffle_0_23818_92
7f4b48f93000-7f4b48f94000 r--s 00000000 ca:50 1082957628                 /mnt2/spark/spark-local-20140227020022-315b/09/shuffle_0_22824_45
7f4b48f94000-7f4b48f95000 r--s 00000000 ca:20 1082199965                 /mnt3/spark/spark-local-20140227020022-5ef5/00/shuffle_0_1429_13
7f4b48f95000-7f4b48f96000 r--s 00000000 ca:20 10940995                   /mnt3/spark/spark-local-20140227020022-5ef5/38/shuffle_0_28705_44
7f4b48f96000-7f4b48f97000 r--s 00000000 ca:10 17456971                   /mnt/spark/spark-local-20140227020022-b372/28/shuffle_0_23055_72
7f4b48f97000-7f4b48f98000 r--s 00000000 ca:30 9853895                    /mnt4/spark/spark-local-20140227020022-227c/08/shuffle_0_22797_42
7f4b48f98000-7f4b48f99000 r--s 00000000 ca:20 1622089728                 /mnt3/spark/spark-local-20140227020022-5ef5/27/shuffle_0_24017_97
7f4b48f99000-7f4b48f9a000 r--s 00000000 ca:50 1082937570                 /mnt2/spark/spark-local-20140227020022-315b/24/shuffle_0_22291_38
7f4b48f9a000-7f4b48f9b000 r--s 00000000 ca:30 10056604                   /mnt4/spark/spark-local-20140227020022-227c/2f/shuffle_0_27408_59
{code}",,ijuma,patrick,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383113,,,Mon Apr 28 00:41:22 UTC 2014,,,,,,,,,,"0|i1u04n:",383381,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/14 00:41;pwendell;Issue resolved by pull request 43
[https://github.com/apache/spark/pull/43];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterSchedulerSuite (soon to be TaskSchedulerImplSuite) does not actually test the ClusterScheduler/TaskSchedulerImpl,SPARK-1143,12704652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,,26/Feb/14 17:15,09/Jan/15 17:47,14/Jul/23 06:25,09/Jan/15 17:47,,,,,,,,,,,,,,,,,,,0,,,,,,This test should probably be both refactored and renamed -- it really tests the Pool / fair scheduling mechanisms and completely bypasses the scheduling code in TaskSchedulerImpl and TaskSetManager.,,apachespark,codingcat,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383059,,,Fri Jan 09 06:12:30 UTC 2015,,,,,,,,,,"0|i1tzsn:",383327,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/14 22:24;codingcat;made  a PR: https://github.com/apache/spark/pull/339
;;;","09/Jan/15 06:12;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/3967;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZK Persistence Engine crashes if stored data has wrong serialVersionUID,SPARK-1137,12704573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,,25/Feb/14 21:47,25/Jul/18 20:44,14/Jul/23 06:25,15/Apr/14 03:33,0.9.0,,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"The ZooKeeperPersistenceEngine contains information about concurrently existing Masters and Workers. This information, as the name suggests, is persistent in the event of a Master failure/restart. If the Spark version is upgraded, the Master will crash with a Java serialization exception when trying to re-read the persisted data.

Instead of crashing (indefinitely), the Master should probably just ignore the prior data.",,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383121,,,Wed Jul 25 20:44:08 UTC 2018,,,,,,,,,,"0|i1u06f:",383389,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/18 20:44;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix FaultToleranceTest for Docker 0.8.1,SPARK-1136,12704584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ilikerps,ilikerps,,25/Feb/14 21:42,15/Apr/14 03:32,14/Jul/23 06:25,15/Apr/14 03:32,1.0.0,,,,,,,,,,,,,Build,,,,,0,,,,,,"Several changes were made between Docker 0.6 (when our spark-test docker files were created) and the current version of Docker, 0.8.1. There are two relevant to the FaultToleranceTest that causes it to fail:

1) A random host name is assigned to Docker containers. This host name, unlike the IP address, is not reachable from outside the container, but by default we'll try to use it as the Worker's Akka host. This fails when a newly-elected Master attempts to recover a Worker, since the Worker is not actually reachable at the host address it connected from.

2) IP addresses are now reassigned immediately upon container recycling. This means that we can confuse ""old"" and ""new"" Workers or Masters that happened to be assigned the same IP address. The main obvious issue that arises is when a Worker gets a ""attempted to re-register"" exception when it takes on a previous Worker's IP address during Master recovery.",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383118,,,2014-02-25 21:42:53.0,,,,,,,,,,"0|i1u05r:",383386,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anchors broken in latest docs due to bad JavaScript code,SPARK-1135,12704566,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,matei,matei,,25/Feb/14 17:22,21/Mar/14 00:12,14/Jul/23 06:25,21/Mar/14 00:12,,,,,,,,,0.9.1,,,,,Documentation,,,,,0,,,,,,"A recent PR that added Java vs Scala tabs for streaming also inadvertently added some bad code to a document.ready handler, breaking our other handler that manages scrolling to anchors correctly with the floating top bar. As a result the section title ended up always being hidden below the top bar.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382944,,,2014-02-25 17:22:30.0,,,,,,,,,,"0|i1tz33:",383212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ipython won't run standalone python script,SPARK-1134,12704690,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dcarroll@cloudera.com,dcarroll@cloudera.com,,25/Feb/14 07:39,03/Apr/14 22:50,14/Jul/23 06:25,03/Apr/14 22:50,0.9.0,0.9.1,,,,,,,0.9.2,1.0.0,,,,PySpark,,,,,0,pyspark,,,,,"Using Spark 0.9.0, python 2.6.6, and ipython 1.1.0.

The problem: If I want to run a python script as a standalone app, the docs say I should execute the command ""pyspark myscript.py"".  This works as long as IPYTHON=0.  But if IPYTHON=1 this doesn't work.

This problem arose for me because I tried to save myself typing by setting IPYTHON=1 in my shell profile script. Which then meant I was unable to execute pyspark standalone scripts.

My analysis: 
in the pyspark script, command line arguments are simply ignored if ipython is used:
{code}if [[ ""$IPYTHON"" = ""1"" ]] ; then
  exec ipython $IPYTHON_OPTS
else
  exec ""$PYSPARK_PYTHON"" ""$@""
fi{code}

I thought I could get around this by changing the script to pass $@.  However, this doesn't work: doing so results in an error saying multiple spark contexts can't be run at once.

This is because of a feature?/bug? of ipython related to the PYTHONSTARTUP environment variable.  the pyspark script sets this variable to point to the python/shell.py script, which initializes the Spark Context.  In regular python, the PYTHONSTARTUP script runs ONLY if python is invoked in interactive mode; if run with a script, it ignores the variable.  iPython runs that script every time, regardless.  Which means it will always execute Spark's shell.py script to initialize the spark context even when it was invoked with a script.

Proposed solution:
short term: add this information to the Spark docs regarding iPython.  Something like ""Note, iPython can only be used interactively.  Use regular Python to execute pyspark script files.""
long term: change the pyspark script to tell if arguments are passed in; if so, just call python instead of pyspark, or don't set the PYTHONSTARTUP variable?  Or maybe fix shell.py to detect if it's being invoked in non-interactively and not initialize sc.
",,dcarroll@cloudera.com,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382911,,,Tue Mar 25 09:27:10 UTC 2014,,,,,,,,,,"0|i1tyvr:",383179,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/14 08:18;dcarroll@cloudera.com;Just tested this change to pyspark script file and it seems to work:

{code}if [[ ""$IPYTHON"" = ""1"" && $# = 0 ]] ; then
  exec ipython $IPYTHON_OPTS
else
  exec ""$PYSPARK_PYTHON"" ""$@""
fi{code};;;","25/Feb/14 15:41;matei;This sounds good, do you want to send in a pull request? Otherwise someone else can fix it as suggested.;;;","27/Feb/14 14:29;dcarroll@cloudera.com;Well, I would if I could.  I tried following the advice and tutorial here:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
But when I attempted to push, I apparently lack the access rights to do
that.  Sorry.



On Tue, Feb 25, 2014 at 6:42 PM, Matei Zaharia (JIRA) <

;;;","25/Mar/14 09:27;dcarroll@cloudera.com;I was able to submit a pull request: 
https://github.com/apache/spark/pull/227 (which makes this fix and also removes IPYTHONOPTS)
(the original pull request was https://github.com/apache/spark/pull/83);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use a predefined seed when seed is zero in XORShiftRandom,SPARK-1129,12704561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,,24/Feb/14 18:10,18/Mar/14 19:30,14/Jul/23 06:25,18/Mar/14 19:30,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"If the seed is zero, XORShift generates all zeros, which would create unexpected result.",,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382972,,,Tue Mar 18 19:30:08 UTC 2014,,,,,,,,,,"0|i1tz9b:",383240,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/14 19:27;mengxr;PR: https://github.com/apache/incubator-spark/pull/645;;;","18/Mar/14 19:28;mengxr;Merged.;;;","18/Mar/14 19:30;mengxr;To mark resolved.;;;","18/Mar/14 19:30;mengxr;Merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hadoop task properties not set while using InputFormat,SPARK-1128,12704531,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,costin.leau,,24/Feb/14 11:55,24/Mar/14 21:56,14/Jul/23 06:25,24/Mar/14 21:56,0.9.0,,,,,,,,1.0.0,,,,,Input/Output,,,,,0,,,,,,"The task specific Hadoop properties, in particular `mapred.task.id` and `mapred.tip.id` (or `mapred.task.partition`) are not set when calling `InputFormat#getRecordReader`.
Implementations that rely on such properties will fail at this point as no information about the current task environment is provided even though the job is 'theoretically' in progress.

I've noticed `SparkHadoopWriter.scala` sets this properties - it would be nice to have the same thing applied for reading data as well.

Thanks!",,codingcat,costin.leau,kasha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382917,,,Fri Mar 07 16:40:25 UTC 2014,,,,,,,,,,"0|i1tyx3:",383185,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 16:40;codingcat;https://github.com/apache/spark/pull/101;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-submit script for running compiled binaries,SPARK-1126,12704526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sandyr,sandyryza,,24/Feb/14 00:42,30/Mar/14 05:40,14/Jul/23 06:25,30/Mar/14 05:40,0.9.0,,,,,,,,1.0.0,,,,,Spark Core,,,,,1,,,,,,"It would be useful to have a script, roughly similar to the ""hadoop jar"" command that is used for running compiled binaries against Spark.

The script would do two things:
* Set up the Spark classpath on the client side, so that users don't need to know where Spark jars are installed or bundle all of Spark inside their app jar.
* Provide a layer over the different modes that apps can be run, so that the same spark-jar invocation could run the driver inside a YARN application master or in the client process, depending on the cluster setup.",,bcwalrus,berngp,crazyjvm,githubbot,matei,patrick,sandy,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1318,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382784,,,Sat Mar 29 22:31:41 UTC 2014,,,,,,,,,,"0|i1ty3j:",383052,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 15:52;matei;Just as another requirement, we might want the same script to work for Python. It can be called spark-app or spark-submit or something.;;;","24/Feb/14 17:40;sandy;That makes sense to me. So I'm thinking:

spark-app <app jar> <main class> [<args>]

where args are:
worker-memory - Memory requested from scheduler per executor.
worker-cores - Cores requested from scheduler per executor.
num-workers - Number of executors.
master-memory - Memory requested from scheduler for driver.  Only applies on yarn-standalone mode.
master-cores - Memory requested from scheduler for driver.  Only applies on yarn-standalone mode.
master-max-heap - Max heap size for the driver JVM.
deploy-mode - yarn-client, yarn-standalone, standalone-standalone, or standalone-client (could maybe use better names here, the confusing thing is that ""standalone"" refers to both a cluster manager and a deploy mode)
supervise - Whether to automatically restart driver on failure.  Only works on standalone-standalone mode, though we should be able to add support for this in yarn-standalone as well.
add-jars - Additional jars that should be on the driver and executor classpaths.
files - Files to place next to all executors.  Only works in yarn-standalone and yarn-client mode.
archives - Archives to extract next to all executors.  Only works in yarn-standalone and yarn-client mode.
queue - Queue/pool to submit the application to.  Only works in yarn-standalone and yarn-client mode.
args - Arguments to pass to the driver.

It would be nice for deploy-mode to be settable by an environment variable.  Passing the option would override it.  This would allow cluster operators to set up a default deploy mode for their cluster and not require users to think about it.  A user could specify a particular deploy mode if it matters to them.

Because many of these don't apply to every mode, it's also worth considering some sort of mechanism for delegating options down to particular modes.  But I think this might be hairier and not add much.
;;;","25/Feb/14 23:29;matei;Hey Sandy, a few comments here:

- As I mentioned above, we probably want this to work for Python too in a complete design. In that case you'd probably take either a .py file or a .egg.

- I think deploy-mode should be separated into two pieces: a cluster URL and a flag for whether to run the driver on the cluster versus locally. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here.

- Supported deploy modes should include Mesos (maybe you don't allow running the driver in Mesos right now, but it can certainly be outside as a client).

- Some of these flags overlap with settings you can put in your SparkConf object today, such as worker memory, cluster URL (if that becomes a flag) and to some extent JARs. How will these be passed through? One option is to set the Java system properties for them when you execute the user's app (e.g. -Dspark.executor.memory=2g), which are going to populate the Conf by default unless the user overwrites them in their program. I don't know how much of an issue the latter will be but we can create a workaround if it is.

- The master-memory and master-cores should really be called driver-memory and driver-cores, and they apply to in-cluster submission on the standalone mode too.

- If we want this to become the standard, the script should also work on Windows. This makes it considerably hairier, to the point where we might want this to be a Scala class, though in that case the JVM startup overhead for launching it is kind of painful.

Anyway, I do think this would be a great feature to have, but before you implement it, it would be great to see a more detailed design that takes into account these points. In particular the main thing I'm worried about is creating inconsistency across languages, operating systems or deployment modes. If we make this change and update all the docs on deploying applications and such, it would be nice to only have to make it once.;;;","26/Feb/14 11:05;sandy;Thanks for taking a look Matei.  I attached a design doc with an amended version of what I posted above.

bq. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here.
In the YARN case, the cluster URL encapsulates both the cluster manager and the deploy mode (client vs. standalone), while in the standalone case, it includes only the former.  The design splits these apart for spark-app's arguments, but I wanted to highlight this.

Another thing I wanted to ask about was memory configuraton.  In Hadoop, the memory requested as a cap from the cluster manager is controlled separately from the memory given as max heap opts to the JVM.  While this is clunky for a lot of reasons, an advantage is that it allows accounting for a process using memory off-heap, either through direct buffers or by forking a subprocess.  If Spark wants to handle these situations, it might make sense to eventually make the amount of padding between the JVM heap and requested memory configurable?

Working on non-Linux platforms will be very difficult if the script is written in bash.  Even across Linux and Macs, there appears to be no shared utility for parsing long-form options.  A python script could avoid the JVM startup overhead.  I suppose this adds a python dependency, but most platforms now include python by default.  Scala also sounds reasonable to me if that makes the most sense to you.;;;","26/Feb/14 14:01;patrick;[~sandy] Hey Sandy - is it not possible to just manually parse the options in bash rather than use a library? Second, what are the semantics of addJars, is this going to add a jar where the driver program is (and then jars are distributed through the normal path through Spark)? Just want to be clear because there is also addJar inside of Spark context... ;;;","26/Feb/14 15:12;sandy;I suppose bash is turing complete, but it would be very painful and error-prone to use it for this.  It also wouldn't solve the Windows issue.

yarn-standalone mode supports an addJars parameter, which will place local jars on the cluster and add them to the YARN distributed cache so that they can be localized for containers.  I misunderstood and thought that other deploy modes had a similar way of specifying jars to add via command line.  So we can just document that it only works for yarn-standalone mode.  To promote consistency, it could also make sense to add environment variables that would allow other deploy modes to add jars outside of code.  We could also possibly reform the functionality in yarn-standalone mode somehow.
;;;","26/Feb/14 16:14;matei;Python isn't available by default on Windows, so we probably can't use that.

Regarding the cluster URL, it's okay if we change URL formats slightly as part of this feature so that you use ""yarn"" for both in-cluster and out-of-cluster clients, but you have a separate flag for ""run the client in the cluster"". I don't think we should combine these two properties (what type of cluster is it and do I want the driver inside) into one flag, because you just end up with flags that are all possible combinations of the two features.

Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs. It's okay if the option is only for YARN at first but it will be very confusing for users if they have to submit one way to YARN and another way to standalone clusters, so I'd look into adding that.;;;","26/Feb/14 17:36;sandy;bq. Regarding the cluster URL, it's okay if we change URL formats slightly as part of this feature
By this, do you mean changing the yarn URL formats in existing code or just as interpreted by the spark-app script?  I.e. should this still work: ""MASTER=yarn-client ./bin/spark-shell""?

bq. Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs.
Cool, I filed SPARK-1142 for this work.;;;","28/Feb/14 16:02;sandy;Would it be best to just go with Scala?  In non-python cases, we can avoid extra JVM startup time by running the user class in-process instead of forking a new JVM.;;;","02/Mar/14 20:23;matei;Yes, I think it's fine to do it in Scala.;;;","06/Mar/14 14:06;berngp;Is there any feature-branch that is covering this work. Will like to contribute.;;;","06/Mar/14 14:07;sandy;Here's the pull request: https://github.com/apache/spark/pull/86;;;","06/Mar/14 17:13;berngp;Thanks, will look into it too.;;;","29/Mar/14 21:30;githubbot;Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/86#discussion_r11095306
  
    --- Diff: docs/cluster-overview.md ---
    @@ -50,6 +50,47 @@ The system currently supports three cluster managers:
     In addition, Spark's [EC2 launch scripts](ec2-scripts.html) make it easy to launch a standalone
     cluster on Amazon EC2.
     
    +# Launching Applications
    --- End diff --
    
    Alright let's punt this to a broader doc clean-up for 1.0 which we can do during the QA phase. I think that ideally yes, we should replace all of the mentions of the other clients with this.
;;;","29/Mar/14 21:41;githubbot;Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/86#issuecomment-39009977
  
    Hey @sryza I'm going to submit a PR with some suggested follow-on changes, but I think we can go ahead and merge this for now as a starting point. Thanks for your work on this!
;;;","29/Mar/14 22:31;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/86
;;;",,,,,,,,,,,,,,,,,
The maven build error for Spark Examples,SPARK-1125,12704522,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gq,,24/Feb/14 00:17,20/Aug/14 08:21,14/Jul/23 06:25,20/Aug/14 08:21,,,,,,,,,,,,,,,,,,,0,,,,,," mvn -v
Apache Maven 3.1.1 (0728685237757ffbf44136acec0402957f723d9a; 2013-09-17 23:22:22+0800)
Maven home: /usr/local/Cellar/maven/3.1.1/libexec
Java version: 1.7.0_51, vendor: Oracle Corporation
Java home: /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre
Default locale: zh_CN, platform encoding: UTF-8
OS name: ""mac os x"", version: ""10.9.1"", arch: ""x86_64"", family: ""mac""",,gq,sowen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383023,,,Wed Mar 12 08:04:50 UTC 2014,,,,,,,,,,"0|i1tzkn:",383291,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 02:34;gq;This error occurs when the maven uses a http proxy. ;;;","24/Feb/14 05:59;sowen;I see you reopened the pull request, but haven't you found that it's just your proxy interfering? then it's nothing to do with Spark, and you already found the Maven settings to make it work, which are not necessary for everyone else that is not on your network.;;;","24/Feb/14 06:15;gq;When someone adds a http proxy configuration maven, he'll get an error when compiling the Spark. We can not determine all of the configuration http proxy, but we can guarantee configure http proxy maven compile spark will not appear this error.;;;","26/Feb/14 23:24;sowen;It's accurate to say that, if your proxy is breaking HTTPS connections, and you do not configure workarounds in Maven, you will get an error from any project that accesses a repo over HTTPS. Your change in the pull request does not fix this problem. This is not something for which some general configuration would resolve the issue.;;;","27/Feb/14 00:49;gq;maven settings.xml :
{code:xml}   
    <proxy>
      <id>optional</id>
      <active>true</active>
      <protocol>http</protocol>
      <host>127.0.0.1</host>
      <port>8087</port>
      <nonProxyHosts>localhost|127.0.0.1|local</nonProxyHosts>
    </proxy>
 {code}

do this
{code}
 mvn -U -Pyarn -Dhadoop.version=2.3.0 -Dyarn.version=2.3.0 compile  -X >> http_proxy.txt
{code};;;","27/Feb/14 01:00;sowen;If I put this in my settings.xml, my build completely fails. I am sure it makes your build work, so you should set this. But this is not something that is an issue with Spark that needs to be patched.;;;","27/Feb/14 04:53;gq;In China, no proxy or vpn is not connected *maven.twttr.com* , therefore there are a lot of people need this http proxy configuratio in settings.xml;;;","10/Mar/14 00:50;gq;Building the current master using Maven has the same compiler error
{code}
git checkout 5d98cfc1c8fb17fbbeacc7192ac21c0b038cbd16 
mvn -U  -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean  install 
{code};;;","12/Mar/14 08:04;gq;Apache Maven 3.2.1 can work.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Infinite NullPointerException failures due to a null in map output locations,SPARK-1124,12704499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,,23/Feb/14 19:51,26/Feb/14 09:56,14/Jul/23 06:25,24/Feb/14 17:04,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Spark Core,,,,,0,,,,,,"The following spark-shell code leads to an infinite retry of the last stage in Spark 0.9:

{code}
val data = sc.parallelize(1 to 100, 2).map(x => {throw new NullPointerException; (x, x)}).reduceByKey(_ + _)

data.count()    // This first one terminates correctly with just an NPE

data.count()    // This second one never terminates, it keeps failing over and over
{code}

The problem seems to be that when there's an NPE in the map stage, we erroneously add map output locations for it, so the next job on the RDD runs only the reduce stage. Those tasks keep failing but they count as a fetch failure, so it keeps retrying.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383119,,,Wed Feb 26 09:56:06 UTC 2014,,,,,,,,,,"0|i1u05z:",383387,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/14 16:51;matei;Fixed in https://github.com/apache/incubator-spark/pull/641.;;;","25/Feb/14 16:59;matei;Test comment;;;","26/Feb/14 09:25;matei;Test comment for email integration;;;","26/Feb/14 09:37;matei;Another test comment for email;;;","26/Feb/14 09:56;matei;Another test comment;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make examples and assembly jar naming consistent between maven/sbt,SPARK-1119,12704442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,,21/Feb/14 22:37,23/Apr/14 17:21,14/Jul/23 06:25,23/Apr/14 17:20,0.9.0,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,"Right now it's somewhat different. Also it should be consistent with what the classpath and example scripts calculate.

This makes them consistent:
spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar
spark-examples-1.0.0-SNAPSHOT-hadoop1.0.4.jar",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382841,,,2014-02-21 22:37:15.0,,,,,,,,,,"0|i1tyg7:",383109,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better error message when python worker process dies,SPARK-1115,12704616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bouk,sandy,,21/Feb/14 01:36,26/Feb/14 14:58,14/Jul/23 06:25,26/Feb/14 14:58,,,,,,,,,0.9.1,1.0.0,,,,PySpark,,,,,0,,,,,,"Right now all I see is:

{code}
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:177)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:55)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:42)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:89)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:53)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
	at org.apache.spark.scheduler.Task.run(Task.scala:53)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{code}",,joshrosen,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383117,,,Wed Feb 26 14:58:34 UTC 2014,,,,,,,,,,"0|i1u05j:",383385,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/14 14:58;joshrosen;Fixed by https://github.com/apache/incubator-spark/pull/644;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
External Spilling Bug - hash collision causes NoSuchElementException,SPARK-1113,12704745,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,,20/Feb/14 15:53,09/Mar/14 17:43,14/Jul/23 06:25,22/Feb/14 17:00,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Shuffle,Spark Core,,,,0,,,,,,"When reading KV pairs back from disk, ExternalAppendOnlyMap maintains a StreamBuffer for each spilled file. These StreamBuffers are ordered by key hash code, and a hash of Int.MAX_VALUE signifies that the corresponding StreamBuffer is empty.

However, Int.MAX_VALUE is a perfectly legitimate hash value. If there exists a key with this value, then ExternalAppendOnlyMap does not differentiate between empty StreamBuffers and StreamBuffers that contain only this key. As a result, a NoSuchElementException is thrown - https://github.com/apache/incubator-spark/blob/95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L304.

java.util.NoSuchElementException (java.util.NoSuchElementException)
org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:277)
org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:212)
org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29)",,andrewor14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1045,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383051,,,Thu Feb 20 19:21:02 UTC 2014,,,,,,,,,,"0|i1tzqv:",383319,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/14 19:21;andrewor14;PR opened at https://github.com/apache/incubator-spark/pull/624;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When spark.akka.frameSize > 10, task results bigger than 10MiB block execution",SPARK-1112,12704733,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mengxr,guillaumepitel,,20/Feb/14 08:34,01/Dec/14 08:29,14/Jul/23 06:25,17/Jul/14 04:31,0.9.0,1.0.0,,,,,,,0.9.2,1.0.1,1.1.0,,,Spark Core,,,,,2,,,,,,"When I set the spark.akka.frameSize to something over 10, the messages sent from the executors to the driver completely block the execution if the message is bigger than 10MiB and smaller than the frameSize (if it's above the frameSize, it's ok)

Workaround is to set the spark.akka.frameSize to 10. In this case, since 0.8.1, the blockManager deal with  the data to be sent. It seems slower than akka direct message though.

The configuration seems to be correctly read (see actorSystemConfig.txt), so I don't see where the 10MiB could come from ",,codingcat,DjvuLee,gq,guillaumepitel,jkleckner,joshrosen,kayousterhout,kzhang,matei,mengxr,patrick,piotrszul,pwendell,reachbach,roshan,sinisa_lyh,swkimme,xiaocai,,,,,,,,,,,,,,,,,,,SPARK-2138,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382997,,,Mon Dec 01 08:29:53 UTC 2014,,,,,,,,,,"0|i1tzev:",383265,,,,,,,,,,,,,0.9.2,1.0.1,1.1.0,,,,,,,,"20/Feb/14 15:47;kayousterhout;Thanks for reporting this!  Does Spark hang, or does the worker throw an exception?  If the former, would you mind uploading the Spark worker log, and if the latter, can you add the stack trace?;;;","20/Feb/14 23:03;guillaumepitel;No Exception, and not ""hanging"" in the bad way the executors can sometime hang : if I kill the driver, the workers receive the shutdown signal and exit cleanly.

Here are the logs :

DRIVER :

14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:13 as 2083 bytes in 0 ms
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:14 as TID 2294 on executor 1: t4.exensa.loc (PROCESS_LOCAL)
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:14 as 2083 bytes in 0 ms
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:15 as TID 2295 on executor 4: t3.exensa.loc (PROCESS_LOCAL)
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:15 as 2083 bytes in 1 ms
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:16 as TID 2296 on executor 0: t0.exensa.loc (PROCESS_LOCAL)
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:16 as 2083 bytes in 2 ms
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:17 as TID 2297 on executor 3: t1.exensa.loc (PROCESS_LOCAL)
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:17 as 2083 bytes in 1 ms
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:18 as TID 2298 on executor 2: t5.exensa.loc (PROCESS_LOCAL)
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:18 as 2083 bytes in 1 ms
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:19 as TID 2299 on executor 5: t6.exensa.loc (PROCESS_LOCAL)
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:19 as 2083 bytes in 1 ms

EXECUTOR :
14/02/19 15:21:53 INFO Executor: Serialized size of result for 2287 is 17229427
14/02/19 15:21:53 INFO Executor: Sending result for 2287 directly to driver
14/02/19 15:21:53 INFO Executor: Serialized size of result for 2299 is 17229262
14/02/19 15:21:53 INFO Executor: Sending result for 2299 directly to driver
14/02/19 15:21:53 INFO Executor: Finished task ID 2299
14/02/19 15:21:53 INFO Executor: Finished task ID 2287
14/02/19 15:21:53 INFO Executor: Serialized size of result for 2281 is 17229426
14/02/19 15:21:53 INFO Executor: Sending result for 2281 directly to driver
14/02/19 15:21:53 INFO Executor: Finished task ID 2281

There is a timezone difference between driver & executor

;;;","20/Feb/14 23:14;roshan;I have a similar issue, I'm on spark-0.9.0 compiled with cdh-4.2.1

For me, serialized tasks over 10 MB do not reach executors. I've tried this with spark.akka.frameSize set to 160 and 10. The workaround suggested (setting spark.akka.frameSize to 10) does not work for me.

I've confirmed that even if serialized tasks are just under 10MB, the executors do get them and the task is completed.

Spark hangs. There are no exceptions or unusual ERROR/WARN/DEBUG logs in the driver, master, executor or worker daemon logs. The executors just don't seem to have received the tasks. The application UI shows the task status as running, but never progresses.

Here are the last few lines from my driver and one executor:

DRIVER:
14/02/21 06:37:00 INFO TaskSetManager: Finished TID 797 in 53897 ms on spark-slave01 (progress: 78/80)
14/02/21 06:37:00 INFO DAGScheduler: Completed ResultTask(9, 58)
14/02/21 06:37:08 INFO TaskSetManager: Finished TID 768 in 75767 ms on spark-slave02 (progress: 79/80)
14/02/21 06:37:08 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool 
14/02/21 06:37:08 INFO DAGScheduler: Completed ResultTask(9, 69)
14/02/21 06:37:08 INFO DAGScheduler: Stage 9 (reduceByKeyLocally at SKMeans.scala:174) finished in 99.048 s
14/02/21 06:37:08 INFO SparkContext: Job finished: reduceByKeyLocally at SKMeans.scala:174, took 99.359019444 s
14/02/21 06:37:09 INFO SparkContext: Starting job: reduceByKeyLocally at SKMeans.scala:174
14/02/21 06:37:09 INFO DAGScheduler: Got job 7 (reduceByKeyLocally at SKMeans.scala:174) with 80 output partitions (allowLocal=false)
14/02/21 06:37:09 INFO DAGScheduler: Final stage: Stage 10 (reduceByKeyLocally at SKMeans.scala:174)
14/02/21 06:37:09 INFO DAGScheduler: Parents of final stage: List()
14/02/21 06:37:09 INFO DAGScheduler: Missing parents: List()
14/02/21 06:37:09 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[28] at reduceByKeyLocally at SKMeans.scala:174), which has no missing parents
14/02/21 06:37:10 INFO DAGScheduler: Submitting 80 missing tasks from Stage 10 (MapPartitionsRDD[28] at reduceByKeyLocally at SKMeans.scala:174)
14/02/21 06:37:10 INFO TaskSchedulerImpl: Adding task set 10.0 with 80 tasks
14/02/21 06:37:10 INFO TaskSetManager: Starting task 10.0:0 as TID 800 on executor 2: spark-slave01 (PROCESS_LOCAL)
14/02/21 06:37:10 INFO TaskSetManager: Serialized task 10.0:0 as 10700743 bytes in 19 ms
...<Starting task and Serialized task lines repeat for each of 40 tasks.>

EXECUTOR: spark-slave01
14/02/21 06:36:08 DEBUG Executor: Task 798's epoch is 3
14/02/21 06:36:08 DEBUG CacheManager: Looking for partition rdd_4_60
14/02/21 06:36:08 DEBUG BlockManager: Getting local block rdd_4_60
14/02/21 06:36:08 DEBUG BlockManager: Level for block rdd_4_60 is StorageLevel(false, true, true, 1)
14/02/21 06:36:08 DEBUG BlockManager: Getting block rdd_4_60 from memory
14/02/21 06:36:08 INFO BlockManager: Found block rdd_4_60 locally
14/02/21 06:36:21 INFO Executor: Serialized size of result for 765 is 1224222
14/02/21 06:36:21 INFO Executor: Sending result for 765 directly to driver
14/02/21 06:36:21 INFO Executor: Finished task ID 765
14/02/21 06:36:34 INFO Executor: Serialized size of result for 790 is 1262463
14/02/21 06:36:34 INFO Executor: Sending result for 790 directly to driver
14/02/21 06:36:34 INFO Executor: Finished task ID 790
14/02/21 06:36:37 INFO Executor: Serialized size of result for 784 is 1394816
14/02/21 06:36:37 INFO Executor: Sending result for 784 directly to driver
14/02/21 06:36:37 INFO Executor: Finished task ID 784
14/02/21 06:36:38 INFO Executor: Serialized size of result for 787 is 1409571
14/02/21 06:36:38 INFO Executor: Sending result for 787 directly to driver
14/02/21 06:36:38 INFO Executor: Finished task ID 787
14/02/21 06:36:41 INFO Executor: Serialized size of result for 798 is 1270321
14/02/21 06:36:41 INFO Executor: Sending result for 798 directly to driver
14/02/21 06:36:41 INFO Executor: Finished task ID 798
14/02/21 06:36:50 INFO Executor: Serialized size of result for 792 is 1175064
14/02/21 06:36:50 INFO Executor: Sending result for 792 directly to driver
14/02/21 06:36:50 INFO Executor: Finished task ID 792
14/02/21 06:36:52 INFO Executor: Serialized size of result for 794 is 1485354
14/02/21 06:36:52 INFO Executor: Sending result for 794 directly to driver
14/02/21 06:36:52 INFO Executor: Finished task ID 794
14/02/21 06:37:00 INFO Executor: Serialized size of result for 797 is 1615486
14/02/21 06:37:00 INFO Executor: Sending result for 797 directly to driver
14/02/21 06:37:00 INFO Executor: Finished task ID 797

Roshan;;;","20/Feb/14 23:54;kayousterhout;Roshan, the issue you're seeing is different -- Guillaume's issue is when task results are too large to be sent using Akka (in which case Spark should use a different code path to send task results to the executor); your issue is when the task itself is too large, in which case Spark (in theory!) gives up and throw an error.  We should fix both problems, but would you mind opening a separate issue?;;;","20/Feb/14 23:56;kayousterhout;Guillaume, just to clarify, the logs you pasted above are for when you set the maximum frame size to 16MiB?  I'm asking because the task results seem to be just slightly larger than 16MiB, which isn't the failure case you mentioned in your description.;;;","21/Feb/14 00:04;guillaumepitel;Sorry, I should have specified it. The frameSize was set to 512 for those logs. I've also tried with 16, and it works when results are over 16MB;;;","21/Feb/14 00:06;kayousterhout;Cool thanks for clarifying!  Looking into this...;;;","21/Feb/14 01:04;guillaumepitel;When I set BOTH the property on driver with 

System.setProperty(""spark.akka.frameSize"", 128) AND I pass the env parameter to SparkContext with SPARK_JAVA_OPTS = ""-Dspark.akka.frameSize=128"" 

Then it seems to works.

So maybe the problem comes from properties not being passed correctly to workers when executors are instanciated ?

Also, I'm using packaged binary distribution for CDH4 on a standalone cluster;;;","21/Feb/14 01:17;roshan;Hi,

I just realized my driver wasn't picking up spark.akka.frameSize value, because of a problem in the way I was passing it in. However, my executor's were picking this value correctly from their conf/spark-env.sh files.

Now, both sides, the driver and executors print the correct value for frameSize with spark.akka.logAkkaConfig=true.

I also noticed that simply starting the driver with java -Dspark.akka.frameSize=200 does not propagate this automatically to the executors. Not that this is an issue. I guess I was just confused about the configuration.

Guillaume, seems like you have the reverse situation as mine, ie. your drivers are correctly configured with the right frameSize, but the executors are still using the 10MB default?

To conclude, after ensuring that the driver is correctly configured with the right frameSize, so far, serialized tasks larger than 10MB are being received by the executors and run successfully.

Roshan;;;","21/Feb/14 05:35;guillaumepitel;You're right Roshan.I was expecting the akka properties set before SparkContext creation to be propagated to the Executors (and based on what the Spark code does, it should be the case). 

I think it should be enforced for the whole akka stuff (timeouts and so on), as well as for the rest of spark properties;;;","21/Feb/14 23:33;patrick;Hey There,

I spent some time playing with this and couldn't reproduce the issue. The driver should capture the options and pass them to executors. I just tested this with a local cluster and was able to verify the akka frame size is passed to executors even if it's not set in spark-env.sh where the executor launches.

[~roshan] - what happens if you remove the setting from spark-env.sh on the executors and only set it at the driver. Does that work correctly?;;;","22/Feb/14 00:16;guillaumepitel;In my code I already had some options passed in SPARK_JAVA_OPTS in the env of the SparkContext, but nothing about the akka frameSize. Could it be related ?

Since it was working correctly in 0.8.1, maybe it's related to the new SparkConf ?;;;","22/Feb/14 00:23;patrick;[~guillaumepitel] If you aren't setting akka.frameSize in SPARK_JAVA_OPTS then where are you setting it?

What I was saying is that if you do System.setProperty(spark.akka.frameSize, XX) before you create the SparkContext it should collect this and send it to the executors correctly. One thing is if you set it after you create the SparkContext it won't work... are you doing this by any chance?;;;","22/Feb/14 00:39;guillaumepitel;No, I create the SparkContext after setting the properties (and I've nothing on my nodes for configuring the frameSize, it's a per-process configuration). 

So before my workaround, I was just setting the property (and the environment in the UI was showing the right value) and passing a SPARK_JAVA_OPTS to the env of the SparkContext with 
{code}
System.setProperty(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
System.setProperty(""spark.kryo.registrator"", registrator)
System.setProperty(""spark.kryo.referenceTracking"", ""false"")
System.setProperty(""spark.kryoserializer.buffer.mb"", bufferSize.toString)
System.setProperty(""spark.locality.wait"", ""10000"")
System.setProperty(""spark.hadoop.mapreduce.output.fileoutputformat.compress"", ""true"")
System.setProperty(""spark.hadoop.mapreduce.output.fileoutputformat.compress.codec"", codec)
System.setProperty(""spark.hadoop.mapreduce.output.fileoutputformat.compress.type"", ""BLOCK"")
System.setProperty(""spark.akka.frameSize"", akkaFrameSize.toString)

val sb = new StringBuilder()
sb.append(""-Dspark.storage.memoryFraction="" + sparkMemoryFraction())
sb.append("" -Dspark.worker.timeout="" + sparkWorkerTimeout())
sb.append("" -Dspark.akka.askTimeout="" + sparkAkkaAskTimeout())
sb.append("" -Dspark.akka.timeout="" + sparkAkkaTimeout())
sb.append("" -Dspark.shuffle.consolidateFiles="" + sparkShuffleConsolidateFiles())

var env = new HashMap[String, String]()
env += ""SPARK_JAVA_OPTS"" -> sb.toString()

val sc = new SparkContext(sparkMaster(), appName, sparkHome(), jars(), env)
{code}

That caused a problem (the akka frame size seemed to be passed to the executor, but only after the creation of the actorSystem, because it was taking the right code path, but the akka system didn't seem to be properly configured).

Now if I add this to my code :

{code}
sb.append("" -Dspark.akka.frameSize="" + akkaFrameSize)
{code}

It works
;;;","22/Feb/14 03:58;roshan;@Patrick Wendell -

Case 1.
DRIVER:
I start the driver with java -Dspark.akka.frameSize=200 -D... -cp .....

EXECUTOR:
spark-env.sh on workers with frameSize specified: 
export SPARK_JAVA_OPTS='-Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.frameSize=160 -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200'

Executor akka config log reports frameSize=160

Case 2. 
DRIVER:
I start the driver with java -Dspark.akka.frameSize=200 -D... -cp ....

EXECUTOR:
spark-env.sh on workers with frameSize NOT specified: 
export SPARK_JAVA_OPTS='-Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200'

Executor akka config log on workers has no frameSize and executor process(by ps waux) has no frameSize. Here, I suppose it picks up the default 10MB.

Case 3. 
DRIVER:
This time I export SPARK_JAVA_OPTS=""-Dspark.akka.frameSize=220"" on the dirver host, before running the driver with java -Dspark.akka.frameSize=200 -D... -cp ....
I'm not reading SPARK_JAVA_OPTS in the driver code.
Driver's Akka config log, says frameSize=200.

EXECUTOR1 on HOST1:
No frameSize specified in spark-env
Executor akka config log says frameSize=220.

EXECUTOR2 on HOST2:
frameSize=160 in spark-env
Executor akka config log says frameSize=220, 

ps waux shows the process was started with this cmd:
/usr/java/default/bin/java -cp sparkJar.jar -Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.frameSize=160 -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200 -Dspark.akka.frameSize=220 -Xms3072M -Xmx3072M org.apache.spark.executor.CoarseGrainedExecutorBackend .....

So the SPARK_JAVA_OPTS from both the executor and the driver are appended, but since java overrides the first frameSize with the second, the executor runs with frameSize=220

Case 4.
DRIVER:
This time I export SPARK_JAVA_OPTS=""-Dspark.akka.frameSize=220"" on the driver host, but don't specify frameSize in the launch command.
Again, I don't SPARK_JAVA_OPTS in the driver code.
No frameSize in driver's akka config log. I expect driver picks up the default 10MB frameSize.

Executors are the same as in case3.

What I'm seeing is that, you can specify the frameSize for a driver in the launch command as java -Dspark.akka.frameSize property, and for the executors in their respective spark-env.sh. If you specify SPARK_JAVA_OPTS on the driver side, then this will override the value for the executors, but not for the driver. I don't use spark-class.sh to launch my driver, because somewhere I read that its meant for spark internal classes and examples. I also currently build my SparkContext directly, without using SparkConf.

This not an issue for me any longer. That being said, it would have saved me loads of time, if the logs had provided some indication that sending serialized tasks to the executors had failed because they were larger than 10MB. Also, the documentation could be a bit clearer about what is set where and which property overrides or is overridden.

Thanks.;;;","29/May/14 01:59;swkimme;Hi all, 

I'm very new to Spark and doing some tests, I've experienced similar issue.
(tested with Spark Shell, 0.9.1, r3.8xlarge instance on EC2 - 32 core / 244GiB MEM)

I was trying to broadcast 700MB of data and Spark hangs when I run collect() method for the data. 

Here's the strange things :
1) when I tried 
{code}val userInfo = sc.textFile(""file:///spark/logs/user_sign_up2.csv"").map{line => val split = line.split("",""); (split(1), split)}
val userInfoMap = userInfo.collectAsMap
{code}
it runs well.
2) when I tried 
{code}val userInfo = sc.textFile(""file:///spark/logs/user_sign_up2.csv"").map{line => val split = line.split("",""); (split(1), split(5))} 
val userInfoMap = userInfo.collectAsMap
{code}
Spark hangs.
3) when I slightly control the data size using sample() method or cutting the data file, it runs well. 

Our team investigated logs from master and worker then we found worker finished all tasks but master couldn't retrieve the result from a task the result size larger than 10MB

We tried to apply the workaround setting spark.akka.frameSize to 9, it works like a charm.

I guess it might hard to reproduce the issue, please contact me if there's need of testing or getting logs. 

Thanks!;;;","29/May/14 02:17;matei;I'm curious, why did you want to make the frameSize this big -- are the tasks themselves also big or just the results? There might be other buffers in Akka that can't be made bigger than this. It's possible that this changed in a newer Akka version (because larger frame sizes used to work before).;;;","29/May/14 02:49;swkimme;[~matei]
I've found the default of spark.akka.frameSize is 10 from the config document, http://spark.apache.org/docs/0.9.1/configuration.html
just tried to slightly larger and smaller (11 and 9) values.

I did collect() method on the userInfo and it might contains large data. (edited the first comment.)

;;;","13/Jun/14 23:00;xiaocai;[~matei] Do you know which akka version we should use to be able to use big frame size. ;;;","16/Jun/14 20:09;xiaocai;To follow up this thread, I have done some experiments when the frameSize is around 10MB .

1) spark.akka.frameSize = 10
If one of the partition size is very close to 10MB, say 9.97MB, the execution blocks without any exception or warning. Worker finished the task to send the serialized result, and then throw exception saying hadoop IPC client connection stops (changing the logging to debug level). However, the master never receives the results and the program just hangs.
But if sizes for all the partitions less than some number btw 9.96MB amd 9.97MB, the program works fine.
2) spark.akka.frameSize = 9
when the partition size is just a little bit smaller than 9MB, it fails as well.

This bug behavior is not exactly what spark-1112 is about, could you please guide me how to open a separate bug when the serialization size is very close to 10MB. 

Thanks a lot;;;","19/Jun/14 00:10;xiaocai;I have filed a bug https://issues.apache.org/jira/browse/SPARK-2156;;;","19/Jun/14 01:38;pwendell;We were able to reproduce this - thanks for reporting it.;;;","19/Jun/14 02:06;xiaocai;Awesome, looking forward to the fix. At least better error or exception message would be helpful. ;;;","19/Jun/14 02:35;mengxr;PR: https://github.com/apache/spark/pull/1124;;;","23/Jun/14 02:49;pwendell;This is fixed in the 1.0 branch via:
https://github.com/apache/spark/pull/1172;;;","25/Jun/14 02:06;pwendell;Fixed in 1.1.0 via:
https://github.com/apache/spark/pull/1132;;;","25/Jun/14 16:48;reachbach;Can a clear workaround be specified for this bug please? For those unable to upgrade to run on 1.0.1  or 1.1.0 in production, general instructions on the workaround are required. This is a huge blocker for current production deployments (even on 1.0.0) otherwise. For instance, running a saveAsTextFile() on an RDD (~400MB) causes execution to freeze with the last log statements seen on the driver being:

14/06/25 16:38:55 INFO spark.SparkContext: Starting job: saveAsTextFile at Test.java:99
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Got job 6 (saveAsTextFile at Test.java:99) with 2 output partitions (allowLocal=false)
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Final stage: Stage 6(saveAsTextFile at Test.java:99)
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Missing parents: List()
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Submitting Stage 6 (MappedRDD[558] at saveAsTextFile at Test.java:99), which has no missing parents
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 6 (MappedRDD[558] at saveAsTextFile at Test.java:99)
14/06/25 16:38:55 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
14/06/25 16:38:55 INFO scheduler.TaskSetManager: Starting task 6.0:0 as TID 5 on executor 1: somehost.corp (PROCESS_LOCAL)
14/06/25 16:38:55 INFO scheduler.TaskSetManager: Serialized task 6.0:0 as 351777 bytes in 36 ms
14/06/25 16:38:55 INFO scheduler.TaskSetManager: Starting task 6.0:1 as TID 6 on executor 0: someotherhost.corp (PROCESS_LOCAL)
14/06/25 16:38:55 INFO scheduler.TaskSetManager: Serialized task 6.0:1 as 186453 bytes in 16 ms

The test setup for reproducing this issue has two slaves (each with 24G) running spark standalone. The driver runs with Xmx 4G.

Thanks.;;;","25/Jun/14 16:57;pwendell;[~reachbach] If you are running on standalone mode, it might work if you go on every node in your cluster and add the following to spark-env.sh:

{code}
export SPARK_JAVA_OPTS=""-Dspark.akka.frameSize=XXX""
{code}

However, this work around will only work if every job in your cluster is using the same frame size (XXX).

The main recommendation is to upgrade to 1.0.1. We are very conservative about what we merge into maintenance branches, so we recommend users upgrade immediately once we release them.;;;","25/Jun/14 16:59;pwendell;This is not resolved yet because it needs to be back ported into 0.9;;;","17/Jul/14 04:01;mengxr;PR for branch-0.9: https://github.com/apache/spark/pull/1455;;;","17/Jul/14 04:31;mengxr;Issue resolved by pull request 1455
[https://github.com/apache/spark/pull/1455];;;","24/Jul/14 14:42;DjvuLee;Does anyone test in version0.9.2，I found it also failed , while  v1.0.1 & v1.1.0 is ok. ;;;","01/Dec/14 08:29;joshrosen;Looks like the ""Fix Versions"" accidentally got overwritten during a backport / cherry-pick, so I've restored them based on the issue history.;;;"
URL Validation Throws Error for HDFS URL's,SPARK-1111,12704706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,patrick,pwendell,,19/Feb/14 22:26,30/Mar/14 04:13,14/Jul/23 06:25,21/Feb/14 11:24,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Deploy,,,,,0,,,,,,The validation method incorrectly assumes Java's URL parser is okay with hdfs urls.,,ilikerps,jobswang,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382832,,,Sat Feb 22 06:53:33 UTC 2014,,,,,,,,,,"0|i1tye7:",383100,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 05:57;jobswang;what error？;;;","21/Feb/14 11:24;ilikerps;The issue was that we used new URL(userURL) as a way to verify that the URL was correct, but the URL constructor throws an Exception if the protocol is hdfs://, which is not good.

With [PR #625|https://github.com/apache/incubator-spark/pull/625], we instead check manually for URL-like syntax.;;;","22/Feb/14 06:53;jobswang;thanks！I know！:-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong API docs for pyspark map function,SPARK-1109,12704506,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,dcarroll@cloudera.com,,19/Feb/14 09:48,07/Feb/20 17:26,14/Jul/23 06:25,04/Mar/14 15:34,0.9.0,,,,,,,,0.9.1,1.0.0,,,,PySpark,,,,,0,,,,,,"The source code/API docs for the pyspark RDD map function says:

    def map(self, f, preservesPartitioning=False):
        """"""
        Return a new RDD containing the distinct elements in this RDD.
        """"""
        def func(split, iterator): return imap(f, iterator)
        return PipelinedRDD(self, func, preservesPartitioning)

I think that was incorrectly cut-and-pasted from the distinct() function, and should actually say ""Return a new RDD by applying a function to each element of this RDD.""",,apachespark,dcarroll@cloudera.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383077,,,Thu Dec 10 15:06:19 UTC 2015,,,,,,,,,,"0|i1tzwn:",383345,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/15 15:06;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/73;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saveAsNewAPIHadoopFile throws NPE with TableOutputFormat,SPARK-1108,12704704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xoltar,xoltar,,18/Feb/14 23:31,09/Mar/14 17:52,14/Jul/23 06:25,09/Mar/14 17:52,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Spark Core,,,,,0,,,,,,"When using HBase's TableOutputFormat, saveAsNewAPIHadoopFile throws NPE because the destination table is not set.",,xoltar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383049,,,Thu Feb 27 22:39:39 UTC 2014,,,,,,,,,,"0|i1tzqf:",383317,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/14 23:33;xoltar;I'll create a pull request for this tomorrow.;;;","27/Feb/14 22:39;xoltar;PR: https://github.com/apache/incubator-spark/pull/638

Merged in https://github.com/apache/spark/commit/4d880304867b55a4f2138617b30600b7fa013b14

Still needs to be merged to branch-0.9 AFAIK;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
config.yml has an error version number,SPARK-1105,12704636,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,,18/Feb/14 16:24,20/Feb/14 11:13,14/Jul/23 06:25,20/Feb/14 11:13,,,,,,,,,0.9.1,1.0.0,,,,,,,,,0,,,,,,"as reported by http://apache-spark-user-list.1001560.n3.nabble.com/question-about-compiling-SimpleApp-td1689.html

the SCALA_VERSION in config.yml is wrong

it should be 2.10.3",,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383136,,,2014-02-18 16:24:11.0,,,,,,,,,,"0|i1u09r:",383404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker should not block while killing executors,SPARK-1104,12704656,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,patrick,,18/Feb/14 15:56,24/Apr/14 22:57,14/Jul/23 06:25,24/Apr/14 22:57,0.9.0,1.0.0,,,,,,,1.0.0,,,,,Deploy,,,,,0,,,,,,"Sometimes due to large shuffles executors will take a long time shutting down. In particular this can happen if large numbers of shuffle files are around (this will be alleviated by SPARK-1103, but nonetheless...).

The symptom is you have DEAD workers sitting around in the UI and the existing workers keep trying to re-register but can't because they've been assumed dead.

If killing the executor happens in its own thread, or if the ExecutorRunner were an actor, this would not be a problem. For 0.9 I'd prefer the former approach since it minimizes code changes.",,codingcat,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383045,,,Mon Mar 10 04:25:51 UTC 2014,,,,,,,,,,"0|i1tzpj:",383313,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 12:34;codingcat;I went through the related code, if my understanding is correct, the difficulty is that how to call process.destroy() and waitFor() in a separate thread, 

my proposal is to have a cleanup thread to maintain the process together with workerThread, 

How do you think about that, Patrick?

;;;","27/Feb/14 10:02;codingcat;made the PR: https://github.com/apache/spark/pull/35;;;","10/Mar/14 04:25;codingcat;any one would like to review the PR?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException,SPARK-1097,12704582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,colorant,Mistobaan,,16/Feb/14 14:31,06/Oct/14 23:06,14/Jul/23 06:25,13/Jun/14 17:53,0.9.0,,,,,,,,1.0.1,1.0.2,1.1.0,,,Spark Core,,,,,0,,,,,,"{noformat}
14/02/16 08:18:45 WARN TaskSetManager: Loss was due to java.util.ConcurrentModificationException
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
	at java.util.HashMap$KeyIterator.next(HashMap.java:960)
	at java.util.AbstractCollection.addAll(AbstractCollection.java:341)
	at java.util.HashSet.<init>(HashSet.java:117)
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:554)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:439)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:110)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:154)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:32)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:72)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.Task.run(Task.scala:53)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}",,colorant,jblomo,kasha,Mistobaan,mubarak.seyed,nravi,ozawa,pwendell,sinisa_lyh,,,,,,,,,,,,,,,,,,,,,,SPARK-1388,,,,,,SPARK-1388,SPARK-2546,,HADOOP-10456,,,,,,,,"02/Apr/14 01:28;nravi;nravi_Conf_Spark-1388.patch;https://issues.apache.org/jira/secure/attachment/12638177/nravi_Conf_Spark-1388.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383110,,,Sat Jul 26 01:58:41 UTC 2014,,,,,,,,,,"0|i1u03z:",383378,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 23:18;kasha;What version of Hadoop is this? ;;;","02/Apr/14 01:29;nravi;Attached is a patch for this issue. Verified with mvn test/compile/install. The fix is to move HashSet initialization to the synchronized block right above it.;;;","02/Apr/14 04:52;srowen;Standard procedure is to provide a pull request. But, you're suggesting a fix to Hadoop code, which belong in your Hadoop JIRA, yes. This can't fix the problem from the Spark end.

(Eyeballing the Hadoop 2.2.0 code, I tend to agree with your patch. Mutation of finalParameters appears consistently synchronized, which means the constructor reading it to copy has to lock on the other Configuration or else exactly this can happen.)

Is a workaround in Spark to synchronize on the Configuration object when calling this constructor? (I smell a deadlock risk.)
Or something crazy like trying the constructor until it doesn't fail this way?;;;","02/Apr/14 05:16;nravi;The problem should be solved at the root. This issue can be exposed by other systems as well, in addition to Spark. The fix is straightforward and harmless. I can initiate a pull request as well.;;;","02/Apr/14 05:20;srowen;I agree, but, we can't patch Hadoop from here. I'm just saying that for purposes of a SPARK-* issue, in anything like the short-term, one would have to propose a workaround within Spark code, if anything. While also trying to fix it at the root, separately, in a HADOOP-* issue. (Spark does not have a copy of Hadoop, yesterday's April Fools joke aside.);;;","02/Apr/14 18:00;nravi;We can consider putting a workaround in Spark as well (for non-CDH users that may be running an older version of Hadoop and not updating it periodically). For now, this fix needs to go upstream, so we can backport it to CDH. The CDH-Spark bundle would then inherit this fix. The same issue has been noted in Hadoop-10456 as well. ;;;","02/Apr/14 22:14;nravi;Patch submitted against Hadoop-10456.;;;","03/Apr/14 09:43;ozawa;A patch by Nishkam on HADOOP-10456 has been already reviewed and will be committed in a few days against hadoop's trunk.;;;","06/Jun/14 19:33;jblomo;FYI still seeing this on spark 1.0, Hadoop 2.4

{code:java}
java.util.ConcurrentModificationException (java.util.ConcurrentModificationException)
java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)
java.util.HashMap$KeyIterator.next(HashMap.java:956)
java.util.AbstractCollection.addAll(AbstractCollection.java:341)
java.util.HashSet.<init>(HashSet.java:117)
org.apache.hadoop.conf.Configuration.<init>(Configuration.java:671)
com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:98)
org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2402)
org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2436)
org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2418)
org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:107)
org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:190)
org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:181)
org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:200)
org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:175)
org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:175)
org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)
org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:174)
{code};;;","06/Jun/14 20:39;ozawa;[~jblomo], thank you for reporting. This issue is fixed in next minor Hadoop release - 2.4.1. Note that 2.4.0 doesn't include the fix.;;;","06/Jun/14 20:53;nravi;Have initiated a PR for a workaround in Spark as well (for developers using < 2.4.1):
https://github.com/apache/spark/pull/1000;;;","01/Jul/14 08:47;colorant;Summit another PR to actually(hopes) workaround this problem
https://github.com/apache/spark/pull/1273
;;;","26/Jul/14 01:58;pwendell;A follow up to this fix is in Spark 1.0.2:
https://github.com/apache/spark/pull/1409/files;;;",,,,,,,,,,,,,,,,,,,,
ADD_JARS regression in Spark 0.9.0,SPARK-1089,12704527,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,codingcat,ash211,,13/Feb/14 14:25,09/Jul/14 18:12,14/Jul/23 06:25,26/Feb/14 23:43,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Spark Core,,,,,1,,,,,,"Using the ADD_JARS environment variable with spark-shell used to add the jar to both the shell and the various workers.  Now it only adds to the workers and importing a custom class in the shell is broken.

The workaround is to add custom jars to both ADD_JARS and SPARK_CLASSPATH.

We should fix ADD_JARS so it works properly again.

See various threads on the user list:
https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201402.mbox/%3CCAJbo4neMLiTrnm1XbyqomWmp0m+EUcg4yE-txuRGSVKOb5KLeA@mail.gmail.com%3E
(another one that doesn't appear in the archives yet titled ""ADD_JARS not working on 0.9"")",,ash211,codingcat,hammer,nchammas,wildfire,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383114,,,Wed Jul 09 18:12:36 UTC 2014,,,,,,,,,,"0|i1u04v:",383382,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/14 17:33;codingcat;I'm interested in fixing this

Can anyone assign it to me?

Thank you!;;;","18/Feb/14 08:30;codingcat;made a PR https://github.com/apache/incubator-spark/pull/614, the analysis on the reason of this bug is over there;;;","21/Feb/14 10:47;codingcat;as a temporary work around, you don't need to set ADD_JARS and SPARK_CLASSPATH at the same time

just SPARK_CLASSPATH is enough ;;;","09/Jul/14 18:12;nchammas;So going forward, is the correct procedure for adding external jars to set {{SPARK_CLASSPATH}}? Is there a doc somewhere that details this process? And what is the difference between setting these environment variables and calling {{sc.addJar()}} from within the shell?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZK PersistenceEngine does not respect zookeeper dir,SPARK-1080,12704475,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,,11/Feb/14 22:25,11/Feb/14 23:25,14/Jul/23 06:25,11/Feb/14 22:51,0.8.1,0.9.0,,,,,,,0.8.2,0.9.1,1.0.0,,,Deploy,,,,,0,,,,,,"ZooKeeperPersistenceEngine is hard-coded to deserialize files from /spark/master_status, so the PersistenceEngine would fail to recover if the user specifies a custom spark.deploy.zookeeper.dir.",,ilikerps,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383160,,,Tue Feb 11 23:25:54 UTC 2014,,,,,,,,,,"0|i1u0f3:",383428,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/14 22:37;markhamstra;Not strictly limited to this issue and related pull request, but something that I've been wondering about wrt ZooKeeperPersistenceEngine that maybe someone can clarify for me: Why is this code using the notoriously difficult ZooKeeper API directly instead of taking advantage of Curator?;;;","11/Feb/14 22:51;ilikerps;https://github.com/apache/incubator-spark/pull/583

Thanks Raymond Liu!;;;","11/Feb/14 22:56;ilikerps;@[~markhamstra] Mostly because I didn't know about Curator until 5 minutes ago! A rewrite would not be unwelcome if it's simpler and more likely to work correctly. Such a rewrite could almost certainly allow us to remove [SparkZooKeeperSession|https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/SparkZooKeeperSession.scala].

A new JIRA should probably be opened for that, though.;;;","11/Feb/14 23:01;markhamstra;Okay, that answers that.

Yes, a rewrite to use Curator does belong in a separate JIRA.;;;","11/Feb/14 23:25;ilikerps;Created SPARK-1082 to track this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix simple doc typo in Spark Streaming Custom Receiver,SPARK-1075,12704716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hsaputra,hsaputra,,10/Feb/14 11:56,11/Feb/14 15:30,14/Jul/23 06:25,11/Feb/14 15:30,,,,,,,,,1.0.0,,,,,Documentation,,,,,0,doc,,,,,"The closing parentheses in the constructor in the first code block example is reversed:

diff --git a/docs/streaming-custom-receivers.md b/docs/streaming-custom-receivers.md
index 4e27d65..3fb540c 100644
--- a/docs/streaming-custom-receivers.md
+++ b/docs/streaming-custom-receivers.md
@@ -14,7 +14,7 @@ This starts with implementing [NetworkReceiver](api/streaming/index.html#org.apa
 The following is a simple socket text-stream receiver.
 
 {% highlight scala %}
-       class SocketTextStreamReceiver(host: String, port: Int(
+       class SocketTextStreamReceiver(host: String, port: Int)
          extends NetworkReceiver[String]
        {
          protected lazy val blocksGenerator: BlockGenerator =",,hsaputra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383161,,,Tue Feb 11 15:30:40 UTC 2014,,,,,,,,,,"0|i1u0fb:",383429,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/14 11:59;hsaputra;PR: 

https://github.com/apache/incubator-spark/pull/577;;;","11/Feb/14 15:30;hsaputra;PR from https://github.com/apache/incubator-spark/pull/577 merged to ASF git repo.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GitHub PR squasher has bad titles,SPARK-1073,12704707,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ash211,ash211,,10/Feb/14 00:27,12/Feb/14 23:29,14/Jul/23 06:25,12/Feb/14 23:29,,,,,,,,,1.0.0,,,,,Build,,,,,0,,,,,,https://github.com/apache/incubator-spark/pull/574,,ash211,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383154,,,Wed Feb 12 23:29:21 UTC 2014,,,,,,,,,,"0|i1u0dr:",383422,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/14 23:29;patrick;Thanks for this Andrew! I consider this a ""Major"" issue - good project infrastructure is important :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use binary search for RangePartitioner when there is more than 1000 partitions,SPARK-1072,12704700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holdenkarau,holdenk_amp,,09/Feb/14 23:25,18/Jun/14 07:57,14/Jul/23 06:25,18/Jun/14 07:57,,,,,,,,,0.9.0,,,,,Spark Core,,,,,0,,,,,,,,holdenk_amp,hsaputra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382963,,,Wed Mar 19 10:58:14 UTC 2014,,,,,,,,,,"0|i1tz7b:",383231,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 10:58;hsaputra;This is fixed with 

https://github.com/apache/incubator-spark/pull/571

?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker.scala should kill drivers in method postStop(),SPARK-1068,12704648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qiuzhuang,Qiuzhuang,,08/Feb/14 00:05,08/Feb/14 13:00,14/Jul/23 06:25,08/Feb/14 13:00,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Spark Core,,,,,0,,,,,,,,Qiuzhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383167,,,Sat Feb 08 00:11:50 UTC 2014,,,,,,,,,,"0|i1u0gn:",383435,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/14 00:11;Qiuzhuang;PR: https://github.com/apache/incubator-spark/pull/561;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark runs out of memory with large broadcast variables,SPARK-1065,12704673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,joshrosen,,07/Feb/14 11:41,17/Aug/14 00:00,14/Jul/23 06:25,17/Aug/14 00:00,0.7.3,0.8.1,0.9.0,,,,,,1.1.0,,,,,PySpark,,,,,2,,,,,,"PySpark's driver components may run out of memory when broadcasting large variables (say 1 gigabyte).

Because PySpark's broadcast is implemented on top of Java Spark's broadcast by broadcasting a pickled Python as a byte array, we may be retaining multiple copies of the large object: a pickled copy in the JVM and a deserialized copy in the Python driver.

The problem could also be due to memory requirements during pickling.

PySpark is also affected by broadcast variables not being garbage collected.  Adding an unpersist() method to broadcast variables may fix this: https://github.com/apache/incubator-spark/pull/543.

As a first step to fixing this, we should write a failing test to reproduce the error.

This was discovered by [~sandy]: [""trouble with broadcast variables on pyspark""|http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-broadcast-variables-on-pyspark-tp1301.html].",,apachespark,davies,diederik,frol,joshrosen,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383168,,,Wed Aug 13 04:57:18 UTC 2014,,,,,,,,,,"0|i1u0gv:",383436,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/14 18:01;sandy;Here's some code that reproduces it.

{code}
theconf = SparkConf().set(""spark.executor.memory"", ""5g"").setAppName(""broadcastfail"").setMaster(cluster_url)
sc = SparkContext(conf=theconf)

broadcast_vals = []
for i in range(5):
  datas = [[float(i) for i in range(200)] for i in range(100000)]
  val = sc.broadcast(datas).value
  broadcast_vals.append(val)

sc.parallelize([i for i in range(80)]).map(lambda x: sum([len(val) for val in broadcast_vals])).collect()
{code}

It generates a few arrays of floats, each of which should take about 160 MB.  The executors never end up using much memory, but the driver uses an enormous amount.  Both the python and java processes ramp up to multiple GB until I start seeing a bunch of ""OutOfMemoryError: java heap space"".

With a single 160MB array, the job completes fine, but the driver still uses about 9 GB.
;;;","11/Aug/14 22:06;frol;I am facing the same issue in my project, where I use PySpark. As a proof of that the big objects I have could easily fit into nodes' memory, I am going to use dummy solution of saving my big objects into HDFS and load them on Python nodes.

Does anybody have an idea how to fix the issue in a better way? I don't have enough either Scala nor Java knowledge to fix this in Spark core. However, I feel like broadcast variables could be reimplemented on Python side though it seems a bit dangerous idea because we don't want to have separate implementations of one thing in both languages. That will also save memory, because while we use broadcasts through Scala we have 1 copy in JVM, 1 pickled copy in Python and 1 constructed object copy in Python.;;;","12/Aug/14 17:18;frol;I have finished my experiment of using HDFS as a temp storage for my big objects. It showed that my mappers do not leak memory and work pretty stable. However, it takes long time to load these objects on each node.

Is there straightforward way to detect memory leaks in Spark and PySpark?;;;","13/Aug/14 00:30;davies;The broadcast was not used correctly in the above code, it should be used like this:

{code}
broadcast_vals = []
for i in range(5):
  datas = [[float(i) for i in range(200)] for i in range(100000)]
  val = sc.broadcast(datas)
  broadcast_vals.append(val)

sc.parallelize([i for i in range(80)]).map(lambda x: sum([len(val.value) for val in broadcast_vals])).collect()
{code}

The reference of object in Python driver in not necessary in most cases, we will make it optional (no reference by default), then it can reduce the memory used in Python driver.;;;","13/Aug/14 00:33;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/1912;;;","13/Aug/14 00:45;frol;[~davies] Will your PR take into account this fix: [SPARK-2521] Broadcast RDD object (instead of sending it along with every task) https://github.com/apache/spark/commit/7b8cd175254d42c8e82f0aa8eb4b7f3508d8fde2 ?
""The patch uses broadcast to send RDD objects and the closures to executors"";;;","13/Aug/14 00:54;frol;[~davies] I have not noticed that there was that mistake in the example, but I have not used that code. I run into the issue in my own code, where I use broadcasts correctly.

I'm building your branch now and will try it right away. Thank you for your fix!;;;","13/Aug/14 00:57;davies;[~frol], I think broadcast the RDD object is already done by that PR.

But the serialized closure will still be sent to JVM by py4j. After using broadcast for large datasets, the serialized closure should not be too huge, so I guess it will not be a big issue.;;;","13/Aug/14 01:02;davies;After this patch, the above test can run successfully with about 700M memory in Python driver, 5xxMB memory in JVM driver, and 3G memory in python worker.

It may triggle another problem when run it with Mesos or YARN, because Spark does not reserve memory for Python worker, this may be fixed in 1.2 release.;;;","13/Aug/14 01:04;frol;[~davies] I understand that if you use broadcast explicitly the closure won't be huge, but the point of that PR was also ""1. Users won't need to decide what to broadcast anymore, unless they would want to use a large object multiple times in different operations"".;;;","13/Aug/14 01:05;frol;[~davies] I use YARN setup so I will see how it goes.;;;","13/Aug/14 01:43;frol;[~davies] I have compiled and run your broadcast branch against my cluster on YARN. It does not leak memory any more! And it is at least 25% faster than my dummy implementation on top of HDFS. Implementation with broadcasts takes 4.5 minutes to finish a task, where my implementation took 6 minutes. More heavy tests are still working.;;;","13/Aug/14 02:37;frol;Heavy tasks completed in 18 minutes each instead of 22 minutes, which is 20% speed up. That is nice!
I don't see any problems on my YARN cluster. Java nodes eat up to 1.5GB RAM (which is my JVM limit) each and Python daemons eat around 650MB each. Though those numbers are still a bit weird, it is obvious to me that workers don't leak now.

Thank you a lot!;;;","13/Aug/14 04:57;davies;Cool, thanks for the tests. If we can compress the data, it will be faster. I will do it in another separate PR.

The closure is serialized by cloudpickle, so it will be much slower if you do not use broadcast explicitly. We can show an warning if the serialized closure is too big.;;;",,,,,,,,,,,,,,,,,,,
JettyUtil is not using host information to start server,SPARK-1060,12704625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,,06/Feb/14 18:55,08/Feb/14 23:39,14/Jul/23 06:25,08/Feb/14 23:39,,,,,,,,,1.0.0,,,,,Web UI,,,,,0,,,,,,"In the current implementation, the webserver in Master/Worker is started with (MasterUI/WorkerUI)

val (srv, bPort) = JettyUtils.startJettyServer(""0.0.0.0"", port, handlers)

inside startJettyServer

val server = new Server(currentPort) //the Server will take ""0.0.0.0"" as the hostname, i.e. will always bind to the IP address of the first NIC

which is actually not using host information got by val host = Utils.localHostName()

this can cause wrong IP binding, e.g. if the host has two NICs, N1 and N2, the user specify the SPARK_LOCAL_IP as the N2's IP address, however, when starting the web server, for the reason stated above, it will always bind to the N1's address",,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383166,,,Thu Feb 06 18:57:44 UTC 2014,,,,,,,,,,"0|i1u0gf:",383434,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 18:57;codingcat;made a PR https://github.com/apache/incubator-spark/pull/556;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Header comment in Executor incorrectly implies it's not used for YARN,SPARK-1056,12704624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sandyr,sandy,,06/Feb/14 15:02,04/Apr/14 20:51,14/Jul/23 06:25,18/Feb/14 13:42,,,,,,,,,1.0.0,,,,,YARN,,,,,0,,,,,,"{code}
/**
 * Spark executor used with Mesos and the standalone scheduler.
 */
{code}
",,ash211,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383141,,,2014-02-06 15:02:39.0,,,,,,,,,,"0|i1u0av:",383409,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark version on Dockerfile does not match release,SPARK-1055,12704547,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,maasg,,05/Feb/14 03:12,22/Feb/14 16:59,14/Jul/23 06:25,22/Feb/14 16:35,0.9.0,,,,,,,,0.9.1,1.0.0,,,,Examples,,,,,0,,,,,,"The used Spark version in the .../base/Dockerfile is stale on 0.8.1 and should be updated to 0.9.x to match the release.
See:
https://github.com/apache/incubator-spark/blob/branch-0.9/docker/spark-test/base/Dockerfile 

PS: Should we add a component for 'Docker'? - not sure under which component I should file this bug.",,maasg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383129,,,Wed Feb 05 03:22:45 UTC 2014,,,,,,,,,,"0|i1u087:",383397,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 03:22;maasg;Scala version is also incorrect;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on Mesos with CDH4.5.0 cannot start the Tasks properly,SPARK-1052,12704604,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bijaybisht,amiorin,,04/Feb/14 13:48,16/Feb/14 16:55,14/Jul/23 06:25,16/Feb/14 16:55,0.9.0,,,,,,,,0.9.1,1.0.0,,,,,,,,,2,,,,,,"Steps :
Start the mesos cluster with the deb 0.15 from mesosphere.
Install cdh 4.5.0.
Start the spark-shell with this code :

val data = 1 to 10000
val distData = sc.parallelize(data)
distData.filter(_< 10).collect()

The tasks die because of an exception.

It used to work with Spark 0.8.0",,amiorin,bijaybisht,ilikerps,mwiewiorka,vinodkone,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383146,,,Sun Feb 16 16:55:31 UTC 2014,,,,,,,,,,"0|i1u0bz:",383414,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 15:01;mwiewiorka;I have exactly the same problem using Hadoop 1.2.1, Spark 0.9 and Mesos 0.15.;;;","08/Feb/14 20:15;vinodkone;I'm able to repro too. I suspect it's networking binding issue.;;;","10/Feb/14 10:59;bijaybisht;Fix is available in the pull request #568

Bug caused by using the new Conf object, for setting spark.dirver.host etc. But the user of those properties still using the System.properties to access it. ;;;","16/Feb/14 16:55;ilikerps;PR #568 merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark on yarn - yarn-client mode doesn't always exit properly,SPARK-1049,12704488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,,30/Jan/14 08:27,08/Apr/14 20:40,14/Jul/23 06:25,08/Apr/14 20:40,0.9.0,,,,,,,,0.9.1,1.0.0,,,,,,,,,0,,,,,,"When running in spark on yarn in yarn-client mode, the application master doesn't always exit in a timely manner if its still waiting on containers from the yarn resource manager.  

",,berngp,tdas,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383180,,,Tue Apr 08 20:37:24 UTC 2014,,,,,,,,,,"0|i1u0jj:",383448,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/14 09:45;tgraves;https://github.com/apache/incubator-spark/pull/526;;;","08/Apr/14 20:03;tdas;[~tgraves] Wasnt this resolved, in master as well as branch 0.9?

This commit https://github.com/apache/spark/commit/b044b0b

 If so, shall we mark this resolved?;;;","08/Apr/14 20:37;tgraves;Yes it was, go ahead and resolve it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to disable the spark ui server (unit tests),SPARK-1047,12704450,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andrewor14,hbraun,,27/Jan/14 07:12,08/Jan/15 23:10,14/Jul/23 06:25,12/Sep/14 00:24,,,,,,,,,1.1.1,1.2.0,,,,Web UI,,,,,0,,,,,,"Provide the ability to disable the internal jetty server that serves the web ui. It's not needed when running unit tests and often conflicts may other ports on the system.
",,andrewor14,hbraun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2100,SPARK-3490,,,,,,,,SPARK-2100,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383185,,,Thu Sep 11 19:05:13 UTC 2014,,,,,,,,,,"0|i1u0kn:",383453,,,,,,,,,,,,,1.2.0,,,,,,,,,,"27/Jan/14 08:09;hbraun;The PR is here: https://github.com/apache/incubator-spark/pull/518/;;;","11/Sep/14 19:05;andrewor14;Here's the more updated PR: https://github.com/apache/spark/pull/2363;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable to build behind a proxy.,SPARK-1046,12704440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,sarutak,,26/Jan/14 22:37,27/Sep/14 19:07,14/Jul/23 06:25,27/Sep/14 19:07,0.8.1,,,,,,,,,,,,,Build,,,,,0,,,,,,"I tried to build spark-0.8.1 behind proxy and failed although I set http/https.proxyHost, proxyPort, proxyUser, proxyPassword.
I found it's caused by accessing  github using git protocol (git://).
The URL is hard-corded in SparkPluginBuild.scala as follows.

{code}
lazy val junitXmlListener = uri(""git://github.com/ijuma/junit_xml_listener.git#fe434773255b451a38e8d889536ebc260f4225ce"")
{code}

After I rewrite the URL as follows, I could build successfully.

{code}

lazy val junitXmlListener = uri(""https://github.com/ijuma/junit_xml_listener.git#fe434773255b451a38e8d889536ebc260f4225ce"")
{code}

I think we should be able to build whether we are behind a proxy or not.",,rxin,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383187,,,Sat Jun 21 21:00:14 UTC 2014,,,,,,,,,,"0|i1u0l3:",383455,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/14 22:41;rxin;Thanks for reporting this. Do you mind submitting a pull request for this?;;;","26/Jan/14 23:26;sarutak;Sure! I'll submit a pull request later.;;;","21/Jun/14 21:00;srowen;Is this stale / resolved? I don't see this in the code at this point.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark RDD's cannot deal with strings greater than 64K bytes.,SPARK-1043,12704474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,tyro89,,24/Jan/14 13:18,28/Jan/14 21:32,14/Jul/23 06:25,28/Jan/14 21:32,0.9.0,,,,,,,,0.9.1,,,,,PySpark,,,,,0,,,,,,"Pyspark uses java.io.DataOutputStream.writeUTF to send data to the python world which causes a problem since java.io.DataOutputStream.writeUTF fails if you pass it strings above 64K bytes. Furthermore a fix to this issue is not straight forward since the java to python protocol actually relies on this and uses it as the separator between items. The offending write happens in 

   core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:220

and the reliance on this to separate items can be found in the  MUTF8Deserializer class in

   python/pyspark/serializers.py:264

The only solution I currently have in mind would be to change the protocol to either extend the number of bytes used to specify the length of the item or to add a boolean flag to every ""packet"" to indicate wether the item is split into multiple parts (although the second option might result in bad data if multiple things are writing to these steams)

",,bouk,joshrosen,tyro89,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383183,,,Tue Jan 28 21:32:55 UTC 2014,,,,,,,,,,"0|i1u0k7:",383451,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/14 22:28;joshrosen;Thanks for catching this.  I'll look into alternative encodings that support longer strings and add a test case for large strings to make sure this doesn't break again in the future.;;;","26/Jan/14 11:57;tyro89;I've submitted a quick fix to this https://github.com/apache/incubator-spark/pull/512 not sure if you guys usually discuss things here or on github so I'm linking this to the pr and the pr to this :);;;","26/Jan/14 14:15;bouk;Note that the current encoding will also break if any of the modified characters described here: http://docs.oracle.com/javase/7/docs/api/java/io/DataInput.html#modified-utf-8 are used. The python side doesn't implement anything to mitigate this, it's probably safest to switch to vanilla UTF-8;;;","26/Jan/14 14:24;joshrosen;There's no reason that PySpark has to use MUTF-8; this was a bad choice that I made when implementing custom serializer support (https://github.com/apache/incubator-spark/pull/146), so I'd switch to vanilla UTF-8 instead.;;;","28/Jan/14 21:32;joshrosen;Fixed in https://github.com/apache/incubator-spark/pull/523;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark cleans all java broadcast variables when it hits the spark.cleaner.ttl ,SPARK-1042,12704767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,qqsun8819,sliwo,,23/Jan/14 09:29,21/Oct/14 06:53,14/Jul/23 06:25,21/Oct/14 06:53,0.8.0,0.8.1,0.9.0,,,,,,0.9.2,,,,,Java API,Spark Core,,,,0,memory_leak,,,,,"When setting spark.cleaner.ttl, spark performs the cleanup on time - but it cleans all broadcast variables, not just the ones that are older than the ttl. This creates an exception when the next mapPartitions runs because it cannot find the broadcast variable, even when it was created immediately before running the task.

Our temp workaround - not set the ttl and suffer from an ongoing memory leak (forces a restart).

We are using JavaSparkContext and our broadcast variables are Java HashMaps.",,liyinan926,pwendell,rxin,sliwo,tsliwowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383008,,,Tue Oct 21 06:53:25 UTC 2014,,,,,,,,,,"0|i1tzhb:",383276,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/14 06:38;qqsun8819;I'll try to solve this . But I can't put myself as Assignee now.;;;","25/Jan/14 09:43;rxin;Thanks. I assigned you the ticket.;;;","28/Jan/14 10:34;liyinan926;If I'm right, you can disable cleaning broadcast variables by setting the property spark.cleaner.ttl.BROADCAST_VARS=-1, when you set spark.cleaner.ttl. Take a look at MetadataCleaner.scala and BlockManager.scala.;;;","28/Jan/14 10:36;liyinan926;Just realized the fix I mentioned above is only in 0.9. So this still needs to be fixed in 0.8.0 and 0.8.1.;;;","28/Jan/14 12:16;sliwo;Thanks. I'd rather not disable broadcast vars entirely but have some ttl for them too. The issue is that with 0.8.1, once the ttl comes, all the broadcast variables get cleaned, not just the old ones. Was this fixed too in 0.9?;;;","30/Jan/14 00:37;qqsun8819;Sorry for latet comment. @Tal Sliwowicz I wonder which mode are you running spark? local or cluster. for local mode, broadcast variables will only be wriiten in BlockManager as a MEMORY_AND_DISK storage level. Otherwise, they will be written both in BlockManager and local files under control of HttpBroadCast (if you didn't change default broadcast), and both ways have their own clean up function callbacks period done by MetadataCleaner. Could u plz show your detail config for spark mode and BroadCast type, so I can do a further lookup of the src;;;","30/Jan/14 00:50;sliwo;We are running spark 0.8.1 in a mesos 0.13 cluster (with zookeeper - so mesos runs in multi master). We did not change any of the cleaner defaults and are using the fault storage level (memory).  Thanks!

;;;","31/Jan/14 05:20;qqsun8819; @Tal Sliwowicz I wonder the value you set for spark.cleaner.ttl (in seconds) and the interval between each time you create broadcast valuse. Becuase I found that the way MetaCleaner works is a little tricky:
1 there is a delay which equal to value set to ttl 
2 there is a period = max(10, delay / 10), so this period is at least 10 seconds
3 there is a Timer schedule for clean tasks (for clean up broadcast vars under ttl.  Cleanup tasks first execute  after (period) secnods and then do every (period) seconds. 
(code is timer.schedule(task, periodSeconds * 1000, periodSeconds * 1000)
What I found is that there is no bugs in cleanup function implementation because each time this task check the formula: ( ( currTime - varCreateTime) > ttl )  == true ), if this happens ,all this var entry will be cleaned.
But the period may have some blocks. For examples , if period = 10 seconds, and your create 3 broadcast vars just in 10 seconds, and your ttl is 5 seconds. So the first time the clean up task is executed after (period  = 10 ) seconds, all broadcast variables just hit the ttl , and will be cleaned.
So maybe your setting for ttl and the resulting period value will affect the clean up result. So check whether you ttl is smaller than the resulting period , and whether it is the reason which cause your issue description. And if you find out that it 's not the reason, I'll just look for other reasons and solutions.Thanks! ;;;","03/Feb/14 00:02;sliwo;Thanks for the detailed reply!

We tried with ttl = 300 . We have a loop were we periodically, each iteration, create new RDDs and run calculations over them. Each loop iteration we also create new broadcast variables. These variables and RDDs are only used inside the specific loop iteration, and never again. When we use the ttl it kept giving exceptions on broadcast variables that were not available to the worker nodes. Is this some thing that does not reproduce for you?;;;","05/Feb/14 19:13;qqsun8819;Hi,@Tal, I'm working on this, it would be wonderful if you give me the detail print stack for what you describle of 'exceptions on broadcast variables that were not available to the worker nodes', at least the class name of the Exception,so I can do a detail debug . Thanks!;;;","18/Feb/14 04:25;sliwo;Hi @OuyangJin 
We tried to reproduce again (on a different driver jar, but it runs the same way as described in the ticket) - this time received the error below. We tried several times, and it is very consistent. Notice - the ""No space left on device"" is bogus. There is 100G+ free space.
We are using Spark 0.8.1.

java.io.FileNotFoundException (java.io.FileNotFoundException: /var/lib/spark/data/spark-local-20140218081428-ae01/2e/merged_shuffle_17562_361_0 (No space left on device))

java.io.FileOutputStream.openAppend(Native Method)
java.io.FileOutputStream.<init>(FileOutputStream.java:192)
org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:114)
org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:173)
org.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:162)
org.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:159)
scala.collection.Iterator$class.foreach(Iterator.scala:772)
scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:159)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:100)
org.apache.spark.scheduler.Task.run(Task.scala:53)
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:215)
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:50)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
java.lang.Thread.run(Thread.java:662);;;","03/Mar/14 06:10;sliwo;We are planning to test this week with 0.9. I will update.;;;","03/Mar/14 19:24;qqsun8819;Hi @Tal, sorry for late reply . What I can see from your exception which was  posted days ago happened during shuffle phase. 
And this code segment (exception report in ShuffleMapTask.runTask) is  a logic which write map output to its associate buckets. This is a pre-phase for standard shuffle which in latter phase ResultTask can read this output using socket or netty depending on your config. I wonder your broadcast variable are for shuffle use? You say""Each loop iteration we also create new broadcast variables"" You use the broadcast to store a result of RDD?(like sc.broadcast(rdd.collect().toMap()) and use it inside of a closure of another RDD? Maybe you can show me the code logic inside of your loop(you can use simplified pseudocode if you think it's not suitable to show your real code relating some copyright)
Thanks!;;;","03/Mar/14 21:55;sliwo;You are correct - there is a mismatch. It seems that there was a corruption of the filesystem which caused the error above. It is now fixed. We will try to reproduce again the issue with the broadcast using the 0.9 we now have running (the original exception was specific to broadcast variables, not a general one);;;","03/Mar/14 22:14;qqsun8819;@Tal. Thanks for your progress. When you mean fixed, you mean you fix a bug in your user code or in your local filesystem? ;;;","03/Mar/14 22:28;sliwo;Local Filesystem (reformatted);;;","03/Mar/14 22:38;qqsun8819;Great. So any progress in your testing for 0.9.0, please let me know so that I can help you track this bug. If this ttl feature really don't work as it declare, I'll fix it with you test support
Thanks;;;","05/Mar/14 06:39;sliwo;Tested on 0.9
This is an exception stack with ttl=120 (the job takes under 60 seconds and then gets rid of all the broadcast variables. It creates new ones each iteration)

2014-03-05 09:01:59,165 ERROR [Executor task launch worker-17] Executor  - Exception in task ID 225722
java.io.FileNotFoundException: http://192.168.10.85:54174/broadcast_312
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1457)
	at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156)
	at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56)
	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)
	at org.apache.spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:69)
	at org.apache.spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:138)
	at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1795)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1754)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
2014-03-05 09:01:59,168 ERROR [Executor task launch worker-11] Executor  - Exception in task ID 225734
java.io.FileNotFoundException: http://192.168.10.85:54174/broadcast_312
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1457)
	at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156)
	at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56)
	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)
	at org.apache.spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:69)
	at org.apache.spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:138)
	at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1795)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1754)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662);;;","05/Mar/14 19:42;qqsun8819;Great, I'll look into it. Any progress will be reported ASAP when I found the reason for it;;;","15/Mar/14 07:33;qqsun8819;@Tal, according to your execption reported, I found out that it happens during task deserialization.So it looks like you put a broadcast variable inside a rdd closure ,and when excutor what to run a task, it has to deserialize a rdd closure together with broadcast variable, and the broadcast varialbe can't be found in the HttpServer
I'm still not sure the reason exactly causes this problem. I'm sorry for that. There are 3 problems I want to be sure for your user code. 1 You say that you create new broadcast variables each time your enter the loop, and each boradcast variable has a ttl for 120 seconds. Are you sure that your rdd closre just use the newly created broadcast variables, not old ones in previous loop . 2 You say your job runs for 60 seconds and all broadcast varialbe cleaned up. In this 60 seconds, how many iterations of your loop happens? Just 1 round of your loop or serveral rounds. 3 When broadcast variables are cleaned up, there will be logs like ""Deleted broadcast file"" , And broadcast variables will be stored on machines which run your driver program. Have your ever see these log in your console or driver machine
Thanks. Sorry for still not finding the exact reason. But previously community also reported several user cases that program surprisingly clean up their broadcast varialbe which are still in use, due to httpserver down which storing the broadcast varialbes, or time not correctly caculated. So now some people are developing a new clean up way which will replace MetadataCleaner and TTL config, in this  PR:https://github.com/apache/spark/pull/126
But I still want to figure out the exact reason because many people are still using 0.9 version. It's best if you can provide some detail information;;;","16/Apr/14 08:59;tsliwowicz;1. Yes, I'm sure it creates and uses a news instance
2. Just 1 round. I can increase the ttl, but it it still happens.
3. I turned logs to WARN. Can you be more specific about the classes I should increase logging on? I will move them to info or even debug ;;;","16/Apr/14 09:05;tsliwowicz;About 3 - the package org.apache.spark.broadcast , right?;;;","16/Apr/14 09:35;qqsun8819;yes is in org.apache.spark.broadcast, ""Deleted broadcast file"" ,this is a info log;;;","20/Oct/14 08:30;tsliwowicz;[~qqsun8819] I think the issue was resolved in 0.9.2. We are not experiencing it any more. Thanks!;;;","21/Oct/14 06:53;pwendell;I think this was fixed back in 0.9.2;;;",,,,,,,,
ec2-related lines in start-*.sh no longer work ,SPARK-1041,12704775,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,,23/Jan/14 08:27,22/Feb/14 20:37,14/Jul/23 06:25,22/Feb/14 20:35,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"Due to the change of hostname of EC2 instances, the following lines in the start-* scripts no longer work 

# Set SPARK_PUBLIC_DNS so slaves can be linked in master web UI
if [ ""$SPARK_PUBLIC_DNS"" = """" ]; then
    # If we appear to be running on EC2, use the public address by default:
    # NOTE: ec2-metadata is installed on Amazon Linux AMI. Check based on that and hostname
    if command -v ec2-metadata > /dev/null || [[ `hostname` == *ec2.internal ]]; then
        export SPARK_PUBLIC_DNS=`wget -q -O - http://instance-data.ec2.internal/latest/meta-data/public-hostname`
    fi
fi

since we have spark-ec2, suggest removing these lines

",,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383127,,,Fri Feb 21 18:55:36 UTC 2014,,,,,,,,,,"0|i1u07r:",383395,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 18:55;codingcat;PR: https://github.com/apache/incubator-spark/pull/588;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collect as Map throws a casting exception when run on a JavaPairRDD object,SPARK-1040,12704761,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,kevinmader,,23/Jan/14 02:03,23/Sep/15 14:29,14/Jul/23 06:25,26/Jan/14 00:24,0.9.0,,,,,,,,0.9.1,,,,,Java API,,,,,0,,,,,,"The error that arises
{code}
Exception in thread ""main"" java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lscala.Tuple2;
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:427)
	at org.apache.spark.api.java.JavaPairRDD.collectAsMap(JavaPairRDD.scala:409)
{code}

The code being executed
{code:java}
public static String ImageSummary(final JavaPairRDD<Integer,int[]> inImg) {
		final Set<Integer> keyList=inImg.collectAsMap().keySet();
		for(Integer cVal: keyList) outString+=cVal+"","";
		return outString;
	}
{code}

The line 426-427 from PairRDDFunctions.scala
{code:java}
 def collectAsMap(): Map[K, V] = {
    val data = self.toArray()
{code}",,glenn.strycker@gmail.com,joshrosen,kevinmader,pulkit.bosco05@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4489,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383188,,,Wed Sep 23 14:29:16 UTC 2015,,,,,,,,,,"0|i1u0lb:",383456,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 11:02;joshrosen;I wonder where that cast is being performed, since there's no explicit cast at that line.  Does the traceback have any more detail?

JavaPairRDD.collectAsMap() is tested at least once in JavaAPISuite: https://github.com/apache/incubator-spark/blob/master/core/src/test/scala/org/apache/spark/JavaAPISuite.java#L290, so I wonder why that test works but your code fails.;;;","25/Jan/14 14:43;kevinmader;I tried it again and it's definitely throws the same error. Could it be a type issue since I am using an array of primitives (int[]) in the JavaPairRDD, which Java doesn't always so eagerly support particularly as a generic?;;;","25/Jan/14 16:23;joshrosen;I was able to reproduce this:

{code}
@Test
  public void collectAsMapWithIntArrayValues() {
    // Regression test for SPARK-1040
    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(new Integer[] { 1 }));
    JavaPairRDD<Integer, int[]> pairRDD = rdd.map(new PairFunction<Integer, Integer, int[]>() {
      @Override
      public Tuple2<Integer, int[]> call(Integer x) throws Exception {
        return new Tuple2<Integer, int[]>(x, new int[] { x });
      }
    });
    pairRDD.collect();  // works fine
    Map<Integer, int[]> map = pairRDD.collectAsMap();  // crashed originally
  }
{code}

I see the same traceback that you reported above.

If I create a JavaPairRDD using SparkContext.parallelizePairs(), collectAsMap() works fine; the problem seems to only affect JavaPairRDDs that have been derived by transforming non-JavaPairRDDs.

Also, it only seems to affect collectAsMap(); calling collect() on the same RDD works fine.  From the error, I suspect that the failing cast has to do with an implicit conversion to PairRDDFunctions.;;;","25/Jan/14 16:45;joshrosen;I found a fix and opened a pull request here: https://github.com/apache/incubator-spark/pull/511;;;","28/Sep/14 21:36;pulkit.bosco05@gmail.com;The github link above is not functional anymore.;;;","22/Sep/15 22:08;glenn.strycker@gmail.com;I am getting a similar error in Spark 1.3.0... see a new ticket I created:  https://issues.apache.org/jira/browse/SPARK-10762;;;","23/Sep/15 14:29;glenn.strycker@gmail.com;My ticket SPARK-10762 may have just been a user error, but was interesting none-the-less... evidently Scala or Spark is not correctly reporting the type of an ArrayBuffer as ArrayBuffer[Any], but claimed it had correctly been cast to ArrayBuffer[(Int,String)].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
.gitignore is overly aggressive,SPARK-1036,12704725,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,mackrorysd,,20/Jan/14 14:02,07/Feb/20 17:19,14/Jul/23 06:25,26/Jul/14 20:22,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"The .gitignore file ignores ""lib/"" but this includes directories named lib anywhere in the tree, including the following:

{code}
./python/lib
./graphx/src/main/scala/org/apache/spark/graphx/lib
./graphx/src/test/scala/org/apache/spark/graphx/lib
{code}

It would be confusing and incorrect if any patches accidentally omitted changes to the contents of these directories.",,joshrosen,mackrorysd,patrick,prashant1,rxin,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383202,,,Sat Jul 26 20:22:01 UTC 2014,,,,,,,,,,"0|i1u0of:",383470,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/14 14:07;rxin;Ooops ....;;;","20/Jan/14 14:08;mackrorysd;Here's one suggestion. I don't know if there are other lib/ directories that show up after some actions that should also be ignored, and there are probably other entries that are intended to be ignored in the root directory only, like /logs/, etc...;;;","20/Jan/14 14:12;rxin;I'm not even sure why we want to exclude the lib folder. Unlike lib_managed, jars in lib are meant to be included. Looks like [~prashant] included it originally. Can you comment on this, [~prashant]?;;;","20/Jan/14 14:45;patrick;[~mackrorysd] Could you submit this patch as a PR?;;;","20/Jan/14 20:36;prashant;Hey Reynold,

I added it initially thinking of the ability to ""provide"" jars in sbt, as in sbt puts everything kept in lib folder on the classpath for run. Since at that point we were not having anything in lib/ so I added it in ignore so that it does not get accidentally committed. But now since we want it we can surely remove the entry from .gitignore or I even liked [~mackrorysd]'s suggestion too. ;;;","21/Jan/14 07:42;mackrorysd;Thanks, everyone. I've submitted Pull Request #488 to remove this line entirely, given Prashant's comments.;;;","26/Jul/14 20:22;joshrosen;Fixed by Patrick in https://github.com/apache/spark/commit/e437069dcecea5b89c2a37f0b7b1f8dd5796b8df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Py4JException on PySpark Cartesian Result,SPARK-1034,12704777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,mrt,,19/Jan/14 12:27,26/Jul/14 22:26,14/Jul/23 06:25,23/Jan/14 19:47,0.9.0,,,,,,,,0.9.0,,,,,PySpark,,,,,1,,,,,,"RDD operations on results of the Pyspark Cartesian method return Py4JException.



Here's a few examples
{code:title=$ bin/pyspark|borderStyle=solid}
>>> rdd1=sc.parallelize([1,2,3,4,5,1])
>>> rdd2=sc.parallelize([11,12,13,14,15,11])
>>> rdd1.cartesian(rdd2).map(lambda x: x[0] + x[1]).collect()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py"", line 446, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py"", line 1041, in _jrdd
    class_tag)
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"", line 669, in __call__
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"", line 304, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace:
py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.CartesianRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202)
	at py4j.Gateway.invoke(Gateway.java:213)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:744)


>>> 
>>> rdd1.cartesian(rdd2).count()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py"", line 525, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py"", line 516, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py"", line 482, in reduce
    vals = self.mapPartitions(func).collect()
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py"", line 446, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py"", line 1041, in _jrdd
    class_tag)
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"", line 669, in __call__
  File ""/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"", line 304, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace:
py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.CartesianRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202)
	at py4j.Gateway.invoke(Gateway.java:213)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:744)
{code}

I see this issue after the custom serializer change.
https://github.com/apache/incubator-spark/commit/cbb7f04aef2220ece93dea9f3fa98b5db5f270d6",,broxton,joshrosen,mrt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2601,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383927,,,Thu Jan 23 16:47:39 UTC 2014,,,,,,,,,,"0|i1u55j:",384195,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 23:12;broxton;I can confirm that I see this bug as well using Spark 0.9.1.  It appears that collect() works correctly:

>>> test_rdd1 = sc.parallelize(range(5), 2)
>>> test_rdd2 = test_rdd1.cartesian(test_rdd1)
>>> print test_rdd2.collect()
[(0, 0), (0, 1), (1, 0), (1, 1), (0, 2), (0, 3), (1, 2), (1, 3), (0, 4), (1, 4), (2, 0), (2, 1), (3, 0), (3, 1), (4, 0), (4, 1), (2, 2), (2, 3), (3, 2), (3, 3), (2, 4), (3, 4), (4, 2), (4, 3), (4, 4)]

However, other operations like map(), count(), and reduce() produce the same exception that Taka has reported above.;;;","23/Jan/14 01:29;broxton;After spending a little bit of time delving into the code, I can say that this problem appears to occur at line 1038 of rdd.py when the PythonRDD object is constructed.

        python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
            bytearray(pickled_command), env, includes, self.preservesPartitioning,
            self.ctx.pythonExec, broadcast_vars, self.ctx._javaAccumulator,
            class_tag)

It appears that Py4J can't find the corresponding Java constructor when this function is called on a Cartesian RDD.  I added the following debugging code immediately before line 1038:

        print '---'
        print self._prev_jrdd.rdd()
        print len(bytearray(pickled_command))
        print env
        print includes
        print self.preservesPartitioning
        print self.ctx.pythonExec
        print broadcast_vars
        print self.ctx._javaAccumulator
        print class_tag
        print '---'

This prints out each argument that is passed to the PythonRDD constructor.   Having made this change, I ran this in Spark:

  test_rdd = sc.parallelize(range(5), 2)  
  print test_rdd.count()                            # THIS WORKS!!

which produced this debugging output on the console:

------
ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:206
1431
{}
[]
False
python
[]
[]
Array[byte]
---

However, when I run this:

  test_rdd1 = sc.parallelize(range(5), 2)  
  test_rdd2= test_rdd1.cartesian(test_rdd1)
  print test_rdd2.count()                          # FAILS WITH Py4J EXCEPTION

I get this debugging output:

---
CartesianRDD[1] at cartesian at NativeMethodAccessorImpl.java:-2
1506
{}
[]
False
python
[]
[]
Object
---

Since both ParallelCollectionRDD and CartesianRDD are subclasses of the same RDD superclass, I suspect that the meaningful difference between the ""working"" and ""not working"" version is the last argument to the constructor: the class_tag.  It appears that the class_tag of a cartesian RDD is a Scala object of some sort, whereas the class_tag of the first RDD was Array[Byte].  

I spent a little bit of time poking around PythonRDD.scala to see if I could understand why the constructor only worked with the first set of arguments, but I'm afraid I cannot see what is causing this bug.  But, hopefully you will find this info useful when you fix this bug.  Thanks!;;;","23/Jan/14 01:56;mrt;Michael, thanks for the info. I'm also looking into the same place.;;;","23/Jan/14 15:19;joshrosen;Thanks for reporting this bug, and for the detailed investigation so far.

Michael, in both cases in your example, {{class_tag}} is actually a Scala ClassTag object and that {{Array\[Byte\]}} and {{Object}} are the results of calling {{toString()}} on those ClassTags.

In cartesian, we're trying to wrap a {{JavaPairRDD<Array<Byte>, Array<Byte>>}} into a PythonRDD.  The ClassTag for this {{JavaRDD<Tuple2<Array<Byte>, Array<Byte>>>}} should be a {{ClassTag<Tuple2<Array<Byte>, Array<Byte>>}} (or a {{ClassTag<Tuple2<?, ?>>}}).  The root problem here may actually be in JavaPairRDD's classTag:

{code}
scala> val x = sc.parallelize(Seq(Array[Byte](0))).map(x => (x, x))
x: org.apache.spark.rdd.RDD[(Array[Byte], Array[Byte])] = MappedRDD[7] at map at <console>:18

scala> JavaPairRDD.fromJavaRDD(x).classTag
res15: scala.reflect.ClassTag[(Array[Byte], Array[Byte])] = Object

scala> JavaRDD.fromRDD(x).classTag
res17: scala.reflect.ClassTag[(Array[Byte], Array[Byte])] = scala.Tuple2
{code}

This turns out to be caused by a line in JavaPairRDD that constructed its ClassTag by casting an AnyRef ClassTag, when it should have just used the underlying ScalaRDD's {{elementClassTag}} (since it's already of the right type):

{code}
override val classTag: ClassTag[(K, V)] =
    implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[Tuple2[K, V]]]
{code}

should be 

{code}
  override val classTag: ClassTag[(K, V)] = rdd.elementClassTag
{code}

I've submitted this fix as part of a pull request: https://github.com/apache/incubator-spark/pull/501;;;","23/Jan/14 15:55;mrt;Josh, 
thanks for the quick fix! I've confirmed this change fixes the problem.;;;","23/Jan/14 16:47;broxton;Hi Josh,

Yes, I can confirm that this fixes it for me as well.  Tremendous thanks for taking a look at this, and for taking the time to explain the fix.  

Now back to data analysis!

Best,

  -Michael;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"If Yarn app fails before registering, app master stays around long after",SPARK-1032,12704678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandy,,19/Jan/14 01:37,04/Apr/14 20:51,14/Jul/23 06:25,28/Feb/14 08:52,0.9.0,,,,,,,,0.9.1,1.0.0,,,,YARN,,,,,0,,,,,,"My Spark/YARN app hit an error while initializing its SparkContext (YARN-1031).   The app stayed around for another 5 minutes continually logging ""14/01/18 23:58:59 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered."" until YARN finally killed it.  This is probably because it died before registering in the first place, but still tried to unregister.",,sandy,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382947,,,Fri Feb 28 08:52:18 UTC 2014,,,,,,,,,,"0|i1tz3r:",383215,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 22:28;sandy;https://github.com/apache/incubator-spark/pull/648;;;","28/Feb/14 08:52;tgraves;pr https://github.com/apache/spark/pull/28 committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException when running simple Spark app on Yarn,SPARK-1031,12704734,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sandy,,19/Jan/14 01:20,20/Jan/14 15:05,14/Jul/23 06:25,19/Jan/14 21:58,0.9.0,,,,,,,,0.9.0,,,,,Spark Core,YARN,,,,0,,,,,,"I hit a FileNotFoundException in the application master when running the SparkPi example as described in the docs against Yarn 2.2.

The problem appears to be that the app master requires the app jar to be in its working directory with the same name as returned by SparkContext.jarOfClass.  A symlink called app.jar to the app jar exists in the working directory, but jar itself lives in the NodeManager local cache dir with the name it was submitted with (e.g. spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar).  When adding the jar, the SparkContext only uses the last component in its path:
{code}
            if (SparkHadoopUtil.get.isYarnMode() && master == ""yarn-standalone"") {
              // In order for this to work in yarn standalone mode the user must specify the 
              // --addjars option to the client to upload the file into the distributed cache 
              // of the AM to make it show up in the current working directory.
              val fileName = new Path(uri.getPath).getName()
              try {
                env.httpFileServer.addJar(new File(fileName))
{code}

If SparkContext.jarOfClass returns the jar in its linked-to location, the file will not be found.

When I modified things to set fileName to use the jar's full path, my app completed successfully.",,sandy,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383203,,,Mon Jan 20 15:05:33 UTC 2014,,,,,,,,,,"0|i1u0on:",383471,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/14 10:23;tgraves;You can work around this by specifying the --addJars option.

To make this more user friendly and not have to specify the --addJars option we made a small change:
https://github.com/apache/incubator-spark/pull/470/;;;","19/Jan/14 10:37;sandy;With that change, an error will still be logged in normal operation, right?  If I am giving a jar through --jar, should I also need to --addJar it?;;;","19/Jan/14 11:06;tgraves;yeah it will still log the error and your application will most likely fail later on when it tries to use it.  

To clarify this is non spark examples case.  The spark examples don't really need the jar distributed again since its the app jar.  In a ""real"" application that uses sc.addJar it will log an error if it doesn't exist and then later on will most likely fail since the jar won't have been distributed.;;;","19/Jan/14 11:07;tgraves;not sure what you mean by --jar option.. If you sc.addJar something then for yarn you have to specify --addJars to distributed it to the application master where it can be distributed through the normal spark addJar HttpFileServer mechanism.;;;","19/Jan/14 11:31;sandy;
Spoke to Patrick offline and I think I understand the fundamental issue behind the strangeness.  Let me know if this is incorrect:
The app jar is distributed through a different mechanism than additional added jars.  The app jar gets to every worker node as a Yarn local resource.  Additional jars only get to the app master, and the app master serves them to workers with the HTTP file server.  The strangeness comes when an application addJar's the app jar, which is a natural thing to do in mesos or standalone mode, but in Yarn mode, will try to distribute the same jar through a different mechanism.

Would it make sense to use the same mechanism for both of these?  I.e. to ship the app jar to workers through the HTTP file server or to ship additional jars to workers as Yarn localized resources?
;;;","19/Jan/14 11:35;sandy;An additional benefit of using the same mechanism is that it would greatly simplify debugging ClassNotFoundExceptions in workers. This is a painful exercise on any framework, but gets harder with each different way that bits can get sent over.;;;","19/Jan/14 11:54;tgraves;This is something Patrick and I spoke about and I plan to look into.;;;","19/Jan/14 11:58;tgraves;I should add feel free to take a look at it if you want.  I was planning on looking more to distributed the app.jar via the spark mechanism rather then the yarn mechamism;;;","20/Jan/14 00:20;sandy;Cool, I will try to take a look.  Filed SPARK-1035 for this.  Is there a reason you think the app.jar mechanism is preferable to the Yarn mechanism?  With the Yarn mechanism we could avoid sending the jars if another app already has them cached. It also requires fewer copies even when they aren't already cached.;;;","20/Jan/14 06:59;tgraves;The main reason I was going to start there was to keep it consistent with the way spark does it. Should be easier to maintain, less changes for the on yarn side, easier for people to understand what is going on if they have used standalone/mesos mode, etc.

But as you say the performance needs to be evaluated.  If its slower or if we thinking people will use the public distributed cache then we should leave it using yarn.   I generally don't see many people using the public distributed cache for their application jars. Either on Spark or MR. That might not be true with other people though.  ;;;","20/Jan/14 13:19;sandy;bq. I generally don't see many people using the public distributed cache for their application jars.
YARN-1492 should make the shared distributed cache a more attractive (and hopefully the default) option.  I've spoken to a few people who say that, on MapReduce, loading jars which many jobs have in common is a significant part of their job time.  It would be nice for Spark not to be behind MR and Tez in this regard. ;;;","20/Jan/14 15:05;tgraves;I agree. Even without YARN-1492 the yarn distributed cache should be better, especially if the incase of a large # of workers.  With the spark HttpFileServer they are fetching from one server vs being distributed to multiple nodes from HDFS. 

The downside again is that its different, even between yarn-client and yarn-standalone mode.  I guess we could add the functionality to yarn-client mode and if they specify the --addJars type parameter then it uses the distributed cache instead of the HttpFileServer.;;;",,,,,,,,,,,,,,,,,,,,,
unneeded file required when running pyspark program using yarn-client,SPARK-1030,12704675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,dcarroll@cloudera.com,,17/Jan/14 11:19,25/Jul/14 01:42,14/Jul/23 06:25,25/Jul/14 01:42,0.8.1,,,,,,,,1.0.0,,,,,Deploy,PySpark,YARN,,,0,,,,,,"I can successfully run a pyspark program using the yarn-client master using the following command:
{code}
SPARK_JAR=$SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.1-incubating-hadoop2.2.0.jar \
SPARK_YARN_APP_JAR=~/testdata.txt pyspark \
test1.py
{code}

However, the SPARK_YARN_APP_JAR doesn't make any sense; it's a Python program, and therefore there's no JAR.  If I don't set the value, or if I set the value to a non-existent files, Spark gives me an error message.  
{code}
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: org.apache.spark.SparkException: env SPARK_YARN_APP_JAR is not set
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:46)
{code}

or

{code}
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.io.FileNotFoundException: File file:dummy.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)
{code}

My program is very simple:

{code}
from pyspark import SparkContext
def main():
    sc = SparkContext(""yarn-client"", ""Simple App"")
    logData = sc.textFile(""hdfs://localhost/user/training/weblogs/2013-09-15.log"")
    numjpgs = logData.filter(lambda s: '.jpg' in s).count()
    print ""Number of JPG requests: "" + str(numjpgs)
{code}

Although it reads the SPARK_YARN_APP_JAR file, it doesn't use the file at all; I can point it at anything, as long as it's a valid, accessible file, and it works the same.

Although there's an obvious workaround for this bug, it's high priority from my perspective because I'm working on a course to teach people how to do this, and it's really hard to explain why this variable is needed!",,berngp,dcarroll@cloudera.com,farrellee,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1053,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383175,,,Fri Jul 25 01:42:23 UTC 2014,,,,,,,,,,"0|i1u0if:",383443,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/14 13:57;farrellee;using pyspark to submit is deprecated in spark 1.0 in favor of spark-submit. i think this should be closed as resolved/workfix. /cc: [~pwendell] [~joshrosen];;;","25/Jul/14 01:42;joshrosen;Closing this now, since it was addressed as part of Spark 1.0's PySpark on YARN patches (including SPARK-1004).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark Window shell script errors regarding shell script location reference,SPARK-1029,12704634,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,Qiuzhuang,,15/Jan/14 23:10,24/Jan/15 22:17,14/Jul/23 06:25,24/Jan/15 22:17,0.9.0,,,,,,,,1.0.0,,,,,Windows,,,,,0,,,,,,"When launch spark-shell.cmd in Window 7, I got following errors

E:\projects\amplab\incubator-spark>bin\spark-shell.cmd
'E:\projects\amplab\incubator-spark\bin\..\sbin\spark-class2.cmd' is not recognized as an internal or external command,
operable program or batch file.

E:\projects\amplab\incubator-spark>bin\spark-shell.cmd
'""E:\projects\amplab\incubator-spark\bin\..\sbin\compute-classpath.cmd""' is not recognized as an internal or external co
mmand,
operable program or batch file.
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/repl/Main

I am attaching my patches,

",,Qiuzhuang,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383206,,,Sat Jan 24 22:17:24 UTC 2015,,,,,,,,,,"0|i1u0pb:",383474,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 23:20;rxin;Thanks for submitting a patch. Do you mind submitting a github pull request against https://github.com/apache/incubator-spark ?;;;","16/Jan/14 22:36;Qiuzhuang;PR #: https://github.com/apache/incubator-spark/pull/451
;;;","24/Jan/15 22:17;srowen;Looks like this was fixed in https://github.com/apache/spark/commit/4e510b0b0c8a69cfe0ee037b37661caf9bf1d057 for 1.0.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell automatically set MASTER  fails,SPARK-1028,12704635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,crazyjvm,,15/Jan/14 19:05,02/Mar/14 18:56,14/Jul/23 06:25,09/Feb/14 23:53,0.8.0,0.8.1,,,,,,,0.9.1,1.0.0,,,,Spark Core,,,,,0,spark-shell,,,,,"spark-shell intends to set MASTER automatically if we do not provide the option when we start the shell , but there's a problem. 
The condition is ""if [[ ""x"" != ""x$SPARK_MASTER_IP"" && ""y"" != ""y$SPARK_MASTER_PORT"" ]];""     we sure will set SPARK_MASTER_IP explicitly, the SPARK_MASTER_PORT option, however, we probably do not set just using spark default port 7077. So if we do not set  SPARK_MASTER_PORT, the condition will never be true. We should just use default port if users do not set port explicitly I think.",,crazyjvm,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383090,,,Sun Mar 02 18:56:01 UTC 2014,,,,,,,,,,"0|i1tzzj:",383358,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 19:06;crazyjvm;if needed , i'll provide a patch. ;;;","15/Jan/14 19:09;rxin;Sure. Please do. Thanks!;;;","15/Jan/14 19:13;crazyjvm;OK, i'll pull request today.;;;","15/Jan/14 20:04;crazyjvm;SPARK-1028 pull request at https://github.com/apache/incubator-spark/pull/449/files;;;","09/Feb/14 23:46;crazyjvm;already merged to master. please close.;;;","02/Mar/14 18:54;crazyjvm;@Reynold, it seems that 0.9.0 did not fix this ?;;;","02/Mar/14 18:56;rxin;I think it was merged into master for 1.0.0 and 0.9.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone cluster should use default spark home if not specified by user,SPARK-1027,12704608,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,patrick,,15/Jan/14 11:03,22/Jan/14 18:58,14/Jul/23 06:25,22/Jan/14 18:58,0.8.1,0.9.0,,,,,,,1.0.0,,,,,Deploy,,,,,0,starter,,,,,"I added a quick fix to this in PR #442 (only in 0.9.0). But a nicer fix would be to have SparkHome be an Option[String] in ApplicationDescription and then to percolate that option all the way to the worker.

Whatever fix we get should merge into both 0.8 and 0.9 branches.",,codingcat,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383200,,,Wed Jan 15 17:47:40 UTC 2014,,,,,,,,,,"0|i1u0nz:",383468,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 17:47;codingcat;made a PR in https://github.com/apache/incubator-spark/pull/447;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark using deprecated mapPartitionsWithSplit,SPARK-1026,12704585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,markhamstra,,14/Jan/14 08:56,23/Jan/14 21:08,14/Jul/23 06:25,23/Jan/14 21:08,0.9.0,,,,,,,,0.9.0,,,,,PySpark,Spark Core,,,,0,,,,,,"In the Scala API, mapPartitionsWithSplit has been deprecated for a long time and really should be removed.  However, PySpark uses mapPartitionsWithSplit instead of mapPartitionsWithIndex, so mapPartitionsWithSplit now can't be removed until it has first been deprecated in PySpark for at least one release.",,joshrosen,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383195,,,Thu Jan 23 19:41:55 UTC 2014,,,,,,,,,,"0|i1u0mv:",383463,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 19:41;joshrosen;I've submitted a pull request for this: https://github.com/apache/incubator-spark/pull/505;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark hangs when parent base file is unavailable,SPARK-1025,12704588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,dcarroll@cloudera.com,,14/Jan/14 08:55,26/Jan/14 00:23,14/Jul/23 06:25,26/Jan/14 00:23,0.8.1,0.9.0,,,,,,,0.9.1,,,,,PySpark,,,,,0,,,,,,"Working with a file in pyspark.  These steps work fine:

{code}
mydata = sc.textFile(""somefile"")
myfiltereddata = mydata.filter(some filter)
myfiltereddata.count()
{code}

But then I delete ""somefile"" from the file system and attempt to run
{code}
myfiltereddata.count()
{code}

This hangs indefinitely.  I eventually hit Ctrl-C and it displayed a stack trace.  (I will attach the output as a file.)  

It works in scala though.  If I do the same thing, the last line produces an expected error message:
{code}
14/01/14 08:41:43 ERROR Executor: Exception in task ID 4
java.io.FileNotFoundException: File file:somefile does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)
{code}",,dcarroll@cloudera.com,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383189,,,Thu Jan 23 18:27:16 UTC 2014,,,,,,,,,,"0|i1u0lj:",383457,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 18:27;joshrosen;I turned on debug-level logging with log4j.properties and found the problem: PythonRDD's stdin writer thread was catching, logging, and ignoring the FileNotFoundException rather than allowing it to propagate and fail the job:

{code}
14/01/23 17:42:54 INFO HadoopRDD: Input split: file:/tmp/test.txt:0+0
14/01/23 17:42:54 INFO PythonRDD: stdin writer to Python finished early
14/01/23 17:42:54 DEBUG PythonRDD: stdin writer to Python finished early
java.io.FileNotFoundException: File file:/tmp/test.txt does not exist.
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:397)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:125)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:283)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:78)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:51)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:168)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:161)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:73)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:81)
{code}

I submitted a pull request to fix this: https://github.com/apache/incubator-spark/pull/504;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
-XX:+UseCompressedStrings is actually dropped in jdk7,SPARK-1024,12704590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,rxin,crazyjvm,,14/Jan/14 01:18,09/Feb/14 22:32,14/Jul/23 06:25,09/Feb/14 22:32,0.8.1,,,,,,,,1.0.0,,,,,Documentation,,,,,0,JVM,UseCompressedStrings,,,,"hi, guys,
     -XX:+UseCompressedStrings is actually dropped in jdk7, but the Tuning Guide(http://spark.incubator.apache.org/docs/latest/tuning.html) is still recommend users to use this. JVM will warn us ""Java HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0"" if we use this option. so,maybe we can remove the point from the guide now. 
     If the option is added back in the future, we also can add it back to the guide.
      more details can access this url : http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7129417",,ash211,crazyjvm,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383165,,,Sun Feb 09 22:32:53 UTC 2014,,,,,,,,,,"0|i1u0g7:",383433,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/14 22:08;rxin;Thanks for reporting. Do you mind submitting a pull request to fix the doc?;;;","15/Jan/14 05:36;crazyjvm;ok, i will fix it.;;;","15/Jan/14 06:51;crazyjvm;pull request SPARK-1024 : https://github.com/apache/incubator-spark/pull/439;;;","09/Feb/14 22:32;ash211;Merged by Reynold on Jan 15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Thread.sleep(5000) from TaskSchedulerImpl,SPARK-1023,12704559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pwendell,,11/Jan/14 16:02,06/Nov/14 17:42,14/Jul/23 06:25,06/Nov/14 17:42,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,This causes the unit tests to take super long. We should figure out why this exists and see if we can lower it or do something smarter.,,codingcat,crazyjvm,kzhang,patrick,rxin,sandy,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382844,,,Fri Jan 17 08:41:13 UTC 2014,,,,,,,,,,"0|i1tygv:",383112,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/14 16:03;patrick;https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala;;;","16/Jan/14 11:09;rxin;I think @Mridul Muralidharan added this. Perhaps he can comment on this.;;;","17/Jan/14 00:11;rxin;From Mridul:

Hi,


  Cant seem to login to that site, but the rationale is to ensure that all pending messages (on the fly) are actually sent out before we shutdown : as in, not just write to output streams/buffers, but to wire and reasonably ensure received on other side.

It is not a graceful fix - just to alleviate immediate concerns : not relevant for local mode, more relevant in cluster mode - particularly helps in congested clusters.


Regards,
Mridul
;;;","17/Jan/14 05:31;codingcat;I found that we have actually waited for a response to StopDriver from the driver

if (driverActor != null) {
        val future = driverActor.ask(StopDriver)(timeout)
        Await.ready(future, timeout)
      }

By intuitive, we should not need to wait for the other 1000 ms. However, when I removed Thread.sleep() and tested on my laptop, it failed...

then I checked the driverActor side code

case StopDriver =>
        sender ! true
        context.stop(self)
  
it sends the response first and then stop itself...so I guess the failed testcase is caused by the scheduling order of threads in my laptop

Thread 1 -> sender!true
Thread 2-> TaskSchedulerImpl.scala, goes forward and jumps out from override def stop() {} -> test case
Thread 1-> context.stop(itself)      

so the test case failed because context.stop(itself) hasn't been executed

we cannot simply change the order of 

sender ! true
context.stop(self)

I'm thinking about how to do this...
;;;","17/Jan/14 08:41;codingcat; confused... I cannot reproduce the bug in both local mode and cluster mode;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add unit tests for kafka streaming,SPARK-1022,12704562,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,pwendell,,11/Jan/14 09:43,06/Aug/15 00:21,14/Jul/23 06:25,06/Aug/14 06:43,,,,,,,,,,,,,,,,,,,0,,,,,,It would be nice if we could add unit tests to verify elements of kafka's stream. Right now we do integration tests only which makes it hard to upgrade versions of kafka. The place to start here would be to look at how kafka tests itself and see if the functionality can be exposed to third party users.,,anujojha,apachespark,jerryshao,patrick,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3615,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382836,,,Thu Aug 06 00:21:53 UTC 2015,,,,,,,,,,"0|i1tyf3:",383104,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/14 00:05;apachespark;User 'tdas' has created a pull request for this issue:
[https://github.com/apache/spark/pull/557|https://github.com/apache/spark/pull/557];;;","03/Aug/14 09:21;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/1751;;;","06/Aug/14 01:41;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/1797;;;","06/Aug/14 06:43;pwendell;There was a follow up to this issue:
https://github.com/apache/spark/pull/1797;;;","06/Aug/14 08:30;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1804;;;","05/Aug/15 21:46;anujojha;[~tdas] and [~jerryshao2015] I was working on writing Integration test for my application, which reads data from kafka, performs transformation on the data and then writes the Dstream back to other kafka topic. To do this I had to get KafkaTestUtils and some other dependent classes locally as I needed embedded zookeeper and kafa queue. I also had to modify sendMessage to accept my specific type of ket and value. I was wondering if it is possible to make the KafkaTestUtils generic? so that others can use it for in-memory  unit/integration testing. I have created generic sendMessage method hopefully it's of some use. 
{code}
class KafkaTestUtils [T,U]{
.......
  // Kafka producer
  private var producer: Producer[T, U] = _

..........

  def sendMessages(topic: String, messageToFreq: JMap[T, U]): Unit = {
    import scala.collection.JavaConversions._
    producer = new Producer[T, U](new ProducerConfig(producerConfiguration))
    for ((k,v) <- messageToFreq) {
      var message = new KeyedMessage(topic, k,v)
      producer.send(message)
    }
    producer.close()
    producer = null
  }

.........
}
{code}

I also had to change producer properties to provide key and value serialization class. It might be worth making possible to set these values from user test. I did not create JIRA for the request as I wanted to see if this is even a acceptable request. Please let me know.;;;","06/Aug/15 00:21;jerryshao;Hi [~anujojha], {{KafkaTestUtils}} is used for Kafka and Spark integration test, it is not public to users to use it, so the API design is not so generic for everyone use. I think Spark itself will not provide such functionality for user to do integration test, if you want to do such thing, from my point it would be better for you to do it yourself, not replying on Spark.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark RDD take() throws NPE,SPARK-1019,12704550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,dcarroll@cloudera.com,,09/Jan/14 14:20,12/Sep/14 20:26,14/Jul/23 06:25,12/Mar/14 23:18,0.8.1,0.9.0,,,,,,,0.9.1,1.0.0,,,,PySpark,,,,,0,,,,,,"I'm getting sporadic NPEs from pyspark that I can't narrow down, but I'm able to reproduce it consistently from the attached data file.  If I delete any single line from the file, it works; but the file as is or larger (it's a snippet of a much larger log file) causes the problem.

Printing the lines read works fine...but afterwards, I get the exception.

Problem occurs with vanilla pyspark and IPython, but NOT with the scala spark shell.

sc.textFile(""testlog13"").take(5) (does not seem to matter how many lines I take).

In [32]: sc.textFile(""testlog16"").take(5)
14/01/09 14:16:45 INFO MemoryStore: ensureFreeSpace(69808) called with curMem=1185875, maxMem=342526525
14/01/09 14:16:45 INFO MemoryStore: Block broadcast_16 stored as values to memory (estimated size 68.2 KB, free 325.5 MB)
14/01/09 14:16:45 INFO FileInputFormat: Total input paths to process : 1
14/01/09 14:16:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:288
14/01/09 14:16:45 INFO DAGScheduler: Got job 23 (runJob at PythonRDD.scala:288) with 1 output partitions (allowLocal=true)
14/01/09 14:16:45 INFO DAGScheduler: Final stage: Stage 23 (runJob at PythonRDD.scala:288)
14/01/09 14:16:45 INFO DAGScheduler: Parents of final stage: List()
14/01/09 14:16:45 INFO DAGScheduler: Missing parents: List()
14/01/09 14:16:45 INFO DAGScheduler: Computing the requested partition locally
14/01/09 14:16:45 INFO HadoopRDD: Input split: file:/home/training/testlog16:0+61124
14/01/09 14:16:45 INFO PythonRDD: Times: total = 14, boot = 1, init = 3, finish = 10
14/01/09 14:16:45 INFO SparkContext: Job finished: runJob at PythonRDD.scala:288, took 0.021146108 s
Out[32]: 
[u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] ""GET KBDOC-00087.html HTTP/1.0"" 200 8681 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ',
 u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] ""GET KBDOC-00087.html HTTP/1.0"" 200 8681 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ',
 u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] ""GET theme.css HTTP/1.0"" 200 8681 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ',
 u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] ""GET KBDOC-00076.html HTTP/1.0"" 200 17546 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ',
 u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] ""GET theme.css HTTP/1.0"" 200 17546 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ']

In [33]: Exception in thread ""stdin writer for python"" java.lang.NullPointerException
	at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:54)
	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:60)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:246)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:275)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:227)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:195)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:167)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:150)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:400)
	at scala.collection.Iterator$class.foreach(Iterator.scala:772)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399)
	at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:98)


",,dcarroll@cloudera.com,joshrosen,mmoroz,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1579,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383003,,,Sun Mar 16 11:17:18 UTC 2014,,,,,,,,,,"0|i1tzg7:",383271,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/14 06:25;dcarroll@cloudera.com;I'm getting this reliably again using the .9 rc build, using a different dataset. Will attach for ease of reproduction. (zip codes with latitude and longitude);;;","30/Jan/14 11:09;patrick;Hey Diana,

I just tried to reproduce this from a fresh spark build and couldn't. If you do a clean checkout of spark like I did below do you get the error?

{code}
cd /tmp
git clone https://git-wip-us.apache.org/repos/asf/incubator-spark.git -b branch-0.9
cd incubator-spark
sbt/sbt assembly
wget https://spark-project.atlassian.net/secure/attachment/12402/testlog13
./bin/pyspark
>>> sc.textFile(""testlog13"").take(5)
[u'165.32.101.206 - 8 [15/Sep/2013:23:59:50 +0100] ""GET theme.css HTTP/1.0"" 200 17446 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] ""GET KBDOC-00087.html HTTP/1.0"" 200 8681 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] ""GET theme.css HTTP/1.0"" 200 8681 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] ""GET KBDOC-00076.html HTTP/1.0"" 200 17546 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] ""GET theme.css HTTP/1.0"" 200 17546 ""http://www.loudacre.com""  ""Loudacre CSR Browser"" ']
{code}

;;;","30/Jan/14 16:20;dcarroll@cloudera.com;I'm not getting the problem from that sample file -- it was sporadic when I was testing.  But I'm now getting it consistently from the latlon.tsv file I just uploaded (a tab-delimited list of zip codes with latitude and longitude).  

I just did a fresh build from your instructions and am still getting the problem  In case it is helpful I will attach a log file with debugging enabled.  (Doesn't seem to give much more info I'm afraid.)

I notice that the actual exception is occurring in Hadoop LineRecordReader.  Maybe my issue has something to do with the version of Hadoop I'm using?  I've tested this in CDH5b1 and the (yet to be released) CDH5b2.


;;;","30/Jan/14 16:21;dcarroll@cloudera.com;debug log;;;","30/Jan/14 17:04;dcarroll@cloudera.com;I just thought to test the latlon.tsv file using HDFS instead of the local file system.  That works fine.  It is only loading it from the local file system that is giving me problems.

I also note that the take() is successful...it displays the requested records and *then* throws NPE for whatever that's worth.;;;","30/Jan/14 18:56;joshrosen;Thanks for reporting this and for including sample data.  I was able to reproduce this bug using the latest master branch, Hadoop 1.0.4, and a local pyspark shell:

{code}
>>> sc.textFile(""latlon.tsv"").take(1)
14/01/30 18:35:00 INFO MemoryStore: ensureFreeSpace(34262) called with curMem=34238, maxMem=311387750
14/01/30 18:35:00 INFO MemoryStore: Block broadcast_1 stored as values to memory (estimated size 33.5 KB, free 296.9 MB)
14/01/30 18:35:00 INFO FileInputFormat: Total input paths to process : 1
14/01/30 18:35:00 INFO SparkContext: Starting job: take at <stdin>:1
14/01/30 18:35:00 INFO DAGScheduler: Got job 1 (take at <stdin>:1) with 1 output partitions (allowLocal=true)
14/01/30 18:35:00 INFO DAGScheduler: Final stage: Stage 1 (take at <stdin>:1)
14/01/30 18:35:00 INFO DAGScheduler: Parents of final stage: List()
14/01/30 18:35:00 INFO DAGScheduler: Missing parents: List()
14/01/30 18:35:00 INFO DAGScheduler: Computing the requested partition locally
14/01/30 18:35:00 INFO HadoopRDD: Input split: file:/Users/joshrosen/Documents/spark/spark/latlon.tsv:0+1127316
14/01/30 18:35:00 INFO PythonRDD: Times: total = 39, boot = 2, init = 37, finish = 0
14/01/30 18:35:00 INFO SparkContext: Job finished: take at <stdin>:1, took 0.043407 s
[u'00210\t43.005895\t-71.013202']
>>> Exception in thread ""stdin writer for python"" java.lang.NullPointerException
	at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48)
	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:214)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:237)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:189)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:134)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:133)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:38)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:164)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:149)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:232)
	at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:85)
{code}

I used this stacktrace with [BrainLeg|http://brainleg.com/], a structural stacktrace search engine, and found a few other projects that have run into similar NPEs:

- https://issues.apache.org/jira/browse/HCATALOG-626
- https://github.com/facebook/presto/pull/950

The exception seems to be dependent on the size of the input file, since the job runs without any exceptions if I run on a small sample of lines from the original input.  I'll check out those links to see how those other projects may have fixed this issue.  I'll also try running under a debugger to see if I can track down where the null is being introduced.;;;","30/Jan/14 20:19;joshrosen;Interestingly, I can only reproduce the NPE when calling an action that only reads a portion of the input file, like first() or take().

If I call

{code}
sc.textFile(""latlon.tsv"").collect()[0]
{code}

which forces the entire file to be read, then I don't see any exception.

My current theory is that there's a race condition between the Python daemon's stdin writer consuming its iterator and the job completion callbacks closing that iterator.

Imagine that I run {{sc.textFile().take(n )}} in Scala Spark.  The data flow here is entirely pull-based and we should only read as much of the file as is necessary to return the first {{n}} records.  When the job completes, the Hadoop iterator is closed using a job completion callback.

Now, consider what happens when I run this locally using PySpark.  PySpark's data flow is a mixture of push and pull with backpressure: the stdin writer thread pulls records from Hadoop or cached RDDs and pushes them into the Python worker process (where network buffer sizes provide backpressure), which pushes its output records back to Java.  If the job finishes before the Python worker has consumed all of its input, the job completion callbacks might close the Hadoop iterator while the PySpark stdin writer thread is still attempting to consume it, leading to a NullPointerException because it calls a closed data stream.

This theory explains why the above {{collect()}} call didn't throw a NPE: since the entire input needed to be consumed before the job could complete, the stdin writer thread would never attempt to read from a closed stream.  It also explains why you didn't see NPEs for small inputs: if the input is small enough, the Python worker will have enough time and network buffer space to receive the entire input before the job completes, even if it only iterates over its first few input records.

I'll try tracing through the execution in more detail to verify if this is what's causing the exception.

When fixing this, we'll have to be careful to avoid introducing race conditions.  If we simply added job completion callbacks to stop the PythonRDD stdin writer thread, we'd have to worry about a race between that callback and the Hadoop iterator callback.  Maybe we could guarantee that completion callbacks are called in reverse order of registration.

It looks like the underlying bug might have been introduced in https://github.com/apache/incubator-spark/commit/b9d6783f36d527f5082bf13a4ee6fd108e97795c, which optimized PySpark's {{take()}} to push the limit into {{mapPartitions()}} calls to avoid computing the entire first partition.  That commit added a try-catch block that seems to imply that IOExceptions could occur if the Python worker didn't consume its entire input:

{code}
        } catch {
          case e: IOException =>
            // This can happen for legitimate reasons if the Python code stops returning data before we are done
            // passing elements through, e.g., for take(). Just log a message to say it happened.
            logInfo(""stdin writer to Python finished early"")
            logDebug(""stdin writer to Python finished early"", e)
        }
 {code}
 
 This seems a little fishy, since it's implicitly detecting the Python worker process being finished rather than using an explicit signal from the worker to PythonRDD.  Also, IOException is too broad of an exception, which was the root cause of SPARK-1025, so we should remove this catch.;;;","09/Mar/14 21:07;patrick;Okay I spent some time today playing with this. Josh I confirmed that the issue is what you suspected - the callback for the HadoopRDD is getting called and invalidating the input stream. I've proposed a patch here that I found fixed the issue for me locally. I think my patch still has a potential race, but it least decreases the odds of this exception substantially so I'd say it's strictly better than what's there now.;;;","09/Mar/14 21:08;patrick;[~dcarroll@cloudera.com] If you could test this patch that would be great.;;;","10/Mar/14 08:32;dcarroll@cloudera.com;Will do.  (Just back from vacation so probably not today!)


On Mon, Mar 10, 2014 at 12:09 AM, Patrick Wendell (JIRA) <

;;;","10/Mar/14 14:42;patrick;Forgot to link the associated pull request:
https://github.com/apache/spark/pull/112;;;","11/Mar/14 07:15;dcarroll@cloudera.com;I tested the patch and it works!  I tested it side by side with unpatched 0.9.  Unpatched = NPE in the latlon.tsv test file; patched = no NPE.

Thanks.;;;","12/Mar/14 23:18;patrick;I think the fixed proposed here is the best we can do given the current cancellation support in Spark. [~joshrosen] please feel free to re-open this if you see a nicer solution. There may very well be one.

https://github.com/apache/spark/pull/112/files;;;","16/Mar/14 11:17;joshrosen;I'm still skeptical of the {{catch IOException}} code, since it's been involved in two bugs now, so we should probably revisit that at some point, but this seems like an okay fix for now for the NPE issues.;;;",,,,,,,,,,,,,,,,,,,
Have DEVELOPERS.txt file with documentation for developers,SPARK-1013,12704452,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,06/Jan/14 11:51,25/Mar/14 15:12,14/Jul/23 06:25,25/Mar/14 15:12,,,,,,,,,,,,,,,,,,,0,,,,,,"Some things to include would be:

- How to run tests
- How to run a single test in sbt or maven
- How to build spark with assemble-deps 
",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1066,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382903,,,2014-01-06 11:51:32.0,,,,,,,,,,"0|i1tytz:",383171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler Exception,SPARK-1012,12704473,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falaki,falaki,,02/Jan/14 16:19,07/Jan/14 22:02,14/Jul/23 06:25,07/Jan/14 22:02,0.9.0,,,,,,,,0.9.0,,,,,Spark Core,,,,,0,DAGScheduler,"MLLib,",,,,"I get 
When running the following code:
{code:java}
import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.Rating

// Load and parse the data
val data = sc.textFile(""mllib/data/als/test.data"")
val ratings = data.map(_.split(',') match {
    case Array(user, item, rate) =>  Rating(user.toInt, item.toInt, rate.toDouble)
})

// Build the recommendation model using ALS
val numIterations = 20
val model = ALS.train(ratings, 1, 20, 0.01)

// Evaluate the model on rating data
val ratesAndPreds = ratings.map{ case Rating(user, item, rate) => (rate, model.predict(user, item))}
val MSE = ratesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce(_ + _)/ratesAndPreds.count
{code}

I get:
{code:java}
org.apache.spark.SparkException: Job aborted: Task 2.0:0 failed 1 times (most recent failure: Exception failure: scala.MatchError: null)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1024)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1024)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:617)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:205)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

The problem is deterministic. In addition ratesAndPreds has exactly 16 elements:
{code:none}
scala> ratesAndPreds.take(16)
res1: Array[(Double, Double)] = Array((5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928))

scala> ratesAndPreds.count()
{code}

Throws the exception again.",,falaki,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383216,,,Tue Jan 07 22:02:31 UTC 2014,,,,,,,,,,"0|i1u0rj:",383484,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/14 16:52;falaki;The problem is that MatrixFactorizationModel has a reference to two RDDs (userFeatures, and productFeatures). As a result, when passed inside a closure all the bad things happen. 

The solution is augmenting MatrixFactorizaitonModel with a bulk prediction method that takes an RDD of test data and returns a prediction RDD.;;;","03/Jan/14 16:02;falaki;Submitted this pull request to offer a viable method for bulk prediction.;;;","07/Jan/14 22:02;falaki;Fixed with PR #328;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MatrixFactorizationModel in pyspark throws serialization error,SPARK-1011,12705122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falaki,falaki,,02/Jan/14 12:42,25/Jul/14 22:53,14/Jul/23 06:25,25/Jul/14 22:42,0.9.0,,,,,,,,0.9.1,,,,,PySpark,,,,,0,,,,,,"When running the following sample code in pyspark, 
{code}
from pyspark.mllib.recommendation import ALS
from numpy import array

# Load and parse the data
data = sc.textFile(""mllib/data/als/test.data"")
ratings = data.map(lambda line: array([float(x) for x in line.split(',')]))

# Build the recommendation model using Alternating Least Squares
model = ALS.train(sc, ratings, 1, 20)

# Evaluate the model on training data
ratesAndPreds = ratings.map(lambda p: (p[2], model.predict(int(p[0]), int(p[1]))))
ratesAndPreds.take(1)
{code}

I get:
{code}
>>> ratesAndPreds.take(1)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/rdd.py"", line 585, in take
    for partition in range(mapped._jrdd.splits().size()):
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/rdd.py"", line 984, in _jrdd
    pickled_command = CloudPickleSerializer().dumps(command)
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/serializers.py"", line 248, in dumps
    def dumps(self, obj): return cloudpickle.dumps(obj, 2)
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 801, in dumps
    cp.dump(obj)
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 140, in dump
    return pickle.Pickler.dump(self, obj)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 224, in dump
    self.save(obj)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 548, in save_tuple
    save(element)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 259, in save_function
    self.save_function_tuple(obj, [themodule])
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 316, in save_function_tuple
    save(closure)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 600, in save_list
    self._batch_appends(iter(obj))
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 633, in _batch_appends
    save(x)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 259, in save_function
    self.save_function_tuple(obj, [themodule])
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 316, in save_function_tuple
    save(closure)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 600, in save_list
    self._batch_appends(iter(obj))
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 633, in _batch_appends
    save(x)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 259, in save_function
    self.save_function_tuple(obj, [themodule])
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 316, in save_function_tuple
    save(closure)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 600, in save_list
    self._batch_appends(iter(obj))
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 636, in _batch_appends
    save(tmp[0])
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 254, in save_function
    self.save_function_tuple(obj, modList)
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 314, in save_function_tuple
    save(f_globals)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 181, in save_dict
    pickle.Pickler.save_dict(self, obj)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 686, in _batch_setitems
    save(v)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 631, in save_reduce
    save(state)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py"", line 181, in save_dict
    pickle.Pickler.save_dict(self, obj)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 681, in _batch_setitems
    save(v)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py"", line 306, in save
    rv = reduce(self.proto)
  File ""build/bdist.macosx-10.8-intel/egg/py4j/java_gateway.py"", line 500, in __call__
  File ""build/bdist.macosx-10.8-intel/egg/py4j/protocol.py"", line 304, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o37.__getnewargs__. Trace:
py4j.Py4JException: Method __getnewargs__([]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)
	at py4j.Gateway.invoke(Gateway.java:251)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:695)
{code}
",,falaki,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383223,,,Fri Jul 25 22:53:26 UTC 2014,,,,,,,,,,"0|i1u0t3:",383491,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/14 15:42;joshrosen;This error suggests that a Py4J object is being referenced inside of a function closure, because pickle is attempting to call __getnewargs__ on a Java object.  I haven't tried running your example, but my hunch is that the private _java_model field in MatrixFactorizationModel is being serialized.

Since we're not using Py4J on the workers, I think we'd need to find a different way to pass model objects to PySpark UDFs for further processing.  This could be tricky and I don't know of any easy solution off the top of my head.;;;","02/Jan/14 15:45;falaki;I was working on examples for MLLib in python that I encountered this issue. One solution might be to add a predict() function to ALS that takes an RDD and returns predicted ratings. If we had such a method, I could avoid passing the model in the closure and void this issue for now (until we fix it properly).;;;","03/Jan/14 09:22;matei;Yeah, actually MatrixFactorizationModel.predict() cannot be called on a cluster even in Scala, as far as I know. This is because it uses RDD operations inside it, and you can't call those within a closure on the cluster. We'd have to add a predict on RDDs if that's what you'd like to do.;;;","03/Jan/14 09:52;falaki;Yes, I added that. After thorough testing I will send the pull request. Hopefully someone can add the python binding for it later.;;;","03/Jan/14 16:40;falaki;It would be great to have python binding for the method added with <https://github.com/apache/incubator-spark/pull/328>;;;","25/Jul/14 22:42;joshrosen;This was fixed in Spark 0.9.1 with the addition of a new {{predictAll}} method for performing bulk predictions.

This was added in commit https://github.com/apache/spark/commit/b2e690f839e7ee47f405135d35170173386c5d13.;;;","25/Jul/14 22:53;joshrosen;Oh, and if you want to combine the actual vs. predicted ratings:

{code}
from pyspark.mllib.recommendation import ALS
from numpy import array

# Load and parse the data
data = sc.textFile(""mllib/data/als/test.data"")
ratings = data.map(lambda line: array([float(x) for x in line.split(',')]))

# Build the recommendation model using Alternating Least Squares
model = ALS.train(ratings, 1, 20)

# Evaluate the model on training data
predictedRatings = model.predictAll(ratings.map(lambda x: (x[0], x[1])))
trueVsPredicted = ratings.map(lambda x: ((x[0], x[1]), x[2])).join(predictedRatings.map(lambda x: ((x[0], x[1]), x[2])))
trueVsPredicted.take(1)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Provide good default logging if log4j properties is not present,SPARK-1008,12705099,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,29/Dec/13 19:08,09/Mar/14 17:54,14/Jul/23 06:25,09/Mar/14 17:54,0.9.0,,,,,,,,0.9.0,,,,,,,,,,0,,,,,,This is an issue now that we've excluded log4j files from our assembly jar.,,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383048,,,2013-12-29 19:08:31.0,,,,,,,,,,"0|i1tzq7:",383316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-class2.cmd should change SCALA_VERSION to be 2.10,SPARK-1007,12705093,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,patrick,Qiuzhuang,,25/Dec/13 22:38,29/Dec/13 19:01,14/Jul/23 06:25,29/Dec/13 19:01,0.9.0,,,,,,,,0.9.0,,,,,Deploy,,,,,0,,,,,,"Most likely a mistake, spark-class2.cm still set scala version to be 2.9.3, it should be 2.10 instead. This is from spark git trunk.",,patrick,Qiuzhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383227,,,Thu Dec 26 23:22:22 UTC 2013,,,,,,,,,,"0|i1u0tz:",383495,,,,,,,,,,,,,,,,,,,,,,,"26/Dec/13 23:22;patrick;Thanks for reporting this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark application is blocked when running on yarn,SPARK-1003,12704799,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,Xicheng Dong,,18/Dec/13 00:18,23/Jan/14 23:41,14/Jul/23 06:25,23/Jan/14 23:41,,,,,,,,,0.9.0,,,,,Deploy,,,,,0,,,,,,"When  running Spark Application on yarn, it is blocked on client side:
SPARK_JAR=./assembly/target/scala-2.9.3/spark-assembly-0.8.1-incubating-hadoop2.2.0.jar \
    ./spark-class org.apache.spark.deploy.yarn.Client \
      --jar ./assembly/target/scala-2.9.3/spark-examples-assembly-0.8.1-incubating.jar \
      --class org.apache.spark.examples.SparkPi \
      --args yarn-standalone \
      --name pi \
      --num-workers 3 \
      --master-memory 2g \
      --worker-memory 2g \
      --worker-cores 1
I check the source and find a bug:
// position: new-yarn/src/main/scala/org/apache/spark/deploy/yarn/ClientArguments.scala
// line 91
        case (""--name"") :: value :: tail =>
          appName = value
          args = tail
the last “args = tail” is missed.",,joshrosen,Xicheng Dong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383192,,,Thu Jan 23 23:41:16 UTC 2014,,,,,,,,,,"0|i1u0m7:",383460,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 23:41;joshrosen;This was fixed in https://github.com/apache/incubator-spark/pull/257;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove binary artifacts from build,SPARK-1002,12704433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,pwendell,,17/Dec/13 10:11,30/Mar/14 04:13,14/Jul/23 06:25,05/Jan/14 23:17,,,,,,,,,0.9.0,,,,,,,,,,0,,,,,,"This is a requirement from Apache and will block our 0.9 release unless dealt with. We should publish any jars we need through sonatype and remove sbt.

- Patrick",,jey,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382828,,,2013-12-17 10:11:19.0,,,,,,,,,,"0|i1tydb:",383096,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors table in application web ui is wrong,SPARK-989,12705094,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ilikerps,,08/Dec/13 17:21,08/Dec/13 17:51,14/Jul/23 06:25,08/Dec/13 17:51,0.8.1,,,,,,,,0.8.1,,,,,Web UI,,,,,0,,,,,,"See attached picture. The (executorId, address) pairs are not associated with the right state data. The most obvious sign of this is that the <driver> executor has run tasks while another executor has not. This happens for other executors as well.",,ilikerps,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383239,,,Sun Dec 08 17:50:44 UTC 2013,,,,,,,,,,"0|i1u0wn:",383507,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/13 17:22;rxin;I think this one was fixed in https://github.com/apache/incubator-spark/pull/181 ?;;;","08/Dec/13 17:50;ilikerps;My apologies, I did not realize that was what that patch was fixing. Also I checked the wrong place for recent updates to the file. Double fail on my part!

Anyway, I've confirmed that that patch works, at least. Sorry about the trouble.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK_TOOLS_JAR not set if multiple tools jars exists,SPARK-984,12704791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,ilikerps,,07/Dec/13 01:33,30/Apr/15 15:55,14/Jul/23 06:25,27/Apr/15 22:00,0.8.1,0.9.0,,,,,,,1.4.0,,,,,Build,,,,,0,,,,,,"If you have multiple tools assemblies (e.g., if you assembled on 0.8.1 and 0.9.0 before, for instance), then this error is thrown in spark-class:
{noformat}./spark-class: line 115: [: /home/aaron/spark/tools/target/scala-2.9.3/spark-tools-assembly-0.8.1-incubating-SNAPSHOT.jar: binary operator expected{noformat}

This is because of a flaw in the bash script:
{noformat}if [ -e ""$TOOLS_DIR""/target/scala-$SCALA_VERSION/*assembly*[0-9Tg].jar ]; then{noformat}
which does not parse correctly if the path resolves to multiple files.

The error is non-fatal, but a nuisance and presumably breaks whatever SPARK_TOOLS_JAR is used for.

Currently, we error if multiple Spark assemblies are found, so we could do something similar for tools assemblies. The only issue is that means that the user will always have to go through both errors (clean the assembly/ jars then tools/ jar) when it appears that the tools/ jar is not actually important for normal operation. The second possibility is to infer the correct tools jar using the single available assembly jar, but this is slightly complicated by the code path if $FWDIR/RELEASE exists.

Since I'm not 100% on what SPARK_TOOLS_JAR is even for, I'm assigning this to Josh who wrote the code initially.",,apachespark,ilikerps,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383197,,,Mon Apr 27 22:00:24 UTC 2015,,,,,,,,,,"0|i1u0nb:",383465,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/13 09:19;joshrosen;SPARK_TOOLS_JAR is the assembly for the spark-tools subproject, which contains developer tools like JavaAPICompletenessChecker; it was added as an alternative to placing those tools in the examples subproject.;;;","23/Jan/14 19:52;joshrosen;Since the Spark Tools project contains tools for use by Spark developers and not ordinary users, we could probably just require developers to access those tools through sbt.  In the sbt shell:

{code}
project tools
run
{code}

I actually prefer to use JavaAPICompletenessChecker this was since it doesn't require me to go through a whole assembly cycle when I make changes.  If nobody has any objections, I'll submit a PR to remove the assemblies for spark-tools and update the wiki to describe the sbt run method.;;;","23/Jan/15 12:29;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4181;;;","27/Apr/15 22:00;joshrosen;It looks like this setting was removed as part of SPARK-4924, Marcelo's Spark Launcher PR, so I'm going to mark this as resolved with 1.4.0 as the fix version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo on Hadoop third-party page,SPARK-982,12705124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,massie,,04/Dec/13 19:13,04/Dec/13 20:31,14/Jul/23 06:25,04/Dec/13 20:31,0.8.0,,,,,,,,0.8.1,0.9.0,,,,Documentation,,,,,0,,,,,,"http://spark.incubator.apache.org/docs/latest/hadoop-third-party-distributions.html

There's a subtle typo which prevents Spark users from compiling against CDH. It has ""chd4"" instead of ""cdh4"".

Release	Version code
CDH 4.X.X (YARN mode)	2.0.0-chd4.X.X
CDH 4.X.X	2.0.0-mr1-chd4.X.X
CDH 3u6	0.20.2-cdh3u6
CDH 3u5	0.20.2-cdh3u5
CDH 3u4	0.20.2-cdh3u4

should be

Release	Version code
CDH 4.X.X (YARN mode)	2.0.0-cdh4.X.X
CDH 4.X.X	2.0.0-mr1-cdh4.X.X
CDH 3u6	0.20.2-cdh3u6
CDH 3u5	0.20.2-cdh3u5
CDH 3u4	0.20.2-cdh3u4

",,massie,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383265,,,Wed Dec 04 20:31:40 UTC 2013,,,,,,,,,,"0|i1u12f:",383533,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/13 20:31;patrick;This was fixed in bef398e5... closing!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException for single-host setup with S3 URLs,SPARK-980,12704630,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,paulrbrown,,03/Dec/13 22:10,23/Jan/15 12:06,14/Jul/23 06:25,23/Jan/15 12:06,0.8.0,,,,,,,,,,,,,Input/Output,,,,,0,,,,,,"Short version:

* The use of {{execSparkHome_}} in [Worker.scala|https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L135] should be checked for {{null}} or that value should be defaulted or plumbed through.
* If the {{sparkHome}} argument to {{new SparkContext(...)}} is non-optional, then it should not be marked as optional.

Long version:

Starting up with {{bin/start-all.sh}} and then connecting from a Scala program and attempting to read two S3 URLs results in the following trace in the worker log:

{code}
13/12/03 21:50:23 ERROR worker.Worker:
java.lang.NullPointerException
	at java.io.File.<init>(File.java:277)
	at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.apply(Worker.scala:135)
	at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.apply(Worker.scala:120)
	at akka.actor.Actor$class.apply(Actor.scala:318)
	at org.apache.spark.deploy.worker.Worker.apply(Worker.scala:39)
	at akka.actor.ActorCell.invoke(ActorCell.scala:626)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)
	at akka.dispatch.Mailbox.run(Mailbox.scala:179)
	at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)
	at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
	at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
	at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
	at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
{code}

This is on Mac OS X 10.9, Oracle Java 7u45, and the Hadoop 1 download from the incubator.

Reading the code, this occurs because {{execSparkHome_}} is {{null}}; see [Worker.scala#L135|https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L135], and setting a value explicitly in the Scala driver allows the computation to complete.",,codingcat,dewshick,paulrbrown,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383208,,,Fri Jan 23 12:06:43 UTC 2015,,,,,,,,,,"0|i1u0pr:",383476,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/13 23:57;rxin;Thanks for reporting. What do you mean by S3 URLs? ;;;","16/Jan/14 05:35;codingcat;I think this has been resolved by PR 442 https://github.com/apache/incubator-spark/pull/442 and PR 447 https://github.com/apache/incubator-spark/pull/447

this is not related to S3, but just due to an empty SPARK_HOME in driver end;;;","16/Jan/14 10:34;paulrbrown;Those two PRs look like they would resolve the issue.  Correct that it is not related to S3  URLs, per se, but it does list the line with the attempt to read the S3 objects as the root of the trace.  (This is common for lazy invocations and stack traces, so c'est la vie.);;;","23/Jan/15 12:06;srowen;I believe Nan is correct that this was subsequently resolved. Although the references to source and PRs aren't live anymore, I see this in Worker.scala:

{code}
new File(sys.env.get(""SPARK_HOME"").getOrElse("".""))
{code}

and this is the only reference to {{SPARK_HOME}}, so it seems to handle the case where this is missing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark's cartesian method throws ClassCastException exception,SPARK-978,12704787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,rxin,,03/Dec/13 13:54,23/Jan/14 19:48,14/Jul/23 06:25,23/Jan/14 19:48,0.8.0,0.8.1,0.9.0,,,,,,0.9.0,,,,,PySpark,,,,,0,,,,,,"Try the following in PySpark:

{code}
a = sc.textFile(""README.md"")
a.cartesian(a).collect()
{code}

exception thrown

{code}
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.writeToFile.
: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.spark.api.python.PythonRDD$.writeToStream(PythonRDD.scala:214)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeToFile$1.apply(PythonRDD.scala:233)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeToFile$1.apply(PythonRDD.scala:232)
	at scala.collection.Iterator$class.foreach(Iterator.scala:772)
	at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:573)
	at org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:232)
	at org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:227)
	at org.apache.spark.api.python.PythonRDD.writeToFile(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:228)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:695)
{code}


But if we convert the rdd to a list of strings it would work.

i.e.

a = a.map(lambda line: str(line))",,joshrosen,mrt,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383198,,,Thu Jan 23 15:18:53 UTC 2014,,,,,,,,,,"0|i1u0nj:",383466,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 15:18;joshrosen;What's happening here is cartesian() produces a JavaPairRDD<String, String> when called on untransformed RDDs that were created with sc.textFile(), but writeToStream() isn't prepared to handle this case.  This fails with a ClassCastException rather than a MatchError because the type parameters of Tuple2 are eliminated by erasure; the compiler had actually warned about this:

{code}
[warn] /Users/joshrosen/Documents/spark/spark/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:214: non-variable type argument Array[Byte] in type pattern (Array[Byte], Array[Byte]) is unchecked since it is eliminated by erasure
[warn]       case pair: (Array[Byte], Array[Byte]) =>
[warn]
{code}

Based on the underlying JavaRDD's ClassTag, we should be able to figure out which Java -> Python serialization method to use, rather than attempting to determine it on a per-element basis (this should be more efficient, too).  Unfortunately, this doesn't work since we need TypeTags rather than ClassTags to work around erasure of Tuple2's parameters.

Rather than breaking a bunch of existing APIs by introducing TypeTags, I changed the scope of the getClass()-based pattern matching to determine types based on the first item in the iterator.

I've submitted my fix as part of this pull request: https://github.com/apache/incubator-spark/pull/501;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error in script spark-0.8.0-incubating/make-distribution.sh,SPARK-974,12705048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,liuluxu1989@gmail.com,,02/Dec/13 18:59,02/Dec/13 21:51,14/Jul/23 06:25,02/Dec/13 21:51,0.8.0,,,,,,,,0.8.1,,,,,Deploy,,,,,0,script,,,,,"file spark-0.8.0-incubating/make-distribution.sh, line 98:
cp ""$FWDIR/conf/*.template"" ""$DISTDIR""/conf 
should be 
cp ""$FWDIR""/conf/*.template ""$DISTDIR""/conf 

Otherwise report file not found error

p.s.:
The script file is included in this version:
http://spark-project.org/download/spark-0.8.0-incubating.tgz",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383269,,,Mon Dec 02 21:51:01 UTC 2013,,,,,,,,,,"0|i1u13b:",383537,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/13 21:27;rxin;Thanks for reporting. Do you mind submitting a pull request against the Apache github mirror to fix this?

https://github.com/apache/incubator-spark;;;","02/Dec/13 21:48;liuluxu1989@gmail.com;@Reynold Xin , never mind.
I's already fixed with the #916 pull request.

https://github.com/mesos/spark/pull/916;;;","02/Dec/13 21:51;rxin;Thanks. Closing this since it's been fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark's saveAsTextFile() throws UnicodeEncodeError when saving unicode strings,SPARK-970,12705032,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,28/Nov/13 22:49,04/Dec/13 11:10,14/Jul/23 06:25,04/Dec/13 11:10,0.7.0,0.7.1,0.7.2,0.7.3,0.8.0,,,,0.8.1,0.9.0,,,,PySpark,,,,,0,,,,,,"PySpark throws a UnicodeEncodeError when trying to save unicode objects to text files.  This is because saveAsTextFile() calls str() to get objects' string representations, when it should be calling unicode() instead.

This is probably a one-line fix.

This was originally reported on the mailing list at https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201311.mbox/%3CCAPS2vjrorZGbxt7Nyqb1ZLZABk2MZy1O1p-KfF%3DxGJzSN0oq9g%40mail.gmail.com%3E",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383266,,,Wed Dec 04 11:10:25 UTC 2013,,,,,,,,,,"0|i1u12n:",383534,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/13 11:10;joshrosen;Fixed in https://github.com/apache/incubator-spark/pull/218;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sometimes DAGScheduler throws NullPointerException ,SPARK-966,12705074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liancheng,Qiuzhuang,,27/Nov/13 01:30,07/Dec/13 14:46,14/Jul/23 06:25,07/Dec/13 14:45,0.9.0,,,,,,,,0.9.0,,,,,,,,,,0,,,,,,"When running some spark examples, I run into NullPointerException  thrown by DAGScheduler. Log is as follows,

13/11/27 17:26:40 ERROR dispatch.TaskInvocation: 
java.lang.NullPointerException
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$3$$anon$1$$anonfun$preStart$1.apply$mcV$sp(DAGScheduler.scala:116)
	at akka.actor.DefaultScheduler$$anon$1.run(Scheduler.scala:142)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:94)
	at akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381)
	at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
	at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
	at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
	at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
13/11/27 17:26:42 WARN util.SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes",,joshrosen,liancheng,markhamstra,Qiuzhuang,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383245,,,Sat Dec 07 14:45:43 UTC 2013,,,,,,,,,,"0|i1u0xz:",383513,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/13 14:35;rxin;What version of Spark is this? Can you paste the code near line 116 in DAGScheduler.scala?;;;","27/Nov/13 18:57;Qiuzhuang;I am using git trunk version. The last git version is e2a43b3 on 11/11/2013 by Lian,Cheng.

line 116 in that class is 
if (failed.size > 0) { 
and here is the code near that line,

  private val eventProcessActor: ActorRef = env.actorSystem.actorOf(Props(new Actor {
    override def preStart() {
      context.system.scheduler.schedule(RESUBMIT_TIMEOUT milliseconds, RESUBMIT_TIMEOUT milliseconds) {
        if (failed.size > 0) {
          resubmitFailedStages()
        }
      }
    }

;;;","27/Nov/13 19:13;rxin;[~liancheng] can you take a look at this? ;;;","27/Nov/13 21:11;liancheng;Hi [~Qiuzhuang], I think I've found the cause, but not fully verified yet.  
Would you please elaborate more on the Spark examples you were running?  I need the command line arguments and possible input data files to verify this bug.  Thanks!

The problematic commit is actually [2539c06|https://github.com/liancheng/incubator-spark/commit/2539c0674501432fb62073577db6da52a26db850], an ancestor of [e2a43b3|https://github.com/liancheng/incubator-spark/commit/e2a43b3dcce81fc99098510d09095e1be4bf3e29].

In that commit, I replaced the daemon thread in {{DAGScheduler}} with an Akka actor {{eventProcessActor}}, and moved the stage re-submission logic into a scheduled task, which references {{DAGScheduler.failed}}.

Furthermore, since {{DAGScheduler}} is always started right after creation, I removed the {{DAGScheduler.start()}} method and started the actor within the {{DAGScheduler}} constructor.

Now comes the problem: when {{eventProcessActor}} is started, {{DAGScheduler}} is not fully constructed yet, but the stage re-submission task is already scheduled to run.  Thus, sometimes, when the scheduled task is executed for the first time, {{DAGScheduler.failed}} may still be {{null}}, thus a {{NullPointerException}} is thrown.

A possible solution would be:

# Add back the {{DAGScheduler.start()}}
# Create and start {{eventProcessActor}} within {{DAGScheduler.start()}} to ensure {{DAGScheduler}} is fully constructed when the scheduled task is executed.

Will fix it ASAP.;;;","27/Nov/13 21:53;Qiuzhuang;Code to simulate the bug.;;;","27/Nov/13 21:53;Qiuzhuang;The example I write by myself to analyze my client's some log which contains all deployed tomcat's log files.  I am attaching the code file for you. As for data, since it's from client I couldn't attach them here, but I think you could just copy some log text files in some folder to simulate this.  Thanks.
;;;","27/Nov/13 22:13;markhamstra;[~liancheng], see also SPARK-965 and [PR215|https://github.com/apache/incubator-spark/pull/215].;;;","27/Nov/13 23:29;liancheng;Hi [~markhamstra], thanks for pointing this out.

By adding back {{DAGScheduler.start()}}, [SPARK-965|https://spark-project.atlassian.net/browse/SPARK-965] can also be solved.  Because {{DAGSchedulerSuite}} never calls {{DAGScheduler.start()}}, thus {{eventProcessActor}} won't be created, the stage re-submission task won't be scheduled, and the race won't exist.

Nevertheless, while reviewing {{DAGScheduler}}, I found that fields like {{failed}}, {{running}} & {{waiting}} are defined to be {{HashSet\[T\]}}, which is not thread safe, but are accessed by multiple threads.  This does introduce race conditions.  They should be defined as something like {{HashSet\[T\] with SynchronizedSet\[T\]}}.;;;","28/Nov/13 00:16;markhamstra;{quote}
fields like failed, running & waiting are defined to be HashSet[T], which is not thread safe, but are accessed by multiple threads
{quote}
I believe that you are mistaken.  Other than in DAGSchedulerSource, those data structures should only be accessed within the DAGScheduler itself and in a way that is carefully maintained to be single-threaded.  In this way, we avoid synchronization overhead and other nasty possibilities such as dead locking.  From DAGSchedulerSource, the sizes of those data structures are read in a way that isn't sensitive to concurrent mutation.

It is only when scheduler package tests (like those in the DAGSchedulerSuite) manipulate the internals of the DAGScheduler directly that there should be any trouble with race conditions or other concurrency issues.

If these and other DAGScheduler data structures are accessed by multiple threads, then those are errors that we would be very much interested in understanding and correcting.;;;","28/Nov/13 01:30;liancheng;bq. Other than in DAGSchedulerSource, those data structures should only be accessed within the DAGScheduler itself and in a way that is carefully maintained to be single-threaded. In this way, we avoid synchronization overhead and other nasty possibilities such as dead locking.

My fault... Just reviewed {{DAGScheduler}} in master HEAD and commit [bf4e613|https://github.com/liancheng/incubator-spark/blob/bf4e6131cceef4fe00fb5693117c0732f181dbd9/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala] (the commit before I replaced the daemon thread with Akka actor).  In the former case, your statement is true.  While in the latter case, I made a mistake that the scheduled stage re-submission task may run in another thread.  I should add a new {{DAGSchedulerEvent}} message {{ResubmitFailedStages}} and send this message to {{eventProcessActor}}, which can then call {{resubmitFailedStages()}} in the same thread that runs {{processEvent()}}.

So yes, embarrassingly, just spotted a bug introduced by myself :-(;;;","28/Nov/13 01:56;liancheng;Hi, [~markhamstra] & [~rxin], please help review [PR-216|https://github.com/apache/incubator-spark/pull/216] that solves both [SPARK-966|https://spark-project.atlassian.net/browse/SPARK-966] and [SPARK-965|https://spark-project.atlassian.net/browse/SPARK-965].  Thanks!;;;","07/Dec/13 14:45;joshrosen;Fixed in https://github.com/apache/incubator-spark/pull/216;;;",,,,,,,,,,,,,,,,,,,,,
Race in DAGSchedulerSuite,SPARK-965,12705067,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liancheng,markhamstra,,25/Nov/13 14:19,07/Dec/13 14:50,14/Jul/23 06:25,07/Dec/13 14:50,0.8.1,0.9.0,,,,,,,0.9.0,,,,,Spark Core,,,,,0,,,,,,"After https://github.com/apache/incubator-spark/pull/159, resubmitFailedStages is scheduled in the eventProcessorActor preStart to run periodically in the actor's thread.  But DAGSchedulerSuite calls resubmitFailedStages directly from its own thread, which means that it is possible for both threads to be concurrently mutating shared data structures (e.g. the set of failed stages and cacheLocs), leading to failure of the test calling resubmitFailedStages.",,joshrosen,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383244,,,Sat Dec 07 14:50:43 UTC 2013,,,,,,,,,,"0|i1u0xr:",383512,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/13 14:50;joshrosen;Fixed in https://github.com/apache/incubator-spark/pull/216;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Races in JobLoggerSuite,SPARK-963,12704977,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,patrick,markhamstra,,24/Nov/13 15:37,08/Dec/13 18:24,14/Jul/23 06:25,08/Dec/13 18:24,0.8.1,0.9.0,,,,,,,0.8.1,,,,,Spark Core,,,,,0,,,,,,"Since SparkListener events are now processed asynchronously from the SparkListenerBus eventQueue, the jobLogger conditions checked in the ""inner variables"" and ""interface functions"" tests of the JobLoggerSuite are not guaranteed to be satisfied just because rdd.reduceByKey(_+_).collect() has returned.  This leads to infrequent failures of these tests when we lose the race.",,markhamstra,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383238,,,Sun Nov 24 16:16:47 UTC 2013,,,,,,,,,,"0|i1u0wf:",383506,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/13 15:41;markhamstra;How come I never see typos until after I have clicked a 'submit' button?  This issue should be entitled ""Races in JobLoggerSuite"".;;;","24/Nov/13 15:49;rxin;I just changed it. Were you not able to edit the title yourself?;;;","24/Nov/13 16:16;markhamstra;Some combination of not being permitted to edit the title and not knowing how to edit it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian package contains old version of executable scripts,SPARK-962,12704955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dhardy,,21/Nov/13 07:40,25/Nov/14 08:58,14/Jul/23 06:25,25/Nov/14 08:58,0.8.0,,,,,,,,,,,,,Build,Deploy,,,,0,deb,debian,jdeb,package,script,"When building debian package with maven
The spark-shell and spark-executor are not usable (packages are not org.apache.*) and seams outdated compare to ./spark-shell and ./spark-exector in repo.

instead of having outdated repl-bin/src/deb/bin/ use a common directory for run spark-shell and spark-executor scripts that jdeb can refer to.",,dhardy,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383237,,,Tue Nov 25 08:58:57 UTC 2014,,,,,,,,,,"0|i1u0w7:",383505,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/13 03:03;dhardy;https://github.com/clearstorydata/incubator-spark/pull/1 is a great way to do it I suppose.;;;","10/Dec/13 16:27;markhamstra;So we've got a ""make it work"" hack in for 0.8.1, but Debian packaging should be handled better in 0.9.  The most glaring issue in the current hack is that the packaged Spark doesn't use {{spark-class}} and the other new scripts that are described in all of the current how-to docs.  Instead, it uses the equivalent of the old {{run}} script.  The link above to the way I changed Debian packaging for ClearStory is adequate to build a package from the {{assembly}} sub-project and to deploy and use the standard scripts.  With perhaps some changes in the package name and metadata, that may be enough for Apache Spark 0.9.

However, this would still be a fair remove from Debian packaging adequate for inclusion in a Debian or Ubuntu distribution.  It would still be just a minimalist wrapping of a fat Spark jar that can be used with a deployment tool such as Chef -- that's all that the Debian packaging of Spark has been intended for to date, and is how we use it at ClearStory.

If we're ready to go further with Spark packaging (i.e. not just proper Debian packaging, but also RPMs and installation packages for Mac and Windows), then we should produce more refined Debian packaging.  That would likely mean producing not just a spark-core package, but separate packages for examples, tools, Java API, Python API, MLlib, etc. as well as a proper source package.

Building and maintaining packages for multiple OSes is a fair amount of work (and I'm not volunteering to do it, since what we've already got at ClearStory is adequate for my needs), so it is worth discussing whether that is something that Spark needs at this point or whether there are easier packaging/distribution targets to hit that are adequate for now.;;;","25/Nov/14 08:58;srowen;This appears fixed. The referenced PR was apparently merged into 0.9, and the outdated scripts referenced in the description are no longer present. Spark uses spark-class from spark-submit et al as desired.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JobCancellationSuite ""two jobs sharing the same stage"" is broken",SPARK-960,12704945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,markhamstra,,18/Nov/13 13:59,26/Jan/15 22:32,14/Jul/23 06:25,26/Jan/15 22:32,0.8.1,0.9.0,,,,,,,1.3.0,,,,,Spark Core,,,,,0,,,,,,"This test doesn't work as it appears to be intended since the map tasks can never acquire sem2.  The simplest way to demonstrate this is to comment out f1.cancel() in the future.  I believe the intention is that f1 and f2 would then complete normally; but they won't.  Instead, both jobs block, waiting on sem2.  It doesn't look like closing over Semaphores works even in a Local context, since sem2.hashCode() is different in each of f1, f2 and in the future containing f1.cancel, so the map jobs never see the sem2.release(10) in the future.

Instead, the test only completes because all of the stages (the two final stages and the common dependent stage) get cancelled and aborted.  When job <--> stage dependencies are fully accounted for and job cancellation changed so that f1.cancel does not abort the common stage, then this test can never finish since it then becomes hung waiting on sem2.",,apachespark,joshrosen,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383273,,,Mon Jan 26 22:32:47 UTC 2015,,,,,,,,,,"0|i1u147:",383541,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/15 12:03;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4180;;;","26/Jan/15 22:32;joshrosen;Issue resolved by pull request 4180
[https://github.com/apache/spark/pull/4180];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ivy fails to download javax.servlet.orbit dependency,SPARK-959,12704813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,,17/Nov/13 22:36,24/Mar/14 22:42,14/Jul/23 06:25,19/Dec/13 00:10,0.7.3,0.8.0,0.8.1,0.9.0,,,,,0.9.0,,,,,Build,,,,,0,,,,,,"Issue: Ivy attempts to download the ""javax.servlet.orbit"" dependency with an extension of "".orbit"" instead of "".jar"" and fails.

(One) Solution: Can add the following to the libraryDependencies in SparkBuild.scala:
""org.eclipse.jetty.orbit"" % ""javax.servlet"" % ""2.5.0.v201103041518"" artifacts Artifact(""javax.servlet"", ""jar"", ""jar"")

Cause: I don't know. This does not occur for everyone, and does not seem directly related to the version of Ant.

See the following for bug reports on the user lists:
http://mail-archives.apache.org/mod_mbox/spark-user/201309.mbox/%3CCAJbo4neXyzQe6zGREQJTzZZ5ZrCoAvfEN+WmBYCed6N1EPftxA@mail.gmail.com%3E
and
http://mail-archives.apache.org/mod_mbox/spark-user/201311.mbox/%3CCANGvG8pXVhcKiGEpXnGHfQeAYTyUygA%3D1nxSe0%3D%2BfRfnKSq88w%40mail.gmail.com%3E",,ilikerps,liancheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382915,,,Mon Mar 24 22:42:56 UTC 2014,,,,,,,,,,"0|i1tywn:",383183,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/13 23:36;ilikerps;This is the ivy bug report related to the issue: https://issues.apache.org/jira/browse/IVY-899
Looks like it was fixed in 2.3.x, but we're still using ivy 2.0, I believe.;;;","19/Dec/13 00:10;ilikerps;https://github.com/apache/incubator-spark/pull/183;;;","24/Mar/14 22:42;liancheng;Seems that some transitive dependencies of javax.servlet also have the same issue. At least under my network environment, I have to add two more lines to get things done:

""org.eclipse.jetty.orbit"" % ""javax.activation"" % ""1.1.0.v201105071233"" artifacts Artifact(""javax.activation"", ""jar"", ""jar""),
""org.eclipse.jetty.orbit"" % ""javax.mail.glassfish"" % ""1.4.1.v201005082020"" artifacts Artifact(""javax.mail.glassfish"", ""jar"", ""jar""),

But I'm not sure whether it's sufficient to workaround the problem...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class path issue in case assembled with specific hadoop version,SPARK-947,12705086,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,shekharbansal,,28/Oct/13 02:33,01/Nov/13 00:32,14/Jul/23 06:25,01/Nov/13 00:32,0.8.0,,,,,,,,0.8.1,0.9.0,,,,Build,,,,,0,,,,,,"assemble with hadoop version 1.2.1 using command SPARK_HADOOP_VERSION=1.2.1 sbt/sbt clean assembly

now we have two jars  $SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar and $SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.0.4.jar

compute class path script put both jars in classpath without adding "":"" between them and results in invalid jar name.

classpath computed by script::
/data/installs/spark-0.8.0-incubating-bin-hadoop1/conf:/data/installs/spark-0.8.0-incubating-bin-hadoop1/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.0.4.jar/data/installs/spark-0.8.0-incubating-bin-hadoop1/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar:/data/installs/hadoop-1.2.1/conf",,rxin,shekharbansal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383293,,,Fri Nov 01 00:31:55 UTC 2013,,,,,,,,,,"0|i1u18n:",383561,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/13 18:39;rxin;How did you generate multiple assembled jars? 

I just tried doing SPARK_HADOOP_VERSION=1.2.1 sbt/sbt clean assembly

and only see one assembled jar:

rxin @ rxin-air : ~/Downloads/spark-0.8.0-incubating-bin-hadoop1 
> find . -name ""spark-assembly*""
./assembly/target/resolution-cache/org.apache.spark/spark-assembly_2.9.3
./assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar

;;;","01/Nov/13 00:28;shekharbansal;now I am not able to reproduce it with sbt clean assembly command but i see two jars when i run stb assembly command. steps to reproduce.


download spark-0.8.0-incubating-bin-hadoop1.tgz
extract it

run find command
./assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar
./assembly/target/spark-assembly_2.9.3-0.8.0-incubating.jar
./assembly/target/spark-assembly_2.9.3-0.8.0-incubating-sources.jar


run assembly command
[admin@COL1 spark-0.8.0-incubating-bin-hadoop1]# SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly


run find command again
./assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar
./assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar
./assembly/target/spark-assembly_2.9.3-0.8.0-incubating.jar
./assembly/target/spark-assembly_2.9.3-0.8.0-incubating-sources.jar
./assembly/target/resolution-cache/org.apache.spark/spark-assembly_2.9.3
;;;","01/Nov/13 00:31;rxin;Yea I think if you don't include ""clean"" and you have a different hadoop version, the assembly would create another jar resulting in two jars in the target directory.

I am going to close this ticket because we already added to the spark-class script to throw a more meaningful error message when this situation happens. 

https://github.com/apache/incubator-spark/commit/4ba32678e04dc687a9f574eeeb1450e4d291ae1f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce memory footprint of DiskBlockManager.blockToFileSegmentMap,SPARK-946,12705110,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,,26/Oct/13 13:53,05/Nov/13 08:13,14/Jul/23 06:25,05/Nov/13 08:13,0.8.1,0.9.0,,,,,,,0.8.1,0.9.0,,,,,,,,,0,,,,,,"blockToFileSegmentMap right now is taking up ~400 bytes per FileSegment. In large shuffles (e.g., >1000 mappers/executor and >1000 reducers), this can lead to several GB used just for this map, which is leading to OOMs.
",,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383290,,,Tue Nov 05 08:13:58 UTC 2013,,,,,,,,,,"0|i1u17z:",383558,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/13 13:56;ilikerps;Due to this issue, it'd also be useful if turning off shuffle file consolidation avoided this map entirely, rather than just further exploding the map size.;;;","05/Nov/13 08:13;ilikerps;Fixed by:
https://github.com/apache/incubator-spark/pull/130
https://github.com/apache/incubator-spark/pull/139;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix confusing behavior when assembly jars already exist,SPARK-945,12705101,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,,25/Oct/13 14:59,30/Mar/14 04:15,14/Jul/23 06:25,26/Oct/13 14:51,0.8.0,,,,,,,,0.8.1,0.9.0,,,,,,,,,0,,,,,,"If you have multiple assembly jars (e.g. from doing builds with different hadoop versions) `spark-class` gives a very confusing message.

We should check for this explicitly. Also we should tell people do to `clean assembly` wherever we document this to avoid this happening.",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382815,,,2013-10-25 14:59:33.0,,,,,,,,,,"0|i1tyaf:",383083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not materialize partitions when DISK_ONLY storage level is used,SPARK-942,12704764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kellrott,kellrott,,20/Oct/13 21:36,17/Mar/14 10:21,14/Jul/23 06:25,17/Mar/14 10:21,0.8.0,0.8.1,0.9.0,,,,,,1.0.0,,,,,Block Manager,Spark Core,,,,1,,,,,,"If an operation returns a generating iterator (i.e. one that creates return values as the 'next' method is called), for example as the result of a 'flatMap' call on an RDD, the CacheManager first completely unrolls the iterator into an Array buffer before passing it to the blockManager (CacheManager.scala:74). Only after the entire iterator has been put into a buffer does it check if there is enough space in memory to store the data (BlockManager.scala:608). 
In the attached test, the code can complete the operation of 'saveAsTextFile' of text strings if it is called directly on the result RDD of a flatMap operation, this is because it is given an iterator result, and works on the map-then-save operation as the results are generated. In the other branch, a 'persist' is called, and the cacheManger first tries to un-roll the entire iterator before deciding to store it too disk, this will cause a Memory Error (on systems with -Xmx512m)

In the cases where storing to disk is an option perhaps the CacheManager(or the BlockManager), can start to scan the iterator, calculating its size as the is pushed into a buffer as it goes (rather pushing everything into a buffer in a single operation), and if it determines that it will run out of memory, start pushing the already buffered portion of the iterator to disk, and then finish scanning the original iterator pushing that onto disk.


Example Code (switch value of 'fail' variable to toggle behavior):

{code:title=MemTest.scala|borderStyle=solid}

import org.apache.spark.SparkContext
import org.apache.spark.storage.StorageLevel

class Expander(base:String, count:Integer) extends Iterator[String] {
  var i = 0;
  def next() : String = {
    i += 1;
    return base + i.toString;
  }
  def hasNext() : Boolean = i < count;
}

object MemTest {
    def expand(s:String, i:Integer) : Iterator[String] = {
      return new Expander(s,i)
    }
    def main(args:Array[String]) = {
        val fail = false;
        val sc = new SparkContext(""local"", ""mem_test"");
        val seeds = sc.parallelize( Array(
          ""This is the first sentence that we will test:"",
          ""This is the second sentence that we will test:"",
          ""This is the third sentence that we will test:""
        ) ); 
        val out = seeds.flatMap(expand(_,10000000));
        if (fail) {
          out.map(_ + ""..."").persist(StorageLevel.MEMORY_AND_DISK).saveAsTextFile(""test.out"")
        } else {
          out.map(_ + ""..."").saveAsTextFile(""test.out"")
        }
    }
} 

{code}",,aht,ash211,jey,kellrott,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1201,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382994,,,Thu Mar 06 14:47:36 UTC 2014,,,,,,,,,,"0|i1tze7:",383262,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/13 15:55;kellrott;This happens with persist set to StorageLevel.DISK_ONLY as well.;;;","16/Nov/13 22:52;kellrott;Fix submitted: https://github.com/apache/incubator-spark/pull/180;;;","06/Mar/14 14:47;patrick;I changed the title slightly here. This is a problem not only for generative iterators but even for normal `map` operations that happen to return large values. In all these cases we materialize the input partition even though, when writing to disk, we don't have to.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in documentation,SPARK-935,12704875,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,rkwasny,,16/Oct/13 08:08,07/Dec/13 14:40,14/Jul/23 06:25,07/Dec/13 14:40,,,,,,,,,0.8.1,,,,,,,,,,0,,,,,,"Small typo on:
http://spark.incubator.apache.org/docs/latest/hadoop-third-party-distributions.html

s/chd4/cdh4/

for cloudera artifacts
",,joshrosen,rkwasny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383246,,,Sat Dec 07 14:40:16 UTC 2013,,,,,,,,,,"0|i1u0y7:",383514,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/13 14:40;joshrosen;Fixed in https://github.com/apache/incubator-spark/pull/171;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"spark-mllib occasionally throw java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509)",SPARK-934,12705142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gq,,15/Oct/13 02:30,20/Aug/14 08:22,14/Jul/23 06:25,20/Aug/14 08:22,0.9.0,,,,,,,,,,,,,,,,,,0,,,,,,"java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509)

com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:346)
com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:192)
com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254)
com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:129)
java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2309)
java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2322)
java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:3012)
java.io.ObjectInputStream.readArray(ObjectInputStream.java:1691)
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342)
java.io.ObjectInputStream.readArray(ObjectInputStream.java:1704)
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342)
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:39)
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:101)
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26)
org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:40)
org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98)
org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:121)
org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:118)
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)
org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:118)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:32)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:32)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:32)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:32)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
org.apache.spark.scheduler.ResultTask.run(ResultTask.scala:99)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:158)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:724)",,gq,Qiuzhuang,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383294,,,Thu Oct 17 04:50:48 UTC 2013,,,,,,,,,,"0|i1u18v:",383562,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/13 11:13;shivaram;Could you provide some more details on what was the program being run and what was the input data etc. ?;;;","16/Oct/13 03:30;gq;code:
System.setProperty(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
System.setProperty(""spark.kryo.registrator"", classOfALSRegistrator.getName)
System.setProperty(""spark.kryo.referenceTracking"", ""false"")
System.setProperty(""spark.kryoserializer.buffer.mb"", ""8"")
System.setProperty(""spark.locality.wait"", ""10000"")
ALS.trainImplicit(ratings, rank, iterations)
rank is 20
iterations is 20 
ratings.count is 3609314 

java.lang.ArrayIndexOutOfBoundsException (java.lang.ArrayIndexOutOfBoundsException: 344)

org.apache.spark.mllib.recommendation.ALS$$anonfun$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:369)
scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:78)
org.apache.spark.mllib.recommendation.ALS$$anonfun$updateBlock$1.apply$mcVI$sp(ALS.scala:366)
scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:78)
org.apache.spark.mllib.recommendation.ALS.updateBlock(ALS.scala:365)
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:337)
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:336)
org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:32)
org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:32)
scala.collection.Iterator$$anon$19.next(Iterator.scala:401)
scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440)
scala.collection.Iterator$class.foreach(Iterator.scala:772)
scala.collection.Iterator$$anon$21.foreach(Iterator.scala:437)
org.apache.spark.rdd.PairRDDFunctions.reducePartition$1(PairRDDFunctions.scala:176)
org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1.apply(PairRDDFunctions.scala:191)
org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1.apply(PairRDDFunctions.scala:191)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:107)
org.apache.spark.scheduler.Task.run(Task.scala:53)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:210)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:724);;;","17/Oct/13 04:50;gq;Seems org.apache.spark.serializer.KryoSerializer causing problems. Switch to org.apache.spark.serializer.JavaSerializer no problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential bug with Spark streaming example,SPARK-931,12704956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,10/Oct/13 20:03,16/Oct/13 10:45,14/Jul/23 06:25,16/Oct/13 10:45,0.8.0,,,,,,,,0.8.1,0.9.0,,,,DStreams,,,,,0,starter,,,,,"See e-mail thread:
https://groups.google.com/forum/#!topic/spark-users/GVPDZOCZMcw",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383304,,,Tue Oct 15 22:58:28 UTC 2013,,,,,,,,,,"0|i1u1b3:",383572,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/13 22:58;patrick;Patch submitted: https://github.com/apache/incubator-spark/pull/63/files;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark sample() doesn't work if numpy is installed on master but not on workers,SPARK-927,12705450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,farrellee,joshrosen,,08/Oct/13 11:03,05/Jan/15 23:05,14/Jul/23 06:25,05/Jan/15 23:05,0.8.0,0.9.1,1.0.2,1.1.2,,,,,1.2.0,,,,,PySpark,,,,,0,,,,,,"PySpark's sample() method crashes with ImportErrors on the workers if numpy is installed on the driver machine but not on the workers.  I'm not sure what's the best way to fix this.  A general mechanism for automatically shipping libraries from the master to the workers would address this, but that could be complicated to implement.",,farrellee,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4477,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383315,,,Mon Jan 05 23:05:05 UTC 2015,,,,,,,,,,"0|i1u1dj:",383583,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/14 15:25;farrellee;it looks like the issue is rddsampler checks for numpy in its constructor instead of when initializing the random number generator;;;","05/Jan/15 23:05;farrellee;PR #2313 was subsumed by PR #3351, which resolved SPARK-4477 and this issue

the resolution was to remove the use of numpy altogether;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSON endpoint URI scheme part (spark://) duplicated,SPARK-920,12705119,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dmccauley,dmccauley,,02/Oct/13 05:00,09/Oct/13 02:25,14/Jul/23 06:25,09/Oct/13 02:25,0.8.1,,,,,,,,0.8.1,,,,,Web UI,,,,,0,,,,,,"Within JsonProtocol the MasterStateResponse URI field is prepended with ""spark://"" but the URI is always already prepended with this string within DeployMessages.

Example output:
spark://spark://host:port",,dmccauley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383313,,,Wed Oct 09 02:25:39 UTC 2013,,,,,,,,,,"0|i1u1d3:",383581,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/13 02:25;dmccauley;Merged, PR: https://github.com/apache/incubator-spark/pull/27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-ec2 launch --resume doesn't re-initialize all modules,SPARK-919,12705079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,26/Sep/13 13:51,04/Oct/13 21:28,14/Jul/23 06:25,04/Oct/13 21:28,0.8.0,,,,,,,,0.8.1,,,,,EC2,,,,,0,,,,,,"I launched a Spark cluster using the new EC2 scripts, stopped it with {{stop}}, then restarted it with {{start}} and ran {{launch --resume}} to re-deploy the Spark configurations.

It looks like the script exits after initializing Spark if it's already installed:

{code}
Initializing spark
~ ~/spark-ec2
Spark seems to be installed. Exiting.
Connection to ec2-*-.amazonaws.com closed.
Spark standalone cluster started at http://ec2-*.compute-1.amazonaws.com:8080
Ganglia started at http://ec2-*-1.amazonaws.com:5080/ganglia
Done!
{code}

It looks like the problem is that the module init scripts are run by {{sourcing}} them into the top-level {{setup.sh}} script rather than running them in subshells, so the module script's own exit command kills the top-level setup script:

{code}
# Install / Init module
for module in $MODULES; do
  echo ""Initializing $module""
  if [[ -e $module/init.sh ]]; then
    source $module/init.sh
  fi
done
{code}",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383320,,,Thu Sep 26 14:13:06 UTC 2013,,,,,,,,,,"0|i1u1en:",383588,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/13 14:13;joshrosen;Pull request here: https://github.com/mesos/spark-ec2/pull/22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
local metrics test has race condition due to new SparkListener architecture,SPARK-908,12705138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,patrick,,15/Sep/13 14:41,25/Nov/13 14:44,14/Jul/23 06:25,25/Nov/13 14:44,,,,,,,,,0.8.1,,,,,,,,,,0,,,,,,"This test assumes that the listener will synchronously process an event once an action is called. Kay's changes made this asynchronous, so this test can now fail sometimes depending on the order in which events occur.

https://github.com/mesos/spark/blob/master/core/src/test/scala/org/apache/spark/scheduler/SparkListenerSuite.scala",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383272,,,2013-09-15 14:41:36.0,,,,,,,,,,"0|i1u13z:",383540,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"How to recover Spark Master in case of machine failure, where Spark Master was running",SPARK-906,12705155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ayushmishra2005,,11/Sep/13 05:10,14/Nov/13 18:15,14/Jul/23 06:25,14/Nov/13 18:15,0.7.3,,,,,,,,0.8.1,0.9.0,,,,Spark Core,,,,,0,,,,,,"I have one Spark master on machine A and two Spark workers on another machines B and C. 
If machine A is failed for any reason, Spark master would die in this case.

Is there any way to recover Spark Master or to create a new Spark Master on another machine automatically?

Can anyone help me to resolve this?

Thanks in advance.
",,ayushmishra2005,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383279,,,Thu Nov 14 18:15:34 UTC 2013,,,,,,,,,,"0|i1u15j:",383547,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/13 18:15;ilikerps;In Spark 0.8.1 and onward, there are two solutions for this (one uses ZooKeeper for full featured fault-tolerance, and the other uses the file system to be able to recover on the same machine). 0.8.1 is not yet released, but if you're interested in reading more about this, you can check out the unpublished docs at https://github.com/apache/incubator-spark/blob/master/docs/spark-standalone.md.

(Even further down the pipeline, we hope to remove the Master entirely...);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent spark assembly,SPARK-903,12705139,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,patrick,mweindel,,11/Sep/13 05:04,15/Sep/13 14:39,14/Jul/23 06:25,15/Sep/13 14:39,0.8.0,,,,,,,,,,,,,Build,,,,,0,,,,,,"I have two problems when using the spark assembly (build via sbt\sbt assembly)

1. HDFS file system is not available
{quote}
java.io.IOException: No FileSystem for scheme: hdfs
{quote}
This seems to be caused by overriding the file
{{META-INF/services/org.apache.hadoop.fs.FileSystem}}

Merging the original files manually resolves this problem:
- hadoop-hdfs-2.0.0-cdh4.3.0.jar and 
- hadoop-common-2.0.0-cdh4.3.0.jar


2. Illegal jar file on windows
The assembly jar contains 
- a directory META-INF/license
- a file META-INF/LICENSE
As Windows file system is case-insensitive, this can cause trouble.",,mweindel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383338,,,Sat Sep 14 06:52:35 UTC 2013,,,,,,,,,,"0|i1u1in:",383606,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/13 06:52;mweindel;HDFS file system problem has already been fixed by Patrick Wendell (commit 0c1985b153a2dc2c891ae61c1ee67506926384ae);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.AbstractMethodError when using FlatMapFunction from Java,SPARK-902,12705076,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,mweindel,,11/Sep/13 04:54,22/Oct/13 16:33,14/Jul/23 06:25,22/Oct/13 16:33,0.7.3,0.8.0,,,,,,,0.8.1,0.9.0,,,,Java API,Spark Core,,,,0,,,,,,"*Important Note*: a patch for _FlatMapFunction.scala_ fixing the problem is attached!

Defining a Java function class based on org.apache.spark.api.java.function.FlatMapFunction compiles without problems, but on execution results in 

{quote}
java.lang.AbstractMethodError: org.apache.spark.api.java.function.WrappedFunction1.call(Ljava/lang/Object;)Ljava/lang/Object;
	at org.apache.spark.api.java.function.WrappedFunction1.apply(WrappedFunction1.scala:31)
{quote}

I have investigated the problem in detail. Using the following small Java class, the issue can easily be reproduced.

{code:java}
import java.lang.reflect.Method;
import java.util.Arrays;

import org.apache.spark.api.java.function.FlatMapFunction;


public class X {
	public static class MyFunction extends FlatMapFunction<String, Long> {
		@Override
		public Iterable<Long> call(String s) throws Exception {
			return Arrays.asList(Long.parseLong(s));
		}		
	}
	
	public static void main(String[] args) {
		//printMethods(WrappedFunction1.class);
		printMethods(FlatMapFunction.class);
		
		MyFunction f = new MyFunction();
		final Iterable<Long> result = f.apply(""1"");
		System.out.println(result);
	}

	private static void printMethods(Class<?> cls) {
		System.out.println(cls.getName() + "" --------------"");
		for (Method m: cls.getDeclaredMethods()) {
			System.out.println(m);
		}
		final Class<?> superCls = cls.getSuperclass();
		if (superCls != null && superCls != Object.class) {
			printMethods(cls.getSuperclass());
		}		
	}

}
{code}

When you run this code, you get following output:
{quote}
org.apache.spark.api.java.function.FlatMapFunction --------------
public abstract java.lang.Iterable org.apache.spark.api.java.function.FlatMapFunction.call(java.lang.Object) throws java.lang.Exception
public scala.reflect.ClassManifest org.apache.spark.api.java.function.FlatMapFunction.elementType()
org.apache.spark.api.java.function.Function --------------
public scala.reflect.ClassManifest org.apache.spark.api.java.function.Function.returnType()
public abstract java.lang.Object org.apache.spark.api.java.function.Function.call(java.lang.Object) throws java.lang.Exception
org.apache.spark.api.java.function.WrappedFunction1 --------------
public final java.lang.Object org.apache.spark.api.java.function.WrappedFunction1.apply(java.lang.Object)
public abstract java.lang.Object org.apache.spark.api.java.function.WrappedFunction1.call(java.lang.Object) throws java.lang.Exception
...
Exception in thread ""main"" java.lang.AbstractMethodError: org.apache.spark.api.java.function.WrappedFunction1.call(Ljava/lang/Object;)Ljava/lang/Object;
	at org.apache.spark.api.java.function.WrappedFunction1.apply(WrappedFunction1.scala:31)
	at X.main(X.java:21)
{quote}

So the problem seems to be that the _call_ method is defined twice.
# returning Object
# returning Iterable

Solution: just delete the definition of the _call_ method in _FlatMapFunction_ (see attached file)",,joshrosen,mweindel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/13 04:54;mweindel;FlatMapFunction.scala;https://issues.apache.org/jira/secure/attachment/12637634/FlatMapFunction.scala",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383297,,,Tue Oct 22 16:33:12 UTC 2013,,,,,,,,,,"0|i1u19j:",383565,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/13 14:21;joshrosen;I tried running your code example using both Maven and sbt, with both Spark 0.7.3 and 0.8.0-incubating, but I wasn't able to reproduce the AbstractMethodError.

I'm running this Java version on OSX:

{code}
java version ""1.7.0_21""
Java(TM) SE Runtime Environment (build 1.7.0_21-b12)
Java HotSpot(TM) 64-Bit Server VM (build 23.21-b01, mixed mode)
{code}

How are you compiling and running this example code?;;;","21/Oct/13 13:13;mweindel;The problem only occurs with the Eclipse compiler!

If I'm using javac, the sample runs without errors. I never expected that the Eclipse compiler is the cause.
So this problem happens if someone builds a JAR package in Eclipse and then deploys it to Spark. In fact, this is my use case.

To reproduce the problem, either compile the sample code in Eclipse or by running the following command line after downloading the Eclipse compiler from e.g. http://repo1.maven.org/maven2/org/eclipse/tycho/org.eclipse.jdt.core/3.9.0.v20130604-1421/org.eclipse.jdt.core-3.9.0.v20130604-1421.jar:

java -jar org.eclipse.jdt.core-3.9.0.v20130604-1421.jar -cp spark-assembly_2.9.3-0.8.0-incubating-hadoop2.0.0-mr1-cdh4.2.0.jar -source 1.7 -target 1.7 Main.java


;;;","22/Oct/13 13:26;joshrosen;Yep, it looks like the Eclipse compiler is the culprit.  I was able to reproduce this bug by compiling my code through Maven, using the Eclipse compiler plugin.  For my own future reference, here's the POM that I used:

{code}
<project>
  <groupId>edu.berkeley</groupId>
  <artifactId>simple-project</artifactId>
  <modelVersion>4.0.0</modelVersion>
  <name>Simple Project</name>
  <packaging>jar</packaging>
  <version>1.0</version>
  <repositories> <!-- Repositories that Spark needs -->
    <repository>
      <id>Spray Repository</id>
      <url>http://repo.spray.cc</url>
    </repository>
    <repository>
      <id>Akka Repository</id>
      <url>http://repo.akka.io/releases</url>
    </repository>
  </repositories>
  <dependencies>
    <dependency> <!-- Spark dependency -->
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.9.3</artifactId>
      <version>0.8.0-incubating</version>
    </dependency>
  </dependencies>
  <build>
    <plugins>
      <plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-compiler-plugin</artifactId>
     <version>3.1</version>
     <configuration>
        <compilerId>eclipse</compilerId>
        <source>1.7</source>
        <target>1.7</target>
     </configuration>
     <dependencies>
        <dependency>
           <groupId>org.codehaus.plexus</groupId>
           <artifactId>plexus-compiler-eclipse</artifactId>
           <version>2.3</version>
        </dependency>
     </dependencies>
      </plugin>
    </plugins>
  </build>
</project>
{code}

I'm going to take a a closer look at the fix in your pull request (https://github.com/apache/incubator-spark/pull/30) to see whether we need to apply it anywhere else, or whether the change breaks the ability to throw exceptions from JavaFunctions. ;;;","22/Oct/13 16:33;joshrosen;Fixed by https://github.com/apache/incubator-spark/pull/100;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UISuite ""jetty port increases under contention"" fails if startPort is in use",SPARK-901,12705066,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,markhamstra,,10/Sep/13 15:58,17/Sep/14 22:17,14/Jul/23 06:25,17/Sep/14 22:17,0.8.0,,,,,,,,,,,,,Build,Spark Core,Web UI,,,0,,,,,,"Recent change of startPort to 3030 conflicts with IANA assignment for arepa-cas.  If 3030 is already in use, the UISuite fails.",,cos,hsaputra,markhamstra,nsundeepreddy,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3555,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383322,,,Wed Sep 17 22:17:07 UTC 2014,,,,,,,,,,"0|i1u1f3:",383590,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/13 16:06;markhamstra;Ah ha!  It's actually Typesafe's Zinc server that is the other half of the conflict.  Zinc declares 3030 to be the default Nailgun port.;;;","10/Sep/13 16:51;cos;the question that Mark raised on the JIRA is essentially about finding which number is greater out of the following two:
 - 'how many people out there are relying on well-known 3030 port'
vs
 - 'how many Zinc server devs are using Spark'

;;;","10/Sep/13 16:59;markhamstra;Doing some more googling, it's pretty hard to find out just what arepa-cas is, much less how many people are actually using it.  I think that it is far more likely that Spark developers trying to use Zinc will run into this conflict than do arepa-cas users.  Zinc is a pretty useful tool that more Scala developers should use, so we should at least warn them of the port conflict if we choose to leave the SparkUI.DEFAULT_PORT at 3030.

Incidentally, shouldn't UISuite be using SparkUI.DEFAULT_PORT instead of 3030 so that it is obvious where this magic number is coming from and why it is important?;;;","10/Sep/13 17:14;cos;bq. Incidentally, shouldn't UISuite be using SparkUI.DEFAULT_PORT instead of 3030 so that it is obvious where this magic number is coming from and why it is important?

won't it cause the port conflict when server and a worker run on the same node? ;;;","03/Oct/13 17:49;nsundeepreddy;Leaving the current test as it is, still causes transient issues. Shouldn't we protect the test setup call server.start() ?
Even though the chance of the port being used is low, it will be good to avoid the setup of a test being dependent on a fixed port.;;;","17/Sep/14 22:17;pwendell;This is fixed by SPARK-3555 since we no longer chose a specific starting port.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add jets3t dependency to Spark Build,SPARK-898,12704697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,patrick,,09/Sep/13 23:10,15/Sep/13 23:07,14/Jul/23 06:25,15/Sep/13 23:07,0.7.3,0.8.0,,,,,,,0.8.0,,,,,,,,,,0,,,,,,This is necessary for s3 reads and writes to work correctly with some hadoop versions.,,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383335,,,2013-09-09 23:10:06.0,,,,,,,,,,"0|i1u1hz:",383603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mvn package doesn't include yarn in the repl-bin shaded jar,SPARK-887,12705097,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,tgraves,,29/Aug/13 15:29,03/Sep/13 12:33,14/Jul/23 06:25,31/Aug/13 13:11,0.8.0,,,,,,,,0.8.0,,,,,Build,,,,,0,,,,,,"If you build with sbt (SPARK_WITH_YARN=true sbt/sbt package assembly) it will add the Yarn classes (YarnClientImpl) into the repl assembled jar ./repl/target/spark-repl-assembly-0.8.0-SNAPSHOT.jar , but if you build with mvn -Phadoop2-yarn package it doesn't add the Yarn class in the repl shaded jar (./repl-bin/target/spark-repl-bin-0.8.0-SNAPSHOT-shaded.jar)

We should keep these consistent.   This matters for https://github.com/mesos/spark/pull/868 as it is relying on the YarnClientImpl being in the jar.",,matei,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383350,,,Tue Sep 03 12:33:10 UTC 2013,,,,,,,,,,"0|i1u1lb:",383618,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/13 06:37;tgraves;I think this might be fixed by https://github.com/mesos/spark/pull/857;;;","31/Aug/13 13:11;matei;Yup, it is.;;;","03/Sep/13 12:33;tgraves;So it looks like the new assembly jar does not include the YarnClientImpl. The only way for me to run the example is to include the yarn jar in the export SPARK_CLASSPATH variable.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullpointerExceptions in InputFormatInfo.computePreferredLocations/SparkHDFSLR ,SPARK-886,12705143,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tgraves,tgraves,,27/Aug/13 13:05,04/Nov/13 09:52,14/Jul/23 06:25,04/Nov/13 09:51,0.8.0,,,,,,,,0.8.1,0.9.0,,,,Spark Core,,,,,0,,,,,,"Since the changes from pr838 to change to hadoop agnostic builds, the SparkHdfsLR example throws a null pointer exception on Spark on Yarn (not sure if it fails on others). It also has the same issue if you call InputFormatInfo.computePreferredLocations before the SparkContext is created.  The reason is that both of those call SparkEnv.get.hadoop which hasn't been created yet if you haven't created the SparkContext.  

This is being used in the SparkHdfsLR example to pass the preferred locations into the SparkContext:

   // This is used only by yarn for now, but should be relevant to other cluster types (mesos, etc) too.
    // This is typically generated from InputFormatInfo.computePreferredLocations .. host, set of data-local splits on host
    val preferredNodeLocationData: scala.collection.Map[String, scala.collection.Set[SplitInfo]] = scala.collection.immutable.Map()",,rxin,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383291,,,Mon Nov 04 09:52:10 UTC 2013,,,,,,,,,,"0|i1u187:",383559,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/13 18:00;rxin;Should we close this one now https://github.com/apache/incubator-spark/pull/124 is merged?;;;","04/Nov/13 06:47;tgraves;Yes we can close it.  I don't seem to have permission to assign to myself or close.;;;","04/Nov/13 09:52;rxin;Thanks. I closed it. I will look into the permission issues ...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When built with Hadoop2, spark-shell and examples don't initialize log4j properly",SPARK-880,12705016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,matei,,19/Aug/13 11:56,06/Nov/14 07:06,14/Jul/23 06:25,06/Nov/14 07:06,,,,,,,,,,,,,,,,,,,0,,,,,,"They print this:

{code}
log4j:WARN No appenders could be found for logger (akka.event.slf4j.Slf4jEventHandler).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{code}

It might have to do with not finding a log4j.properties file. I believe hadoop1 had one in its own JARs (or depended on an older log4j that came with a default)? but hadoop2 doesn't. We should probably have our own default one in conf.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383371,,,Thu Sep 11 09:02:51 UTC 2014,,,,,,,,,,"0|i1u1pz:",383639,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/14 09:02;srowen;This should be resolved/obsoleted by subsequent updates to SLF4J and log4j integration, and the props file.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Typo in slaves file: ""listes"" instead of ""listed""",SPARK-879,12704742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,seanm,billmcn,,15/Aug/13 14:54,01/Sep/13 15:35,14/Jul/23 06:25,01/Sep/13 15:35,0.7.3,,,,,,,,0.8.0,,,,,Deploy,,,,,0,,,,,,"The comment at the top of the conf/slaves file reads ""A Spark Worker will be started on each of the machines listes below."" It should say ""listed"" instead of ""listes"".",,billmcn,seanm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383352,,,Thu Aug 15 20:12:49 UTC 2013,,,,,,,,,,"0|i1u1lr:",383620,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/13 20:12;seanm;resolved by: https://github.com/mesos/spark/pull/843;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark does not add Python *.zip and *.egg files to PYTHONPATH,SPARK-878,12704983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,schumach,schumach,,15/Aug/13 11:45,19/Aug/13 15:31,14/Jul/23 06:25,19/Aug/13 15:31,0.8.0,,,,,,,,,,,,,PySpark,,,,,0,Starter,,,,,"When a list of *.zip or *.egg files is passed to SparkContext via pyFiles these do not get added to PYTHONPATH on the worker. The situation is different for *.py files since for these it is sufficient to add the working directory to PYTHONPATH (via sys.path).

For the original discussion see:

https://groups.google.com/forum/#!searchin/spark-users/pyfiles/spark-users/jG8VC17vTe4/z_DhBoRWuAMJ",,joshrosen,schumach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383370,,,Mon Aug 19 14:58:47 UTC 2013,,,,,,,,,,"0|i1u1pr:",383638,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/13 14:00;joshrosen;Does Jey's patch fix this?  Want to test that and submit it as a pull request?;;;","15/Aug/13 14:07;schumach;It does but it does also add all other *.zip files and such to the path. I actually talked to him and he suggested that it could be a nice case for learning PySpark internals. I now have an alternative solution (adding an include list to PythonRDD which is serialized into the worker input stream) which I still need to test though. Then it would be great to hear your comments on that, Josh.;;;","15/Aug/13 16:42;schumach;Added pull request. Thanks Jey for discussing various approaches and helpful comments.;;;","19/Aug/13 14:58;schumach;Issue was fixed in pull request #840. I tried to mark it as ""resolved"" but possibly my access right don't allow me to(?!);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.UnsupportedOperationException: empty.reduceLeft in UI,SPARK-877,12704960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,rxin,,13/Aug/13 23:00,14/Aug/13 08:33,14/Jul/23 06:25,14/Aug/13 08:33,,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,"I opened stage's job progress UI page which had no active tasks and saw the following exception:

{code}
java.lang.UnsupportedOperationException: empty.reduceLeft
        at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:152)
        at scala.collection.mutable.ArrayOps.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayOps.scala:38)
        at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:69)
        at scala.collection.mutable.ArrayOps.reduceLeft(ArrayOps.scala:38)
        at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:180)
        at scala.collection.mutable.ArrayOps.reduce(ArrayOps.scala:38)
        at spark.ui.exec.ExecutorsUI.render(ExecutorsUI.scala:41)
        at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)
        at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)
        at spark.ui.JettyUtils$$anon$1.handle(JettyUtils.scala:61)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1040)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:976)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:363)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982
)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:628)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:722)
{code}",,patrick,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383373,,,Tue Aug 13 23:12:44 UTC 2013,,,,,,,,,,"0|i1u1qf:",383641,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/13 23:09;patrick;Hey this stack trace suggests you were accessing the Executors page rather than the job progress page. Is that right? 

{code}
  at spark.ui.exec.ExecutorsUI.render(ExecutorsUI.scala:41)
        at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)
        at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)
{code};;;","13/Aug/13 23:12;rxin;That was probably right (otherwise the stack wouldn't make sense). I had multiple pages open at the time.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobs UI shows incorrect task count if #tasks is not #partitions,SPARK-870,12704914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,12/Aug/13 22:15,25/Oct/13 10:20,14/Jul/23 06:25,25/Oct/13 09:54,0.8.0,,,,,,,,0.8.1,0.9.0,,,,Web UI,,,,,0,,,,,,"{code}
val rdd = sc.textFile(""/tpch10g/lineitem"")
sc.runJob(rdd, (it: Iterator[String]) => it.take(10).toArray, Array(0), false)
{code}",,Grace Huang,patrick,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383295,,,Fri Oct 25 10:20:18 UTC 2013,,,,,,,,,,"0|i1u193:",383563,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/13 16:17;patrick;I'm bumping this to 0.8.1. It only affects things in a few cases and it's not super simple to fix.;;;","25/Oct/13 10:15;rxin;Which PR fixed this?;;;","25/Oct/13 10:20;patrick;https://github.com/apache/incubator-spark/commit/fa9a0e40b2d76b85918958cf7d57ec95f766e785#diff-6ddec7f06d0cf5392943ecdb80fcea24R55;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered",SPARK-850,12705132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,billz,billz,,02/Aug/13 17:43,05/Aug/13 12:10,14/Jul/23 06:25,05/Aug/13 12:10,0.7.3,0.8.0,,,,,,,0.8.0,,,,,Spark Core,,,,,0,,,,,,"When you running the spark job with SPARK_MEM set too large, you will receive the following error  ""WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered""  Could you give a better error message in the console.

Step to recreate the issue:

1. set the SPARK_MEM in conf/spark-env.sh close to your memory limit on the box.
2. run the interactive Spark shell against the local cluster; for me is ""MASTER=spark://billz-retina.local:7077 ./spark-shell""
3. run the spark job; i.e. ""val a = sc.parallelize(1 to 100)"", a.count();

you will see console error:
13/08/02 16:45:01 WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered
13/08/02 16:45:16 WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered


In logs/spark-bill-spark.deploy.master.Master-1-billz-retina.local.out:

13/08/02 16:43:22 INFO master.Master: Registering app Spark shell
13/08/02 16:43:22 WARN master.Master: Could not find any workers with enough memory for app-20130802163627-0000
13/08/02 16:43:22 INFO master.Master: Registered app Spark shell with ID app-20130802164322-0002",,billz,ianoc,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383419,,,Mon Aug 05 09:53:24 UTC 2013,,,,,,,,,,"0|i1u20n:",383687,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/13 20:54;patrick;Hey Bill,

It might be a good idea to append ""and have sufficient memory"" to the warning message at the driver. Would you mind submitting a pull request for this?

- Patrick;;;","05/Aug/13 09:53;billz;I will submit  a pull request.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zombie workers,SPARK-847,12705358,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,markhamstra,,01/Aug/13 14:04,14/Nov/13 18:21,14/Jul/23 06:25,14/Nov/13 18:21,0.7.3,0.8.0,,,,,,,,,,,,Deploy,Spark Core,,,,0,,,,,,"spark.deploy.master.Worker contains 'workers', a HashSet[WorkerInfo].  The only place where a worker is ever removed from that set is within Worker.addWorker.  Within addWorker, DEAD workers are only removed from the set if they were using both the same host address and port as the new worker being added.  The result of this is that DEAD workers hang around forever if a new worker is never added or is added with a different host:port; and every WORKER_TIMEOUT interval, timeOutDeadWorkers again tries to remove them.

It looks like one or both of two things needs to happen at least some of the time (proper conditions?): 1) Worker.removeWorker(worker) should remove the worker from the workers HashSet in addition to removing its associated entries in idToWorker, actorToWorker and addressToWorker; 2) addWorker should remove a DEAD worker from the set of workers even when there isn't an exact host:port match.

Worker.removeWorker being called for zombies each WORKER_TIMEOUT is at least one of the reasons for core counts going negative: SPARK-845
",,ilikerps,markhamstra,mbautin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383278,,,Thu Nov 14 18:21:48 UTC 2013,,,,,,,,,,"0|i1u15b:",383546,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/13 15:07;mbautin;It looks like worker ports are random in practice, but in theory there could be more than one worker on the same node, which would probably make removing dead worker(s) based only on the hostname infeasible. However, a timeout-based approach to cleaning up dead workers sounds reasonable.;;;","01/Aug/13 15:15;markhamstra;https://github.com/markhamstra/spark/commit/8e86fa5a11102e113559b7e200e594a05c9d16ef
;;;","14/Nov/13 18:21;ilikerps;Just marking as fixed -- thanks Mark!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set `spark.job.annotation` and display it in the web UI.,SPARK-846,12705056,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,01/Aug/13 13:31,10/Aug/13 16:17,14/Jul/23 06:25,10/Aug/13 16:17,,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,,,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383385,,,2013-08-01 13:31:15.0,,,,,,,,,,"0|i1u1t3:",383653,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing an executor can result in a negative number of cores used ,SPARK-845,12704837,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,markhamstra,,01/Aug/13 11:11,14/Nov/13 18:22,14/Jul/23 06:25,14/Nov/13 18:22,0.7.3,0.8.0,,,,,,,,,,,,Deploy,Spark Core,,,,0,,,,,,"Under some circumstances, an executor dying or being removed will result in a negative number of cores granted to and reported as used by an application.  Still working on isolating the code paths that produce this result, but one know way to reproduce the problem is to kill a running executor with SIGKILL (kill -9).  If the executor is killed with SIGTERM, then the correct number of cores are removed from running applications; but SIGKILL seems to result in spark.deploy.master.ApplicationInfo.removeExecutor being called multiple times.

A proposed fix is available at https://github.com/clearstorydata/spark/commit/96f5e70fdbf21e188f4fc76a30957cc78302037f",,ilikerps,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383277,,,Thu Nov 14 18:22:39 UTC 2013,,,,,,,,,,"0|i1u153:",383545,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/13 18:22;ilikerps;Just marking as fixed -- thanks Mark!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Occasional hang on shuffle fetches in master branch,SPARK-844,12705046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,matei,,31/Jul/13 22:59,17/May/20 18:30,14/Jul/23 06:25,10/Aug/13 16:17,0.8.0,,,,,,,,0.8.0,,,,,Shuffle,Spark Core,,,,0,,,,,,"In running some iterative jobs with lots of shuffles, I've occasionally seen a few tasks hung with this kind of stack trace:

{noformat}

""pool-5-thread-1"" prio=10 tid=0x00007f6530049000 nid=0x1217 waiting on condition [0x00007f65d7cfa000]
   java.lang.Thread.State: WAITING (parking)
  at sun.misc.Unsafe.park(Native Method)
  - parking to wait for  <0x00007f6d17a1e298> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
  at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
  at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
  at spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.next(BlockFetcherIterator.scala:246)
  at spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.next(BlockFetcherIterator.scala:71)
  at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440)
  at spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26)
  at scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:457)
  at scala.collection.Iterator$class.foreach(Iterator.scala:772)
  at scala.collection.Iterator$$anon$22.foreach(Iterator.scala:451)
  at spark.Aggregator.combineValuesByKey(Aggregator.scala:37)
  at spark.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98)
  at spark.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98)
  at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:141)
  at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:138)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
  at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)
  at spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:138)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.FlatMappedValuesRDD.compute(PairRDDFunctions.scala:768)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:141)
  at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:138)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
  at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)
  at spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:138)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.FlatMappedValuesRDD.compute(PairRDDFunctions.scala:768)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:32)
  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)
  at spark.RDD.iterator(RDD.scala:241)
  at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:161)
  at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:93)
  at spark.executor.Executor$TaskRunner.run(Executor.scala:129)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:724)
{noformat}",,haoyuan,matei,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/13 13:32;haoyuan;jstack;https://issues.apache.org/jira/secure/attachment/12637644/jstack","05/Aug/13 13:32;haoyuan;stderr;https://issues.apache.org/jira/secure/attachment/12637645/stderr",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383384,,,Mon Aug 05 13:32:25 UTC 2013,,,,,,,,,,"0|i1u1sv:",383652,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/13 19:31;patrick;Hey [~matei] did you look at the logs at all for these nodes? It would be helpful to know whether there was any log output.;;;","05/Aug/13 13:32;haoyuan;Attach a hung machine's stderr and jstack.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven assembly is including examples libs and dependencies,SPARK-842,12704749,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,cos,,31/Jul/13 14:32,10/Oct/13 18:05,14/Jul/23 06:25,10/Oct/13 18:05,0.7.3,,,,,,,,0.8.0,,,,,Build,,,,,0,,,,,,"According to this [email exchange|http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201307.mbox/%3C62DB7E7A-0547-4090-BB9A-0182829A0D19%40gmail.com%3E] 

final assembly has to include ""...libraries that users' client programs need to run. These are core, repl (needed if they use the shell), and likely bagel and streaming and mllib,""

Hence, current Maven assembly needs to be fixed accordinly to exclude examples/.  This fix will also affect BIGTOP-715.",,cos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383308,,,2013-07-31 14:32:51.0,,,,,,,,,,"0|i1u1bz:",383576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use a smaller job UI port than 33000 by default,SPARK-841,12705024,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,matei,matei,,31/Jul/13 11:04,01/Sep/13 15:33,14/Jul/23 06:25,01/Sep/13 15:33,0.8.0,,,,,,,,0.8.0,,,,,Web UI,,,,,0,Starter,,,,,"33000 is in the ephemeral port range so it makes it more likely that your job sometimes binds to a higher port than expected.

Note that this change should also be reflected in the EC2 scripts, as they open port 33000-33010 currently to let users view the job UI.",,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383353,,,Sun Sep 01 15:33:14 UTC 2013,,,,,,,,,,"0|i1u1lz:",383621,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/13 15:33;joshrosen;Fixed in https://github.com/mesos/spark/commit/498a26189b197bdaf4be47e6a8baca7b97fe9064 and https://github.com/mesos/spark/commit/793a722f8e14552b8d36f46cca39d336dc2df9dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail.,SPARK-840,12705019,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,benh,benh,,31/Jul/13 10:45,31/Jul/13 16:40,14/Jul/23 06:25,31/Jul/13 16:40,,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,"Even though the distribution has everything it needs invoking `spark-shell` now errors with:

SCALA_HOME is not set and scala is not in PATH

It looks like the offending commit is f4d514810e6fd9f42868ebb9a89390c62c3b42e1.",,benh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383425,,,Wed Jul 31 16:32:15 UTC 2013,,,,,,,,,,"0|i1u21z:",383693,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/13 12:52;benh;Add a fix for this into the pull request at https://github.com/mesos/spark/pull/749. ;;;","31/Jul/13 16:32;benh;This can be marked as resolved, it was included in https://github.com/mesos/spark/pull/749.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in how failed executors are removed by ID from standalone cluster,SPARK-839,12705010,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,markhamstra,,31/Jul/13 09:54,09/Feb/15 02:51,14/Jul/23 06:25,09/Feb/15 02:51,0.7.3,0.8.0,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"ClearStory data reported the following issue, where some hashmaps are indexed by executorId and some by appId/executorId, and we use the wrong string to search for an executor: https://github.com/clearstorydata/spark/pull/9. This affects FT on the standalone mode.",,Grace Huang,markhamstra,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383372,,,Mon Feb 09 02:51:40 UTC 2015,,,,,,,,,,"0|i1u1q7:",383640,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/13 00:29;markhamstra;Related user list report: https://groups.google.com/forum/?fromgroups=#!topic/spark-users/DXIOjT_DP0Y;;;","09/Feb/15 02:51;markhamstra;Fixed long ago.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ResultTask's serialization forget about handling ""generation"" field, while ShuffleMapTask does",SPARK-837,12704991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,andyyehoo,,30/Jul/13 07:29,07/Dec/13 14:37,14/Jul/23 06:25,09/Sep/13 18:03,0.7.3,0.8.0,,,,,,,0.8.0,,,,,Spark Core,,,,,0,,,,,,"In ResultTask's serialization relative method: writeExternal and readExternal, they didn't do anything to generation. 

But in ShuffleMapTask's method, writeExternal and readExternal, they do something like ""partition = in.readInt()"" and "" out.writeLong(generation)"" to them. 

As we know ResultTask will be used after ShuffleMapTask, if right after ShuffleMapTask finish and the work failed for some reason, It will be recomputed, with a ""generation"" bigger than -1. The ResultTask can't get the right data again with default generation, that it will ask DAGScheduler to recompter ShuffleMapTask again. This will last until the whole job crash.


",,andyyehoo,Grace Huang,matei,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-836,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383248,,,Mon Sep 09 18:03:19 UTC 2013,,,,,,,,,,"0|i1u0yn:",383516,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/13 12:10;matei;This does seem like a bug -- thanks for reporting it! Going to bump it up in priority.;;;","09/Sep/13 18:03;patrick;This is now fixed as far as I can tell because the epoch is serialized with the ResultTask.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskMetrics should report compressed (not uncompressed) shuffle read bytes.,SPARK-834,12704984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,29/Jul/13 16:07,11/Aug/13 19:48,14/Jul/23 06:25,11/Aug/13 19:48,0.8.0,,,,,,,,0.8.0,,,,,Web UI,,,,,0,,,,,,This should be consistent with the shuffle write bytes.,,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383382,,,2013-07-29 16:07:01.0,,,,,,,,,,"0|i1u1sf:",383650,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Web UI's ""Application Detail UI"" link may use internal EC2 hostname",SPARK-833,12704797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,29/Jul/13 10:54,30/Mar/14 05:59,14/Jul/23 06:25,30/Mar/14 05:59,0.8.0,,,,,,,,1.0.0,,,,,Web UI,,,,,0,,,,,,"In the new (0.8) web UI, the ""Application Detail UI"" link on the Application Info page is derived from Utils.getLocalHostName().  When running on EC2, this link will break if the local hostname isn't the public EC2 hostname.

In SPARK-613, we fixed a similar problem by introducing a masterPublicAddress variable that's populated from SPARK_PUBLIC_DNS if it's set.  We should probably do something similar here.",,ilikerps,joshrosen,matei,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382860,,,Sun Mar 30 05:58:57 UTC 2014,,,,,,,,,,"0|i1tykf:",383128,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/13 16:00;joshrosen;This is still broken; some users ran across it during the AMP Camp 3 exercises.;;;","02/Sep/13 12:15;matei;The code is still actually in SparkUI to use SPARK_PUBLIC_DNS; all that remains is to set it. I've sent a PR at https://github.com/mesos/spark-ec2/pull/18.;;;","25/Sep/13 22:04;joshrosen;This is still a problem, even in the {{0.8.0-incubating}} release.  When I view the Master web UI, the application name link in the ""Running Applications"" row still links to an {{ip-*.ec2.internal}} address.  Similarly, the link labeled ""Application Detail UI"" on the Master's application details page uses these internal addresses.

I don't think that SPARK_PUBLIC_DNS is the problem here, since this is only happening with links added for the new per-application UI.  My guess is that those links are using a different mechanism to get the master's hostname.;;;","25/Sep/13 22:31;joshrosen;Actually, the problem is a bit more complicated.  I'm launching a PySpark session through IPython notebook, and it looks like the application WebUI is actually binding to the internal address:

{code}
13/09/26 04:54:15 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
13/09/26 04:54:15 INFO ui.SparkUI: Started Spark Web UI at http://ip-10-182-188-66.ec2.internal:4040
{code}

This is because the UI binds to the either SPARK_PUBLIC_DNS or the local hostname:

{code}
val host = Option(System.getenv(""SPARK_PUBLIC_DNS"")).getOrElse(Utils.localHostName())
{code}

One solution would be to have the web UI listen on all interfaces but generate links using SPARK_PUBLIC_DNS.  Even if user applications (like my IPython notebook) launch without setting SPARK_PUBLIC_DNS, we can rely on the Spark master having it set, so the master web UI will still be able to generate correct links to the application UIs.  However, this assumes that both web UIs are running on the same machine.;;;","27/Sep/13 14:46;joshrosen;This problem potentially affects all Spark driver programs, not just PySpark.  For PySpark, I probably can fix this by setting SPARK_PUBLIC_DNS in the {{spark-class}} scripts, and we could do something similar in the {{run-example}} scripts.  For arbitrary user programs, such as drivers launched through sbt, this wouldn't work.  Maybe the best solution is to just export SPARK_PUBLIC_DNS in the {{.bash_profile}} script.;;;","04/Oct/13 22:01;joshrosen;This should (hopefully finally!) be fixed by https://github.com/mesos/spark-ec2/pull/23.;;;","08/Dec/13 17:25;ilikerps;I am seeing this issue again. The ""Application Detail UI"" link is pointing to an EC2 internal. Other links are working, except for the ""Name"" column in the Master ""Running Applications"" table. Will attach screenshot.;;;","08/Dec/13 17:26;ilikerps;Attached an image to show this bug is still at large. Note that I am hovering over the Application Detail UI link and the URL displayed in the status bar is an internal ec2 IP.;;;","30/Mar/14 05:58;pwendell;I don't think there is a general way to fix this other than setting SPARK_PUBLIC_DNS in provisioning frameworks such as the spark ec2 scripts and the amp camp scripts. AFAIK those have all been updated to do this correctly, so I'm going to close this for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,
PySpark should set worker PYTHONPATH from SPARK_HOME instead of inheriting it from the master,SPARK-832,12704938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,28/Jul/13 17:52,06/Aug/13 23:21,14/Jul/23 06:25,06/Aug/13 23:21,0.7.0,0.7.1,0.7.2,0.7.3,,,,,0.8.0,,,,,PySpark,,,,,0,,,,,,"In current versions of PySpark, the worker Python processes inherit the master's PYTHONPATH environment variable.  This can lead to ImportErrors when running the PySpark worker processes if the master and workers use different SPARK_HOME paths.  Instead, the workers should append SPARK_HOME/python/pyspark to their own PYTHONPATHs.

To support customization of the PYTHONPATH on the workers (e.g. to add a NFS folder containing shared libraries), users would still be able to set a custom PYTHONPATH in spark-env.sh.",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383395,,,2013-07-28 17:52:52.0,,,,,,,,,,"0|i1u1vb:",383663,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move certain classes into more appropriate packages,SPARK-831,12704444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,rxin,,26/Jul/13 17:07,15/Sep/13 23:07,14/Jul/23 06:25,15/Sep/13 23:07,,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,"We will need to move the package name in Spark to org.apache.spark. We should take this opportunity to rename some classes and move them into more appropriate packages.

Let's use this ticket to track what needs to move / have been moved.

{code}
RDD -> rdd.RDD
PairRDDFunctions -> rdd.PairRDDFunctions
MappedValuesRDD -> rdd.MappedValuesRDD
FlatMappedValuesRDD -> rdd.FlatMappedValuesRDD
OrderedRDDFunctions -> rdd.OrderedRDDFunctions
DoubleRDDFunctions -> rdd.DoubleRDDFunctions

KryoSerializer -> serializer.KryoSerializer
JavaSerializer -> serializer.javaSerializer

SizeEstimator -> util.SizeEstimator

{code}

",,matei,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383337,,,Sun Sep 01 00:46:02 UTC 2013,,,,,,,,,,"0|i1u1if:",383605,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/13 00:46;matei;Added these onto https://github.com/mesos/spark/pull/882 now. I also moved Utils and ClosureCleaner to spark.util.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scheduler shouldn't hang if a task contains unserializable objects in its closure,SPARK-829,12705073,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,rxin,,23/Jul/13 14:00,05/Aug/13 16:31,14/Jul/23 06:25,05/Aug/13 16:31,,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,"Try

{code}
val a = new Object
sc.parallelize(1 to 10, 2).map(x => a).count
{code}

The following exception is uncaught:

{code}
java.io.NotSerializableException: java.lang.Object
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:331)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:28)
	at spark.scheduler.ResultTask$.serializeInfo(ResultTask.scala:43)
	at spark.scheduler.ResultTask.writeExternal(ResultTask.scala:115)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1442)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1411)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:331)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:28)
	at spark.JavaSerializerInstance.serialize(JavaSerializer.scala:48)
	at spark.scheduler.Task$.serializeWithDependencies(Task.scala:78)
	at spark.scheduler.local.LocalTaskSetManager.slaveOffer(LocalTaskSetManager.scala:115)
	at spark.scheduler.local.LocalScheduler$$anonfun$resourceOffer$2.apply(LocalScheduler.scala:134)
	at spark.scheduler.local.LocalScheduler$$anonfun$resourceOffer$2.apply(LocalScheduler.scala:131)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at spark.scheduler.local.LocalScheduler.resourceOffer(LocalScheduler.scala:131)
	at spark.scheduler.local.LocalActor$$anonfun$receive$1.apply(LocalScheduler.scala:46)
	at spark.scheduler.local.LocalActor$$anonfun$receive$1.apply(LocalScheduler.scala:44)
	at akka.actor.Actor$class.apply(Actor.scala:318)
	at spark.scheduler.local.LocalActor.apply(LocalScheduler.scala:43)
	at akka.actor.ActorCell.invoke(ActorCell.scala:626)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)
	at akka.dispatch.Mailbox.run(Mailbox.scala:179)
	at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)
	at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
	at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
	at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
	at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
{code}",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383418,,,2013-07-23 14:00:49.0,,,,,,,,,,"0|i1u20f:",383686,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DoubleRDDFunctions.sampleStdev() actually computes regular stdev(),SPARK-825,12705038,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,,22/Jul/13 13:09,30/Mar/14 23:33,14/Jul/23 06:25,22/Jul/13 16:09,0.6.0,0.6.1,0.6.2,0.7.0,0.7.1,0.7.2,0.7.3,0.8.0,0.7.3,0.8.0,,,,Spark Core,,,,,0,,,,,,"Due to a typo, DoubleRDDFunctions.sampleStdev() returns the regular, non-sample stdev():

{code}
  /** 
   * Compute the sample standard deviation of this RDD's elements (which corrects for bias in
   * estimating the standard deviation by dividing by N-1 instead of N).
   */
  def sampleStdev(): Double = stats().stdev
{code}",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383441,,,2013-07-22 13:09:12.0,,,,,,,,,,"0|i1u25j:",383709,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.default.parallelism's default is inconsistent across scheduler backends,SPARK-823,12704709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ilganeli,joshrosen,,19/Jul/13 09:24,17/May/20 17:48,14/Jul/23 06:25,09/Feb/15 16:31,0.7.3,0.8.0,0.9.1,,,,,,,,,,,Documentation,PySpark,Scheduler,Spark Core,,1,,,,,,"The [0.7.3 configuration guide|http://spark-project.org/docs/latest/configuration.html] says that {{spark.default.parallelism}}'s default is 8, but the default is actually max(totalCoreCount, 2) for the standalone scheduler backend, 8 for the Mesos scheduler, and {{threads}} for the local scheduler:

https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/cluster/StandaloneSchedulerBackend.scala#L157
https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/mesos/MesosSchedulerBackend.scala#L317
https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/local/LocalScheduler.scala#L150

Should this be clarified in the documentation?  Should the Mesos scheduler backend's default be revised?",,ash211,dcarroll@cloudera.com,ilganeli,joshrosen,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383443,,,Mon Feb 09 16:07:11 UTC 2015,,,,,,,,,,"0|i1u25z:",383711,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 18:27;dcarroll@cloudera.com;Yes, please clarify the documentation, I just ran into this.  the Configuration guide (http://spark.apache.org/docs/latest/configuration.html) says the default is 8.

In testing this on Standalone Spark, there actually is no default value for the variable:
>sc.getConf.contains(""spark.default.parallelism"")
>res1: Boolean = false

It looks like if the variable is not set, then the default behavior is decided in code, e.g. Partitioner.scala:
{code}
    if (rdd.context.conf.contains(""spark.default.parallelism"")) {
      new HashPartitioner(rdd.context.defaultParallelism)
    } else {
      new HashPartitioner(bySize.head.partitions.size)
    }
{code};;;","30/Apr/14 19:08;dcarroll@cloudera.com;Okay, this is definitely more than a documentation bug, because PySpark and Scala work differently if spark.default.parallelism isn't set by the user.  I'm testing using wordcount.

Pyspark: reduceByKey will use the value of sc.defaultParallelism.  That value is set to the number of threads when running locally.  On my Spark Standalone ""cluster"" which has a single node with a single core, the value is 2.  If I set spark.default.parallelism, it will set sc.defaultParallelism to that value and use that.

Scala: reduceByKey will use the number of partitions in my file/map stage and ignore the value of sc.defaultParallelism.  sc.defaultParallism is set by the same logic as pyspark (number of threads for local, 2 for my microcluster), it is just ignored.

I'm not sure which approach is correct.  Scala works as described here: http://spark.apache.org/docs/latest/tuning.html
{quote}
Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to SparkContext.textFile, etc), and for distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest parent RDD’s number of partitions. You can pass the level of parallelism as a second argument (see the spark.PairRDDFunctions documentation), or set the config property spark.default.parallelism to change the default. In general, we recommend 2-3 tasks per CPU core in your cluster.
{quote}

;;;","09/Feb/15 16:07;ilganeli;Hi [~joshrosen] I believe the documentation is up to date and I reviewed all usages of spark.default.parallelism and found no inconsistencies with the documentation. The only thing that is un-documented with regards to the usage of spark.default.parallelism is how it's used within the Partitioner class in both Spark and Python. If defined, the default number of partitions created is equal to spark.default.parallelism - otherwise, it's the local number of partitions. I think this issue can be closed - I don't think that particular case needs to be publicly documented (it's clearly evident in the code what is going on). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
netty: ChannelInboundByteHandlerAdapter no longer exist in 4.0.3.Final,SPARK-819,12705002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tgraves,,18/Jul/13 13:07,17/May/20 18:30,14/Jul/23 06:25,08/Nov/14 09:52,0.8.0,,,,,,,,,,,,,Shuffle,Spark Core,,,,0,,,,,,"It appears the netty shuffle code uses netty version 4.0.0.Beta2, which by the tag was in beta. They now have 4.0.2.Final which doesn't include the api ChannelInboundByteHandlerAdapter which is used by the FileClientHandler. We should move to use a stable api.  It looks like it was replaced with ChannelInboundHandlerAdapter.",,ngbinh,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383224,,,Sat Nov 08 09:52:17 UTC 2014,,,,,,,,,,"0|i1u0tb:",383492,,,,,,,,,,,,,,,,,,,,,,,"31/Dec/13 17:37;ngbinh;Should be fixed in https://github.com/apache/incubator-spark/pull/238;;;","08/Nov/14 09:52;srowen;This must have been fixed along the way, as the class ChannelInboundByteHandlerAdapter  is not used in the code now, nor is that version of Netty.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark's parallelize() should batch objects after partitioning (instead of before),SPARK-815,12704541,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,joshrosen,,15/Jul/13 19:09,30/Mar/14 23:33,14/Jul/23 06:25,28/Jul/13 23:56,0.7.0,0.7.1,0.7.2,0.7.3,,,,,0.7.3,0.8.0,,,,PySpark,,,,,1,,,,,,"PySpark uses batching when serializing and deserializing Python objects.  By default, it serializes objects in groups of 1024.

The current batching code causes SparkContext.parallelize() to behave counterintuitively when parallelizing small datasets.  The current code batches the objects, then parallelizes the batches, so calls to parallelize() with small inputs will be unaffected by the number of partitions:

{code}
>>> rdd = sc.parallelize([1, 2, 3, 4], 2)
>>> rdd.glom().collect()
[[], [1, 2, 3, 4]]
{code}

Instead, parallelize() should first partition the elements and then batch them:

{code}
>>> rdd = sc.parallelize([1, 2, 3, 4], 2)
>>> rdd.glom().collect()
[[1, 2], [3, 4]]
{code}

Maybe parallelize() should accept an option to control the batch size (right now, it can only be set when creating the SparkContext).",,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383431,,,Mon Jul 29 00:10:23 UTC 2013,,,,,,,,,,"0|i1u23b:",383699,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/13 23:56;matei;I've fixed this here: https://github.com/mesos/spark/commit/feba7ee540fca28872957120e5e39b9e36466953, though the solution is not completely perfect because it requires materializing the list if you pass a generator. On the other hand, that's what the Scala/Java parallelize does as well.

Maybe a slightly better solution would be to materialize the first context.batchSize * numSplits elements from the generator, write those out with even splitting if you've reached the end of the generator, or write them out as full batches otherwise. But I'm not sure we want to go to this level of complexity now.

A better thing to add by the way would be smart splitting support for xrange, similar to that of Range in Scala.;;;","29/Jul/13 00:03;joshrosen;Funny timing: I wrote my own fix for this tonight, but I like your fix better: https://github.com/JoshRosen/spark/compare/spark-815.  

In my branch, I cleaned up some old test code that was setting the batch size to 2 to work around this issue.  We should probably clean that up in master now, too.;;;","29/Jul/13 00:07;matei;I tried the tests and they all work with this, but it could be nice to update them if you'd like.;;;","29/Jul/13 00:10;matei;Sorry for not giving a heads-up BTW, I just fixed a bunch of small issues since I was coming back from this conference: http://www.pydata.org/bos2013/ and had some time on the plane. One of the other annoying things I've fixed is that first() and take() now stop after returning the first few elements, instead of computing the rest of the partition anyway, which makes them quite a bit faster.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Result stages should be named after the action that invoked them,SPARK-814,12704478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,matei,matei,,15/Jul/13 09:12,16/Jul/13 11:02,14/Jul/23 06:25,16/Jul/13 11:02,,,,,,,,,0.8.0,,,,,Spark Core,,,,,0,,,,,,"Right now it looks like jobs (as printed in the log) are named after this action, but result stages aren't. E.g. if you do sc.parallelize(1 to 10).count() there will be a stage called ""parallelize at <console>:1"" and a job called ""count at <console>:1"".",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383446,,,2013-07-15 09:12:03.0,,,,,,,,,,"0|i1u26n:",383714,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Poor locality in master due to ClusterScheduler changes,SPARK-813,12704841,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,,14/Jul/13 21:53,09/Sep/13 16:19,14/Jul/23 06:25,09/Sep/13 16:19,0.8.0,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"The ClusterScheduler in master attempts to launch non-local tasks on a node if it has finished launching all local tasks for it, but unfortunately this causes it to miss the chance to assign those tasks to other nodes later. We should switch back to a delay scheduling strategy as before, with possibly two levels (process-local and then node- or rack-local depending on configuration), and a way to bypass process-local if it clearly won't be met for a stage.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383342,,,2013-07-14 21:53:54.0,,,,,,,,,,"0|i1u1jj:",383610,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure thread safety of Spark UI,SPARK-810,12705454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,pwendell,,13/Jul/13 22:59,30/Mar/14 04:15,14/Jul/23 06:25,10/Aug/13 16:32,0.8.0,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,The datastructures in the web ui may not be thread safe if a stage is active and the details of the stage are being mutated. I need to check this and possibly add synchronization.,,kayousterhout,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382813,,,2013-07-13 22:59:11.0,,,,,,,,,,"0|i1ty9z:",383081,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Time columns in web UI tables don't sort properly,SPARK-801,12705423,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,matei,,13/Jul/13 15:58,06/Aug/13 23:03,14/Jul/23 06:25,06/Aug/13 23:03,,,,,,,,,,,,,,Web UI,,,,,0,,,,,,"They seem to be sorted by the string value in them instead of the number of seconds, milliseconds, etc. It's possible to add a sort key in the HTML that the table sorter plugin will recognize and sort by.",,matei,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383400,,,Tue Aug 06 23:03:14 UTC 2013,,,,,,,,,,"0|i1u1wf:",383668,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/13 18:17;patrick;Hey [~matei] - do you know which view and column was causing trouble? We use custom sort keys now in many places but maybe we missed one.;;;","06/Aug/13 23:03;matei;Looks like this was actually fine, so closing the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Quickstart Docs to Make Full Deployment More Clear,SPARK-800,12705112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,,11/Jul/13 22:55,23/Apr/14 02:18,14/Jul/23 06:25,23/Apr/14 02:18,,,,,,,,,1.0.0,,,,,,,,,,0,,,,,,"Some things are not super clear to people that should be in the quickstart standalone job section.

1. Explain that people need to package their dependencies either by making an uber jar (which is added to the SC) or by copying to each slave node if they build an actual project. This isn't clear since there are no dependencies in the quickstart itself. Warn them about issues creating an uber-jar with akka.

2. Explain that people need to set the required resources in an env variable before launching (or whatever new way Matei added to do this). ",,ianoc,joshrosen,patrick,pwendell,romain,seanm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383359,,,Wed Apr 23 02:18:29 UTC 2014,,,,,,,,,,"0|i1u1nb:",383627,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/13 13:07;patrick;Another thing we should include here is the ugliness of dealing with Akka configurations inside of an uber jar. This is something [~joshrosen] ran into inside of the Spark build and I just ran into today. I independently arrived at the same fix Josh did.

https://github.com/mesos/spark/pull/158

;;;","03/Aug/13 21:13;ianoc;Probably not the most common/best way, but since asked for ways:

I mostly run on EMR clusters of mostly spot instances few hundred XL nodes normally,

I use https://github.com/ianoc/SparkEMRBootstrap to build a custom bootstrap image with any extra global dependencies if they are large/common across work bundled into the RPM via the installer script.

Then build assembly jars with projects like https://github.com/ianoc/ExampleSparkProject (excludes spark and scala from assembly),(does some stuff to probe the mesos cluster to figure out number of nodes at launch to set default parallelism) .

And just used a simple script like https://github.com/ianoc/SparkEMRBootstrap/blob/master/sample_remote_deploy.sh to launch the job remotely inside a screen session on the cluster.;;;","04/Aug/13 21:24;seanm;We use a simple bash script (2 lines of code) that finds and passes in the dependencies.  It works great for us.;;;","23/Apr/14 02:18;pwendell;This has been fixed with the recent changes to the docs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML failures if libfortran is not installed,SPARK-797,12705005,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,pwendell,,08/Jul/13 21:00,23/Jan/15 12:47,14/Jul/23 06:25,23/Jan/15 12:47,0.8.0,,,,,,,,,,,,,,,,,,0,,,,,,"When running tests with the new ML package, I get:

{quote}
[info]   spark.SparkException: Job failed: Task 5.0:0 failed more than 4 times; aborting job java.lang.UnsatisfiedLinkError: org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V
{quote}

Shivaram mentioned I should install a Fortran package:

{quote}
sudo apt-get install libgfortran3
{quote}

And it worked after that. We should figure out exactly what the dependencies are here list in the build instructions that they exist. Also, if these are runtime dependencies we should mention them clearly in the main Spark docs.",,andyk,matei,patrick,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382816,,,Fri Jan 23 12:47:11 UTC 2015,,,,,,,,,,"0|i1tyan:",383084,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/13 16:30;patrick;Hey [~shivaram] this would be something good to mention if you are going to add an MLLib doc.;;;","17/Aug/13 16:41;andyk;Yeah, I also just saw this now when I tried building/testing Spark master on the Ubuntu Jenkins node (it was set to only build on the CentOS boxes before) - see https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT-Hadoop2/223/console

It's a little bit of a shame to require this extra dependency, no?;;;","17/Aug/13 16:42;andyk;BTW, here is the snippet of output including the point of failure:

...

[info] RidgeRegressionSuite:
-- org.jblas ERROR Couldn't load copied link file: java.lang.UnsatisfiedLinkError: /tmp/jblas6680700493603641651libjblas_arch_flavor.so: libgfortran.so.3: cannot open shared object file: No such file or directory.

On Linux 64bit, you need additional support libraries.
You need to install libgfortran3.

For example for debian or Ubuntu, type ""sudo apt-get install libgfortran3""

For more information, see https://github.com/mikiobraun/jblas/wiki/Missing-Libraries
[info] - multi-collinear variables *** FAILED ***
[info]   spark.SparkException: Job failed: Task 5.0:0 failed more than 4 times; aborting job java.lang.UnsatisfiedLinkError: org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V
[info]   at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:735)
[info]   at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:733)
[info]   at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)
[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
[info]   at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:733)
[info]   at spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:363)
[info]   at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:425)
[info]   at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:135)
[info]   ...;;;","18/Aug/13 12:46;shivaram;It is a bit of a shame -- but short of adding a layer of indirection over JBLAS I am not sure if there is an easy way to avoid this. FWIW, this shouldn't affect anybody not using ML Lib and the error message is pretty informative on what needs to be done. Any suggestions other than improving documentation and/or FAQs ?;;;","18/Aug/13 17:06;patrick;This is the kind of thing that would be good to list in a ""requirements"" or ""dependencies"" section. We don't really have that for Spark right now, but if we add MLBase documentation, it would be good to list it up-front at the very beginning of the MLBase introduction. ;;;","18/Aug/13 20:07;matei;Yeah, let's add it when we add a doc page on MLlib. I don't think it's worth switching matrix libraries -- we'd have to try linking to some native code anyway and JBLAS went to a lot of trouble to make this easy.;;;","18/Aug/13 20:09;matei;BTW, we may also want to make it ""fail early"" by trying to do a JBLAS operation in the driver and throwing a more meaningful exception if it fails. We can add a simple MLUtils.testJblas() call in the training objects.;;;","18/Aug/13 20:50;shivaram;There is a `loadLibrariesAndCheckErrors()` call in JBLAS that we can use. https://github.com/mikiobraun/jblas/blob/master/src/main/java/org/jblas/NativeBlas.java#L77

Do we want to do this in the driver ? Programs which don't use ML Lib should just work without the libgfortran, so adding it to just the MLLib objects might be better. ;;;","09/Sep/13 22:22;patrick;Okay this is at least discussed now in the docs. It would be better to do some detection of this, especially for the tests.;;;","23/Jan/15 12:47;srowen;I suggest this can be resolved. The dependency isn't going to be removed, and it's documented. I can't figure out where to put a check for these libs since not even every app that imports MLlib will end up touching jblas. A warning is a little nicer but the result is still that the program can't continue.;;;",,,,,,,,,,,,,,,,,,,,,,,
Jobs are always marked as SUCCEEDED even it's actually failed on Yarn.,SPARK-796,12704643,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,blackniuza,blackniuza,,08/Jul/13 20:07,06/Aug/13 23:13,14/Jul/23 06:25,06/Aug/13 23:13,0.8.0,,,,,,,,0.8.0,,,,,Deploy,,,,,0,,,,,,"When I submit a spark job that will throw an exception on yarn, the job is marked as SUCCEEDED at FinalStatus. This will mislead some users.",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383928,,,2013-07-08 20:07:51.0,,,,,,,,,,"0|i1u55r:",384196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Numerous tests failing in maven build,SPARK-795,12705047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,markhamstra,markhamstra,,08/Jul/13 11:16,07/Dec/13 14:23,14/Jul/23 06:25,07/Dec/13 14:23,0.8.0,,,,,,,,0.8.0,,,,,Build,,,,,0,,,,,,"After the merge of 1ffadb2d9e87169ccc406cd34dab6bd7beda70f1 pwendell/ui-updates, numerous tests are failing when run from maven (e.g. mvn -Phadoop2 test.)  First test to fail is in core PartitionSuite, and is typical of the other test failures.  Looks like a classpath issue...

Run starting. Expected test count is: 245
DiscoverySuite:
PartitioningSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: scala/util/Try$
  at spark.ui.JettyUtils$.connect$1(JettyUtils.scala:101)
  at spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:113)
  at spark.ui.SparkUI.bind(SparkUI.scala:33)
  at spark.SparkContext.<init>(SparkContext.scala:108)
  at spark.SharedSparkContext$class.beforeAll(SharedSparkContext.scala:14)
  at spark.PartitioningSuite.beforeAll(PartitioningSuite.scala:9)
  at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:150)
  at spark.PartitioningSuite.beforeAll(PartitioningSuite.scala:9)
  at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:211)
  at spark.PartitioningSuite.run(PartitioningSuite.scala:9)
  ...
  Cause: java.lang.ClassNotFoundException: scala.util.Try$
  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
  at spark.ui.JettyUtils$.connect$1(JettyUtils.scala:101)
  at spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:113)
  at spark.ui.SparkUI.bind(SparkUI.scala:33)
  ...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM .......................... SUCCESS [1.200s]
[INFO] Spark Project Core ................................ FAILURE [8.966s]
[INFO] Spark Project Bagel ............................... SKIPPED
[INFO] Spark Project Streaming ........................... SKIPPED
[INFO] Spark Project Examples ............................ SKIPPED
[INFO] Spark Project REPL ................................ SKIPPED
[INFO] Spark Project REPL binary packaging ............... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------",,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383251,,,Mon Jul 08 16:00:28 UTC 2013,,,,,,,,,,"0|i1u0zb:",383519,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/13 16:00;markhamstra;See https://github.com/mesos/spark/pull/688;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove sleep() in ClusterScheduler.stop,SPARK-794,12705170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,boyork,matei,,06/Jul/13 17:09,17/May/20 17:47,14/Jul/23 06:25,26/Feb/15 22:08,0.9.0,,,,,,,,1.2.2,1.3.0,,,,Scheduler,Spark Core,,,,0,,,,,,This temporary change made a while back slows down the unit tests quite a bit.,,aash,apachespark,boyork,joshrosen,matei,mubarak.seyed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383252,,,Thu Feb 26 22:08:29 UTC 2015,,,,,,,,,,"0|i1u0zj:",383520,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/14 10:12;aash;I don't see a {{ClusterScheduler}} class on master -- was that refactored to {{TaskSchedulerImpl}}?

There is still a {{Thread.sleep(1000)}} in that class's {{stop()}} though, which may be what this ticket refers to: https://github.com/apache/spark/blob/1df05a40ebf3493b0aff46d18c0f30d2d5256c7b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L399;;;","31/Dec/14 00:04;apachespark;User 'brennonyork' has created a pull request for this issue:
https://github.com/apache/spark/pull/3851;;;","31/Dec/14 22:24;joshrosen;It looks like this was added by [~mridulm@yahoo-inc.com] all the way back in 2013: https://github.com/apache/spark/commit/d90d2af1036e909f81cf77c85bfe589993c4f9f3#diff-4e188f32951dc989d97fa7577858bc7cR397;;;","04/Jan/15 20:42;joshrosen;I'm merged [~boyork]'s PR into master and am going to monitor the pull request builder for a bit to see whether this change introduced any flakiness.  If not, I'd consider backporting this change into other branches in order to speed up the tests.;;;","26/Jan/15 19:41;boyork;[~joshrosen] How is this PR holding up? I haven't seen any issues on the dev board. Think we can close this JIRA ticket? Trying to help prune the JIRA tree :);;;","23/Feb/15 22:41;boyork;[~srowen] [~joshrosen] bump on this. Would assume things are stable with the removal of the sleep method, but want to double check. Thinking we can close this ticket out.;;;","24/Feb/15 10:53;srowen;[~joshrosen] would it be OK if I tried my hand at backporting into 1.2? further?;;;","26/Feb/15 19:43;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4793;;;","26/Feb/15 22:08;srowen;Backported to 1.2;;;",,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler doesn't release ActiveJob inside idToActiveJob,SPARK-793,12704802,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,blackniuza,andyyehoo,,06/Jul/13 02:55,23/Jan/14 23:37,14/Jul/23 06:25,23/Jan/14 23:37,0.7.0,0.7.1,0.7.2,0.7.3,0.8.0,,,,0.8.0,,,,,Spark Core,,,,,0,,,,,,"In DAGScheduler, idToActiveJob is a HashMap to Store relationship between JobId and Active Job. But we found that there's only places that adding and reading ActiveJob from it, but never remove. 

1.   val idToActiveJob = new HashMap[Int, ActiveJob]
2.   idToActiveJob(runId) = job
3.   val properties = idToActiveJob(stage.priority).properties

This will cause a memory leak in Master, when we call collect() method for multipal iterations.  All ActiveJob will be kept in memory and can't not be return even with full GC, which will finally cause Master to die, no matter in Standalone cluster or Yarn cluster.",,andyyehoo,Grace Huang,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383193,,,Thu Jan 23 23:37:11 UTC 2014,,,,,,,,,,"0|i1u0mf:",383461,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/13 02:59;andyyehoo;Here are the log of our job. On iteration 5, App master on Yarn lost...

-------> Iteration 1 is running!!
194.375: [Full GC [PSYoungGen: 554284K->360219K(700544K)] [PSOldGen: 1527886K->1593407K(1768704K)] 2082171K->1953627K(2469248K) [PSPermGen: 47015K->47015K(55936K)], 6.5237410 secs] [Times: user=5.97 sys=0.55, real=6.52 secs] 
-----> Iteration 1 finished, loss is: 0.6931471805593069

381.753: [Full GC [PSYoungGen: 603580K->251971K(1054464K)] [PSOldGen: 3174220K->3352512K(3683264K)] 3777800K->3604483K(4737728K) [PSPermGen: 47161K->47161K(50880K)], 8.2145200 secs] [Times: user=7.65 sys=0.56, real=8.22 secs] 
-----> Iteration 2 finished, loss is: 0.6516189398401027

495.914: [Full GC [PSYoungGen: 547426K->348782K(1445440K)] [PSOldGen: 4412187K->4412351K(4831552K)] 4959614K->4761134K(6276992K) [PSPermGen: 47200K->47197K(49664K)], 13.8350190 secs] [Times: user=13.30 sys=0.51, real=13.83 secs] 
-----> Iteration 3 finished, loss is: 0.6219949536858732


-----> Iteration 4 finished, loss is: 0.5995576249445631
652.047: [Full GC [PSYoungGen: 1205541K->1053760K(1773120K)] [PSOldGen: 6138589K->6107303K(6558720K)] 7344130K->7161064K(8331840K) [PSPermGen: 47213K->47210K(48512K)], 20.3188240 secs] [Times: user=19.84 sys=0.45, real=20.31 secs] 

-------> Iteration 5 is running!!
1164.810: [Full GC [PSYoungGen: 600459K->597840K(1806080K)] [PSOldGen: 7233536K->7233536K(7233536K)] 7833995K->7831376K(9039616K) [PSPermGen: 47268K->47268K(47552K)], 15.7865770 secs] [Times: user=15.27 sys=0.48, real=15.79 secs] ;;;","23/Jan/14 23:37;joshrosen;This was fixed in https://github.com/mesos/spark/pull/681;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[pyspark] operator.getattr not serialized,SPARK-791,12704789,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,davies,jblomo,,04/Jul/13 16:27,29/Jul/14 08:18,14/Jul/23 06:25,29/Jul/14 08:18,0.7.2,0.9.0,1.0.0,,,,,,0.9.3,1.0.3,1.1.0,,,PySpark,,,,,0,,,,,,"Using operator.itemgetter as a function in map seems to confuse the serialization process in pyspark.  I'm using itemgetter to return tuples, which fails with a TypeError (details below).  Using an equivalent lambda function returns the correct result.

Use a test file:
{code:sh}
echo 1,1 > test.txt
{code}

Then try mapping it to a tuple:
{code:python}
import csv
sc.textFile(""test.txt"").mapPartitions(csv.reader).map(lambda l: (l[0],l[1])).first()
Out[7]: ('1', '1')
{code}

But this does not work when using operator.itemgetter:
{code:python}
import operator
sc.textFile(""test.txt"").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first()
# TypeError: list indices must be integers, not tuple
{code}

This is running with git master, commit 6d60fe571a405eb9306a2be1817901316a46f892
IPython 0.13.2 
java version ""1.7.0_25""
Scala code runner version 2.9.1 
Ubuntu 12.04

Full debug output:
{code:python}
In [9]: sc.textFile(""test.txt"").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first()
13/07/04 16:19:49 INFO storage.MemoryStore: ensureFreeSpace(33632) called with curMem=201792, maxMem=339585269
13/07/04 16:19:49 INFO storage.MemoryStore: Block broadcast_6 stored as values to memory (estimated size 32.8 KB, free 323.6 MB)
13/07/04 16:19:49 INFO mapred.FileInputFormat: Total input paths to process : 1
13/07/04 16:19:49 INFO spark.SparkContext: Starting job: takePartition at NativeMethodAccessorImpl.java:-2
13/07/04 16:19:49 INFO scheduler.DAGScheduler: Got job 4 (takePartition at NativeMethodAccessorImpl.java:-2) with 1 output partitions (allowLocal=true)
13/07/04 16:19:49 INFO scheduler.DAGScheduler: Final stage: Stage 4 (PythonRDD at NativeConstructorAccessorImpl.java:-2)
13/07/04 16:19:49 INFO scheduler.DAGScheduler: Parents of final stage: List()
13/07/04 16:19:49 INFO scheduler.DAGScheduler: Missing parents: List()
13/07/04 16:19:49 INFO scheduler.DAGScheduler: Computing the requested partition locally
13/07/04 16:19:49 INFO scheduler.DAGScheduler: Failed to run takePartition at NativeMethodAccessorImpl.java:-2
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-9-1fdb3e7a8ac7> in <module>()
----> 1 sc.textFile(""test.txt"").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first()

/home/jim/src/spark/python/pyspark/rdd.pyc in first(self)
    389         2
    390         """"""
--> 391         return self.take(1)[0]
    392 
    393     def saveAsTextFile(self, path):

/home/jim/src/spark/python/pyspark/rdd.pyc in take(self, num)
    372         items = []
    373         for partition in range(self._jrdd.splits().size()):
--> 374             iterator = self.ctx._takePartition(self._jrdd.rdd(), partition)
    375             # Each item in the iterator is a string, Python object, batch of
    376             # Python objects.  Regardless, it is sufficient to take `num`

/home/jim/src/spark/python/lib/py4j0.7.egg/py4j/java_gateway.pyc in __call__(self, *args)
    498         answer = self.gateway_client.send_command(command)
    499         return_value = get_return_value(answer, self.gateway_client,
--> 500                 self.target_id, self.name)
    501 
    502         for temp_arg in temp_args:

/home/jim/src/spark/python/lib/py4j0.7.egg/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:spark.api.python.PythonRDD.takePartition.
: spark.api.python.PythonException: Traceback (most recent call last):
  File ""/home/jim/src/spark/python/pyspark/worker.py"", line 53, in main
    for obj in func(split_index, iterator):
  File ""/home/jim/src/spark/python/pyspark/serializers.py"", line 24, in batched
    for item in iterator:
TypeError: list indices must be integers, not tuple

	at spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:117)
	at spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:139)
	at spark.api.python.PythonRDD.compute(PythonRDD.scala:82)
	at spark.RDD.computeOrReadCheckpoint(RDD.scala:232)
	at spark.RDD.iterator(RDD.scala:221)
	at spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:423)
	at spark.scheduler.DAGScheduler$$anon$2.run(DAGScheduler.scala:410)
{code}",,airhorns,davies,distobj,jblomo,joshrosen,nchammas,schumach,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1091,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383151,,,Tue Jul 29 01:10:37 UTC 2014,,,,,,,,,,"0|i1u0d3:",383419,,,,,,,,,,,,,1.0.2,,,,,,,,,,"28/Jul/13 21:00;joshrosen;I think this is a problem with how operator.itemgetter is serialized, because

{code}
sc.textFile(""test.txt"").mapPartitions(csv.reader).map(lambda x: operator.itemgetter(0, 1)(x)).first()
{code}

works for me, while

{code}
sc.textFile(""test.txt"").mapPartitions(csv.reader).map(operator.itemgetter(0, 1)).first()
{code}

gives the error that you saw.  Interestingly, the example works fine if I only specify a single index, e.g. operator.itemgetter(0).

To get a bit more insight into what's going on, I used pickletools.dis to log the pickled functions (specifically, I used pickletools.dis(pickletools.optimize(cloudpickle.dumps(func))) with the extra optimization added to remove unnecessary PUTs in order to make the output easier to read).  For 

{code}
sc.textFile(""test.txt"").mapPartitions(csv.reader).cache().map(operator.itemgetter(0, 1)).first()
{code}

the second map function's pickled form disassembles into

{code}
     0: \x80 PROTO      2
    2: (    MARK
    3: c        GLOBAL     'pyspark.cloudpickle _modules_to_main'
   41: q        BINPUT     0
   43: ]        EMPTY_LIST
   44: U        SHORT_BINSTRING 'pyspark.rdd'
   57: q        BINPUT     2
   59: a        APPEND
   60: \x85     TUPLE1
   61: R        REDUCE
   62: 1        POP_MARK   (MARK at 2)
   63: c    GLOBAL     'pyspark.cloudpickle _fill_function'
   99: q    BINPUT     4
  101: (    MARK
  102: c        GLOBAL     'pyspark.cloudpickle _make_skel_func'
  139: q        BINPUT     5
  141: c        GLOBAL     'new code'
  151: q        BINPUT     6
  153: (        MARK
  154: K            BININT1    2
  156: K            BININT1    2
  158: K            BININT1    4
  160: K            BININT1    19
  162: U            SHORT_BINSTRING 't\x00\x00\x88\x00\x00|\x00\x00|\x01\x00\x83\x02\x00\x88\x01\x00\x83\x02\x00S'
  186: N            NONE
  187: \x85         TUPLE1
  188: U            SHORT_BINSTRING 'batched'
  197: q            BINPUT     9
  199: \x85         TUPLE1
  200: U            SHORT_BINSTRING 'split'
  207: q            BINPUT     11
  209: U            SHORT_BINSTRING 'iterator'
  219: q            BINPUT     12
  221: \x86         TUPLE2
  222: U            SHORT_BINSTRING '/Users/joshrosen/Documents/spark/spark/python/pyspark/rdd.py'
  284: U            SHORT_BINSTRING 'batched_func'
  298: M            BININT2    725
  301: U            SHORT_BINSTRING '\x00\x01'
  305: U            SHORT_BINSTRING 'oldfunc'
  314: U            SHORT_BINSTRING 'batchSize'
  325: \x86         TUPLE2
  326: )            EMPTY_TUPLE
  327: t            TUPLE      (MARK at 153)
  328: R        REDUCE
  329: K        BININT1    2
  331: }        EMPTY_DICT
  332: q        BINPUT     22
  334: \x87     TUPLE3
  335: R        REDUCE
  336: }        EMPTY_DICT
  337: h        BINGET     9
  339: c        GLOBAL     'pyspark.serializers batched'
  368: s        SETITEM
  369: N        NONE
  370: ]        EMPTY_LIST
  371: (        MARK
  372: (            MARK
  373: h                BINGET     0
  375: ]                EMPTY_LIST
  376: h                BINGET     2
  378: a                APPEND
  379: \x85             TUPLE1
  380: R                REDUCE
  381: 1                POP_MARK   (MARK at 372)
  382: h            BINGET     4
  384: (            MARK
  385: h                BINGET     5
  387: h                BINGET     6
  389: (                MARK
  390: K                    BININT1    2
  392: K                    BININT1    2
  394: K                    BININT1    3
  396: K                    BININT1    19
  398: U                    SHORT_BINSTRING 't\x00\x00\x88\x00\x00|\x01\x00\x83\x02\x00S'
  413: N                    NONE
  414: \x85                 TUPLE1
  415: U                    SHORT_BINSTRING 'imap'
  421: q                    BINPUT     32
  423: \x85                 TUPLE1
  424: h                    BINGET     11
  426: h                    BINGET     12
  428: \x86                 TUPLE2
  429: U                    SHORT_BINSTRING '/Users/joshrosen/Documents/spark/spark/python/pyspark/rdd.py'
  491: U                    SHORT_BINSTRING 'func'
  497: K                    BININT1    87
  499: U                    SHORT_BINSTRING ''
  501: U                    SHORT_BINSTRING 'f'
  504: \x85                 TUPLE1
  505: )                    EMPTY_TUPLE
  506: t                    TUPLE      (MARK at 389)
  507: R                REDUCE
  508: K                BININT1    1
  510: h                BINGET     22
  512: \x87             TUPLE3
  513: R                REDUCE
  514: }                EMPTY_DICT
  515: h                BINGET     32
  517: c                GLOBAL     'itertools imap'
  533: s                SETITEM
  534: N                NONE
  535: ]                EMPTY_LIST
  536: c                GLOBAL     'operator itemgetter'
  557: K                BININT1    0
  559: K                BININT1    1
  561: \x86             TUPLE2
  562: \x85             TUPLE1
  563: R                REDUCE
  564: a                APPEND
  565: }                EMPTY_DICT
  566: t                TUPLE      (MARK at 384)
  567: R            REDUCE
  568: M            BININT2    1024
  571: e            APPENDS    (MARK at 371)
  572: }        EMPTY_DICT
  573: t        TUPLE      (MARK at 101)
  574: R    REDUCE
  575: .    STOP
{code}

It looks like something strange is happening to operator.itemgetter:

{code}
  536: c                GLOBAL     'operator itemgetter'
  557: K                BININT1    0
  559: K                BININT1    1
  561: \x86             TUPLE2
  562: \x85             TUPLE1
  563: R                REDUCE
{code}

It's supposed to be constructed using two arguments, 0 and 1, but instead it looks like it's being constructed with (0, 1).  To verify this:

{code}
import pickle
s = ""coperator\nitemgetter\nK\x00K\x01\x86\x85R.""
op = pickle.loads(s)
t = [1, 2, 2, 3]
print op(t)
{code}

gives ""TypeError: list indices must be integers, not tuple"".

If I leave off the extra TUPLE1 opcode, then this works as expected:

{code}
import pickle
s = ""coperator\nitemgetter\nK\x00K\x01\x86R.""
op = pickle.loads(s)
t = [1, 2, 2, 3]
print op(t)
{code}

gives (1, 2).

I'm guessing this is a problem in the CloudPickle library, but it might take me a while to find and fix it.;;;","27/Aug/13 14:31;schumach;Just an observation: the same error occurs with the cloudpickler from cloud-2.8.5 from 2013-08-21. If it's a problem with the library, maybe one should file a bug report there?;;;","23/Jan/14 16:56;joshrosen;Quick update: It looks like the Dill serialization library handles this case properly, but there are a couple of issues to work out before we can consider switching to it: https://mail-archives.apache.org/mod_mbox/spark-dev/201312.mbox/%3CCAOEPXP5hu-dhGnjQq=RYdT35G-eeEYPVopgqQdf2NFxRB7vA_g@mail.gmail.com%3E;;;","03/May/14 14:20;airhorns;Hey Josh, looks like that issue mentioned in that thread has been fixed in Dill: https://github.com/uqfoundation/dill/issues/18. Anecdotally playing around with Dill I've found it a bit more friendly than Cloudpickle and as you mentioned the actively-maintained status of it is really encouraging. Would you mind if I tried to rebase your port to Dill on top of current master and opened a PR?;;;","18/Jun/14 03:52;distobj;I began porting Pyspark to Python 3, but with my modest Python-fu, hit a wall at cloudpickle. Dill supports Python 3, so seems like a big win in that direction too.;;;","29/Jul/14 01:10;davies;This will be fixed by PR-1627[1]

[1] https://github.com/apache/spark/pull/1627;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationRemoved message shouldn't be sent to terminated application driver actor,SPARK-789,12704644,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liancheng,liancheng,,04/Jul/13 09:46,06/Aug/13 23:13,14/Jul/23 06:25,06/Aug/13 23:13,0.7.1,0.7.2,,,,,,,0.8.0,,,,,Deploy,,,,,0,,,,,,"Once received a Terminated message, the {{Master}} actor:

# Calls {{finishApplication}} if the terminated actor is an app
# The {{finishApplication}} method calls {{removeApplication(app, ApplicationState.FINISHED)}}.
#  Within {{removeApplication}}, an {{ApplicationRemoved}} message is sent to {{app.driver}}

But {{app.driver}} has already terminated, thus causes Akka report log errors complaining about {{RemoteClientError}} and {{RemoteClientWriteFailed}} etc..

Within {{removeApplication}}, when the {{state}} argument is {{ApplicationState.FINISHED}}, we should not send the {{ApplicationRemoved}} message, since {{app.driver}} is terminated.

A sample error log provided by Jason Dai is attached.",,alexf,liancheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/13 09:46;liancheng;spark-hadoop-spark.deploy.master.Master-1-cii-8l06-05.bj.intel.com.out;https://issues.apache.org/jira/secure/attachment/12637643/spark-hadoop-spark.deploy.master.Master-1-cii-8l06-05.bj.intel.com.out",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383398,,,Thu Jul 04 10:01:22 UTC 2013,,,,,,,,,,"0|i1u1vz:",383666,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/13 10:01;liancheng;Just sent a pull request to fix this issue: https://github.com/mesos/spark/pull/674;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClosureCleaner not invoked on most PairRDDFunctions,SPARK-785,12705395,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,matei,,23/Jun/13 16:21,17/Dec/14 20:21,14/Jul/23 06:25,17/Dec/14 20:21,,,,,,,,,0.9.3,1.0.3,1.1.2,1.2.1,1.3.0,Spark Core,,,,,0,,,,,,"It's pretty weird that we've missed this so far, but it seems to be the case. Unfortunately it may not be good to fix this in 0.7.3 because it could change behavior in unexpected ways; I haven't decided yet. But we should definitely do it for 0.8, and add tests.",,apachespark,ilikerps,joshrosen,matei,mubarak.seyed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383276,,,Wed Dec 17 20:21:32 UTC 2014,,,,,,,,,,"0|i1u14v:",383544,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/13 18:25;ilikerps;Is this still an open issue?;;;","06/Nov/14 07:04;matei;[~adav] it still seems to be, weirdly enough: for instance combineByKey doesn't do it. Unless I'm missing something.;;;","14/Dec/14 01:10;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3690;;;","16/Dec/14 00:10;joshrosen;I've merged https://github.com/apache/spark/pull/3690 to fix this in the maintenance branches and have tagged this for a 1.2.1 backport.;;;","17/Dec/14 20:21;joshrosen;I've merged this into {{branch-1.2}}, so it will be included in Spark 1.2.1.  Since this was the last backport, I'm marking this as Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"foldByKey does not clone the ""zero value"" for each key, leading to overwriting",SPARK-784,12705062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,,22/Jun/13 13:37,23/Jun/13 10:27,14/Jul/23 06:25,23/Jun/13 10:27,0.7.2,0.8.0,,,,,,,0.7.3,0.8.0,,,,Spark Core,,,,,0,,,,,,"This is not a problem with the normal fold() because each task is deserialized separately and gets a separate version of the ""zero value"". We probably need to do that manually same in foldByKey.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383463,,,Sun Jun 23 10:27:37 UTC 2013,,,,,,,,,,"0|i1u2af:",383731,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/13 10:27;matei;Fixed here: https://github.com/mesos/spark/commit/78ffe164b33c6b11a2e511442605acd2f795a1b5 and in branch-0.7.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple versions of ASM being put on classpath,SPARK-782,12704511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,matei,,20/Jun/13 08:07,04/Apr/14 20:51,14/Jul/23 06:25,09/Mar/14 13:18,,,,,,,,,0.9.1,1.0.0,,,,,,,,,0,,,,,,"Since the update to ASM 4, something is still pulling in ASM 3. We need to exclude it in SBT or just find a better way to manage lib_managed.",,glenn.murray,klmarkey,matei,sandy,santhoma,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382878,,,Fri Mar 28 02:53:42 UTC 2014,,,,,,,,,,"0|i1tyof:",383146,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/13 16:21;matei;This is now fixed in the SBT build but not Maven.;;;","23/Feb/14 23:52;sandy;It looks like Hadoop depends on asm 3.2.  I've seen this cause IncompatibleClassChangeErrors a couple times recently, depending on which order the jars are placed on the classpath (even when built with SBT).  Would it make sense to shade asm in Spark?;;;","05/Mar/14 20:44;sandy;https://github.com/apache/spark/pull/90;;;","11/Mar/14 13:09;klmarkey;Thank you!  Not following this issue earlier, and having to deal with multiple ASM conflicts (Hadoop, Jersey servlet, and more), and not being able to wait for shaded Maven packages (spark-core), we needed to shade other instances in other libraries, but given how Spark is deployed, it was impossible for us to shade Spark independency.  ;;;","24/Mar/14 16:49;tdas;The PR that fixes this https://github.com/apache/spark/pull/100/ ;;;","25/Mar/14 08:13;klmarkey;Thanks. I understand there's a fix and pull for it but my comment was to include it in. 0.9.1. It's lots handier if it's in Maven central than making a special build and pushing it into our Maven repository and pushing the fixes to the clusters. 

Thanks again. 

;;;","25/Mar/14 16:18;glenn.murray;I'd like to +1 Kevin's request to push this into 0.9.1.  This week I actually had a coding exercise for a job application that requested deployment of the exercise to AWS, and having it fail with 0.9.0 was painful.

Thanks,
Glenn;;;","25/Mar/14 17:39;tdas;The PR (https://github.com/apache/spark/pull/100/) that fixed this for master branch did the following:
1. Made Spark use existing shaded asm (Kryo's) and removed Spark's direct dependency to asm
2. Excluded ASM from all Spark dependencies so that asm is not Spark binary any more. 

For branch 0.9, I am afraid to do (2), because there is a remote possibility that completely remove asm from Spark binaries may fail something. For example, Hadoop has asm as dependency and removing asm completely may cause Hadoop to fail in some scenario. Hence, I am porting only (1). This would allow downstream projects to safely exclude asm from Spark, as Spark in itself does not depend on asm (only shaded asm).


;;;","25/Mar/14 17:47;tdas;I opened a PR with this https://github.com/apache/spark/pull/232;;;","25/Mar/14 18:07;klmarkey;It is precisely the Hadoop transitive dependencies on ASM that broke us first. It depends on what Hadoop version. We were using 2,2,0 which required 3.2 ASM. Hive, too. More recent Hadoop versions might be in line with 
4.0 but I've no checked. If they are in conflict, we are sunk in any case, and your concern is moot. Hadoop shouldn't depend on Spark's transitive dependencies. 

Thanks 
Kevin

;;;","25/Mar/14 19:01;tdas;But shouldnt this change in theh PR make it easier to deal with such conflicts as you can now exclude ASM from Spark, without affecting the execution of Spark (since Spark does not use ASM any more)?

Since Spark is designed to build against many different version of Hadoop, short of blindly eliminating ASM completely from all dependencies, there isnt any super-smart thing that can be done in the Spark build to deal with ASM conflicts. Explicitly removing a transitive dependency without have a full knowledge of how it may affect dependencies (Hadoop in this case) is something that should not be done in a maintenance release.
;;;","26/Mar/14 14:50;tdas;[~klmarkey] Since 0.9.1-RC2 with this fix is out, if possible, you can try compiling your project with ASM excluded from Spark.
;;;","26/Mar/14 15:07;klmarkey;Thank you for including 782.  Yes, I'm in the process of ramping up a 
test.  Compilation is insufficient.  I'll need to test runtime, too.  
Which will take a few days because some of the relevant code in flux.

Kevin



;;;","28/Mar/14 02:32;santhoma;Thanks for the fix.
I was getting below exception while trying with CDH 5.2 beta.

java.lang.IncompatibleClassChangeError: class org.apache.spark.util.InnerClosureFinder has interface org.objectweb.asm.ClassVisitor as super class
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:792)

The embedded version with CDH 5.2b is spark 0.9, and after building from  0.9.1 branch version the above error was gone.;;;","28/Mar/14 02:53;tdas;Great!;;;",,,,,,,,,,,,,,,,,,
run script should try java executable from JAVA_HOME first,SPARK-778,12704939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joshrosen,,17/Jun/13 11:28,27/Jun/13 22:19,14/Jul/23 06:25,27/Jun/13 22:19,0.7.2,,,,,,,,0.7.3,0.8.0,,,,,,,,,0,,,,,,"See https://groups.google.com/d/msg/spark-users/uuw2e9d1l74/_bCyDfq6dywJ;

It looks like the script tries to run `java` first and falls back on `$JAVA_HOME/bin/java` only if `java` cannot be found.  This should probably be the other way around, trying $JAVA_HOME first.  This is important for supporting users with multiple JDKs.

We should probably do the same thing for $SCALA_HOME, checking for a user-supplied SCALA_HOME before falling back on `scala`.",,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383460,,,Thu Jun 27 22:19:23 UTC 2013,,,,,,,,,,"0|i1u29r:",383728,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/13 22:19;matei;Fixed in https://github.com/mesos/spark/commit/4974b658edf2716ff3c6f2e6863cddb2a4ddf891;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SPARK_EXAMPLES_JAR in 0.7.2,SPARK-764,12705455,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,joshrosen,,07/Jun/13 13:26,22/Jun/13 10:22,14/Jul/23 06:25,22/Jun/13 10:22,0.7.2,,,,,,,,0.7.3,,,,,Examples,,,,,0,,,,,,"Users are reporting that SPARK_EXAMPLES_JAR is not set right in Spark 0.7.2 (see https://groups.google.com/d/msg/spark-users/nQ6wB2lcFN8/gWfBd6fLWHQJ for a recent example).

A new post in https://groups.google.com/d/msg/spark-users/x5UczgI-Xm8/qInsC0ww-NAJ reports that SPARK_EXAMPLES_JAR is not set  because the run script searches for the wrong filename.  Here's the suggested fix in that message:

{code}
144 if [ -e ""$EXAMPLES_DIR/target/scala-$SCALA_VERSION/spark-examples_""!(*sources    |*javadoc) ]; then
145   # Use the JAR from the SBT build
146   export SPARK_EXAMPLES_JAR=`ls ""$EXAMPLES_DIR/target/scala-$SCALA_VERSION/spark-examples_""!(*sources|*javadoc).jar`
147 fi
{code}

The new part here is the addition of the underscore between ""spark-examples"" and the Scala version, e.g. `spark-examples_2.9.3-0.7.2.jar`.

I'm not sure whether the real problem is with the run script or the Maven or SBT assembly scripts; could someone more familiar with Spark's packaging look into this?",,ctn,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383465,,,Sat Jun 22 10:22:19 UTC 2013,,,,,,,,,,"0|i1u2av:",383733,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/13 10:53;matei;Just curious, did this happen with SBT or Maven?;;;","14/Jun/13 10:58;joshrosen;In the first thread, they say that they compiled Spark using sbt/sbt.

I haven't checked whether we have this bug with the pre-compiled 0.7.2 binaries.

It's worth confirming that Maven and sbt produce build artifacts with the same filenames.;;;","14/Jun/13 18:19;ctn;I can confirm that I get this when using ""sbt package"":
{code}
-rw-rw-r-- 1 ctn ctn 347561 Jun 14 17:47 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar
{code}
--

Separately, the run script breaks when there are more than one matching jar files, e.g.:
{code}
ctn@ctnu:~/src/spark$ ls -l examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.*
-rw-rw-r-- 1 ctn ctn 347561 Apr 17 16:02 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar
-rw-rw-r-- 1 ctn ctn 347561 Jun 14 17:47 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar

ctn@ctnu:~/src/spark$ ./run spark.examples.SparkPi local
+ '[' -e /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar ']'
./run: line 145: [: /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar: binary operator expected
+ '[' -e '/home/ctn/src/spark/examples/target/spark-examples-!(*sources|*javadoc).jar' ']'
+ exit 0
{code};;;","22/Jun/13 10:22;matei;I've fixed this here: https://github.com/mesos/spark/commit/e6a14e73b93fe30fcae7913262506b4a03fedd11. It looks like it was broken on a commit that existed in 0.7.x but not in master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The simple tutorial doesn't say anything about including the Spark jar or the Spark codepath,SPARK-762,12704778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andyk,rxin,,06/Jun/13 12:02,11/Aug/13 20:40,14/Jul/23 06:25,11/Aug/13 20:40,,,,,,,,,0.8.0,,,,,Documentation,,,,,0,,,,,,"Fairly confusing to users. I am not even sure if this is a valid item, but you can certainly find it in the compiled html.

http://spark-project.org/docs/latest/spark-simple-tutorial.html",,joshrosen,patrick,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383381,,,Sun Aug 11 20:11:10 UTC 2013,,,,,,,,,,"0|i1u1s7:",383649,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/13 12:56;joshrosen;It also doesn't mention setting SPARK_MEM (or any of the other settings from spark-env.sh).;;;","11/Aug/13 20:01;patrick;Hey [~rxin] [~andyk] [~joshrosen],

This document that's linked to here - I'm not sure it's actually referenced anywhere in the docs. This is also nearly completely subsumed by the Quickstart guide. Am I missing something? Probably we should just delete the doc.;;;","11/Aug/13 20:11;rxin;Yes - we should just delete it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterSchedulerSuite unit test will failed in some scenarios,SPARK-753,12704482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrew xia,andrew xia,,31/May/13 00:53,07/Jan/14 17:59,14/Jul/23 06:25,07/Jan/14 17:59,0.8.0,,,,,,,,0.8.1,,,,,Spark Core,,,,,0,,,,,,"ClusterSchedulerSuite unit test will failed in some scenarios

[info] ClusterSchedulerSuite:
[info] - FIFO Scheduler Test
[INFO] [05/30/2013 14:56:15.332] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [info] - Fair Scheduler Test *** FAILED ***
[info]   3 did not equal 0 (ClusterSchedulerSuite.scala:111)

[INFO] [05/30/2013 14:56:15.540] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [info] - Nested Pool Test *** FAILED ***
[info]   7 did not equal 0 (ClusterSchedulerSuite.scala:111)

[INFO] [05/30/2013 14:56:15.712] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [error] Failed: : Total 3, Failed 2, Errors 0, Passed 1, Skipped 0 [error] Failed tests:
[error]         spark.scheduler.ClusterSchedulerSuite
[error] (core/test:test-only) sbt.TestsFailedException: Tests unsuccessful [error] Total time: 5 s, completed May 30, 2013 2:56:15 PM
",,andrew xia,Grace Huang,tedyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383217,,,Tue Jan 07 17:59:22 UTC 2014,,,,,,,,,,"0|i1u0rr:",383485,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/14 16:35;tedyu;Was the test failure based on hadoop1 or hadoop2 ?

Can you attach full test output ?

Thanks;;;","07/Jan/14 17:58;andrew xia;I have fixed this bug, it has nothing to do with Hadoop, thanks.;;;","07/Jan/14 17:59;andrew xia;fix this bug by changing scheduler algorithm.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-ec2 fails to detect cluster after ssh error during launch,SPARK-749,12704981,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,joshrosen,,14/May/13 15:11,19/Sep/13 14:58,14/Jul/23 06:25,19/Sep/13 14:58,,,,,,,,,0.8.1,,,,,EC2,,,,,0,,,,,,"I tried to launch an EC2 cluster using Patrick's new version of the EC2 script, running this command

{code}
 ./spark-ec2 -i ~/.ssh/id_rsa -s 10 -t m1.large --spot-price 0.25 --region us-west-1 launch test_cluster
{code}

This launched a cluster, but ssh failed because I didn't configure things properly

{code}
Setting up security groups...
Searching for existing cluster test_cluster...
Spark AMI: ami-61ffd024
Launching instances...
Requesting 10 slaves as spot instances with price $0.250
Waiting for spot instances to be granted...
0 of 10 slaves granted, waiting longer
...
All 10 slaves granted
Launched master in us-west-1a, regid = r-790f2320
Waiting for instances to start up...
Waiting 120 more seconds...
Copying SSH key /Users/joshrosen/.ssh/id_rsa to master...
Warning: Permanently added 'ec2-54-241-179-230.us-west-1.compute.amazonaws.com,54.241.179.230' (RSA) to the list of known hosts.
Permission denied (publickey).
{code}

When I tried to shut this cluster down, the EC2 script claims that it couldn't find a cluster.  I see the same message when using the 0.7 version of spark-ec2.

If I view the EC2 administration console, it tells me that I have a number of machines running in us-west-1 with the right security groups.

Although the script finds the active instance requests, it can't find any security group names for those requests, so it fails to identify those machines as belonging to our spark-ec2 cluster.

I think the culprit is the line

{code}
group_names = [g.name for g in res.groups]
{code}

in {{get_existing_cluster()}}.

If I replace this with

{code}
group_names = list(set(g.name for g in i.groups for i in res.instances))
{code}

then it finds the group names and detects the cluster.

Does anyone know what's going on here?  I'm not familiar enough with Boto to explain why the Instance would have groups attribute while its Request's groups are empty.",,ilikerps,joshrosen,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383331,,,Thu Sep 19 14:58:23 UTC 2013,,,,,,,,,,"0|i1u1h3:",383599,,,,,,,,,,,,,,,,,,,,,,,"19/May/13 11:41;shivaram;From my limited knowledge of boto, the first command looks like it retrieves all the groups for a user, while the second one lists the groups for instances. So my guess is that the while the instances were assigned a group, the group was not created in the list of all groups for a user (maybe an effect of eventual consistency ?).  ;;;","19/Sep/13 14:58;ilikerps;Fixed in [938|https://github.com/mesos/spark/pull/938].
The exact cause of this discrepancy is still not known, but this PR should fix it nevertheless.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw an exception on slaves if a message is sent that is larger than akka's max frame size,SPARK-747,12704754,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,patrick,,10/May/13 22:29,17/Mar/14 10:23,14/Jul/23 06:25,17/Mar/14 10:23,0.6.2,0.7.0,,,,,,,0.7.3,1.0.0,,,,,,,,,0,,,,,,"This can be really hard to debug for users, until we fix SPARK-669 we should at least warn users when this happens with an exception:

https://groups.google.com/forum/#!msg/spark-users/WG87fG8rrKY/6VpJ1AFXS-cJ",,joshrosen,patrick,rxin,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-669,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382991,,,Mon Mar 17 10:23:05 UTC 2014,,,,,,,,,,"0|i1tzdj:",383259,,,,,,,,,,,,,,,,,,,,,,,"11/May/13 17:55;joshrosen;It looks like earlier versions of Akka had some bugs that make this issue very difficult to diagnose: https://www.assembla.com/spaces/akka/tickets/3038-silently-dropped-big-messages

It looks like this has been fixed in Akka 2.1.1, so I'll see about upgrading to a newer Akka version.;;;","11/May/13 17:58;rxin;I saw that ticket before - the problem is Spark needs to be aware that this is happening too ... and hopefully remind the user.

It is hard to diagnose problems right now, but it wouldn't be too hard to check how big the task result is. It doesn't have to be very accurate, but as long as it is close to the max frame size, we can at least give a warning.

But yes, if you have time to just fix SPARK-669, it might be easier to just do that one.;;;","19/May/13 13:18;joshrosen;Pull request https://github.com/mesos/spark/pull/610 modifies Spark to abort  the job if any task fails due to a TaskResult that exceeds the Akka frame size.

Do we want to backport this fix into branch-0.7?;;;","12/Jun/13 15:08;shivaram;I just saw a case where sc.parallelize failed when the array being parallelized is larger than 10MB. We should also print an error message or warning for that.;;;","17/Mar/14 10:23;patrick;Subsumed by SPARK-669.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task Metrics should not employ per-record timing by default,SPARK-742,12705135,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,patrick,patrick,,29/Apr/13 08:51,01/May/13 17:38,14/Jul/23 06:25,01/May/13 17:38,0.7.0,,,,,,,,0.7.1,0.8.0,,,,,,,,,0,,,,,,"The per-record timing inside of TimedIterator causes substantial performance overhead in certain cases.

It should be an optional feature that is turned off by default, rather than hard-coded feature that is not possible to disable.",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383497,,,2013-04-29 08:51:19.0,,,,,,,,,,"0|i1u2hz:",383765,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark block manager UI has bug when enabling Spark Streaming,SPARK-740,12704971,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,saisai_shao,saisai_shao,,25/Apr/13 19:40,17/May/20 18:21,14/Jul/23 06:25,07/Aug/13 10:50,0.7.0,0.7.1,0.7.2,0.7.3,,,,,0.7.3,0.8.0,,,,Block Manager,Spark Core,,,,1,,,,,,"currently Spark storage ui aggregate RDDInfo using block name, and in block manger, all the block name is rdd_*_*. But in Spark Streaming, block name changes to input-*-*, this will cause a exception when group rdd info using block name in StorageUtils.scala:

    val groupedRddBlocks = infos.groupBy { case(k, v) =>
      k.substring(0,k.lastIndexOf('_'))
    }.mapValues(_.values.toArray)

according to '_' to get rdd name will meet exception when using Spark Streaming.

java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1958)
        at spark.storage.StorageUtils$$anonfun$3.apply(StorageUtils.scala:49)
        at spark.storage.StorageUtils$$anonfun$3.apply(StorageUtils.scala:48)
        at scala.collection.TraversableLike$$anonfun$groupBy$1.apply(TraversableLike.scala:315)
        at scala.collection.TraversableLike$$anonfun$groupBy$1.apply(TraversableLike.scala:314)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:178)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:347)
        at scala.collection.TraversableLike$class.groupBy(TraversableLike.scala:314)
        at scala.collection.immutable.HashMap.groupBy(HashMap.scala:38)
        at spark.storage.StorageUtils$.rddInfoFromBlockStatusList(StorageUtils.scala:48)
        at spark.storage.StorageUtils$.rddInfoFromStorageStatus(StorageUtils.scala:40)
        at spark.storage.BlockManagerUI$$anonfun$5.apply(BlockManagerUI.scala:54)
....

there has two methods:
1. filter out all the Spark Streaming's input block RDD.
2. treat Spark Streaming's input RDD as a special case, add code to support this case.",,matei,noootsab,saisai_shao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383391,,,Wed Aug 07 10:49:53 UTC 2013,,,,,,,,,,"0|i1u1uf:",383659,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/13 06:51;noootsab;The same goes with 0.7.3 (fyi ;-));;;","28/Jul/13 17:36;saisai_shao;It is fixed in master branch, but do not backport to 0.7.3.;;;","29/Jul/13 00:02;matei;Oh, can you show me the commit where it's fixed?;;;","29/Jul/13 00:09;saisai_shao;hi Matei, this issue is fixed in in PR 581(https://github.com/mesos/spark/pull/581).;;;","07/Aug/13 10:49;matei;Thanks. I've now fixed this in branch-0.7 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark should detect and squash nonserializable exceptions,SPARK-738,12705404,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,patrick,pwendell,,24/Apr/13 21:38,30/Mar/14 04:14,14/Jul/23 06:25,02/Jun/13 09:21,0.7.1,0.8.0,,,,,,,,,,,,,,,,,0,,,,,,"If user exception is thrown which a Spark executor cannot serialize, it causes the executor to crash. The executor should wrap the attempt to serialize the exception with a try/catch and return a default exception in the case where it can't be serialized.

The fix should include a test case",,patrick,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382825,,,Wed Apr 24 21:42:04 UTC 2013,,,,,,,,,,"0|i1tycn:",383093,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/13 21:42;rxin;Pushing the priority to Critical - since a user level bug can actually destroy the cluster this way ...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silence expected exceptions for unit tests,SPARK-737,12705390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,patrick,,22/Apr/13 10:55,11/Jul/13 15:54,14/Jul/23 06:25,11/Jul/13 15:54,0.7.0,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,"The unit tests make a lot of noise, mostly from Akka logs. It would be good to figure out how to disable these so they don't confuse people.",,joshrosen,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383453,,,Thu Jul 11 15:54:50 UTC 2013,,,,,,,,,,"0|i1u287:",383721,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/13 14:52;joshrosen;The 

{code}
[INFO] [04/13/2013 09:48:09.453] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped
{code}

log messages are particularly annoying; I'd love to disable these if we can.;;;","23/Apr/13 20:21;patrick;Spent some time on this today - unfortunately it's non-trivial.

The Spray logging internally hooks into Akka's event logging. We have the Akka logging configured to delegate to slf4j which then delegates to log4j (since we include the log4j jar in our build).

With a slight modification to the way Spray is initialized, it's possible to silence this with the Spark logging properties, as seen here:

https://github.com/pwendell/spark/commit/b5a5c2562d32ab65669e94dfc19002ac2d2059bf

The problem is that, while this works great for running Spark in the shell, it doesn't work for tests. For some reason, during the tests something breaks down in the Spray->Akka->Slf4j->Log4j pipeline, possibly because of the way the classloading happens inside of Scalatest.

To actually silence this for the tests, I had to manually hook into the Spray logger and disable all info/debug level messages. This works but it's pretty brittle:

https://github.com/pwendell/spark/commit/40410247d898cbde9299089ef92f79eb7991ff25

I'm going to have to put this aside for a bit, that's as far as I got for now.
;;;","11/Jul/13 15:54;patrick;Thanks Shivaram for finally cracking this. The last annoying bit has been fixed here: 

https://github.com/mesos/spark/pull/683;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memory leak in KryoSerializer,SPARK-735,12705388,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rams,rxin,,17/Apr/13 16:00,21/Oct/14 07:47,14/Jul/23 06:25,21/Oct/14 07:47,0.7.0,,,,,,,,,,,,,,,,,,0,,,,,,"KryoSerializer uses a ThreadLocal object to store a kryo buffer, which is never cleaned up.

This becomes a serious problem in projects like Shark, where new threads are created constantly by the thrift server. As long as the new thread references the kryo serializer, it will create a new kryo buffer.

A simple solution is to remove the ThreadLocal reference, and create a new buffer object every time a new kryo serializer instance is created.

This is not very expensive because large writes actually go through the serialization stream interface, which reuse the buffer anyway. ",,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383502,,,Tue Oct 21 07:47:00 UTC 2014,,,,,,,,,,"0|i1u2j3:",383770,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/13 16:00;rxin;Ram already has a fix and he will submit a PR to Spark.;;;","21/Oct/14 07:47;pwendell;I think this was fixed a long time ago.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation on use of accumulators in lazy transformation,SPARK-733,12705427,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joshrosen,,14/Apr/13 22:59,16/Jan/15 21:33,14/Jul/23 06:25,16/Jan/15 21:33,,,,,,,,,1.2.1,1.3.0,,,,Documentation,,,,,0,,,,,,"Accumulators updates are side-effects of RDD computations.  Unlike RDDs, accumulators do not carry lineage that would allow them to be computed when their values are accessed on the master.

This can lead to confusion when accumulators are used in lazy transformations like `map`:

{code}
    val acc = sc.accumulator(0)
    data.map(x => acc += x; f(x))
    // Here, acc is 0 because no actions have cause the `map` to be computed.
{code}

As far as I can tell, our  documentation only includes examples of using accumulators in `foreach`, for which this problem does not occur.

This pattern of using accumulators in map() occurs in Bagel and other Spark code found in the wild.

It might be nice to document this behavior in the accumulators section of the Spark programming guide.",,apachespark,ilganeli,irashid,joshrosen,rektide,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3642,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383503,,,Fri Jan 16 21:33:51 UTC 2015,,,,,,,,,,"0|i1u2jb:",383771,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/15 17:55;apachespark;User 'ilganeli' has created a pull request for this issue:
https://github.com/apache/spark/pull/4022;;;","16/Jan/15 21:33;irashid;Fixed by https://github.com/apache/spark/pull/4022;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit testing failure,SPARK-730,12705442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,matei,markhamstra,,09/Apr/13 12:05,11/Apr/13 19:34,14/Jul/23 06:25,11/Apr/13 19:34,0.7.1,,,,,,,,0.7.1,0.8.0,,,,Build,,,,,0,,,,,,"After, I believe, the switch to using sbt 0.12.x, multiple unit tests fail.  Most of the failures are of the form:

.
.
.
org.jboss.netty.channel.ChannelException: Failed to bind to: /10.0.0.35:61448
.
.
.
Cause: java.net.BindException: Address already in use
at sun.nio.ch.Net.bind0(Native Method)
.
.
.

*** And a meta-bug: 0.8.0 isn't yet available as an ""Affects Version/s"" option in JIRA.",,andyk,markhamstra,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383504,,,Thu Apr 11 19:34:35 UTC 2013,,,,,,,,,,"0|i1u2jj:",383772,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/13 12:38;andyk;I've added 0.8.0 as an unreleased version.;;;","10/Apr/13 07:54;matei;I didn't see these failures myself when I updated. Are you sure they're not due to something like your hostname not mapping to the right IP?;;;","10/Apr/13 07:56;matei;Actually, the effect might be nondeterministic due to SBT's parallel execution, which changed slightly in 0.12: https://github.com/harrah/xsbt/wiki/Parallel-Execution. Maybe we need to limit the degree of parallelism more than the build file already does.;;;","10/Apr/13 08:31;markhamstra;It really does look like a concurrency issue, since tests that fail using 'sbt/sbt test' succeed when run individually using, e.g, 'sbt/sbt ""test-only spark.AccumulatorSuite""'.;;;","11/Apr/13 19:34;matei;I've fixed this in https://github.com/mesos/spark/commit/ed336e0d44d27e9be66adb0962f82af7d1ac4d87. It was slightly tricky because SBT itself had a bug in this setting until 0.12.3 (https://github.com/sbt/sbt/issues/692).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible bugs in zip() transformation,SPARK-726,12705415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,joshrosen,,05/Apr/13 20:12,07/Apr/13 18:16,14/Jul/23 06:25,07/Apr/13 18:16,,,,,,,,,0.7.1,,,,,,,,,,0,,,,,,"A couple of bugs in the {{zip()}} transformation were reported on the mailing list, so I thought I'd link them here so they aren't forgotten:

- https://groups.google.com/d/msg/spark-users/EofjLb_xm5Y/HPSNXBakZegJ
- https://groups.google.com/d/msg/spark-users/demrmjHFnoc/oYaEOLqFsqYJ",,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383508,,,Sun Apr 07 18:16:33 UTC 2013,,,,,,,,,,"0|i1u2kf:",383776,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/13 18:16;matei;Fixed this in https://github.com/mesos/spark/commit/054feb6448578de5542f9ef54d4cc88f706c22f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests fail out of the box,SPARK-716,12705043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,andyk,,17/Mar/13 16:00,19/Feb/14 13:44,14/Jul/23 06:25,07/Dec/13 14:16,,,,,,,,,,,,,,,,,,,0,,,,,,"At commit c1e9cdc49f89222b366a14a20ffd937ca0fb9adc

$ sbt/sbt test

...

[info] DriverSuite:
[info] - driver should exit after finishing *** FAILED ***
[info]   SparkException was thrown during property evaluation. (DriverSuite.scala:15)
[info]   Message: Process List(./run, spark.DriverWithoutCleanup, local-cluster[2,1,512]) exited with code 1
[info]   Occurred at table row 1 (zero based, not counting headings), which had values (
[info]     master = local-cluster[2,1,512]
[info]   )

...

[error] Failed: : Total 203, Failed 1, Errors 0, Passed 202, Skipped 0
[error] Failed tests:
[error] 	spark.DriverSuite

...

[error] {file:/Users/andyk/Development/spark/}core/test:test: Tests unsuccessful
[error] Total time: 676 s, completed Mar 17, 2013 1:47:55 PM",,andyk,joshrosen,matei,sowen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383137,,,Wed Feb 19 13:44:32 UTC 2014,,,,,,,,,,"0|i1u09z:",383405,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/13 15:59;joshrosen;We might just want to drop this test or convert it into a ""benchmark"" or something, so that failures don't mark the test suites as failed.  I added this test after fixing a bug where some ""daemon"" threads weren't actually being constructed as daemon threads, which might have prevented clients from exiting properly without calling System.exit().;;;","07/Apr/13 18:18;matei;This probably failed because you need to create a spark-env.sh and set SCALA_HOME to run it. We should warn better about that situation. I want these tests to run, so disabling them isn't an option.;;;","19/Feb/14 13:44;sowen;Hi all, Sandy and I are seeing the same error when running the Maven-based tests in the CDH integration environment. To reproduce, you can just run the DriverSuite test from within core/, without building 'sbt assembly'. It works after running 'sbt assembly' but the Maven build doesn't ensure that has been run.

I suppose I would imagine the Maven build should not depend on sbt. If that's the case I think this test might be removed? i.e. I don't think it's just a matter of setting env variables.

Or if the intent is that the Maven build needs to depend on 'sbt assembly', then we can try to figure out a way to execute that in the Maven build. What's the preferred solution?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Link to YARN document broken in ""Launching Spark on YARN"" doc",SPARK-714,12705350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andyk,andyk,,13/Mar/13 02:35,16/Mar/13 11:44,14/Jul/23 06:25,16/Mar/13 11:44,0.7.0,,,,,,,,,,,,,Documentation,,,,,0,,,,,,"The first hyperlink in the body of the ""Launching Spark on YARN"" doc (see http://www.spark-project.org/docs/0.7.0/running-on-yarn.html) currently links to http://hadoop.apache.org/docs/r2.0.1-alpha/hadoop-yarn/hadoop-yarn-site/YARN.html which now returns a 404 page not found.

Should we link to http://hadoop.apache.org/docs/r2.0.3-alpha/hadoop-yarn/hadoop-yarn-site/YARN.html instead (which does currently work)?",,andyk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383522,,,Sat Mar 16 11:44:57 UTC 2013,,,,,,,,,,"0|i1u2nj:",383790,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/13 11:44;andyk;Fixed in https://github.com/mesos/spark/pull/524

Though the link we updated to use might break as well since it is still an alpha version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken link in quick start guide,SPARK-713,12705412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andyk,andyk,,13/Mar/13 02:04,13/Mar/13 12:11,14/Jul/23 06:25,13/Mar/13 12:11,0.6.0,0.6.1,0.6.2,0.7.0,,,,,,,,,,Documentation,,,,,0,,,,,,"the link to ""Java Programming Guide"" in the quick start (i.e. docs/ links to http://spark-project.org/docs/0.6.2/""java-programming-guide"" (with the quotes) when it should point to http://spark-project.org/docs/0.6.2/java-programming-guide.html",,andyk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383523,,,Wed Mar 13 12:11:40 UTC 2013,,,,,,,,,,"0|i1u2nr:",383791,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/13 02:24;andyk;Just found another broken link in same document, this time to the Python programming guide.;;;","13/Mar/13 12:11;andyk;Fixed in merge https://github.com/mesos/spark/pull/523;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka OutOfMemoryError,SPARK-712,12705008,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,seanm,seanm,,12/Mar/13 22:22,18/Jul/13 15:48,14/Jul/23 06:25,18/Jul/13 15:48,0.7.0,,,,,,,,0.7.3,0.8.0,,,,DStreams,,,,,0,,,,,,"Using KafkaInputDStream causes OutOfMemoryErrors after running for a period of time (1-2 minutes in my case).  The bug is not within Spark Streaming, but rather has something to do with how the provided jar under /streaming/lib/ was packaged.


If it is helpful- We have packaged Kafka 0.7.2 /w scala 2.9.2 that we run in many prod environments that we would be happy to provide.


The issue is easily reproducible, here is code I ran using the kafka jar that is bundled with Spark Streaming-
{code}
import java.util.concurrent.Executors
import java.util.Properties
import kafka.consumer._
import kafka.message.{Message, MessageSet, MessageAndMetadata}
import kafka.serializer.StringDecoder
import kafka.utils.{Utils, ZKGroupTopicDirs}


private class MessageHandler(stream: KafkaStream[String]) extends Runnable {

  var index = 0

  def run() {
    stream.takeWhile { msgAndMetadata =>
      if (index%1000 == 0) {
        println(""got: "" + index)
      }
      index += 1
      true
    }
  }
}
object KafkaTest {
  def main(args: Array[String]) {
    val props = new Properties()
    props.put(""zk.connect"", ""ozoo01.staging.dmz,ozoo02.staging.dmz,ozoo03.staging.dmz"")
    props.put(""groupid"", ""my-cool-consumer-group"")
    props.put(""zk.connectiontimeout.ms"", ""100000"")

    val executorPool = Executors.newFixedThreadPool(1)



    val consumerConfig = new ConsumerConfig(props)
    val consumerConnector: ZookeeperConsumerConnector = Consumer.create(consumerConfig).asInstanceOf[ZookeeperConsumerConnector]
    val topicMessageStreams = consumerConnector.createMessageStreams(Map(""las_01_scsRawHits"" -> 1), new StringDecoder())

    topicMessageStreams.values.foreach { streams =>
      streams.foreach { stream => executorPool.submit(new MessageHandler(stream)) }
    }
  }
}
{code}


Here is the output:
got: 0
got: 1000
got: 2000
got: 3000
got: 4000
...
got: 158000
got: 159000
got: 160000
got: 161000
got: 162000
13/03/12 22:47:14 ERROR network.BoundedByteBufferReceive: OOME with size 4194330
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80)
	at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63)
	at kafka.network.Receive$class.readCompletely(Transmission.scala:55)
	at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
	at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177)
	at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117)
	at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115)
	at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60)
13/03/12 22:47:14 ERROR consumer.FetcherRunnable: error in FetcherRunnable 
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80)
	at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63)
	at kafka.network.Receive$class.readCompletely(Transmission.scala:55)
	at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
	at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177)
	at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117)
	at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115)
	at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60)
13/03/12 22:47:14 INFO consumer.FetcherRunnable: stopping fetcher FetchRunnable-1 to host oagg01.staging.dmz
got: 163000
13/03/12 22:47:16 ERROR network.BoundedByteBufferReceive: OOME with size 4194330
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80)
	at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63)
	at kafka.network.Receive$class.readCompletely(Transmission.scala:55)
	at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
	at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177)
	at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117)
	at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115)
	at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60)
13/03/12 22:47:16 ERROR consumer.FetcherRunnable: error in FetcherRunnable 
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80)
	at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63)
	at kafka.network.Receive$class.readCompletely(Transmission.scala:55)
	at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
	at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177)
	at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117)
	at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115)
	at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60)
13/03/12 22:47:18 INFO consumer.FetcherRunnable: stopping fetcher FetchRunnable-0 to host oagg02.staging.dmz",,matei,seanm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383444,,,Thu Jul 18 15:48:11 UTC 2013,,,,,,,,,,"0|i1u267:",383712,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/13 23:22;seanm;The OOM issue is caused by using takeWhile.  https://github.com/mesos/spark/pull/527 has the fix.

Thanks;;;","18/Jul/13 15:26;seanm;https://github.com/mesos/spark/pull/527 was merged in, and the issue is fixed.  So this can be marked as resolved (I don't see an option to close this issue.);;;","18/Jul/13 15:48;matei;I've marked it as resolved. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All PairRDDFunctions should accept JFunction (not Function),SPARK-702,12705091,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,22/Feb/13 12:51,22/Feb/13 14:59,14/Jul/23 06:25,22/Feb/13 14:59,,,,,,,,,0.7.0,,,,,,,,,,0,,,,,,There are a few that accept Scala's Function instead of JFunction. This should be fixed.,,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383528,,,2013-02-22 12:51:23.0,,,,,,,,,,"0|i1u2ov:",383796,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong SPARK_MEM setting with different EC2 master and worker machine types,SPARK-701,12705145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,joshrosen,,21/Feb/13 16:33,30/Jun/15 06:07,14/Jul/23 06:25,14/May/13 15:38,0.7.0,,,,,,,,0.7.0,,,,,EC2,,,,,0,,,,,,"When launching a spark-ec2 cluster using different worker and master machine types, SPARK_MEM in spark-env.sh is set based on the master's memory instead of the worker's.  This causes jobs to hang if the master has more memory than the workers (because jobs will request too much memory). ",,joshrosen,parmesan,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383484,,,Tue Jun 30 06:07:42 UTC 2015,,,,,,,,,,"0|i1u2f3:",383752,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/13 10:21;shivaram;Should be fixed by https://github.com/mesos/spark-ec2/pull/4;;;","13/Apr/13 10:03;joshrosen;I'm pretty sure that this made it into the 0.7 AMI.  Could we maybe tag commits in the spark-ec2 repo whenever we release a new public AMI, so that it's easier to tell what gets released?;;;","14/May/13 15:38;joshrosen;Yep, this fix is present in the current 0.7 AMI.  Marking as fixed.;;;","29/Jun/15 22:53;parmesan;[~shivaram] it looks like this issue reappeared, in a way (talking about 1.4.0 now, not tested on previous versions): if you create a cluster with an {{m1.small}} master (1.7GB RAM) and one {{m1.large}} worker (7.5GB RAM), {{spark.executor.memory}} will be set to 512MB, and that's because of [system_ram_kb = min(slave_ram_kb, master_ram_kb)|https://github.com/mesos/spark-ec2/blob/e642aa362338e01efed62948ec0f063d5fce3242/deploy_templates.py#L32]

Quite often you use a smaller master instance compared to workers; smaller means fewer RAM, and the line of code I linked above shows that the minimum between the master and the worker(s) memory is used as {{spark_mb}}, which in turn is used as {{default_spark_mem}} to generate the [spark-defaults.conf|https://github.com/mesos/spark-ec2/blob/e642aa362338e01efed62948ec0f063d5fce3242/templates/root/spark/conf/spark-defaults.conf].

If we read the title of this bug, it would seem like the issue reappeared, and we should reopen it; if we instead read the description, we'll notice it's not 100% the same issue: it says ""SPARK_MEM in spark-env.sh is set based on the master's memory instead of the worker's"", while now ""it's set on the minimum between the master's memory and the worker's, which is quite often the master's"".

What do you suggest? Should we reopen this three-digits issue, or create a new one?;;;","29/Jun/15 23:09;shivaram;Yeah so SPARK_MEM used to be used for both master and executors before. Right now we have two separate variables spark.executor.memory and spark.driver.memory that we can set. Lets open a new issue for this.;;;","30/Jun/15 06:07;parmesan;Posting the link to the new issue for reference: https://issues.apache.org/jira/browse/SPARK-8726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark Standalone Mode is leaving a java process ""spark.executor.StandaloneExecutorBackend"" open on Windows",SPARK-698,12705424,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cgrothaus,cgrothaus,,13/Feb/13 02:11,25/Jun/13 19:15,14/Jul/23 06:25,25/Jun/13 19:15,0.6.2,,,,,,,,0.8.0,,,,,Deploy,,,,,0,,,,,,"The java process runnig ""spark.executor.StandaloneExecutorBackend"" fails to end after a task is finished.

Under Max OS X and Unix, there is a single shell script ""run"" to start Spark master, worker, and executor. Under Windows, there is a cascade: ""run.cmd"" calls ""run2.cmd"" which calls java. So when the spark.deploy.worker.ExecutorRunner (which runs in the worker process) wants to kill the executor process via process.destroy(), it actually only kills the process of ""run.cmd"", and the process of ""run2.cmd"" (=> java running the executor) stays alive.

See this thread on spark-users for all details: https://groups.google.com/forum/#!topic/spark-users/NrdhVlrUDtU/discussion

",,cgrothaus,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383462,,,Tue Jun 25 19:15:03 UTC 2013,,,,,,,,,,"0|i1u2a7:",383730,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/13 08:14;cgrothaus;Did some internet search on this: there is a bug open at Oracle: ""4770092 : (process) Process.destroy does not kill multiple child processes"" http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4770092 It affects Windows platforms.

All solutions discussed on StackOverflow that I found involve JNA, like this one: http://stackoverflow.com/questions/4912282/java-tool-method-to-force-kill-a-child-process/6032734#6032734

Another suggestion: maybe it is possible to send a stop message to the StandaloneExecutorBackend, so that it can stop the actor system and initiate its own shutdown properly.;;;","14/Feb/13 23:31;matei;Thanks for looking into it. The stop message would work in many cases, but unfortunately it would fail if the StandaloneExecutorBackend somehow freezes. Another option I'd consider is to execute the Java child process directly by running {{java}} instead of {{run.cmd}}. We would need to replicate the code for setting environment variables and classpaths that's there, but it shouldn't be too bad (in fact we can consider exporting the same variables that were used to launch the Worker).;;;","14/Feb/13 23:34;cgrothaus;Well, that would indeed work. Maybe it's the best solution in the face of the aforementioned Bug on Windows.;;;","25/Jun/13 19:15;matei;Merged Christoph's commit to fix this in 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sortByKey(ascending: Boolean) ignores ascending parameter,SPARK-696,12705387,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,11/Feb/13 17:42,11/Feb/13 18:53,14/Jul/23 06:25,11/Feb/13 18:53,0.6.1,0.6.2,0.7.0,,,,,,,,,,,Java API,,,,,0,,,,,,"It should pass the ascending parameter on. Instead, it always passes ""true"".

{code}
  /**
   * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling
   * `collect` or `save` on the resulting RDD will return or output an ordered list of records
   * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in
   * order of the keys).
   */
  def sortByKey(ascending: Boolean): JavaPairRDD[K, V] = {
    val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]]
    sortByKey(comp, true)
  }
{code}",,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383533,,,2013-02-11 17:42:18.0,,,,,,,,,,"0|i1u2pz:",383801,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exponential recursion in getPreferredLocations,SPARK-695,12705055,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,staple,matei,,11/Feb/13 13:27,06/Nov/14 07:02,14/Jul/23 06:25,01/Aug/14 19:06,,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"This was reported to happen in DAGScheduler for graphs with many paths from the root up, though I haven't yet found a good test case for it.",,matei,staple,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-682,,,,,,,,,SPARK-682,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383511,,,Thu Jul 31 23:23:18 UTC 2014,,,,,,,,,,"0|i1u2l3:",383779,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/14 23:23;staple;Progress has been made on a PR here:
https://github.com/apache/spark/pull/1362;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"All references to [K, V] in JavaDStreamLike should be changed to [K2, V2]",SPARK-694,12705382,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,patrick,,11/Feb/13 10:01,12/Feb/13 14:01,14/Jul/23 06:25,12/Feb/13 14:01,0.7.0,,,,,,,,0.7.0,,,,,,,,,,0,,,,,,"The type identifiers {code}K, V{code} are also used in JavaPairDStream, which causes a conflict with some compilers. This reveals itself whenever you want to create a JavaPairDStream from another, where the resulting types are different.

This may be related to SPARK-668, I'm not sure if that's the same problem. For example, trying to revers the key and value data on a stream doesn't compile correctly:

{code}
JavaPairDStream<Integer, String> reversed = pairStream.map(
    new PairFunction<Tuple2<String, Integer>, Integer, String>() {
      @Override
      public Tuple2<Integer, String> call(Tuple2<String, Integer> in) throws Exception {
        return new Tuple2(in._2(),  in._1());
      }
});
{code}",,joshrosen,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-668,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383530,,,Mon Feb 11 11:13:50 UTC 2013,,,,,,,,,,"0|i1u2pb:",383798,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/13 10:19;joshrosen;Ah, interesting.  In principle, type identifiers in different namespaces shouldn't cause conflicts.  In practice, I've seen some weird issues.  For example, I reported [SI-6067|https://issues.scala-lang.org/browse/SI-6057] while working on the original Java API.

What compiler error did you see when trying to compile that example?;;;","11/Feb/13 10:43;patrick;Here is the compiler error

{code}
[error] /home/patrick/Documents/spark/streaming/src/test/java/spark/streaming/JavaAPISuite.java:650: cannot find symbol
[error] symbol  : method map(<anonymous spark.api.java.function.PairFunction<scala.Tuple2<java.lang.String,java.lang.Integer>,java.lang.Integer,java.lang.String>>)
[error] location: class spark.streaming.api.java.JavaPairDStream<java.lang.String,java.lang.Integer>
[error]     JavaPairDStream<Integer, String> reversed = pairStream.map(
{code}

Another note - you need to {code}sbt/sbt clean{code} after applying this patch. Whatever scala/sbt uses to detect whether the code has changed doesn't pick up on this naming difference.;;;","11/Feb/13 11:13;joshrosen;That's the same error that I saw in SPARK-668.  I wrote a hack workaround for that issue because renaming type parameters didn't seem to work when I tried it.

I tried again just now after running {{sbt/sbt clean}} and the renamed type parameters solved the problem, so I'll submit a separate pull request to remove my hack workaround.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinite recursion in doCheckpoint when running Bagel,SPARK-691,12705343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,,08/Feb/13 14:20,11/Feb/13 13:26,14/Jul/23 06:25,11/Feb/13 13:26,0.7.0,,,,,,,,0.7.0,,,,,,,,,,0,,,,,,,,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383534,,,Mon Feb 11 13:26:00 UTC 2013,,,,,,,,,,"0|i1u2q7:",383802,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/13 13:26;matei;Fixed in https://github.com/mesos/spark/commit/ea08537143d58b79b3ae5d083e9b3a5647257da8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use separate SPARK_DAEMON_MEMORY setting in Windows run script too,SPARK-687,12705332,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,matei,matei,,06/Feb/13 14:40,07/Feb/13 21:51,14/Jul/23 06:25,07/Feb/13 21:51,0.6.2,,,,,,,,0.6.2,,,,,Windows,,,,,0,,,,,,"Commit https://github.com/mesos/spark/commit/c0d2ea111c17d9dde579c1b3bd79e5a8098f011a switches the standalone worker and master to use a different environment variable for their memory, since allocating them a huge amount of memory is a common pitfall. The Windows run script needs to receive the corresponding changes.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383537,,,2013-02-06 14:40:23.0,,,,,,,,,,"0|i1u2qv:",383805,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port FT heartbeat and fixes from 0.6 branch to master,SPARK-686,12705431,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,matei,,06/Feb/13 14:02,07/Dec/13 14:14,14/Jul/23 06:25,07/Dec/13 14:14,0.7.0,,,,,,,,,,,,,,,,,,0,,,,,,"Commits https://github.com/mesos/spark/commit/4b53f145b49fd8228129edef9a1f4f3f3488b865 through https://github.com/mesos/spark/commit/f886b42cecc8097ab33b2ba5ac39445c103a9ba7 on branch 0.6 fix a few issues with fault tolerance, including detecting ""hard crashes"" of nodes faster than a TCP timeout in the standalone cluster, and properly removing block locations for failed nodes. These need to be ported to master, which change the code from using slave IDs to executor IDs.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383254,,,2013-02-06 14:02:48.0,,,,,,,,,,"0|i1u0zz:",383522,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 0.7 with Hadoop 1.0 does not work with current AMI's HDFS installation,SPARK-683,12705386,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tdas,,04/Feb/13 17:13,29/Oct/14 09:17,14/Jul/23 06:25,11/Sep/14 08:59,0.7.0,,,,,,,,,,,,,EC2,,,,,0,,,,,,"A simple saveAsObjectFile() leads to the following error.

org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NoSuchMethodException: org.apache.hadoop.hdfs.protocol.ClientProtocol.create(java.lang.String, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, boolean, boolean, short, long)
	at java.lang.Class.getMethod(Class.java:1622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)
",,bachbui,patrick,shivaram,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383529,,,Wed Oct 29 09:17:12 UTC 2014,,,,,,,,,,"0|i1u2p3:",383797,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/13 20:37;patrick;I think the default hadoop version might have changed in the build. Are you sure you compiled spark with the same hadoop version as is on the AMI?;;;","04/Feb/13 20:42;shivaram;That was the problem. We changed the default hadoop version to 1.0 in 0.7.0 -- We should either change the AMI to run HDFS v1.0 or change Spark on the AMI to make sure users don't run into this.;;;","04/Feb/13 20:51;patrick;Ya I ran into this testing streaming code. Probably same as TD.;;;","21/Feb/13 15:07;shivaram;Hopefully this will help whoever makes the AMI for 0.7. I tried setting up Hadoop 1.0.3 on the existing AMI and the configuration we have right now works fine out of the box. All I had to do was:

wget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.3/hadoop-1.0.3.tar.gz
tar -xf hadoop-1.0.3.tar.gz
# Copy conf files from existing hdfs setup
;;;","11/Sep/14 08:59;srowen;I think this is likely long since obsolete or fixed, since Spark, Hadoop and AMI Hadoop versions have moved forward, and have not heard of this issue in recent memory.;;;","29/Oct/14 09:17;srowen;PS I think this also turns out to be the same as SPARK-4078;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Gateway JVM's should not be launched on slave,SPARK-674,12705400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,patrick,,31/Jan/13 14:22,01/Feb/13 11:45,14/Jul/23 06:25,01/Feb/13 11:45,,,,,,,,,0.7.0,,,,,PySpark,,,,,0,,,,,,"The slaves seem to launch a new JVM for each task (to run the Gateway Java program). This is a bug since that program is only needed on the driver. It causes increased latency for tasks - due to JVM launch - and also memory pressure, since the gateway asks for SPARK_MEM memory on launch.",,joshrosen,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383543,,,Fri Feb 01 11:45:24 UTC 2013,,,,,,,,,,"0|i1u2s7:",383811,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/13 11:45;joshrosen;Fixed in https://github.com/mesos/spark/pull/438;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark runs out of memory on fork/exec (affects both pipes and python),SPARK-671,12705147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jey,patrick,,31/Jan/13 12:52,02/Nov/15 12:11,14/Jul/23 06:25,28/Jun/13 20:30,0.5.0,0.5.1,0.5.2,0.6.0,0.6.1,,,,0.7.3,0.8.0,,,,PySpark,Spark Core,,,,0,,,,,,"Because the JVM uses fork/exec to launch child processes, any child process initially has the memory footprint of its parent. In the case of a large Spark JVM that spawns many child processes (for Pipe or Python support), this quickly leads to kernel memory exhaustion.

This problem is discussed here:
https://gist.github.com/1970815

It results in errors like this:
{code}
13/01/31 20:18:48 INFO cluster.TaskSetManager: Loss was due to java.io.IOException: Cannot run program ""cat"": java.io.IOException: error=12, Cannot allocate memory
       at java.lang.ProcessBuilder.start(ProcessBuilder.java:475)
       at spark.rdd.PipedRDD.compute(PipedRDD.scala:38)
       at spark.RDD.computeOrReadCheckpoint(RDD.scala:203)
       at spark.RDD.iterator(RDD.scala:192)
       at spark.scheduler.ResultTask.run(ResultTask.scala:76)
{code}

I was able to workaround by allowing for memory over-commitment by the kernel on all slaves,

{code}
echo 1 > /proc/sys/vm/overcommit_memory
{code}

but we should try to include a more robust solution, such as the one here:
https://github.com/axiak/java_posix_spawn",,ash211,joshrosen,lisendong,matei,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-620,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383459,,,Mon Nov 02 12:11:18 UTC 2015,,,,,,,,,,"0|i1u29j:",383727,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/13 12:39;patrick;You might want to make sure this still happens now that you fixed the bug which launches JVM's for every task.;;;","01/Feb/13 12:40;patrick;Actually, since we saw this even with rdd.pipe(), probably it's still an issue.;;;","01/Feb/13 14:52;joshrosen;It seems like this is a JVM problem that only affects some platforms.  It sounds like Jenkins and Hadoop don't work around this, so maybe a fix is out of scope for us.

I propose that we add some configuration documentation on how to work around this issue (e.g. through overcommit_memory or adding extra virtual memory), then resolve this issue as ""Won't Fix."";;;","14/Feb/13 23:32;matei;Don't put a fix version on this since it's not yet fixed. You're supposed to only assign that once you fix it.;;;","14/Feb/13 23:33;matei;And yes I agree that we probably don't want to work around this if Hadoop and Jenkins don't. We might just leave it as an open issue and document it in various places.;;;","15/Feb/13 10:25;patrick;Matei - that's not how Fix versions are used in JIRA. Fix versions are for the version of the intended fix, regardless of whether the issue is completed, it says so clearly in the JIRA docs:

https://confluence.atlassian.com/display/JIRA/What+is+an+Issue

This is necessary for using ""Roadmap"" features of JIRA - to answer questions like: ""How many outstanding issues are there for 0.7"". That way people will actually know what remains before a release comes out and can track projects. The other projects (STREAMING, SHARK) also use fix versions like this pervasively.;;;","15/Feb/13 21:04;matei;Ah, I was actually going by what I saw happen in Hadoop. In this case though, let's not assign fix versions unless we actually make a roadmap where we agree we'll do this for a particular version. (At least for Spark.);;;","28/Jun/13 20:30;joshrosen;This was fixed by Jey in https://github.com/mesos/spark/pull/563, which uses a separate process to fork PySpark's {{python}} processes.;;;","28/Jun/13 20:32;joshrosen;AFAIK, this has only been fixed for PySpark, not for general pipe() calls in Spark.  If we still need to fix that, please re-open this issue (or open a new linked issue).;;;","02/Nov/15 12:11;lisendong;is it still a problem in latest version?
I 'm using pipe() operation, and found that if I use pipe() before any shuffle task, the memory always grows very high.
maybe the memory usage of sub process is not a constant volume ? is it affected by the memory of the parent process?;;;",,,,,,,,,,,,,,,,,,,,,,,
Send back task results through BlockManager instead of Akka messages,SPARK-669,12705136,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,matei,,27/Jan/13 22:49,04/Oct/13 15:11,14/Jul/23 06:25,04/Oct/13 15:11,0.6.2,0.7.0,,,,,,,0.8.1,,,,,,,,,,0,,,,,,"A common problem for users is that their task results are multiple MB in size, and Akka cannot send messages larger than its frameSize. It would be better to avoid this altogether by sending them through the BlockManager. The driver would then delete the result from the remote node after it fetched it.",,joshrosen,kayousterhout,matei,patrick,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-747,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383321,,,Fri Oct 04 15:11:54 UTC 2013,,,,,,,,,,"0|i1u1ev:",383589,,,,,,,,,,,,,,,,,,,,,,,"10/May/13 22:18;joshrosen;Looks like this has affected some users: https://groups.google.com/d/msg/spark-users/WG87fG8rrKY/6VpJ1AFXS-cJ;;;","10/May/13 22:46;rxin;For small task results, it is better to piggyback the akka task status message. For large ones, better using our own layer.;;;","30/Jul/13 14:35;patrick;Josh has a mock-up of this in a branch (borrowed from comment in SPARK-747).
https://github.com/mesos/spark/pull/610

Also see his comment here:
https://github.com/mesos/spark/pull/610#issuecomment-18021295;;;","04/Oct/13 15:11;kayousterhout;Fixed by https://github.com/apache/incubator-spark/pull/10;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors,SPARK-668,12705392,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,26/Jan/13 14:40,18/Nov/14 20:12,14/Jul/23 06:25,26/Jan/13 16:34,0.6.1,0.7.0,,,,,,,0.6.2,0.7.0,,,,Java API,,,,,0,,,,,,"As described in https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion, calls to JavaRDdLike.flatMap(PairFlatMapFunction) may be falsely rejected by the compiler with ""cannot find symbol; method: flatMap"" errors.

Here's a complete standalone example that reproduces the problem:

https://gist.github.com/4640356

I tried implementing a similar example in pure-Java (no Spark code) and was able to get the proper typechecking, so I suspect that this might be a Scala compiler bug.",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-694,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383535,,,Tue Nov 18 20:12:56 UTC 2014,,,,,,,,,,"0|i1u2qf:",383803,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/13 16:34;joshrosen;Fixed in https://github.com/mesos/spark/pull/417;;;","18/Nov/14 20:12;joshrosen;For reference, I think that this was caused by https://issues.scala-lang.org/browse/SI-6057;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Spark's master debug level logging consumable,SPARK-666,12705219,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,rxin,,24/Jan/13 19:03,29/Jan/13 21:31,14/Jul/23 06:25,29/Jan/13 21:11,,,,,,,,,0.7.0,,,,,,,,,,0,,,,,,"Right now if you turn debug level logging on, the console gets flooded with debug messages that basically make it impossible to use the debug level.

",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383547,,,2013-01-24 19:03:16.0,,,,,,,,,,"0|i1u2t3:",383815,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The master web interface is broken for Scala 2.10,SPARK-659,12705146,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,emchristiansen,,18/Jan/13 16:27,14/May/13 15:30,14/Jul/23 06:25,14/May/13 15:30,0.7.0,,,,,,,,,,,,,,,,,,0,,,,,,"To reproduce:

Build branch scala-2.10.

Edit the ""run"" script to use Scala 2.10 and comment out the REPL stuff.

Launch with ""./run spark.deploy.master.Master"".

Visit localhost:8080 in a browser.

This generates a MatchError: None at MasterWebUI.scala:28

Thanks!

Also, it would be nice if you used a Github issue tracker.",,emchristiansen,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383485,,,Tue May 14 15:30:16 UTC 2013,,,,,,,,,,"0|i1u2fb:",383753,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/13 20:53;emchristiansen;This is fixed in the current 2.10 branch. Hrm, I don't think I have permissions to close this.;;;","14/May/13 15:30;joshrosen;Closing this issue, since [~emchristiansen]'s comment says that it's fixed.  If this is still broken, please re-open this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Spark execution time logging more obvious and easier to read,SPARK-658,12705322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,patrick,rxin,,16/Jan/13 11:40,29/Jan/13 21:30,14/Jul/23 06:25,29/Jan/13 21:11,,,,,,,,,0.7.0,,,,,,,,,,0,,,,,,"I believe we log some execution time, but they are hard to read. Making those execution time (by stages) easier to read would be a great start to performance analysis.",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383548,,,2013-01-16 11:40:54.0,,,,,,,,,,"0|i1u2tb:",383816,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use multiple loopback IP addresses in unit tests,SPARK-657,12705325,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,matei,matei,,16/Jan/13 11:07,27/Jan/13 23:18,14/Jul/23 06:25,27/Jan/13 23:18,0.7.0,,,,,,,,0.7.0,,,,,,,,,,0,,,,,,"Right now we require users to manually alias their interface as 127.100.0.1, 2, etc, at least on some systems (Mac OS X).",,matei,mbautin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383549,,,Sun Jan 27 23:18:05 UTC 2013,,,,,,,,,,"0|i1u2tj:",383817,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/13 14:16;mbautin;By the way, the work-around for this is 

sudo ifconfig lo0 add 127.100.0.1
sudo ifconfig lo0 add 127.100.0.2
sudo ifconfig lo0 add 127.100.0.3

etc.;;;","27/Jan/13 23:18;matei;Closed by commit https://github.com/mesos/spark/commit/44b4a0f88fcb31727347b755ae8ec14d69571b52;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use ID of hash function when comparing Python partitioner objects in equals(),SPARK-654,12705378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,matei,,13/Jan/13 20:24,20/Jan/13 17:09,14/Jul/23 06:25,20/Jan/13 17:09,,,,,,,,,0.7.0,,,,,PySpark,,,,,0,,,,,,Right now we might compare two Python partitioners as equal even if they have different hash functions.,,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383555,,,Mon Jan 14 15:20:29 UTC 2013,,,,,,,,,,"0|i1u2uv:",383823,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/13 15:20;joshrosen;We should fix this, although it isn't currently a correctness problem because joins / groups / cogroups are not implemented in terms of the Java / Scala implementations, so PythonPartitioners are never compared for equality.

This reminds me that I should implement a co-partitioning-aware joins in PySpark, for which I'll open a different issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
takeSample() with repetitions should be able to return more items than an RDD contains,SPARK-648,12704498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,matei,joshrosen,,08/Jan/13 15:27,07/Aug/13 10:45,14/Jul/23 06:25,06/Aug/13 23:17,0.6.1,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,"If I use takeSample() _with repetition_ and attempt to take more items than the RDD contains, I may receive fewer than {{num}} items:

{code}
scala> sc.parallelize(0 to 1).takeSample(true, 10, 42)
res17: Array[Int] = Array(1, 0)

scala> sc.parallelize(0 to 4).takeSample(true, 10, 42)
res33: Array[Int] = Array(3, 2, 0, 3, 0)
{code}

If we fix this, we should add more tests for sample() and takeSample(), since right now they're only called in one test in the JavaAPISuite.",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383392,,,Wed Aug 07 10:45:42 UTC 2013,,,,,,,,,,"0|i1u1un:",383660,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/13 10:45;joshrosen;When / where was this fixed?  Is there a unit test for the fix?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionManager.sendMessage may create too many unnecessary connections,SPARK-647,12705384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shane.huang,shane.huang,,07/Jan/13 22:59,10/Jan/13 17:54,14/Jul/23 06:25,10/Jan/13 17:54,0.6.1,,,,,,,,0.6.2,0.7.0,,,,Spark Core,,,,,0,,,,,,"In ConnectionManager, sendMessage creates a new SendingConnection when connection host key is not found in connectionsById. But there might be too many unnecessary connections created before connectionsById is updated in the connection-manager-thread.run. In out test ConnectionMangerTest fails on ""connection reset by peer"" or ""timeout"" when there're too many message sending threads. We should make connectionRequests a map to track the connections by key so that connections can be reused.  ",,matei,shane.huang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383569,,,Thu Jan 10 17:54:04 UTC 2013,,,,,,,,,,"0|i1u2xz:",383837,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/13 00:01;shane.huang;Submitted a pull request for this issue @ https://github.com/mesos/spark/pull/356;;;","10/Jan/13 17:54;matei;Shane fixed this in https://github.com/mesos/spark/pull/356. Will also merge it to 0.6 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Floating point overflow/underflow in LR examples,SPARK-646,12704952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,joshrosen,,29/Dec/12 17:59,06/Aug/13 23:24,14/Jul/23 06:25,06/Aug/13 23:24,0.5.2,0.6.1,,,,,,,0.8.0,,,,,Examples,,,,,0,,,,,,"The SparkLR examples call scala.math.exp() with very large or small exponents, causing its result to be rounded to 0 or Infinity.  Is this a bug?

I discovered this while porting the LR example to Python, because Python's math.exp() function rounds very small results to 0 but raises OverflowError for large results.

In Scala:

{code}
scala> import math.exp
import math.exp

scala> math.exp(10000)
res4: Double = Infinity

scala> math.exp(-10000)
res5: Double = 0.0
{code}

Python:

{code}
from math import exp
exp(10000)
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
OverflowError: math range error
exp(-10000)
0.0
{code}

I added a call to println("""" + (-p.y * (w dot p.x))) in the map UDF in SparkLR and SparkHdfsLR to log the exponents, and in both cases I saw small exponents like
{code}
-4.967736504527945
-1.0153344192159428
0.4639647012587064
{code}

in the first round and huge exponents like

{code}
-3731.0565020800145
469.3852842964799
-2838.8348220771445
{code}

in all later rounds.

The examples calculate the gradients using

{code}
(1 / (1 + exp(-p.y * (w dot p.x))) - 1) * p.y * p.x.
{code}

The exponent (w dot p.x) grows rapidly because the magnitudes of w's components grow rapidly.

I'm not familiar enough with logistic regression to know whether this is common or how to fix this.

This could be a problem because a model whose weights have large magnitudes would always make predictions with extremely high confidence (e.g. p(y = 1 | x) is always 0 or 1, due to rounding).",,aht,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383393,,,Tue Aug 06 23:24:46 UTC 2013,,,,,,,,,,"0|i1u1uv:",383661,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/13 16:26;joshrosen;This is still a problem, particularly in PySpark: https://groups.google.com/d/msg/spark-users/ngHhosQN2MY/WZmjHzCtfU0J;;;","06/Aug/13 23:24;matei;This is no longer a problem in the new numpy-based LR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling distinct() without parentheses fails,SPARK-645,12705256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,markhamstra,markhamstra,,24/Dec/12 01:47,24/Dec/12 08:05,14/Jul/23 06:25,24/Dec/12 08:05,0.6.2,0.7.0,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"While distinct now supports a number of splits parameter and has a default value for that parameter, it only supports default calls of a form similar to rdd.distinct().persist and not rdd.distinct.persist.  The without-parentheses form should be allowed -- as it is for persist().",,markhamstra,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383571,,,Mon Dec 24 08:05:46 UTC 2012,,,,,,,,,,"0|i1u2yf:",383839,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/12 02:44;markhamstra;Pull request submitted.;;;","24/Dec/12 08:05;matei;Committed it. Thanks Mark!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobs canceled due to repeated executor failures may hang,SPARK-644,12705396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,20/Dec/12 23:55,06/Nov/14 17:33,14/Jul/23 06:25,06/Nov/14 17:33,0.6.1,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"In order to prevent an infinite loop, the standalone master aborts jobs that experience more than 10 executor failures (see https://github.com/mesos/spark/pull/210).  Currently, the master crashes when aborting jobs (this is the issue that uncovered SPARK-643).  If we fix the crash, which involves removing a {{throw}} from the actor's {{receive}} method, then these failures can lead to a hang because they cause the job to be removed from the master's scheduler, but the upstream scheduler components aren't notified of the failure and will wait for the job to finish.

I've considered fixing this by adding additional callbacks to propagate the failure to the higher-level schedulers.  It might be cleaner to move the decision to abort the job into the higher-level layers of the scheduler, sending an {{AbortJob(jobId)}} method to the Master.  The Client is already notified of executor state changes, so it may be able to make the decision to abort (or defer that decision to a higher layer).",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-643,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383550,,,2012-12-20 23:55:35.0,,,,,,,,,,"0|i1u2tr:",383818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone master crashes during actor restart,SPARK-643,12704451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,14/Dec/12 19:19,06/Nov/14 17:33,14/Jul/23 06:25,06/Nov/14 17:33,0.6.1,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,"The standalone master will crash if it restarts due to an exception:

{code}
12/12/15 03:10:47 ERROR master.Master: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times.
spark.SparkException: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times.
        at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:103)
        at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:62)
        at akka.actor.Actor$class.apply(Actor.scala:318)
        at spark.deploy.master.Master.apply(Master.scala:17)
        at akka.actor.ActorCell.invoke(ActorCell.scala:626)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)
        at akka.dispatch.Mailbox.run(Mailbox.scala:179)
        at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)
        at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
        at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
        at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
        at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
12/12/15 03:10:47 INFO master.Master: Starting Spark master at spark://ip-10-226-87-193:7077
12/12/15 03:10:47 INFO io.IoWorker: IoWorker thread 'spray-io-worker-1' started
12/12/15 03:10:47 ERROR master.Master: Failed to create web UI
akka.actor.InvalidActorNameException:actor name HttpServer is not unique!
[05aed000-4665-11e2-b361-12313d316833]
        at akka.actor.ActorCell.actorOf(ActorCell.scala:392)
        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.liftedTree1$1(ActorRefProvider.scala:394)
        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:394)
        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:392)
        at akka.actor.Actor$class.apply(Actor.scala:318)
        at akka.actor.LocalActorRefProvider$Guardian.apply(ActorRefProvider.scala:388)
        at akka.actor.ActorCell.invoke(ActorCell.scala:626)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)
        at akka.dispatch.Mailbox.run(Mailbox.scala:179)
        at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)
        at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
        at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
        at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
        at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
{code}

When the Master actor restarts, Akka calls the {{postRestart}} hook.  [By default|http://doc.akka.io/docs/akka/snapshot/general/supervision.html#supervision-restart], this calls {{preStart}}.  The standalone master's {{preStart}} method tries to start the webUI but crashes because it is already running.

I ran into this after a job failed more than 11 times, which causes the Master to throw a SparkException from its {{receive}} method.

The solution is to implement a custom {{postRestart}} hook.",,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-644,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383184,,,Thu Dec 20 23:29:09 UTC 2012,,,,,,,,,,"0|i1u0kf:",383452,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/12 11:59;joshrosen;I'd like to add unit tests for executor failures, but I can't seem to find a way to reproduce them in tests.  Writing a UDF that throws an exception just results in task failures; I want to completely kill the executor processes.

I tried adding a call to System.exit() in a UDF, hoping that it would only be executed in the Executor's JVM, but this caused test runner to exit.  I was using local-cluster mode, which looks like it spawns separate JVMs for Executors.  Any ideas?;;;","20/Dec/12 09:12;matei;You could add extra testing in the code to allow Executors to crash. Or, you can use the ""local-cluster"" mode, where System.exit will indeed crash an executor without killing the whole test runner.;;;","20/Dec/12 09:13;matei;You should add extra testing in the code to allow Executors to crash. Or, you can use the ""local-cluster"" mode, where System.exit will indeed crash an executor without killing the whole test runner.

Matei




;;;","20/Dec/12 23:29;joshrosen;Restarted Akka actors have fresh states (see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion and http://doc.akka.io/docs/akka/snapshot/general/supervision.html#What_Restarting_Means), so allowing actors to restart might lead to unexpected behavior.  I propose that we add {{postRestart}} methods so that actor restarts always lead to crashes:

{code}
  override def postRestart(reason: Throwable) {
    logError(""Spark worker actor failed: "" + reason)
    // Allowing the actor to restart will cause problems, because the new actor will have a fresh
    // state; see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion
    // We could try copying the state, but it's probably safer to just exit:
    logError(""Exiting because we do not restart failed worker actors"")
    System.exit(1)
  }
{code}

This should go in at least the standalone Worker and Master actors (where I've observed problems related to actor restarts), but we might want to add it elsewhere.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-ec2 standalone launch should set SPARK_MEM and SPARK_JAVA_OPTS,SPARK-642,12705363,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivaram,joshrosen,,14/Dec/12 10:47,05/Apr/13 19:48,14/Jul/23 06:25,05/Apr/13 19:48,0.6.1,,,,,,,,0.7.0,,,,,EC2,,,,,0,,,,,,"spark-ec2's standalone cluster launch script does not configure SPARK_MEM and SPARK_JAVA_OPTS.  This is done automatically when running under Mesos mode by using the scripts included in the AMI.

We should either have feature parity between the --cluster-type modes or remove the feature if it's too difficult to support both modes.",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383512,,,Fri Apr 05 19:48:04 UTC 2013,,,,,,,,,,"0|i1u2lb:",383780,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/13 19:48;joshrosen;This was fixed in 0.7, which fixed standalone mode support in spark-ec2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-ec2 standalone launch should create ~/mesos-ec2/slaves,SPARK-641,12705394,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivaram,joshrosen,,14/Dec/12 10:05,05/Apr/13 19:45,14/Jul/23 06:25,05/Apr/13 19:45,0.6.1,,,,,,,,0.7.0,,,,,EC2,,,,,0,,,,,,"When launching a standalone cluster using the --cluster-type option, the ~/mesos-ec2/slaves file is not populated (this is normally done by the ~/mesos-ec2/setup script).  We should fix this, since users may still wish to use scripts like copy-dir to copy files.",,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383514,,,Fri Apr 05 19:45:07 UTC 2013,,,,,,,,,,"0|i1u2lr:",383782,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/12 18:26;matei;I actually would prefer not to support a standalone mode EC2 cluster, or else to switch the default cluster to be the standalone one. The reason is that it's annoying to maintain two types of AMIs and two ways of running on EC2, and it confuses users (they will wonder why go for one type over the other). So maybe we should just switch the AMIs to use the standalone mode at some point.;;;","14/Dec/12 18:51;joshrosen;It might be useful to support this feature for our own testing purposes.

It's possible to support both cluster modes using a single AMI by running a different EC2-side configuration script from spark-ec2 depending on the cluster mode.  After launching the cluster, spark-ec2 could just run something similar to ~/mesos-ec2/setup, but for standalone mode.  The two scripts could share most of their code.  This would block on SPARK-521.;;;","05/Apr/13 19:45;joshrosen;The EC2 scripts in 0.7 properly support both standalone and Mesos clusters, so I'm marking this as fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone --cluster-type option broken in spark-ec2 due to SPARK_MASTER_IP setting,SPARK-638,12705003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,13/Dec/12 17:04,13/Dec/12 18:08,14/Jul/23 06:25,13/Dec/12 18:08,0.6.1,,,,,,,,0.6.2,,,,,,,,,,0,,,,,,"spark-ec2 has a --cluster-type option to launch standalone clusters, but this is broken because SPARK_MASTER_IP is set in the start-slaves.sh script but not in start-master.sh.  This causes the workers to connect to the master on the wrong address, which fails.",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383577,,,Thu Dec 13 17:32:36 UTC 2012,,,,,,,,,,"0|i1u2zr:",383845,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/12 17:32;joshrosen;Pull request: https://github.com/mesos/spark/pull/330;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Akka system names need to be normalized (since they are case-sensitive),SPARK-632,12705311,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,massie,,07/Dec/12 11:44,11/Nov/14 09:13,14/Jul/23 06:25,11/Nov/14 09:13,,,,,,,,,,,,,,,,,,,0,,,,,,"The ""system"" name of the Akka full path is case-sensitive (see http://akka.io/faq/#what_is_the_name_of_a_remote_actor).

Since DNS names are case-insensitive and we're using them in the ""system"" name, we need to normalize them (e.g. make them all lowercase).  Otherwise, users will find the ""workers"" will not be able to connect with the ""master"" even though the URI appears to be correct.

For example, Berkeley DNS occasionally uses names e.g. foo.Berkley.EDU. If I used foo.berkeley.edu as the master adddress, the workers would write to their logs that they are connecting to foo.berkeley.edu but failed to. They never show up in the master UI.  If use the foo.Berkeley.EDU address, everything works as it should. ",,aash,massie,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383585,,,Tue Nov 11 09:13:23 UTC 2014,,,,,,,,,,"0|i1u31j:",383853,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/14 08:43;aash;// link moved to http://doc.akka.io/docs/akka/current/additional/faq.html#what-is-the-name-of-a-remote-actor

I believe having the hostname change case will still break Spark.  But after a search of the dev and user mailing lists over the past year I haven't seen any other users with this issue.

A potential fix could be to call .toLower on the hostname in the Akka string across the cluster, but it's a little dirty to make this assumption everywhere.

Technically [hostnames ARE case insensitive|http://serverfault.com/questions/261341/is-the-hostname-case-sensitive] so Spark's behavior is wrong, but the issue is in the underlying Akka library.  This is the same underlying behavior where Akka requires that hostnames exactly match as well -- you can't use an IP address to refer to a Akka listening on a hostname -- SPARK-625.

Until Akka handles differently-cased hostnames I think can only be done with an ugly workaround.

Possibly relevant Akka issues:
- https://github.com/akka/akka/issues/15990
- https://github.com/akka/akka/issues/15007

My preference would be to close this as ""Won't Fix"" until it's raised again as a problem from the community.

cc [~rxin];;;","11/Nov/14 09:13;rxin;Sounds good. In the future we might roll our own RPC rather than using Actor for RPC. I think the current RPC library built for the shuffle service is already ok with case insensitive hostnames.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK_LOCAL_IP environment variable should also affect spark.master.host,SPARK-631,12705277,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,,06/Dec/12 18:10,08/Dec/12 01:11,14/Jul/23 06:25,08/Dec/12 01:11,0.6.1,,,,,,,,0.6.2,,,,,,,,,,0,,,,,,So that we can have a single variable for configuring the IP address that Spark uses.,,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383584,,,Sat Dec 08 01:11:16 UTC 2012,,,,,,,,,,"0|i1u31b:",383852,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/12 01:11;matei;This was actually happening correctly by default. I also updated it so that Spark prefers to bind to a non-loopback address in case InetAddress.getLocalHost returns a loopback one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master web UI shows some finished/killed executors as running,SPARK-630,12705421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,,04/Dec/12 18:15,04/May/13 22:52,14/Jul/23 06:25,04/May/13 22:52,0.6.1,,,,,,,,0.8.0,,,,,,,,,,0,,,,,,"When I view a finished job's Job Details page on the standalone master, it shows all executors as RUNNING.  However, when I view any of the workers' pages, the same executor appears under the ""Finished Executors"" list.",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383493,,,Sat May 04 22:52:12 UTC 2013,,,,,,,,,,"0|i1u2h3:",383761,,,,,,,,,,,,,,,,,,,,,,,"04/May/13 22:52;joshrosen;Fixed in https://github.com/mesos/spark/pull/597;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone job details page has strange value for number of cores,SPARK-629,12705443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,,04/Dec/12 14:50,04/May/13 22:52,14/Jul/23 06:25,04/May/13 22:52,0.6.1,0.7.0,,,,,,,0.8.0,,,,,,,,,,0,,,,,,"When I view the job details page of a job running on a standalone cluster, I see the following strange output:

{code}
Cores: 2147483647 (400 Granted )
{code}

I'm not sure where 2147483647 is coming from (it's Integer.MAX_VALUE).

Looking at the code for this job details page, this is generated by the following:

{code}
        <li><strong>Cores:</strong>                                             
          @job.desc.cores                                                       
          (@job.coresGranted Granted                                            
          @if(job.desc.cores == Integer.MAX_VALUE) {                            
                                                                                
          } else {                                                              
            , @job.coresLeft                                                    
          }                                                                     
          )                                                                     
        </li>     
{code}

I'm not sure what this is supposed to do; is the idea to display something like ""Cores: totalCores (x granted, y pending)""?  Does Integer.MAX_VALUE have any special significance when used as the number of cores in a JobDescription?",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383492,,,Sat May 04 22:52:32 UTC 2013,,,,,,,,,,"0|i1u2gv:",383760,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/13 19:54;joshrosen;This is still a problem in 0.7.;;;","04/May/13 22:52;joshrosen;Fixed in https://github.com/mesos/spark/pull/597;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deleting security groups gives me a 400 error,SPARK-626,12705385,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pas256,rxin,,29/Nov/12 21:40,11/Dec/12 12:13,14/Jul/23 06:25,11/Dec/12 12:13,,,,,,,,,0.6.2,,,,,,,,,,0,,,,,,"Filing on behalf of Shivaram:


Deleting security group tinytasks-test-zoo
ERROR:boto:400 Bad Request
ERROR:boto:<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response>
Traceback (most recent call last):
  File ""./spark_ec2.py"", line 637, in <module>
    main()
  File ""./spark_ec2.py"", line 571, in main
    conn.delete_security_group(group.name)
  File ""/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py"", line 2039, in delete_security_group
  File ""/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py"", line 944, in get_status
boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request
<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response>",,joshrosen,matei,pas256,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383582,,,Tue Dec 11 12:02:37 UTC 2012,,,,,,,,,,"0|i1u30v:",383850,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/12 23:32;joshrosen;I ran into the same issue and tried to investigate it:

There may be dependencies between security groups (e.g. allow traffic to/from another group), so these dependencies must be removed before the groups can be deleted.  The current script tries to do this, but it needs an additional for-loop: the current script removes an individual group's rules then deletes that group, but it should first remove all rules from all groups then delete all groups.

I tried modifying the script to do this (https://gist.github.com/4187604), but it ran into the same error.  When I ran the destroy command a second time, it successfully deleted the groups, so there may be a race condition there.;;;","04/Dec/12 11:45;pas256;I think that Gist will work, but perhaps there needs to be a delay between the steps. I get the feeling that behind the scenes the AWS backend is using eventual consistency, which is why I didn't experience this issues but others are.;;;","10/Dec/12 16:20;matei;Have you tried that, Peter? Would be nice to see this fixed.;;;","10/Dec/12 17:46;pas256;Here you go:
https://github.com/mesos/spark/pull/323

I tried lowering the sleep delay in between deleting the rules and deleting the groups, but anything below 30 seconds caused issues. 30 seconds seems to be as low as we can consistently go.;;;","11/Dec/12 10:54;pas256;It turns out this is far more error prone than I first though. There are multiple dependencies in play:
 - dependencies between rules in security groups
 - instances may not have been terminated completely when trying to delete a security group
 - AWS back-end eventual consistency
 - group.revoke() returns True even when it fails

The unfortunate result is the code is somewhat messy. This is as clean as I can make - there are 3 retries to delete the all groups, and exceptions are caught.

By default, groups will not be deleted, so most people won't experience this. You can delete groups when destroying a cluster by adding `--delete-groups`;;;","11/Dec/12 12:02;matei;Alright, I've merged your commit. Thanks for looking into this and fixing it!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Client hangs when connecting to standalone cluster using wrong address,SPARK-625,12704941,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,joshrosen,,27/Nov/12 14:13,07/Feb/15 22:48,14/Jul/23 06:25,07/Feb/15 22:48,0.7.0,0.7.1,0.8.0,,,,,,,,,,,Spark Core,,,,,0,,,,,,"I launched a standalone cluster on my laptop, connecting the workers to the master using my machine's public IP address (128.32.*.*:7077).  If I try to connect spark-shell to the master using ""spark://0.0.0.0:7077"", it successfully brings up a Scala prompt but hangs when I try to run a job.

From the standalone master's log, it looks like the client's messages are being dropped without the client discovering that the connection has failed:

{code}
12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message RegisterJob(JobDescription(Spark shell)) for non-local recipient akka://spark@0.0.0.0:7077/user/Master at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077
12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message DaemonMsgWatch(Actor[akka://spark@128.32.*.*:57518/user/$a],Actor[akka://spark@0.0.0.0:7077/user/Master]) for non-local recipient akka://spark@0.0.0.0:7077/remote at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077
{code}",,aash,ianoc,joshrosen,kzhang,seanm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383466,,,Sat Feb 07 22:48:32 UTC 2015,,,,,,,,,,"0|i1u2b3:",383734,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/13 22:35;seanm;I have run into this as well. From what it appears is that akka is just very sensitive to hostnames.

Since these are different, you got that error:
akka://spark@0.0.0.0:7077
akka://spark@128.32.*.*:7077


I ran into this because my slaves file was using fqdn, while akka was expecting just hostname.  As soon as I switched my slaves file to just using hostnames, things started working great for me.;;;","19/May/13 13:44;joshrosen;Fixed by Matei in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7, which also fixed SPARK-617;;;","19/May/13 13:47;joshrosen;Actually, I take that back:  If I run

{code}
MASTER=spark://0.0.0.0:7077 ./spark-shell
{code}

on my laptop, I see the same hang.  This is with the current (0.8) master branch.;;;","17/Jun/13 16:29;ianoc;What OS is your laptop Josh? 

something like http://stackoverflow.com/questions/11982562/socket-connect-to-0-0-0-0-windows-vs-mac suggests that java just won't like connecting to it.

Chrome on windows for me says 0.0.0.0 is an invalid address, it works on OS X however. so you could just be hitting that?;;;","14/Nov/14 09:43;aash;Spark is very sensitive to hostnames in Spark URLs, and that comes from Akka being very sensitive.  I've personally been bitten by hostnames vs FQDNs vs external IP address vs loopback IP address, and it's really a pain.

On current master branch (1.2) with the Spark standalone master listening on {{spark://aash-mbp.local:7077}} as confirmed by the master web UI, and the spark shell attempting to connect to {{spark://127.0.01:7077}} with the {{--master}} parameter, the driver tries 3 attempts and then fails with this message:

{noformat}
14/11/14 01:37:56 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077...
14/11/14 01:37:56 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077
14/11/14 01:37:56 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077
14/11/14 01:38:16 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077...
14/11/14 01:38:16 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077
14/11/14 01:38:16 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077
14/11/14 01:38:36 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077...
14/11/14 01:38:36 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077
14/11/14 01:38:36 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077
14/11/14 01:38:56 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.
14/11/14 01:38:56 WARN SparkDeploySchedulerBackend: Application ID is not initialized yet.
14/11/14 01:38:56 ERROR TaskSchedulerImpl: Exiting due to error from cluster scheduler: All masters are unresponsive! Giving up.
{noformat}

So the hang seems to be gone and replaced with a reasonable 3x attempts and fail.

[~joshrosen], short of changing Akka ourselves to make it less strict on exact URL matches, is there anything else we can do for this ticket?  I think we can reasonably close as fixed.;;;","07/Feb/15 22:48;joshrosen;Let's resolve this as ""Fixed"" for now.  Reducing Akka's sensitivity to hostnames is a more general issue and we may have a fix for this in the future by either upgrading to a version of Akka that differentiates between bound and advertised addressed or by replacing Akka with a different communications layer.  I don't think we've observed the ""hang indefinitely"" behavior described in this ticket for many versions, so I think this should be safe to close.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't hardcode log location for standalone UI,SPARK-623,12705264,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cgrothaus,dennybritz,,14/Nov/12 15:41,13/May/13 14:37,14/Jul/23 06:25,13/May/13 14:37,0.6.2,0.7.0,,,,,,,0.7.1,0.8.0,,,,,,,,,0,,,,,,,,dennybritz,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383486,,,Mon May 13 14:37:09 UTC 2013,,,,,,,,,,"0|i1u2fj:",383754,,,,,,,,,,,,,,,,,,,,,,,"13/May/13 14:22;joshrosen;Does anyone know which log / directory this issue is talking about?;;;","13/May/13 14:25;rxin;In WorkerArguments.scala, I do see


{code}
    if (System.getenv(""SPARK_WORKER_DIR"") != null) {
      workDir = System.getenv(""SPARK_WORKER_DIR"")
    }
{code};;;","13/May/13 14:37;joshrosen;I think that this issue referred to the WorkerWebUI assuming that worker logs would be in $SPARK_HOME/work.  This was fixed by https://github.com/mesos/spark/pull/539;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop MapReduce should be configured to use all local disks for shuffle on AMI,SPARK-619,12705244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,matei,,11/Nov/12 11:22,06/Nov/14 07:00,14/Jul/23 06:25,06/Nov/14 07:00,,,,,,,,,,,,,,,,,,,0,,,,,,"It used to be, but that got lost at some point.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383591,,,2012-11-11 11:22:40.0,,,,,,,,,,"0|i1u32v:",383859,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Driver program can crash when a standalone worker is lost,SPARK-617,12704785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,,10/Nov/12 22:46,11/Nov/12 21:21,14/Jul/23 06:25,11/Nov/12 21:21,0.6.0,,,,,,,,0.6.1,0.7.0,,,,,,,,,0,,,,,,Seems to be due to an uncaught communication timeout in Akka.,,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383589,,,Sun Nov 11 21:21:41 UTC 2012,,,,,,,,,,"0|i1u32f:",383857,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/12 21:21;matei;Fixed in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone web UI links to internal IPs when running on EC2,SPARK-613,12705254,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,07/Nov/12 23:01,25/May/13 23:42,14/Jul/23 06:25,25/May/13 23:42,0.6.0,0.7.0,,,,,,,0.7.1,0.8.0,,,,EC2,,,,,0,,,,,,"When I visit the standalone cluster web UI on EC2, the links to worker UIs are based on internal addresses (e.g. http://10.159.2.115:8081/) instead of externally-accessible addresses (e.g. http://ec2-*-*-*-*.compute-1.amazonaws.com/).  This makes it hard to view the worker logs.",,joshrosen,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383480,,,Sat May 25 23:42:28 UTC 2013,,,,,,,,,,"0|i1u2e7:",383748,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/12 15:40;joshrosen;Fixed by https://github.com/mesos/spark/pull/316;;;","14/Dec/12 10:38;joshrosen;It looks like there are hostnames like domU-12-31-39-09-F5-02.compute-1.internal and ip-10-226-87-193 which aren't recognized by the pattern used in that pull request.

We should probably add an environment variable in spark-env.sh to specify that the machines are running on EC2, and automatically set this variable in the EC2 scripts.;;;","28/Jan/13 10:30;shivaram;Fixed for Spark AMI by https://github.com/mesos/spark/pull/419;;;","08/Feb/13 14:54;joshrosen;Can https://github.com/mesos/spark/pull/419 be backported to 0.6?;;;","05/May/13 12:27;joshrosen;This may still be broken.  I noticed a link to a worker with domu-*-*-*-*=*.compute-1.internal:8081, an internal address, when running the latest Spark master on the 0.7 AMI.;;;","24/May/13 13:10;joshrosen;Found what's hopefully the last issue here and opened a new PR: https://github.com/mesos/spark/pull/621;;;","25/May/13 23:42;joshrosen;Merged and cherry-picked that PR into branch-0.7, so this is resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Timeout while fetching map statuses may cause job to hang,SPARK-607,12705071,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,06/Nov/12 10:32,16/Jan/13 21:54,14/Jul/23 06:25,16/Jan/13 21:53,0.6.0,,,,,,,,0.6.2,0.7.0,,,,Spark Core,,,,,1,,,,,,"Jobs may hang if workers time out while fetching map output locations from the MapOutputTracker.

I ran into this issue while running under Mesos on EC2, but I was able to reproduce it on my own machine using a 1-node standalone cluster.

After applying the attached patch to generate random timeout failures, my groupByKey job lost a task due to the timeout.  It looks like the master is notified of the failure, since it appears in its log:

{code}
12/11/06 10:19:39 INFO TaskSetManager: Serialized task 0.0:7 as 3095 bytes in 1 ms
12/11/06 10:19:39 INFO TaskSetManager: Lost TID 10 (task 0.0:2)
12/11/06 10:19:40 INFO TaskSetManager: Loss was due to spark.SparkException: Error communicating with MapOutputTracker
	at spark.MapOutputTracker.askTracker(MapOutputTracker.scala:78)
	at spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:154)
	at spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:14)
	at spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:38)
	at spark.RDD.iterator(RDD.scala:161)
	at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:18)
	at spark.RDD.iterator(RDD.scala:161)
	at spark.scheduler.ResultTask.run(ResultTask.scala:18)
	at spark.executor.Executor$TaskRunner.run(Executor.scala:76)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
12/11/06 10:19:40 INFO TaskSetManager: Starting task 0.0:2 as TID 16 on slave worker-20121106101845-128.32.130.156-50931: 128.32.130.156 (preferred)
12/11/06 10:19:40 INFO TaskSetManager: Serialized task 0.0:2 as 3095 bytes in 1 ms
{code}

The job hangs here; perhaps the failure leaves some inconsistent state on the worker.",,akopich,glenn.strycker@gmail.com,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/12 10:32;joshrosen;test_case.patch;https://issues.apache.org/jira/secure/attachment/12637626/test_case.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383565,,,Wed Jan 16 21:54:26 UTC 2013,,,,,,,,,,"0|i1u2x3:",383833,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/12 05:05;akopich;I've got the same problem. 

Is there any workaround? ;;;","23/Nov/12 10:19;matei;I believe this was fixed in 0.6.1, when we increased the Akka message timeout. Can you try that? ;;;","23/Nov/12 13:47;joshrosen;My test case patch can still produce a hang under 0.6.1.  Increasing the Akka timeout addresses the cause of the TimeoutException but doesn't fix the hang itself.  The exception should cause the task to fail rather than hanging.;;;","13/Dec/12 21:24;joshrosen;I found a potential cause for the freeze:

If an exception is throwing while communicating with the MapOutputTracker, the requested shuffleId will never be removed from the {{fetching}} set, which tracks in-progress requests.  As a result, subsequent fetches for the block will block while waiting for the failed fetch to finish.

What's the right fix here?

We could add a try-finally block around the call to the MapOutputTracker to ensure that the failed fetch removes the shuffleId from the {{fetching}} set, but this will just lead to a NullPointerException when the waiting thread tries to read the missing value.  I suppose that this is okay, since the chain of failures will eventually be caught when the RDD partitions that were being computed are discovered to be missing.  Does this sound reasonable?;;;","13/Dec/12 22:04;joshrosen;I tested that fix locally and it works, so I submitted a pull request: https://github.com/mesos/spark/pull/332;;;","16/Jan/13 21:54;joshrosen;Pull request merged into master and branch-0.6, so I'm marking this as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
PairRDDFunctions.lookup fails unnecessarily when self.partitioner is None,SPARK-601,12704935,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,markhamstra,,,31/Oct/12 16:02,06/Aug/13 23:20,14/Jul/23 06:25,06/Aug/13 23:20,0.5.0,0.5.1,0.6.0,0.6.1,0.7.0,,,,0.7.2,,,,,Spark Core,,,,,0,,,,,,"If a lookup(k) is attempted on an RDD[(K, V)] with no partitioner, an UnsupportedOperationException is thrown even when the operation should succeed:

  scala> val rdd = sc.parallelize(List((1, 'a'), (1, 'b'), (2, 'c')))
  rdd: spark.RDD[(Int, Char)] = spark.ParallelCollection@73f4117b

  scala> rdd.lookup(1)
  java.lang.UnsupportedOperationException: lookup() called on an RDD without a partitioner
  	  at spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:315)

At a minimum, a filter.map.collect over the whole RDD works when the optimized path using a partitioner is not available:

         case None =>
-          throw new UnsupportedOperationException(""lookup() called on an RDD without a partitioner"")
+          self.filter(kv => kv._1 == key).map(kv => kv._2).collect",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383397,,,2012-10-31 16:02:31.0,,,,,,,,,,"0|i1u1vr:",383665,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext.stop and clearJars delete local JAR files,SPARK-600,12705060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,matei,,31/Oct/12 14:43,06/Nov/14 06:58,14/Jul/23 06:25,06/Nov/14 06:58,,,,,,,,,,,,,,,,,,,0,,,,,,"If you happen to pass a JAR that's in your current working directory to SparkContext, clearJars() will delete it. I'm not exactly sure why it's deleting files to begin with (maybe it was meant to deal with JAR files that are somehow copied in local mode?) but it's certainly not something that should be done in SparkContext.",,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383597,,,Thu Nov 06 06:58:50 UTC 2014,,,,,,,,,,"0|i1u347:",383865,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/14 06:58;matei;Should no longer be a problem since 1.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemoryErrors can cause workers to hang indefinitely,SPARK-599,12705373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joshrosen,,30/Oct/12 22:06,26/Nov/12 12:32,14/Jul/23 06:25,26/Nov/12 12:32,0.6.0,,,,,,,,0.7.0,,,,,,,,,,0,,,,,,"While running Shark with an insufficient number of reduce tasks, an overloaded worker machine raised {{java.lang.OutOfMemoryError : GC overhead limit exceeded}}.  This caused that Java process to hang at 100% CPU, spending all of its time in the garbage collector.  This failure wasn't detected by the master, causing the entire job to hang.

Handling and reporting failures due to {{OutOfMemoryError}} can be complicated because the {{OutOfMemoryError}} exception can be raised at many different locations, depending on which allocation caused the error.

I'm not sure that it's safe to recover from {{OutOfMemoryError}}, so worker processes should probably die once they raise that error.  We might be able to do this in an uncaught exception handler.",,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383586,,,Mon Nov 26 12:32:12 UTC 2012,,,,,,,,,,"0|i1u31r:",383854,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/12 14:53;matei;I think we might want to deal with this using a timeout instead of trying to catch the error. Do you know whether any threads continue running at all when there's an OOM? Somehow I doubt it.;;;","31/Oct/12 14:59;joshrosen;Here's a forced stack trace from the hanging worker: https://gist.github.com/3984822.  Looks like all threads are blocked, but I don't know if this always happens.

Could we take both approaches, killing the worker if we catch OOM while using a watchdog timeout in case that fails?;;;","26/Nov/12 09:46;joshrosen;Did https://github.com/mesos/spark/pull/305 fix this?;;;","26/Nov/12 12:32;matei;Good point, I think it does. Going to close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashPartitioner incorrectly partitions RDD[Array[_]],SPARK-597,12704982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,,28/Oct/12 22:51,08/Oct/14 22:27,14/Jul/23 06:25,17/Jan/13 10:43,,,,,,,,,0.6.2,0.7.0,,,,,,,,,0,,,,,,"Java arrays have {{hashCodes}} that are based on the arrays' identities rather than their contents [1].  As a result, attempting to partition an {{RDD[Array[_]]}} using a {{HashPartitioner}} will produce an unexpected/incorrect result.

This was the cause of a bug in PySpark, where I hash partition PairRDDs with {{Array[Byte]}} keys.  In PySpark, I fixed this by using a custom {{Partitioner}} that calls {{Arrays.hashCode(byte[])}} when passed an {{Array[Byte]}} [2].

I would like to address this issue more generally in Spark.

We could add logic to {{HashPartitioner}} to perform special handling for arrays, but I'm not sure whether the additional branching would add a significant performance overhead.  The logic could become messy because the {{Arrays}} module defines {{Arrays.hashCode()}} for primitive arrays and {{Arrays.deepHashCode()}} for Object arrays.  Perhaps Guava or Apache Commons has an implementation of this.

An alternative would be to keep the current {{HashPartitioner}} and add logic to print warnings (or to fail with an error) when shuffling an {{RDD[Array[_]]}} using the default {{HashPartitioner}}.


[1] http://stackoverflow.com/questions/744735/java-array-hashcode-implementation
[2] https://github.com/JoshRosen/spark/commit/2ccf3b665280bf5b0919e3801d028126cb070dbd",,aht,joshrosen,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3847,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383564,,,Thu Jan 17 10:43:12 UTC 2013,,,,,,,,,,"0|i1u2wv:",383832,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/12 22:59;joshrosen;There might be similar problems if arrays are used as {{Map}} keys or are stored in a {{Set}}.;;;","29/Oct/12 14:14;matei;This is a good catch, but it seems tough to do the right thing automatically in HashPartitioner, as you said. I think one interim fix is to add an ArrayPartitioner (or even ByteArrayPartitioner) and ask people to use that when their keys are byte arrays. I guess this is what you did in PySpark? We might be able to warn people of this condition in the PairRDDFunctions constructor, where we have a ClassManifest for the key K.

Another option would be to choose the partitioner automatically based on the type of K, but let's try the other approach first and see whether there are any implications.;;;","29/Oct/12 15:58;joshrosen;If we add ArrayPartitioner or ByteArrayPartitioner, then it might be useful to perform type checking to prevent mistakes like using a {{ByteArrayPartitioner}} on an RDD with integer keys.  We might do this by adding a type parameter {{K}} to {{Partitioner}}.

The {{partitioner}} field is defined for all RDDs, not just key-value RDDs, so its type would have to be {{Partitioner[Any]}}.  However, {{partitioner}} is a {{val}}, so it only matters that a partitioner of the right type is passed in the RDD's constructor.

I would declare {{Partitioner[-K]}} to be contravariant in the key type, so that a {{Partitioner[Any]}} can be used with an {{RDD[Array[Byte]]}} but a {{Partitioner[Array[Byte]]}} cannot be used with an {{RDD[Int]}}.

If we take this approach, then we might want to split this into two commits: one that adds a warning when using {{HashPartitioner}} with arrays of any kind, and another that changes {{Partition}} and adds the type parameters.  This would retain backwards-compatibility for the existing releases, while allowing us to provide better type checking in future releases.

Should I take a stab at this and submit a pull request?;;;","29/Oct/12 16:08;joshrosen;Also, array keys will cause problems with map-side combiners, which use RDD keys as {{HashMap}} keys.

Is it worth supporting map-side combiners with array keys?  If not, an easy solution would be to fail with an error.  We could probably detect the error before the job ever runs.  This would require separate patches for the 0.5/0.6 and 0.7 branches, due to my shuffle refactoring changes in 0.7.
;;;","31/Dec/12 20:35;joshrosen;On reflection, it's probably a bad idea to use contravariant types in user-facing APIs because they could cause issues in the Java API.

I submitted a pull request that causes Spark to throw exceptions in cases where we know that hashing arrays will produce incorrect results: https://github.com/mesos/spark/pull/348;;;","17/Jan/13 10:43;matei;Merged this into 0.6 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Log task size when it's too large on master,SPARK-590,12705408,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,patrick,,24/Oct/12 09:58,07/Dec/13 13:04,14/Jul/23 06:25,07/Dec/13 13:04,0.6.0,0.7.0,0.8.0,,,,,,0.9.0,,,,,,,,,,0,Starter,,,,,"For now, log once per job if the closure is > 100Kb. ",,joshrosen,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383261,,,Sat Dec 07 13:04:53 UTC 2013,,,,,,,,,,"0|i1u11j:",383529,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/13 13:04;joshrosen;Fixed in https://github.com/apache/incubator-spark/pull/207;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MESOS_NATIVE_LIBRARY env var needs to be set when running on Mesos,SPARK-589,12704953,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,patrick,,24/Oct/12 09:57,10/Aug/13 16:31,14/Jul/23 06:25,10/Aug/13 16:31,0.6.0,,,,,,,,,,,,,Documentation,,,,,0,,,,,,,,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383383,,,Sat Aug 10 16:31:04 UTC 2013,,,,,,,,,,"0|i1u1sn:",383651,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/13 16:31;patrick;This was fixed a while ago.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test issue,SPARK-587,12705183,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,matei,,23/Oct/12 21:53,23/Oct/12 21:56,14/Jul/23 06:25,23/Oct/12 21:56,,,,,,,,,,,,,,,,,,,0,,,,,,,,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383602,,,Tue Oct 23 21:54:53 UTC 2012,,,,,,,,,,"0|i1u35b:",383870,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/12 21:54;matei;Test comment;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos may not work with mesos:// URLs,SPARK-585,12705349,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,,23/Oct/12 12:01,21/Nov/12 11:43,14/Jul/23 06:25,21/Nov/12 11:43,,,,,,,,,,,,,,,,,,,0,,,,,,According to some users. I guess we should strip the mesos:// at the front.,,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383587,,,2012-10-23 12:01:53.0,,,,,,,,,,"0|i1u31z:",383855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failures in BlockStore may lead to infinite loops of task failures,SPARK-583,12704925,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,woggle,joshrosen,,20/Oct/12 15:54,19/May/13 13:31,14/Jul/23 06:25,19/May/13 13:31,0.6.0,0.6.1,,,,,,,0.7.0,,,,,Spark Core,,,,,0,,,,,,"Summary: failures in BlockStore may lead to infinite loops of task failures.

I ran into a situation where a block manager operation failed:
{code}
12/10/20 21:25:13 ERROR storage.BlockManagerWorker: Exception handling buffer message
com.esotericsoftware.kryo.SerializationException: Buffer limit exceeded writing object of type: shark.ColumnarWritable
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:492)
	at spark.KryoSerializationStream.writeObject(KryoSerializer.scala:78)
	at spark.serializer.SerializationStream$class.writeAll(Serializer.scala:58)
	at spark.KryoSerializationStream.writeAll(KryoSerializer.scala:73)
	at spark.storage.BlockManager.dataSerialize(BlockManager.scala:834)
	at spark.storage.MemoryStore.getBytes(MemoryStore.scala:72)
	at spark.storage.BlockManager.getLocalBytes(BlockManager.scala:311)
	at spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:79)
	at spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:58)
	at spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33)
	at spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.Iterator$class.foreach(Iterator.scala:772)
	at scala.collection.IndexedSeqLike$Elements.foreach(IndexedSeqLike.scala:54)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:73)
	at spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:12)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at spark.storage.BlockMessageArray.map(BlockMessageArray.scala:12)
	at spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:33)
	at spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23)
	at spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23)
	at spark.network.ConnectionManager.spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:276)
	at spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
{code}

This failure appears to have been detected via a fetch failure in the following stage:

{code}
12/10/20 21:25:12 INFO scheduler.DAGScheduler: Marking Stage 2 (mapPartitions at Operator.scala:197) for resubmision due to a fetch failure
12/10/20 21:25:12 INFO scheduler.DAGScheduler: The failed fetch was from Stage 3 (mapPartitions at Operator.scala:197); marking it for resubmission
{code}

The failed task was retried on the same machine, and it executed without exceptions.

However, the job is unable to make forward progress; the scheduler gets stuck in an infinite loop of the form
{code}
12/10/20 22:23:08 INFO spark.CacheTrackerActor: Asked for current cache locations
12/10/20 22:23:08 INFO scheduler.DAGScheduler: Resubmitting Stage 3 (mapPartitions at Operator.scala:197) because some of its tasks had failed: 220
12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting Stage 3 (mapPartitions at Operator.scala:197), which has no missing parents
12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 3
12/10/20 22:23:08 INFO cluster.ClusterScheduler: Adding task set 3.4080 with 1 tasks
12/10/20 22:23:08 INFO cluster.TaskSetManager: Starting task 3.4080:0 as TID 5484 on slave 201210202106-1093469194-5050-5222-43: domU-12-31-39-14-5E-5E.compute-1.internal (preferred)
12/10/20 22:23:08 INFO cluster.TaskSetManager: Serialized task 3.4080:0 as 7605 bytes in 0 ms
12/10/20 22:23:09 INFO cluster.TaskSetManager: Finished TID 5484 in 820 ms (progress: 1/1)
12/10/20 22:23:09 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(3, 220)
12/10/20 22:23:09 INFO scheduler.DAGScheduler: ShuffleMapTask finished with host domU-12-31-39-14-5E-5E.compute-1.internal
12/10/20 22:23:09 INFO scheduler.DAGScheduler: Stage 3 (mapPartitions at Operator.scala:197) finished; looking for newly runnable stages
12/10/20 22:23:09 INFO scheduler.DAGScheduler: running: Set()
12/10/20 22:23:09 INFO scheduler.DAGScheduler: waiting: Set(Stage 2, Stage 1)
12/10/20 22:23:09 INFO scheduler.DAGScheduler: failed: Set()
12/10/20 22:23:09 INFO spark.CacheTrackerActor: Asked for current cache locations
{code}

If I look at the worker that is running these tasks, I see infinite loop of the form
{code}
12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5
12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_0 already exists on this machine; not re-adding it
12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_1 already exists on this machine; not re-adding it
12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_2 already exists on this machine; not re-adding it
[..]
12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_199 already exists on this machine; not re-adding it
12/10/20 21:29:29 INFO executor.Executor: Serialized size of result for 1677 is 350
12/10/20 21:29:29 INFO executor.Executor: Finished task ID 1677
12/10/20 21:29:29 INFO executor.Executor: Running task ID 1678
12/10/20 21:29:29 INFO executor.Executor: Its generation is 3
12/10/20 21:29:29 INFO spark.CacheTracker: Cache key is rdd_4_220
12/10/20 21:29:29 INFO spark.CacheTracker: Found partition in cache!
12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Running Pre-Shuffle Group-By
12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Mapside hash aggregation enabled
12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5
{code}

I'm not sure of the exact cause of this behavior, but I have a guess:

During the original failed execution, the task's output blocks were not stored but their block ids were added to the BlockManager's {{blockInfo}} map.  This prevents these blocks from being recomputed and causes the ""{{Block shuffle_*_*_* already exists on this machine; not re-adding it}}"" warnings.  As a result, the block is never stored and the master is never informed of its location.

This causes the DAG scheduler to repeatedly launch the same task in an infinite loop, saying that it is ""{{Resubmitting Stage * because some of its tasks had failed: *}}"".",,aht,joshrosen,matei,patrick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-706,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383482,,,Sun May 19 13:31:08 UTC 2013,,,,,,,,,,"0|i1u2en:",383750,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/12 15:54;joshrosen;I haven't been able to reproduce the infinite loop of task retrials, but I did try modifying the {{DiskStore}} to randomly fail with {{IOException}} while trying to write blocks.  On a local standalone cluster, these failures were detected but the job hung after trying to recompute the failed tasks.

If I catch the exception and remove the block's {{blockId}} from the {{BlockManager}}'s {{blockInfo}} map, then the job is eventually able to succeed.  This suggests that we should add exception handling inside the {{BlockManager}}'s {{put()}} and {{putBytes()}} calls.

There is one subtle race condition to watch for, though: between the time that we store the {{blockId}} and the time that the write fails, another task may have retrieved that block's {{BlockInfo}} object and began to wait by calling {{waitForReady()}}.  To avoid deadlock, this waiter needs to notified of the failure.;;;","14/Dec/12 16:03;joshrosen;I managed to run across this issue again today, using a branch based on Spark 0.6.1.  Even though no exceptions occurred, I saw the same infinite loop of task resubmissions and ""Block already exists"" messages.

Two machines exhibited loops, and both loops were preceded by a block being dropped from the memory store to free up memory.  This is a bit strange, since it looks like the machine had plenty of free memory:

{code}
12/12/14 19:43:36 INFO storage.MemoryStore: Block rdd_323_43 of size 199481138 dropped from memory (free 8579469803)
12/12/14 19:43:36 INFO executor.StandaloneExecutorBackend: Got assigned task 76480
12/12/14 19:43:36 INFO executor.Executor: Running task ID 76480
12/12/14 19:43:36 INFO executor.Executor: Its generation is 325
12/12/14 19:43:36 INFO spark.CacheTracker: Cache key is rdd_645_43
12/12/14 19:43:36 INFO storage.BlockManager: Getting local block rdd_645_43
12/12/14 19:43:36 INFO storage.BlockManager: Getting remote block rdd_645_43
12/12/14 19:43:36 INFO spark.CacheTracker: Computing partition spark.ParallelCollectionSplit@6de0
12/12/14 19:43:39 INFO storage.MemoryStore: ensureFreeSpace(199481138) called with curMem=0, maxMem=8579469803
12/12/14 19:43:39 INFO storage.MemoryStore: Block rdd_645_43 stored as values to memory (estimated size 190.2 MB, free 7.8 GB)
{code}

My job looked something like this:

{code}
val samples = sc.parallelize(1 to numMachines, numMachines).flatMap(_ => generate_pairs)
samples.cache()
samples.count() // Force evaluation

// In a loop:
samples.groupByKey().count()
{code}

The real code used preshuffle() and the CoalescedRDD class in my skew-handling branch instead of performing the regular groupByKey().

It looks like a partition of {{samples}} was lost and recomputed.  Following this, the resubmitted tasks were ShuffleMapTasks; the master log said that they correspond to the flatMap() line, so they are shuffling the recomputed {{samples}} partition.  However, it looks like these shuffle blocks are already in the BlockManager:

{code}
12/12/14 19:43:39 INFO executor.StandaloneExecutorBackend: Got assigned task 76530
12/12/14 19:43:39 INFO executor.Executor: Running task ID 76530
12/12/14 19:43:39 INFO executor.Executor: Its generation is 325
12/12/14 19:43:39 INFO spark.CacheTracker: Cache key is rdd_645_43
12/12/14 19:43:39 INFO storage.BlockManager: Getting local block rdd_645_43
12/12/14 19:43:39 INFO spark.CacheTracker: Found partition in cache!
12/12/14 19:43:43 INFO executor.Executor: Serialized size of result for 76530 is 184
12/12/14 19:43:43 INFO executor.Executor: Finished task ID 76530
12/12/14 19:43:43 INFO executor.StandaloneExecutorBackend: Got assigned task 76538
12/12/14 19:43:43 INFO executor.Executor: Running task ID 76538
12/12/14 19:43:43 INFO executor.Executor: Its generation is 325
12/12/14 19:43:43 INFO spark.CacheTracker: Cache key is rdd_645_43
12/12/14 19:43:43 INFO storage.BlockManager: Getting local block rdd_645_43
12/12/14 19:43:43 INFO spark.CacheTracker: Found partition in cache!
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_0 already exists on this machine; not re-adding it
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_1 already exists on this machine; not re-adding it
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_2 already exists on this machine; not re-adding it
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_3 already exists on this machine; not re-adding it
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_4 already exists on this machine; not re-adding it
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_5 already exists on this machine; not re-adding it
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_6 already exists on this machine; not re-adding it
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_7 already exists on this machine; not re-adding it
12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_8 already exists on this machine; not re-adding it
...
{code}

My theory:

* A block being dropped somehow results in shuffle blocks on the same machine being marked as missing in the master's MapOutputTracker (or some other bookkeeping structure on the master).
* This causes the master to resubmit tasks, because blocks required by the next phase seem to be missing.
* The blocks were never actually lost, so they cannot be recomputed.
* The master is not notified of the existence of these blocks, because put() returns before updating the master if the block already exists.
* This leads to an infinite loop.;;;","14/Dec/12 17:04;joshrosen;On closer inspection of the master logs, it looks like the looping machines were marked as dead hosts, causing the master CacheTracker to mark their caches as lost.  It looks like the dead host comes back, causing tasks to be scheduled on it:

{code}
12/12/14 19:23:37 INFO cluster.TaskSetManager: Lost TID 38348 (task 484.0:37)
12/12/14 19:23:37 INFO cluster.TaskSetManager: Loss was due to fetch failure from BlockManagerId(ip-10-12-129-38, 52252)
taskLaunchOverhead is 0 milliseconds
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO scheduler.DAGScheduler: Marking Stage 484 (CoalescedShuffleFetcherRDD at SkewBenchmark.scala:77) for resubmision due to a fetch failure
12/12/14 19:23:37 INFO scheduler.DAGScheduler: The failed fetch was from Stage 483 (flatMap at SkewBenchmark.scala:43); marking it for resubmission
12/12/14 19:23:37 INFO scheduler.DAGScheduler: Host lost: ip-10-12-129-38
12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Trying to remove the host: ip-10-12-129-38 from BlockManagerMaster.
12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Previous hosts: ArrayBuffer(BlockManagerId(ip-10-144-65-13, 57952), BlockManagerId(ip-10-12-130-28, 38283), BlockManagerId(ip-10-152-157-13, 57580), BlockManagerId(ip-10-60-97-229, 60271), BlockManagerId(ip-10-152-161-81, 51219), BlockManagerId(ip-10-12-130-82, 37488), BlockManagerId(ip-10-144-141-107, 51761), BlockManagerId(ip-10-152-155-24, 44270), BlockManagerId(ip-10-152-146-166, 44224), BlockManagerId(ip-10-152-150-78, 34278), BlockManagerId(ip-10-145-205-182, 50214), BlockManagerId(ip-10-152-151-66, 59577), BlockManagerId(ip-10-145-205-217, 47570), BlockManagerId(ip-10-145-179-146, 55921), BlockManagerId(ip-10-144-72-168, 37861), BlockManagerId(ip-10-152-166-209, 35915), BlockManagerId(ip-10-145-203-221, 60217), BlockManagerId(ip-10-144-140-91, 48968), BlockManagerId(ip-10-152-148-80, 57024), BlockManagerId(ip-10-144-132-152, 42273), BlockManagerId(ip-10-147-128-53, 44072), BlockManagerId(ip-10-152-146-72, 50587), BlockManagerId(ip-10-60-99-32, 53933), BlockManagerId(ip-10-145-212-30, 58755), BlockManagerId(ip-10-147-130-32, 34711), BlockManagerId(ip-10-152-150-104, 60430), BlockManagerId(ip-10-152-156-131, 40720), BlockManagerId(ip-10-152-148-92, 43706), BlockManagerId(ip-10-147-129-249, 45660), BlockManagerId(ip-10-12-129-38, 52252), BlockManagerId(ip-10-152-151-151, 51417), BlockManagerId(ip-10-145-203-198, 44413), BlockManagerId(ip-10-152-159-35, 41803), BlockManagerId(ip-10-145-203-17, 53531), BlockManagerId(ip-10-147-129-235, 40164), BlockManagerId(ip-10-60-99-168, 54819), BlockManagerId(ip-10-147-129-219, 38611), BlockManagerId(ip-10-152-165-130, 46365), BlockManagerId(ip-10-144-135-87, 50616), BlockManagerId(ip-10-152-138-241, 45156), BlockManagerId(ip-10-152-146-74, 45560), BlockManagerId(ip-10-145-230-152, 39402), BlockManagerId(ip-10-152-157-159, 58826), BlockManagerId(ip-10-10-189-14, 39648), BlockManagerId(ip-10-152-156-104, 42281), BlockManagerId(ip-10-144-65-37, 36194), BlockManagerId(ip-10-152-148-69, 48816), BlockManagerId(ip-10-62-94-62, 41136), BlockManagerId(ip-10-60-71-25, 42670), BlockManagerId(ip-10-144-65-33, 36063))
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38350 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Current hosts: ArrayBuffer(BlockManagerId(ip-10-144-65-13, 57952), BlockManagerId(ip-10-12-130-28, 38283), BlockManagerId(ip-10-152-157-13, 57580), BlockManagerId(ip-10-60-97-229, 60271), BlockManagerId(ip-10-152-161-81, 51219), BlockManagerId(ip-10-12-130-82, 37488), BlockManagerId(ip-10-144-141-107, 51761), BlockManagerId(ip-10-152-155-24, 44270), BlockManagerId(ip-10-152-146-166, 44224), BlockManagerId(ip-10-152-150-78, 34278), BlockManagerId(ip-10-145-205-182, 50214), BlockManagerId(ip-10-152-151-66, 59577), BlockManagerId(ip-10-145-205-217, 47570), BlockManagerId(ip-10-145-179-146, 55921), BlockManagerId(ip-10-144-72-168, 37861), BlockManagerId(ip-10-152-166-209, 35915), BlockManagerId(ip-10-145-203-221, 60217), BlockManagerId(ip-10-144-140-91, 48968), BlockManagerId(ip-10-152-148-80, 57024), BlockManagerId(ip-10-144-132-152, 42273), BlockManagerId(ip-10-147-128-53, 44072), BlockManagerId(ip-10-152-146-72, 50587), BlockManagerId(ip-10-60-99-32, 53933), BlockManagerId(ip-10-145-212-30, 58755), BlockManagerId(ip-10-147-130-32, 34711), BlockManagerId(ip-10-152-150-104, 60430), BlockManagerId(ip-10-152-156-131, 40720), BlockManagerId(ip-10-152-148-92, 43706), BlockManagerId(ip-10-147-129-249, 45660), BlockManagerId(ip-10-152-151-151, 51417), BlockManagerId(ip-10-145-203-198, 44413), BlockManagerId(ip-10-152-159-35, 41803), BlockManagerId(ip-10-145-203-17, 53531), BlockManagerId(ip-10-147-129-235, 40164), BlockManagerId(ip-10-60-99-168, 54819), BlockManagerId(ip-10-147-129-219, 38611), BlockManagerId(ip-10-152-165-130, 46365), BlockManagerId(ip-10-144-135-87, 50616), BlockManagerId(ip-10-152-138-241, 45156), BlockManagerId(ip-10-152-146-74, 45560), BlockManagerId(ip-10-145-230-152, 39402), BlockManagerId(ip-10-152-157-159, 58826), BlockManagerId(ip-10-10-189-14, 39648), BlockManagerId(ip-10-152-156-104, 42281), BlockManagerId(ip-10-144-65-37, 36194), BlockManagerId(ip-10-152-148-69, 48816), BlockManagerId(ip-10-62-94-62, 41136), BlockManagerId(ip-10-60-71-25, 42670), BlockManagerId(ip-10-144-65-33, 36063))
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38329 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO storage.BlockManagerMaster: Removed ip-10-12-129-38 successfully in notifyADeadHost
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38341 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38360 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38351 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO spark.CacheTrackerActor: Memory cache lost on ip-10-12-129-38
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38334 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38323 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO spark.CacheTracker: CacheTracker successfully removed entries on ip-10-12-129-38
12/12/14 19:23:37 INFO spark.CacheTrackerActor: Asked for current cache locations
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38327 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38333 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38333 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:37 INFO scheduler.DAGScheduler: Resubmitting failed stages
12/12/14 19:23:37 INFO spark.CacheTrackerActor: Asked for current cache locations
12/12/14 19:23:37 INFO scheduler.DAGScheduler: Submitting Stage 483 (flatMap at SkewBenchmark.scala:43), which has no missing parents
12/12/14 19:23:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 483
12/12/14 19:23:37 INFO cluster.ClusterScheduler: Adding task set 483.1 with 1 tasks
12/12/14 19:23:37 INFO cluster.TaskSetManager: Starting task 483.1:0 as TID 38361 on slave worker-20121214185650-ip-10-144-65-37-40073: ip-10-144-65-37 (preferred)
12/12/14 19:23:37 INFO cluster.TaskSetManager: Serialized task 483.1:0 as 1398 bytes in 0 ms
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38346 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38332 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38352 because its task set is gone
taskLaunchOverhead is 0 milliseconds
12/12/14 19:23:38 INFO storage.BlockManagerMasterActor: Registering block manager ip-10-12-129-38:52252 with 8.0 GB RAM
12/12/14 19:23:38 INFO storage.BlockManagerMasterActor: Added rdd_323_39 in memory on ip-10-12-129-38:52252 (size: 190.2 MB, free: 7.8 GB)
{code}

When the DAGScheduler handles task completion, it does not store records of output locations that are on dead hosts:
{code}
          case smt: ShuffleMapTask =>                                           
            val stage = idToStage(smt.stageId)                                  
            val status = event.result.asInstanceOf[MapStatus]                   
            val host = status.address.ip                                        
            logInfo(""ShuffleMapTask finished with host "" + host)                
            if (!deadHosts.contains(host)) {   // TODO: Make sure hostnames are consistent with Mesos
              stage.addOutputLoc(smt.partition, status)                         
            }  
{code}

Hosts are permanently marked as dead, so this leads to an infinite loop as tasks scheduled on those machines can never register their output locations.

It looks like the solution is to properly handle the return of dead hosts.;;;","14/Dec/12 18:28;matei;Charles may have recently fixed this in https://github.com/mesos/spark/pull/317. Talk with him to see whether it handles this case.;;;","14/Dec/12 18:42;joshrosen;My branch already includes Charles' pull request.  His code allows the dead node's block manager to reconnect (which actually takes place in the above log), but doesn't address my specific problem.

The problem is that the dead node's hostname is never removed from DAGScheduler's {{deadHosts}} set, even if that node returns.;;;","19/May/13 13:31;joshrosen;It looks like Charles fixed this in https://github.com/mesos/spark/pull/408, which made it into 0.7.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
